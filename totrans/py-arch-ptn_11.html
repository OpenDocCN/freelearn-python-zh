<html><head></head><body><div><p>&#13;&#13;
    <h1 class="chapterNumber">9</h1>&#13;&#13;
    <h1 id="_idParaDest-169" class="chapterTitle">Microservices vs Monolith</h1>&#13;&#13;
    <p class="normal">In this chapter, we will present and comment on two of the most common architectures for complex systems. Monolithic architecture<a id="_idIndexMarker577"/> creates a single block where the whole system is contained, and is simple to operate. Microservices architecture, on <a id="_idIndexMarker578"/>the other hand, divides the system into smaller microservices that talk to each other, aiming to allow different teams to take ownership of different elements, and helping big teams to work in parallel.</p>&#13;&#13;
    <p class="normal">We will discuss when to choose each one, based on its different characteristics. We will also go through the teamwork aspect of them, as they have different requirements in terms of how the work needs to be structured.</p>&#13;&#13;
    <div>&#13;&#13;
      <p class="Tip--PACKT-">Remember that the architecture is not only related to tech, but to a significant degree to how communication is structured! Refer to <em class="chapterRef">Chapter 1</em>, <em class="italic">Introduction to Software Architecture</em>, for a further discussion of Conway's Law.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">A common pattern is to migrate from an old monolithic architecture to a microservices one. We will talk about the stages involved in such a change.</p>&#13;&#13;
    <p class="normal">We will also introduce Docker as a way of containerizing services, something very useful when it comes to creating microservices, but that can also be applied to monoliths. We will containerize the web application presented in <em class="chapterRef">Chapter 5</em>, <em class="italic">The Twelve-Factor App Methodology</em>.</p>&#13;&#13;
    <p class="normal">Finally, we will briefly describe how to deploy and operate multiple containers using an orchestration tool, and describe the most popular one these days – Kubernetes.</p>&#13;&#13;
    <p class="normal">In this chapter, we'll cover the following topics:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Monolithic architecture</li>&#13;&#13;
      <li class="bullet">The microservices architecture</li>&#13;&#13;
      <li class="bullet">Which architecture to choose</li>&#13;&#13;
      <li class="bullet">Moving from a monolith to microservices</li>&#13;&#13;
      <li class="bullet">Containerizing services</li>&#13;&#13;
      <li class="bullet">Orchestration and Kubernetes</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Let's start by talking in more depth about monolithic architecture.</p>&#13;&#13;
    <h1 id="_idParaDest-170" class="title">Monolithic architecture</h1>&#13;&#13;
    <p class="normal">When a system is designed organically, the tendency is to generate a single unitary block of software that contains the whole functionality of the system. </p>&#13;&#13;
    <p class="normal">This is a logical <a id="_idIndexMarker579"/>progression. When a software system is designed, it starts small, typically with a simple functionality. But, as the software is used, it grows in terms of its usage and starts getting requests for new functionality to complement the existing ones. Unless there are sufficient resources and planning to structure the growth, the path of least resistance will be to keep adding everything into the same code structure, with little modularity.</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_09_01.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated with low confidence" width="826" height="302"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 9.1: A monolithic application</p>&#13;&#13;
    <p class="normal">This process ensures that all the code and functionality are tied together in a single block, hence the name <em class="italic">monolithic architecture</em>.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">And, by extension, software that follows this pattern is called a monolith.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Although this kind of structure is quite common, in general, monolithic structures have a better modularity and internal structure. Even if the software is composed of a single block, it can be divided logically into different parts, assigning different responsibilities to different modules.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">For example, in previous chapters we discussed the MVC architecture. This is a monolithic architecture. The Models, Views, and Controllers are all under the same process, but there is a definitive structure in place that differentiates the responsibilities and functions.</p>&#13;&#13;
      <p class="Information-Box--PACKT-">Monolithic architecture is not synonymous with a lack of structure.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">The defining characteristic of a <a id="_idIndexMarker580"/>monolith is that all the calls between modules are through <em class="italic">internal</em> APIs, within the same process. This affords the advantage of being very flexible. The strategy for deploying a new version of the monolith is also easy. Restarting the process will ensure full deployment.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">Keep in mind that a monolithic application can have multiple copies running. For example, a monolithic web application can have multiple copies of the same software running in parallel, with a load balancer sending requests to all of them. A restart, in this case, will be in multiple stages.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">The version of the monolith is easy to know, as all the code is part of the same structure. The code, if it's under source control, will all be under the same repo.</p>&#13;&#13;
    <h1 id="_idParaDest-171" class="title">The microservices architecture</h1>&#13;&#13;
    <p class="normal">The microservices <a id="_idIndexMarker581"/>architecture was developed as an alternative to having a single block containing all the code.</p>&#13;&#13;
    <p class="normal">A system following a microservices architecture is <em class="italic">a collection of loosely coupled specialized services that work in unison to provide a comprehensive service</em>. Let's divide the definition up in order to be clearer:</p>&#13;&#13;
    <ol>&#13;&#13;
      <li class="numbered">A <strong class="keyword">collection of specialized services</strong>, meaning that there are different and well-defined modules</li>&#13;&#13;
      <li class="numbered"><strong class="keyword">Loosely coupled</strong>, so each microservice can be independently deployed and developed</li>&#13;&#13;
      <li class="numbered">That <strong class="keyword">work in unison</strong>. Each microservice needs to communicate with others</li>&#13;&#13;
      <li class="numbered">To <strong class="keyword">provide a comprehensive service</strong>, meaning that the whole system creates a full system that has a clear motive and functionality</li>&#13;&#13;
    </ol>&#13;&#13;
    <p class="normal">Compared with a <a id="_idIndexMarker582"/>monolith, instead of grouping the whole software under the same process, it uses multiple, separate functional parts (each microservice) that communicate through well-defined APIs. These elements can be in different processes and typically are moved out from different servers to allow proper scaling of the system.</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_09_02.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="335"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 9.2: Note that not all microservices will be connected to the storage. Each microservice may have its own individual storage</p>&#13;&#13;
    <p class="normal">The defining characteristic is that the calls between different services are all through <em class="italic">external</em> APIs. These APIs act as a clear, defined barrier between functionalities. Because of this, microservices architecture requires advanced planning and needs to define clearly the differences between components.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">In particular, microservices architecture requires a good upfront design to be sure that the different elements connect together correctly, as any problem that is cross-service will be costly to work with.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">A system that follows the <a id="_idIndexMarker583"/>microservices architecture doesn't happen organically, but it's the result of a plan created beforehand and executed carefully. This architecture is not typically started for systems from scratch, but instead, they are migrated from a previously existing, successful, monolithic architecture.</p>&#13;&#13;
    <h1 id="_idParaDest-172" class="title">Which architecture to choose</h1>&#13;&#13;
    <p class="normal">There's a tendency to think that a more evolved architecture, like the microservices architecture, is better, but that's an oversimplification. Each one has its own set of strengths and weaknesses.</p>&#13;&#13;
    <p class="normal">The first one is the fact that almost every small application will start as a monolithic application. This is because it is the most natural way to start a system. Everything is at hand, the number of modules is reduced, and it's an easy starting point.</p>&#13;&#13;
    <p class="normal">Microservices, on the<a id="_idIndexMarker584"/> other hand, require the creation of a plan to divide the functionality carefully into different modules. This task may be complicated, as some designs may prove inadequate later on.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">Keep in mind that no design can be totally future-proof. Any perfectly valid architectural decision may prove incorrect a year or two later when changes in the system require adjustments. While it is a good question to think about the future, trying to cover every possibility is futile. The proper balance between designing for the current feature and designing for the future vision of the system is a constant challenge in software architecture.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">This requires quite a lot of work to be done beforehand, which requires an investment in the microservices architecture.</p>&#13;&#13;
    <p class="normal">That said, as monoliths grow, they can start presenting problems just through the sheer size of the code. The main characteristic of a monolithic architecture is that all the code is found together, and it can start presenting a lot of connections that can cause developers to be confused. Complexity can be reduced by good practices and constant vigilance to ensure good internal structure, but that requires a lot of work in place by existing developers to enforce it. When dealing with a big and complex system, it may be easier to present clear and strict boundaries just by dividing different areas into different processes.</p>&#13;&#13;
    <p class="normal">The modules can also require different specific knowledge, making it natural to assign different team members to different areas. To create a proper sense of ownership of the modules, they can have different opinions in terms of code standards, an adequate programming language for the job, ways of performing tasks, and so on; for example, a photosystem that has an interface for uploading photos and an AI system for categorizing them. While the first module will work as a web service, the abilities required for training and handling an AI model to categorize the data will be very different, making the module separation natural and productive. Both of them in the same code base may generate problems by trying to work at the same time.</p>&#13;&#13;
    <p class="normal">Another problem of <a id="_idIndexMarker585"/>monolithic applications is the inefficient utilization of resources, as each deployment of the monolith carries over every copy of every module. For example, the RAM required will be determined for the worst-case scenario across multiple modules. When there are multiple copies of the monolith, that will waste a lot of RAM preparing for worst-case scenarios that will likely be rare. Another example is the fact that, if any module requires a connection to the database, a new connection will be created, whether that's used or not.</p>&#13;&#13;
    <p class="normal">In comparison, using microservices can adjust each service according to its own worst-case use case, and independently control the number of replicas for each. When viewed as a whole, that can lead to big resource saves in big deployments.</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_09_03.png" alt="Shape&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="427"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 9.3: Notice that using different microservices allows us to reduce RAM usage by dividing requests into different microservices, while in a monolithic application, the worst-case scenario drives RAM utilization</p>&#13;&#13;
    <p class="normal">Deployments also work very differently between monoliths and microservices. As the monolithic application needs to be deployed in a single go, every deployment is, effectively, a task for the whole team. If the team is small, creating a new deployment and ensuring that the new features are properly coordinated between modules and not interfering incorrectly is not very complicated. However, as the teams grow bigger, this can present a serious challenge if the code is not strictly structured. In particular, a bug in a small part of the system may bring down the whole system completely, as any critical error in the monolith affects the whole of the code.</p>&#13;&#13;
    <p class="normal">Monolith deployments<a id="_idIndexMarker586"/> require coordination between modules, meaning that they need to work with each other, which normally leads to teams working closely together until the feature is ready to be released, and require some sort of supervision until the deployment is ready. This is noticeable when several teams are working on the same code base, with competing goals, and this blurs the ownership and responsibility of deployments.</p>&#13;&#13;
    <p class="normal">By comparison, different microservices are deployed independently. The API should be stable and backward compatible with older releases, and that's one of the strong requisites that need to be enforced. However, the boundaries are very clear, and in the event of a critical bug, the worst that can happen is that the particular microservice goes down, while other unrelated microservices continue unaffected.</p>&#13;&#13;
    <p class="normal">This makes the system work in a "degraded state," as compared to the "all-or-none" approach of the monolith. It limits the scope of a catastrophic failure.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Of course, certain microservices may be more critical than others, making them worthy of extra attention and care regarding their stability. But, in that case, they can be defined as critical in advance, with stricter stability rules enforced.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Of course, in both cases, solid testing techniques can be used to increase the quality of the software released.</p>&#13;&#13;
    <p class="normal">In comparison with the monolith, microservices can be deployed independently, without coordinating closely with other services. This brings independence to the teams working on them and allows for faster, continuous deployments that require less central coordination.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">The keyword here is <em class="italic">less</em> coordination. Coordination is still required, but the objective of a microservices architecture is necessarily that each microservice can be independently deployed and owned by a team, so the majority of changes can be dictated exclusively by the owner without requiring a process of warning other teams.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Monolithic applications, because <a id="_idIndexMarker587"/>they communicate with other modules through internal operations, mean that they typically can perform these operations much faster than through the external APIs. This allows a very high level of interaction between modules without paying a significant performance price.</p>&#13;&#13;
    <p class="normal">There is an overhead related to the usage of external APIs and communication through a network that can produce a noticeable delay, especially if there are too many internal requests made to different microservices. Careful consideration is required to try to avoid repeating external calls and to limit the number of services that can be contacted in a single task.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">In some cases, the usage of tools to abstract the contact with other microservices may produce extra calls that will be absolutely necessary. For example, a task to process a document needs to obtain some user information, which requires calling a different microservice. The name is required at the start of the document, and the email at the end of it. A naïve implementation may produce two requests to obtain the information instead of requesting it all in a single go.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Another interesting advantage of <a id="_idIndexMarker588"/>microservices is the independence of technical requirements. In a monolithic application, problems may arise as a result of requiring different versions of libraries for different modules. For example, updating the version of Python requires the whole code base to be prepared for that. These library updates can be complicated as different modules may have different requirements, and one module can effectively mingle with another by requiring an upgrade of the version of a certain library that's used by both.</p>&#13;&#13;
    <p class="normal">Microservices, on the other hand, contain their own set of technical requirements, so there's not this limitation. Because of the external APIs used, different microservices can even be programmed in different programming languages. This allows the use of specialized tools for different microservices, tailoring each one for each purpose and thereby avoiding conflicts.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">Just because different microservices can be programmed in different languages doesn't mean that they should. Avoid the temptation of using too many programming languages in a microservices architecture as this will complicate maintenance and make it difficult for a member of a different team to be able to help, thereby creating more isolated teams. </p>&#13;&#13;
      <p class="Tip--PACKT-">Having one or two default languages and frameworks available and then allowing special justified cases is a sensible way to proceed.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">As we see, most of the characteristics of microservices make it more suited for a bigger operation, when the number of developers is high enough that they need to be split into different teams and coordination needs to be more explicit. The high change of pace in a big application also requires better ways to deploy and work independently, in general.</p>&#13;&#13;
    <p class="normal">A small team can self-coordinate very well and will be able to work quickly and efficiently in a monolith.</p>&#13;&#13;
    <p class="normal">This is not to say that a monolith can be very big. Some are. But, in a general sense, microservices architecture only makes sense if there are enough developers such that different teams are working in the same system and are required to achieve a good level of independence between them.</p>&#13;&#13;
    <h2 id="_idParaDest-173" class="title">A side note about similar designs</h2>&#13;&#13;
    <p class="normal">While the decision of monolith versus microservices is normally discussed in the context of web services, it's not exactly a new idea and it's not the only environment where there are similar ideas and structures.</p>&#13;&#13;
    <p class="normal">The kernel of an OS can also be monolithic. In this case, a kernel structure is called monolithic if it all operates within kernel space. A program running in kernel space in a computer can access the whole memory and hardware directly, something that is critical for the usage of an OS, while at the same time, this is dangerous as it has big security and safety implications. Because the code in kernel space<a id="_idIndexMarker589"/> works so closely with the hardware, any failure here can result in the total failure of the system (a kernel panic). The alternative is to run in user space, which is the area where a program only has access to its own data, and has to interact explicitly with the OS to retrieve information.</p>&#13;&#13;
    <p class="normal">For example, a program in user space that wants to read from a file needs to make a call to the OS, and the OS, in kernel space, will access the file, retrieve the information, and return it to the requested program, copying to a part of the memory where the program can access.</p>&#13;&#13;
    <p class="normal">The idea of the monolithic kernel is that it can minimize this movement and context switch between different kernel elements, such as libraries or hardware drivers.</p>&#13;&#13;
    <p class="normal">The alternative to a monolithic kernel is called a <a id="_idIndexMarker590"/>microkernel. In a microkernel structure, the kernel part is greatly reduced and elements such as filesystems, hardware drivers, and network stacks are executed in user space instead of in kernel space. This requires these elements to communicate by passing messages through the microkernel, which is less efficient. </p>&#13;&#13;
    <p class="normal">At the same time, it can improve the modularity and security of the elements, as any crash in user space can be restarted easily.</p>&#13;&#13;
    <p class="normal">There was a famous argument between Andrew S. Tanenbaum and Linus Torvalds about what architecture is better, given that Linux was created as a monolithic kernel. In the long run, kernels have evolved toward hybrid models, where they take aspects of both elements, incorporating microkernel ideas into existing monolithic kernels for flexibility.</p>&#13;&#13;
    <p class="normal">Discovering and analyzing related architectural ideas can help to improve the tools at the disposal of a good architect and improve architectural understanding and knowledge.</p>&#13;&#13;
    <h1 id="_idParaDest-174" class="title">The key factor – team communication</h1>&#13;&#13;
    <p class="normal">A key element of the difference between microservices and monolithic architecture is the difference in the communication structure that they support.</p>&#13;&#13;
    <p class="normal">If the monolithic application<a id="_idIndexMarker591"/> has grown organically from a small project, as usually happens, the internal structure can become messy, and requires developers with experience in the system who can change and adapt it for any change. In bad cases, the code can become very chaotic and be more and more complicated to work with.</p>&#13;&#13;
    <p class="normal">Increasing the size of the development team becomes complicated, as each engineer requires a lot of contextual information, and learning how to navigate the code is difficult. The older teammates who have been around can help to train new team members, but they'll act as bottlenecks, and mentoring is a slow process that has limits. Each new member of the team will require a significant amount of training time until they can be productive in fixing bugs and adding new features.</p>&#13;&#13;
    <p class="normal">Teams also have a maximum natural size limit. Managing a team with too many members, without dividing it into smaller groups, is difficult.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">The ideal size of a team depends on a lot of different factors, but between 5 and 9 is generally considered the ideal size to work efficiently.</p>&#13;&#13;
      <p class="Tip--PACKT-">Teams that are bigger than that tend to self-organize into their own smaller groups, losing focus as a unit and creating small information silos where parts of the team are not aware of what's going on.</p>&#13;&#13;
      <p class="Tip--PACKT-">Teams with fewer members create too much overhead in terms of management and communication with other teams. They will be able to work faster with a slightly bigger size.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">If the growing size of the code requires it, this is the time to employ all the techniques that we are describing in this book to generate more structure, architecting the system. This will involve defining modules with clear responsibilities and clear boundaries. This division allows the team to be divided into groups and allows them to work at creating ownership and explicit goals for each team.</p>&#13;&#13;
    <p class="normal">This allows the teams to work in parallel without too much interference, so the extra members can increase the throughput in terms of features. As we commented before, clear boundaries will help in defining the work for each team.</p>&#13;&#13;
    <p class="normal">In a monolith, however, these limitations are <em class="italic">soft</em>, as the whole system is accessible. Sure, there is a certain discipline in terms of focusing on certain areas, and the tendency will be that one team <a id="_idIndexMarker592"/>will be able to access everything, and will tweak and bend internal APIs.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">This characteristic is not necessarily a bad thing, especially on a smaller scale. This way of working with a small, focused team can produce fantastic results, as they'll be able to adjust quickly all the related parts of the software. The drawback is that the members of the team need to be highly experienced and know their way around the software, which normally becomes more and more difficult over time.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">When moving to a <a id="_idIndexMarker593"/>microservices architecture, the division of work becomes way more explicit. The APIs between teams become hard limitations and there is a need for more work upfront to communicate between teams. The trade-off is that teams are way more independent, as they can:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Own the microservice completely without other teams coding in the same code base </li>&#13;&#13;
      <li class="bullet">Deploy independently from other teams</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">As the code base will be smaller, new members of the team will be able to learn it quickly and be productive earlier. Because the external APIs to interact with other microservices will be explicitly defined, a higher level of abstraction will be applied, making it easier to interact.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">Note this also means that different teams will know less about the internals of other microservices compared with monolithic applications when there's at least a superficial knowledge of it. This can create some friction when moving people from one team to another.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">As we saw in the first chapter, Conway's law is something to keep in mind when making architectural decisions that affect communication within the organization. Let's remember that this software law states that the structure of the software will replicate the communication structure of the organization.</p>&#13;&#13;
    <p class="normal">A good example of Conway's law is the creation of DevOps practices. The older way of dividing work was to have different teams, one related to developing new features, and another in charge of deploying and operating the software. The abilities required for each task are different, after all.</p>&#13;&#13;
    <p class="normal">The risk of this structure is the "I don't know what it is / I don't know where it runs" division, which can cause the team responsible for developing new features to be unaware of bugs and problems associated with operating the software, while the operations team finds changes with little reaction time, and identifies bugs without understanding the inside operation of the software.</p>&#13;&#13;
    <p class="normal">This division is still in place in many organizations, but the idea behind DevOps is that the same team that develops the software is responsible for deploying it, thereby creating a virtuous feedback loop where developers are aware of the complexities of the deployment and can react and fix bugs in production and improve the operation of the software.</p>&#13;&#13;
    <p class="normal">Note that this <a id="_idIndexMarker594"/>normally involves creating a multi-functional team with people who both understand operations and development, though they don't necessarily need to be the same. Sometimes, an external team is responsible for creating a set of common tools for other teams to use in their operations.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">This is a big change, and changing from the older structure to the DevOps one involves mixing teams in a way that can be very disruptive for the corporate culture. As we've tried to highlight here, this involves people changes, which are slow and have a significant amount of pain associated with them. For example, there may be a good operations culture where they share their knowledge and have fun together, and now they'll need to break up those teams and integrate them with new people.</p>&#13;&#13;
      <p class="Tip--PACKT-">This kind of process is difficult and should be planned carefully, understanding both its human and social scale.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Communication within the same team is different from the communication between different teams. Communicating with other teams is always more difficult and costlier. This is probably easy to say, but the implications of it for teamwork are big. Examples include the following:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Because APIs to be used externally from the team are going to be used by other engineers without the same level of expertise in the internals, it makes sense to make them generic and easy to use, as well as creating proper documentation.</li>&#13;&#13;
      <li class="bullet">If a new design follows the structure of already existing teams, it will be easier to implement than the other way around. Architectural changes that lie between teams require organizational changes. Changing the structure of an organization is a long and painful process. Anyone who has been involved in a company reorganization can attest to that. These organizational changes will be reflected in the software naturally, so ideally a plan will be generated to allow for it.</li>&#13;&#13;
      <li class="bullet">Two teams working in the same service will create problems because each team will try to pull it to their own goals. This is a situation that can happen with some common libraries or with "core" microservices that are used by multiple teams. Try to enforce clear owners for them to be sure that a single team is in charge of any changes.&#13;&#13;
    &#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">Explicit owners establish clarity about who is responsible for changes and new features. Even if something is implemented by someone else, the owner should be responsible for approving it and giving direction and feedback. They should also be prepared to have a long-term vision and handle any technical debt.</p>&#13;&#13;
    </p>&#13;&#13;
</li></ul>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Given that<a id="_idIndexMarker595"/> different physical locations and time zones naturally impose their own communication barrier, they normally are used to set up different teams, describing their own structured communication, like the API definition, between time zones.<p>&#13;&#13;
          <p class="Tip--PACKT-">Working remotely has increased significantly as a result of the COVID-19 crisis. This has also created the need to structure communication differently compared with a team working together in the same room. This has developed and improved communication skills, which can lead to better ways of organizing work. In any case, team division is not only a matter of being physically located in the same place but creating the bonds and structure to work as a team.</p>&#13;&#13;
        </p>&#13;&#13;
      </li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Communication aspects of development are an important part of the work and should not be underestimated. Keep in mind that changes to them are "people changes," which are more difficult to implement than tech changes.</p>&#13;&#13;
    <h1 id="_idParaDest-175" class="title">Moving from a monolith to microservices</h1>&#13;&#13;
    <p class="normal">A usual case is the need to <a id="_idIndexMarker596"/>migrate from an existing monolithic architecture to a new microservices one.</p>&#13;&#13;
    <p class="normal">The main reason for wanting to implement this change is the size of the system. As we've discussed before, the main advantage of a microservice system is the creation of multiple independent parts that can be developed in parallel, enabling the development to be scaled and the pace increased by allowing more engineers to work at the same time.</p>&#13;&#13;
    <p class="normal">This is a move that makes sense if the monolith has grown to exceed a manageable size and there are enough problems with releases, interfering features, and stepping on each other's toes. But, at the same time, it's a very huge and painful transition to perform.</p>&#13;&#13;
    <h2 id="_idParaDest-176" class="title">Challenges for the migration</h2>&#13;&#13;
    <p class="normal">While the final result <a id="_idIndexMarker597"/>may be much better than a monolithic application that shows its age, migrating to a new architecture is a big undertaking. We'll now look at some of the challenges and problems that we can expect in the process:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Migrating to microservices will require a huge amount of effort, actively changing the way the organization operates, and will require a big upfront investment until it starts to pay off. The transition time will be painful and will require compromises between the speed of migration and the regular operation of the service, as stopping the operation completely won't be an option. It will require a good deal of meetings and documentation to plan and communicate the migration to everyone. It needs to have active support at the executive level to ensure full commitment to get it done, with a clear understanding of why it is being done.</li>&#13;&#13;
      <li class="bullet">It also requires a profound cultural change. As we've seen above, the key element of microservices is the interaction between teams, which will change significantly compared with the way of operating in a monolithic architecture. This will likely involve changing teams and changing tools. Teams will have to be stricter in their usage and documentation of external APIs. <p class="bullet">They'll need to be more formal in their interaction with other teams and probably take attributions they didn't have before. In general, people don't like change, and that could be responded to in the form of resistance by members of some teams. Be sure that these elements are taken into account.</p>&#13;&#13;
      </li>&#13;&#13;
      <li class="bullet">Another challenge is the training aspect. New tools will surely be used (we will cover Docker and Kubernetes later in this chapter), so some teams will likely need to adapt to use them. Managing a cluster of services can be complicated to wrap one's head around, and it will likely involve different tools than the ones used previously. For example, local developers will likely be very different. Learning how to operate and work with containers, if going down that route, will take some time. This requires planning and the need to support team members until they are comfortable with the new system.&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">A very clear example of this is the extra complexity for debugging a request coming into the system, as it can be jumping around different microservices. Previously, this request was probably easier to track in the monolith. Understanding how a request moves and finding subtle bugs produced by that can be difficult. To be certain of fixing this, they will likely need to be replicated and fixed in local development, which, as we've seen, will entail the use of different tools and systems.</p>&#13;&#13;
    </p>&#13;&#13;
</li>&#13;&#13;
    </ul>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Dividing the <a id="_idIndexMarker598"/>existing monolith into different services requires careful planning. A bad division between services can make two services tightly coupled, thereby not allowing independent deployment. This can result in a situation where practically any change to one service will require a change in the other, even if, theoretically, this could be done independently. This creates duplication of work, as routinely working on a single feature requires multiple microservices to be changed and deployed. Microservices can be mutated later and boundaries redefined, but there's a high cost associated with that. The same care should be taken later when adding new services.</li>&#13;&#13;
      <li class="bullet">There's an overhead in creating microservices, as there is some work that gets replicated on each service. That overhead gets compensated for by allowing independent and parallel development. But, to take full advantage of that, you need numbers. A small development team of up to 10 people can coordinate and handle a monolith very efficiently. It's only when the size grows and independent teams are formed that migrating to microservices starts to make sense. The bigger the company, the more it makes sense.</li>&#13;&#13;
      <li class="bullet">A balance between allowing each team to make their own decisions and standardize some common elements and decisions is necessary. If teams have too little direction, they'll keep reinventing the wheel over and over. They'll also end up creating knowledge silos where the knowledge in a section of the company is wholly non-transferable to another team, making it difficult to learn lessons collectively. Solid communication between teams is required to allow consensus and the reuse of common solutions. Allow controlled experimentation, label it as such, and get the lessons learned across the board so that the other teams benefit. There will be tension between shared and reusable ideas and independent, multiple-implementation ideas.&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Be careful when introducing shared code across services. If the code grows, it will make services dependent on each other. This can reduce the independence of the microservices.</p>&#13;&#13;
    </p>&#13;&#13;
</li>&#13;&#13;
    </ul>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Following the Agile principles, we know that working software is more important than extensive documentation. However, in microservices, it's important to maximize the usability of each individual microservice to reduce the amount of support between teams. That involves some degree of documentation. The best approach is to create self-documenting services.</li>&#13;&#13;
      <li class="bullet">As we've <a id="_idIndexMarker599"/>discussed earlier, each call to a different microservice can increase the delay of responses, as multiple layers will have to be involved. This can produce latency problems, with external responses taking longer. They will also be affected by the performance and capacity of the internal network connecting the microservices.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">A move to microservices should be taken with care and by carefully analyzing its pros and cons. It is possible that it will take years to complete the migration in a mature system. But for a big system, the resulting system will be much more agile and easy to change, allowing you to tackle technical debt effectively and to empower developers to take full ownership and innovate, structuring communication and delivering a high-quality, reliable service.</p>&#13;&#13;
    <h2 id="_idParaDest-177" class="title">A move in four acts</h2>&#13;&#13;
    <p class="normal">The migration from one architecture to another should be considered in four steps:</p>&#13;&#13;
    <ol>&#13;&#13;
      <li class="numbered" value="1"><strong class="keyword">Analyze</strong> the existing system carefully.</li>&#13;&#13;
      <li class="numbered"><strong class="keyword">Design</strong> to determine what the desired destination is.</li>&#13;&#13;
      <li class="numbered"><strong class="keyword">Plan</strong>. Create a route to move, step by step, from the current system to the vision designed in the first stage.</li>&#13;&#13;
      <li class="numbered"><strong class="keyword">Execute</strong> the plan. This stage will need to be done slowly and deliberately, and at each step, the design and plan will need to be re-evaluated.</li>&#13;&#13;
    </ol>&#13;&#13;
    <p class="normal">Let's look at each of the steps in greater detail.</p>&#13;&#13;
    <h3 id="_idParaDest-178" class="title">1. Analyze</h3>&#13;&#13;
    <p class="normal">The very first step <a id="_idIndexMarker600"/>is to have a good understanding of our starting point with the existing monolith. This may appear trivial, but the fact is that it is quite conceivable that no particular person has a good understanding of all the details of the system. It may require information gathering, compilation, and digging deep to understand the intricacies of the system.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">The existing code can be described as <em class="italic">legacy code</em>. While a debate is currently taking place on exactly what code can be categorized as legacy, the main property of it is code that is already in place and doesn't follow the best and new practices that new code has.</p>&#13;&#13;
      <p class="Information-Box--PACKT-">In other words, legacy code is old code from some time ago and that is very likely not up to date with current practices. However, legacy code is critical, as it is in use and probably key for the day-to-day operations of the organization.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">The main<a id="_idIndexMarker601"/> objective of this phase should be to determine whether a change will actually be beneficial and get a preliminary idea of what microservices will result from the migration. Performing this migration is a big commitment, and it's always a good idea to double-check that tangible benefits will result. Even if, at this stage, it won't be possible to estimate the effort required, it will start shaping the size of the task.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">This analysis will benefit greatly from good metrics and actual data showing the number of requests and interactions that are actually being produced in the system. This can be achieved through good monitoring, and adding metrics and logs to the system to allow the current behavior to be measured. This can lead to insights about what parts are commonly used, and, even better, parts that are almost never used and can perhaps be deprecated and removed. Monitoring can continue to be used to ensure that the process is going according to plan.</p>&#13;&#13;
      <p class="Tip--PACKT-">We will discuss monitoring in more detail in <em class="chapterRef">Chapter 11</em>, <em class="italic">Package Mangement</em>, and <em class="chapterRef">Chapter 12</em>, <em class="italic">Logging</em>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">This analysis can be almost instant if the system is already well-architected and properly maintained, but may extend to months of meetings and digging into code if the monolith is a mess of chaotic code. However, this stage will allow us to build on solid foundations, knowing what the current system is.</p>&#13;&#13;
    <h3 id="_idParaDest-179" class="title">2. Design</h3>&#13;&#13;
    <p class="normal">The next <a id="_idIndexMarker602"/>stage of the process is to generate a vision in terms of what the system will look like after breaking the monolith up into multiple microservices.</p>&#13;&#13;
    <p class="normal">Each microservice needs to be considered in isolation, and as part of the rest. Think in terms of what makes sense to separate. Some questions that may help you to structure the design are as follows:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">What microservices should be created? Can you describe each microservice with a clear objective and area to control?</li>&#13;&#13;
      <li class="bullet">Is there any critical or core microservice that requires more attention or special requirements? For example, higher security or performance requirements.</li>&#13;&#13;
      <li class="bullet">How will the teams be structured to cover the microservices? Are there too many for the team to support? If that's the case, can multiple requests or areas be joined as part of the same microservice?</li>&#13;&#13;
      <li class="bullet">What are the prerequisites of each microservice?</li>&#13;&#13;
      <li class="bullet">What new technologies will be introduced? Is any training required?</li>&#13;&#13;
      <li class="bullet">Are microservices independent? What are the dependencies between microservices? Is there any microservice that is accessed more than others?</li>&#13;&#13;
      <li class="bullet">Can microservices be deployed independently from each other? What's the process <a id="_idIndexMarker603"/>if a new change is introduced that requires a change in a dependent dependency?</li>&#13;&#13;
      <li class="bullet">What microservices are going to be exposed externally? What microservices are only exposed internally?</li>&#13;&#13;
      <li class="bullet">Is there any prerequisite in terms of required API limitations? For example, is there any service that requires specific APIs, such as a SOAP connection?</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Other things that can be useful in informing the design can be to draw expected flow diagrams of requests that need to interact with multiple microservice, so as to analyze the expected movement between services.</p>&#13;&#13;
    <p class="normal">Special care should be taken regarding whatever storage is decided for each microservice. In general, storage for one microservice should not be shared with another, to isolate the data.</p>&#13;&#13;
    <p class="normal">This has a very concrete application, that is, to not access a database or other kind of raw storage directly by two or more microservices. Instead, one microservice should control the format and expose the data, and allow changes to the data by an accessible API.</p>&#13;&#13;
    <p class="normal">For example, let's imagine that there are two microservices, one that controls reports and another that controls users. For certain reports, we may need to access the user information to pull, for example, the name and email of a user who generated a report. We can break the microservice's responsibility by allowing the report service to access directly a database that contains user information.</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_09_04.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="427"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 9.4: An example of incorrect usage, accessing the information directly from storage </p>&#13;&#13;
    <p class="normal">Instead, the <a id="_idIndexMarker604"/>report service needs to access the user microservice through an API and pull the data. That way, each microservice is responsible for its own storage and format.</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_09_05.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="475"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 9.5: This is the correct structure. Each microservice keeps its own independent storage. This way, any information is only shared through well-defined APIs</p>&#13;&#13;
    <p class="normal">As we<a id="_idIndexMarker605"/> commented before, creating a flow diagram of some requests will help enforce this separation and find possible points of improvement; for example, returning data from an API that is not required until later in the process.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">While a prerequisite is not to mix storage, and to retain separation, you can use the same backend service to provide support for different microservices. The same database server can handle two or more logical databases that can store different information.</p>&#13;&#13;
      <p class="Tip--PACKT-">Generally, though, most microservices won't require their own data to be stored and can work in a completely stateless way, relying instead on other microservices to store the data.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">At this stage, there's no need to design detailed APIs between microservices, but some general ideas on what services handle what data and what the required flows between microservices are would be beneficial.</p>&#13;&#13;
    <h3 id="_idParaDest-180" class="title">3. Plan</h3>&#13;&#13;
    <p class="normal">Once the <a id="_idIndexMarker606"/>general areas are clear, it's time to get into more detail and start planning how the system is going to be changed from the starting point to the end line.</p>&#13;&#13;
    <p class="normal">The challenge here is to iteratively move into the new system while the system simultaneously remains functional at all times. New features are likely being introduced, but let's park that for the moment and talk only about the migration itself.</p>&#13;&#13;
    <p class="normal">To be able to do so, we need to use what is known <a id="_idIndexMarker607"/>as the <strong class="keyword">strangler pattern</strong>. This pattern aims to gradually replace parts of the system with new ones until the entire previous system is "strangled" and can be removed safely. This pattern gets applied iteratively, slowly, migrating the functionality from the old system to the new one in small increments.</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_09_06.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="422"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 9.6: The strangler pattern</p>&#13;&#13;
    <p class="normal">To create new microservices, there are three possible strategies:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Replace the functionality with new code that substitutes the old code, functionally producing the same result. Externally, the code reacts exactly the same to external requests, but internally, the implementation is new. This strategy allows you to start from scratch and fix some of the oddities of the old code. It can even be done in newer tools such as frameworks or even programming languages.<p class="bullet-para">At the same time, this approach can be very time-consuming. If the legacy system is undocumented and/or untested, it can be difficult to guarantee the same functionality. Also, if the functionality covered by this microservice is changing quickly, it may enter a game of catch-up between the new system and the old one, where there's no time to replicate any new functionality.</p>&#13;&#13;
        <p>&#13;&#13;
          <p class="Tip--PACKT-">This approach makes the most sense where the legacy parts to be replicated are small and obsolete, like using a tech stack that is considered to be deprecated.</p>&#13;&#13;
        </p>&#13;&#13;
      </li>&#13;&#13;
      <li class="bullet">Divide the <a id="_idIndexMarker608"/>functionality, copying and pasting code that exists in the monolith into a new microservice structure. If the existing code is in good shape and structured, this approach is relatively fast, only requiring some internal calls to be replaced with external API calls.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">It may be necessary to include in the monolith new access points to ensure that a new microservice can call back to obtain some information. </p>&#13;&#13;
      <p class="Tip-Within-Bullet--PACKT-">It's also possible that the monolith needs to be refactored to clarify elements and divide them into a structure that's more in line with the new system. </p>&#13;&#13;
    </p>&#13;&#13;
    <p class="bullet-para">This process can also be made iterative by first starting with a single functionality migrated to the new microservice, and then, one by one, moving the code until the functionality is completely migrated. At that point, it is safe to delete the code from the old system.</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">A <strong class="keyword">combination</strong> of both divide and replace. Some parts of the same functionality can likely be copied directly, but for others, a new approach is preferred.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">This will inform each microservice plan, although we will need to create a global view to determine which microservices to create in what order. </p>&#13;&#13;
    <p class="normal">Here are some useful points to think about to determine what the best course of action is:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">What microservices need to be available first, taking into account dependencies that will be produced.</li>&#13;&#13;
      <li class="bullet">An idea of what the biggest pain points are, and whether working on them is a priority. Pain points are the code or other elements that are changed frequently and the current way of dealing with them in a monolith makes them difficult. This can produce great benefits following migration.</li>&#13;&#13;
      <li class="bullet">What are the difficult points and the cans of worms? There will likely be some. Acknowledge that they exist and minimize their impact on other services. Note that they may be the same as the pain points, or they may differ. For example, old systems that are very stable are difficult points, but not painful as per our definition, as they don't change.</li>&#13;&#13;
      <li class="bullet">Now for a couple of quick wins that will keep the momentum of the project going. Show the advantages to your teams and stakeholders quickly! This will also allow everyone to understand the new mode of operation you want to move to and start working that way.</li>&#13;&#13;
      <li class="bullet">An idea of the training that teams will require and what the new elements are that you want to introduce. Also, whether any skills are lacking in your team – you may be planning to hire.</li>&#13;&#13;
      <li class="bullet">Any team<a id="_idIndexMarker609"/> changes and ownership of the new services. It's important to consider feedback from the teams so that they can express their concerns regarding any oversights during the creation of the plan. Involve the team and value their feedback.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Once we have a plan on how we are going to proceed, it's time to do it.</p>&#13;&#13;
    <h3 id="_idParaDest-181" class="title">4. Execute</h3>&#13;&#13;
    <p class="normal">Finally, we need to act on our plan to start the move from the outdated monolith to the new wonderful land of microservices!</p>&#13;&#13;
    <p class="normal">This will <a id="_idIndexMarker610"/>actually be the longest stage of the four, and arguably the most difficult. As we said before, the objective is to keep the service running all throughout the process.</p>&#13;&#13;
    <p class="normal">The key element for a successful transition is to maintain <strong class="keyword">backward compatibility</strong>. This means that the system keeps behaving like the monolithic system from an external point of view. That way, we can change the internals in terms of how the system works without affecting customers.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">Ideally, the new architecture will allow us to be faster, meaning the only perceived change will be that the system is more responsive!</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">This is obviously easier said than done. Software development in a production environment has been referred to as starting an automobile race driving a Ford T and crossing the finishing line in a Ferrari, changing every single piece of it without stopping. Fortunately, software is so flexible that this is something we can even discuss.</p>&#13;&#13;
    <p class="normal">To be able to make the change, from the monolith to the new microservice or microservices that handle the same functionality, the key tool is to use a load balancer at the top level, right on the ingress of requests. This is especially useful if the new microservice is directly replacing the requests. The load balancer can take the intake of requests and redirect them to the proper service, in a controlled manner.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">We will assume that all incoming requests are HTTP requests. A load balancer can handle other kinds of requests, but HTTP is by far the most common.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">This can be used to <a id="_idIndexMarker611"/>migrate the requests from the monolith slowly to the new microservice that should receive this request. Keep in mind that the load balancer can be configured by a different URL to direct the request to a different service, so it can use that small granularity to distribute the load properly across the different services.</p>&#13;&#13;
    <p class="normal">The process will look a little like this. First, the load balancer is directing all the requests to the legacy monolith. Once the new microservice is deployed, the requests can be load-balanced by introducing the new microservice. Initially, the balance should only be forwarding a few requests to the new system, to be sure that the behavior is the same.</p>&#13;&#13;
    <p class="normal">Slowly, over time, it can grow until all requests are migrated. For example, the first week can only move 10% of the requests, the second week 30%, the third week 50%, and then 100% of all requests the week after.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">The migration period is 4 weeks. During that time, no new features and changes should be introduced as the interface needs to be stable between the legacy monolith and the new microservice. Be sure that all the parties involved are aware of the plan and each of the steps.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">At that point, the handling of the requests in the legacy monolith is unused and can be removed to cleanup if this makes sense.</p>&#13;&#13;
    <p class="normal">This process is similar to the strangler pattern that we discussed before, but in this case applied to individual requests. The load balancer will be an invaluable ally for implementing the pattern in full form, extending this procedure in a greater mode, as we are adding more functionality and slowly migrating it to be certain that any problem can be detected early and without affecting a large number of requests. </p>&#13;&#13;
    <h4 class="title">Execution phases</h4>&#13;&#13;
    <p class="normal">The whole execution plan should consist of three phases:</p>&#13;&#13;
    <ol>&#13;&#13;
      <li class="numbered" value="1"><strong class="keyword">The pilot phase.</strong> Any <a id="_idIndexMarker612"/>plan will need to be tested with care. The pilot phase will be when the plan is checked in terms of its feasibility and the tools tested. A single team should lead this effort, to be sure that they are focused on it, and can learn and share quickly. Try to start on a couple of small services and low-hanging fruit, so that the improvement is obvious for the team. Good candidates are non-critical services, so if there's a problem, it doesn't present a big impact. This phase will allow you to prepare for the migration and to make adjustments and learn from inevitable mistakes.</li>&#13;&#13;
      <li class="numbered"><strong class="keyword">Consolidation phase.</strong> At<a id="_idIndexMarker613"/> this point, the basics of the migration are understood, but there's still a lot of code to migrate. The pilot team can then start training other teams and spread the knowledge, so everyone understands how it should proceed. By this time, the basic infrastructure will be in place, and hopefully the most obvious issues have been corrected or at least there's a good understanding of how to deal with them.<p class="bullet-para">To help with the spreading of knowledge, documenting standards will help teams to coordinate and depend less on asking the same questions over and over. Enforcing a list of prerequisites for a new microservice to be deployed and running in production will give clarity on what is required. Be sure also to keep a feedback channel, so new teams can share their findings and improve the process.</p>&#13;&#13;
        <p class="bullet-para">This phase will probably see some plan changes, as reality will overcome whatever plan has been laid out in advance. Be sure to adapt and keep an eye on the objective while navigating through the problems.</p>&#13;&#13;
        <p class="bullet-para">At this phase, the pace will be increased, as the uncertainty is being reduced as more and more code is migrated. At some point, creating and migrating a new microservice will be routine for the team.</p>&#13;&#13;
      </li>&#13;&#13;
      <li class="numbered"><strong class="keyword">Final phase</strong>. In this<a id="_idIndexMarker614"/> phase, the monolithic architecture has been split, and any new development is done in the microservices. There may still be some remains of the monolith that are regarded as unimportant or low priority. If that's the case, the boundaries should be clear to contain the old way of doing things.<p class="bullet-para">Now, teams can take full ownership of their microservices and start taking more ambitious tasks, such as replacing a microservice completely by creating an equivalent one in another programming language or changing the architecture by merging or splitting microservices. This is the end stage where, from now on, you live in a microservices architecture. Be sure to celebrate it with the team accordingly.</p>&#13;&#13;
      </li>&#13;&#13;
    </ol>&#13;&#13;
    <p class="normal">That's roughly the process. Of course, this may be a long and arduous process that can span many months or even years. Be sure to keep a sustainable pace and a long-term view on the objective to be able to continue until the goal is reached.</p>&#13;&#13;
    <h1 id="_idParaDest-182" class="title">Containerizing services</h1>&#13;&#13;
    <p class="normal">The traditional way of operating services is to use a server using a full OS, such as Linux, and then install on it all the required packages (for example, Python or PHP) and services (for example, nginx, uWSGI). The server acts as the unit, so each physical machine needs to be independently maintained <a id="_idIndexMarker615"/>and managed. It also may not be optimal from the point of view of hardware utilization.</p>&#13;&#13;
    <p class="normal">This can be improved by replacing the physical server with virtual machines, so a single physical server can handle multiple VMs. This helps with hardware utilization and flexibility, but still requires each server to be managed as an independent physical machine.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Multiple tools help with this management, for example, configuration management tools such as Chef or Puppet. They can manage multiple servers and guarantee that they have installed the proper versions and are running the proper services.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Containers <a id="_idIndexMarker616"/>bring a different approach to this area. Instead of using a full-fledged computer (a server), with an installed OS, packages, and dependencies, and then installing your software on top of that, which mutates more often than the underlying system, it creates a package (the container image) that brings it all.</p>&#13;&#13;
    <p class="normal">The container has its own filesystem, including the OS, dependencies, packages, and code, and is deployed as a whole. Instead of having a stable platform and running services on top of them, containers run as a whole, self-containing everything required. The platform (host machine) is a thin layer that only needs to be able to run the containers. Containers share the same kernel with the host, making them very efficient to run, compared with VMs, which may require simulating the whole server.</p>&#13;&#13;
    <p class="normal">This allows, for example, different containers to be run in the same physical machine and have each container run a different OS, with different packages, and different versions of the code.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Sometimes, containers are thought of as "lightweight virtual machines." This is not correct. Instead, think of them as <em class="italic">a process wrapped in its own filesystem</em>. This process is the main process of the container, and when it finishes, the container stops running.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">The most popular tool for building and running containers<a id="_idIndexMarker617"/> is Docker (<a href="https://www.docker.com/">https://www.docker.com/</a>). We will now examine how to operate with it.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">To install Docker, you can go to the documentation at <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a> and follow the instructions. Use version 20.10.7 or later.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Once installed, you should be <a id="_idIndexMarker618"/>able to check the version running and get something similar to the following:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ docker version&#13;&#13;
Client:&#13;&#13;
 Cloud integration: 1.0.17&#13;&#13;
 Version:           20.10.7&#13;&#13;
 API version:       1.41&#13;&#13;
 Go version:        go1.16.4&#13;&#13;
 Git commit:        f0df350&#13;&#13;
 Built:             Wed Jun  2 11:56:22 2021&#13;&#13;
 OS/Arch:           darwin/amd64&#13;&#13;
 Context:           desktop-linux&#13;&#13;
 Experimental:      true&#13;&#13;
Server: Docker Engine - Community&#13;&#13;
 Engine:&#13;&#13;
  Version:          20.10.7&#13;&#13;
  API version:      1.41 (minimum version 1.12)&#13;&#13;
  Go version:       go1.13.15&#13;&#13;
  Git commit:       b0f5bc3&#13;&#13;
  Built:            Wed Jun  2 11:54:58 2021&#13;&#13;
  OS/Arch:          linux/amd64&#13;&#13;
  Experimental:     false&#13;&#13;
 containerd:&#13;&#13;
  Version:          1.4.6&#13;&#13;
  GitCommit:        d71fcd7d8303cbf684402823e425e9dd2e99285d&#13;&#13;
 runc:&#13;&#13;
  Version:          1.0.0-rc95&#13;&#13;
  GitCommit:        b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7&#13;&#13;
 docker-init:&#13;&#13;
  Version:          0.19.0&#13;&#13;
  GitCommit:        de40ad0&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Now we need to build a container image that we can run.</p>&#13;&#13;
    <h2 id="_idParaDest-183" class="title">Building and running an image</h2>&#13;&#13;
    <p class="normal">The container image<a id="_idIndexMarker619"/> is the whole filesystem and instructions to run when it's started. To start using containers we need to build the proper images that form the basis of the system.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Remember the description presented previously, that a container is a process surrounded by its own filesystem. Building the image creates this filesystem.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">An image is created<a id="_idIndexMarker620"/> by applying a <code class="Code-In-Text--PACKT-">Dockerfile</code>, a recipe that creates the image by executing different layers, one by one.</p>&#13;&#13;
    <p class="normal">Let's see a very simple <code class="Code-In-Text--PACKT-">Dockerfile</code>. Create a file called <code class="Code-In-Text--PACKT-">sometext.txt</code> containing some small example text, and another file called <code class="Code-In-Text--PACKT-">Dockerfile.simple</code> containing the following text:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">FROM ubuntu &#13;&#13;
RUN mkdir -p /opt/&#13;&#13;
COPY sometext.txt /opt/sometext.txt&#13;&#13;
CMD cat /opt/sometext.txt&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The first line, <code class="Code-In-Text--PACKT-">FROM</code>, will start the image by using the Ubuntu image.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">There are many images that you can use as a starting point. You have all the usual Linux distributions, such as Ubuntu, Debian, and Fedora, but also images for full-fledged systems such as storage systems (MySQL, PostgreSQL, and Redis) or images to work with specific tools, such as Python, Node.js, or Ruby. Check<a id="_idIndexMarker621"/> Docker Hub (<a href="https://hub.docker.com">https://hub.docker.com</a>) for all the available images.</p>&#13;&#13;
      <p class="Information-Box--PACKT-">An interesting starting point is to use the<a id="_idIndexMarker622"/> Alpine Linux distribution, which is designed to be small and focused on security. Check out <a href="https://www.alpinelinux.org">https://www.alpinelinux.org</a> for further information.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">One of the main advantages of containers is the ability to use and share already created containers, either directly or as a starting point to enhance them. Nowadays, it is very common to create and push a container to Docker Hub to allow others to use it directly. That's one of the great things about containers! They are very easy to share and use.</p>&#13;&#13;
    <p class="normal">The second line runs a command inside the container. In this case, it creates a new subdirectory in <code class="Code-In-Text--PACKT-">/opt</code>:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">RUN mkdir -p /opt/&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Next, we copy the current <code class="Code-In-Text--PACKT-">sometext.txt</code> file inside, in the new subdirectory:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">COPY sometext.txt /opt/sometext.txt&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Finally, we define the command to execute when the image is run:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">CMD cat /opt/sometext.txt&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">To build the image, we run the following command:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">docker build -f &lt;Dockerfile&gt; --tag &lt;tag name&gt; &lt;context&gt;&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">In our case, we use the <a id="_idIndexMarker623"/>defined Dockerfile and <code class="Code-In-Text--PACKT-">example</code> as a tag. The context is <code class="Code-In-Text--PACKT-">.</code> (current directory), which defines the root point in terms of where to refer to all the <code class="Code-In-Text--PACKT-">COPY</code> commands:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ docker build -f Dockerfile.sample -–tag example .&#13;&#13;
[+] Building 1.9s (8/8) FINISHED&#13;&#13;
 =&gt; [internal] load build definition from Dockerfile.sample                                              &#13;&#13;
 =&gt; =&gt; transferring dockerfile: 92B                                                                                   &#13;&#13;
 =&gt; [internal] load .dockerignore                                                                                     &#13;&#13;
 =&gt; =&gt; transferring context: 2B                                                                                       &#13;&#13;
 =&gt; [internal] load metadata for docker.io/library/ubuntu:latest                                                      &#13;&#13;
 =&gt; [1/3] FROM docker.io/library/ubuntu@sha256:82becede498899ec668628e7cb0ad87b6e1c371cb8a1e597d83a47fac21d6af3       &#13;&#13;
 =&gt; [internal] load build context                                                                                     &#13;&#13;
 =&gt; =&gt; transferring context: 82B                                                                                      &#13;&#13;
 =&gt; CACHED [2/3] RUN mkdir -p /opt/                                                                                   &#13;&#13;
 =&gt; CACHED [3/3] COPY sometext.txt /opt/sometext.txt                                                                  &#13;&#13;
 =&gt; exporting to image                                                                                                &#13;&#13;
 =&gt; =&gt; exporting layers                                                                                               &#13;&#13;
 =&gt; =&gt; writing image sha256:e4a5342b531e68dfdb4d640f57165b704b1132cd18b5e2ba1220e2d800d066cb                          &#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">If we list the available images, you will be able to see the <code class="Code-In-Text--PACKT-">example</code> one:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ docker images&#13;&#13;
REPOSITORY      TAG          IMAGE ID       CREATED         SIZE&#13;&#13;
example         latest       e4a5342b531e   2 hours ago     72.8MB&#13;&#13;
ubuntu          latest       1318b700e415   47 hours ago    72.8MB&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">We can now run the<a id="_idIndexMarker624"/> container, which will execute the <code class="Code-In-Text--PACKT-">cat</code> command inside:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ docker run example&#13;&#13;
Some example text&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The container will stop the execution as the command finishes. You can see the stopped containers using the <code class="Code-In-Text--PACKT-">docker ps -a</code> command, but a stopped container is generally not very interesting. </p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">A common exception to this is that the resulting filesystem is stored onto disk, so the stopped container may have interesting files generated as part of the command.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">While this way of running containers can be useful sometimes to compile binaries or other kinds of operations of a similar kind, normally, it's more common to create <code class="Code-In-Text--PACKT-">RUN</code> commands that are always running. In that case, it will run until stopped externally, as the command will run forever.</p>&#13;&#13;
    <h2 id="_idParaDest-184" class="title">Building and running a web service</h2>&#13;&#13;
    <p class="normal">A web service container<a id="_idIndexMarker625"/> is the most common type of microservice, as we have seen. To be able to build and run one, we need to have the following parts:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Proper infrastructure that runs the web service to a port in the container</li>&#13;&#13;
      <li class="bullet">Our code, which will run</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Following the usual architecture presented in previous chapters, we will use the following tech stack:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Our code will be written in Python and use Django as the web framework</li>&#13;&#13;
      <li class="bullet">The Python code will be executed through uWSGI</li>&#13;&#13;
      <li class="bullet">The service will be exposed in port 8000 through an nginx web server</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Let's take a look at the different elements.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">The code is available at <a href="https://github.com/PacktPublishing/Python-Architecture-Patterns/tree/main/chapter_09_monolith_microservices/web_service">https://github.com/PacktPublishing/Python-Architecture-Patterns/tree/main/chapter_09_monolith_microservices/web_service</a>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">The code is structured in <a id="_idIndexMarker626"/>two main directories and one file:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet"><code class="Code-In-Text--PACKT-">docker</code>: This subdirectory contains the files related to the operation of Docker and other infrastructure.</li>&#13;&#13;
      <li class="bullet"><code class="Code-In-Text--PACKT-">src</code>: The source code of the web service itself. The source code is the same as we saw in <em class="chapterRef">Chapter 5</em>, <em class="italic">The Twelve-Factor App Methodology</em>.</li>&#13;&#13;
      <li class="bullet"><code class="Code-In-Text--PACKT-">requirements.txt</code>: The file with the Python requirements for running the source code.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">The Dockerfile image is located in the <code class="Code-In-Text--PACKT-">./docker</code> subdirectory. We will follow it to explain how the different parts connect:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">FROM ubuntu AS runtime-image&#13;&#13;
# Install Python, uwsgi and nginx&#13;&#13;
RUN apt-get update &amp;&amp; apt-get install -y python3 nginx uwsgi uwsgi-plugin-python3&#13;&#13;
RUN apt-get install -y python3-pip&#13;&#13;
# Add starting script and config&#13;&#13;
RUN mkdir -p /opt/server&#13;&#13;
ADD ./docker/uwsgi.ini /opt/server&#13;&#13;
ADD ./docker/nginx.conf /etc/nginx/conf.d/default.conf&#13;&#13;
ADD ./docker/start_server.sh /opt/server&#13;&#13;
# Add and install requirements&#13;&#13;
ADD requirements.txt /opt/server&#13;&#13;
RUN pip3 install -r /opt/server/requirements.txt&#13;&#13;
# Add the source code&#13;&#13;
RUN mkdir -p /opt/code&#13;&#13;
ADD ./src/ /opt/code&#13;&#13;
WORKDIR /opt/code&#13;&#13;
# compile the static files&#13;&#13;
RUN python3 manage.py collectstatic --noinput&#13;&#13;
EXPOSE 8000&#13;&#13;
CMD ["/bin/sh", "/opt/server/start_server.sh"]&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The first part of the file starts the container from the standard Ubuntu Docker image and install the basic requirements: Python interpreter, nginx, uWSGI, and a couple of complementary packages – the uWSGI plugin to run <code class="Code-In-Text--PACKT-">python3</code> code and <code class="Code-In-Text--PACKT-">pip</code> to be able to install Python packages:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">FROM ubuntu AS runtime-image&#13;&#13;
# Install Python, uwsgi and nginx&#13;&#13;
RUN apt-get update &amp;&amp; apt-get install -y python3 nginx uwsgi uwsgi-plugin-python3&#13;&#13;
RUN apt-get install -y python3-pip&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The next stage is<a id="_idIndexMarker627"/> to add all the required scripts and config files to start the server and configure uWSGI and nginx. All these files are in the <code class="Code-In-Text--PACKT-">./docker</code> subdirectory and are stored inside the container in <code class="Code-In-Text--PACKT-">/opt/server </code>(except for the nginx configuration that is stored in the default <code class="Code-In-Text--PACKT-">/etc/nginx </code>subdirectory). </p>&#13;&#13;
    <p class="normal">We ensure that the start script is executable:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"># Add starting script and config&#13;&#13;
RUN mkdir -p /opt/server&#13;&#13;
ADD ./docker/uwsgi.ini /opt/server&#13;&#13;
ADD ./docker/nginx.conf /etc/nginx/conf.d/default.conf&#13;&#13;
ADD ./docker/start_server.sh /opt/server&#13;&#13;
RUN chmod +x /opt/server/start_server.sh&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The Python requirements are installed next. The <code class="Code-In-Text--PACKT-">requirements.txt</code> file is added and then installed through the <code class="Code-In-Text--PACKT-">pip3</code> command:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"># Add and install requirements&#13;&#13;
ADD requirements.txt /opt/server&#13;&#13;
RUN pip3 install -r /opt/server/requirements.txt&#13;&#13;
</code></pre>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">Some Python packages may need certain packages to be installed in the container in the first stage to be sure that some tools are available; for example, installing certain database connection modules will require the proper client libraries to be installed.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">We add the source code to <code class="Code-In-Text--PACKT-">/opt/code</code> next. With the <code class="Code-In-Text--PACKT-">WORKDIR</code> command, we execute any <code class="Code-In-Text--PACKT-">RUN</code> command in that subdirectory and then run <code class="Code-In-Text--PACKT-">collectstatic</code> with the Django <code class="Code-In-Text--PACKT-">manage.py</code> command to generate the static files in the proper subdirectory:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"># Add the source code&#13;&#13;
RUN mkdir -p /opt/code&#13;&#13;
ADD ./src/ /opt/code&#13;&#13;
WORKDIR /opt/code&#13;&#13;
# compile the static files&#13;&#13;
RUN python3 manage.py collectstatic --noinput&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Finally, we describe the<a id="_idIndexMarker628"/> exposed port (8000) and the <code class="Code-In-Text--PACKT-">CMD</code> to run to start the container, the <code class="Code-In-Text--PACKT-">start_server.sh</code> script copied previously:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">EXPOSE 8000&#13;&#13;
CMD ["/bin/bash", "/opt/server/start_server.sh"]&#13;&#13;
</code></pre>&#13;&#13;
    <h3 id="_idParaDest-185" class="title">uWSGI configuration</h3>&#13;&#13;
    <p class="normal">The uWSGI configuration is <a id="_idIndexMarker629"/>very similar to the one presented in <em class="chapterRef">Chapter 5</em>,<em class="chapterRef"> </em><em class="italic">The Twelve-Factor App Methodology</em>:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">[uwsgi]&#13;&#13;
plugins=python3&#13;&#13;
chdir=/opt/code&#13;&#13;
wsgi-file = microposts/wsgi.py&#13;&#13;
master=True&#13;&#13;
socket=/tmp/uwsgi.sock&#13;&#13;
vacuum=True&#13;&#13;
processes=1&#13;&#13;
max-requests=5000&#13;&#13;
uid=www-data&#13;&#13;
# Used to send commands to uWSGI&#13;&#13;
master-fifo=/tmp/uwsgi-fifo&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The only difference is the need to include the <code class="Code-In-Text--PACKT-">plugins</code> parameter to indicate that it runs the <code class="Code-In-Text--PACKT-">python3</code> plugin (this is because the Ubuntu-installed <code class="Code-In-Text--PACKT-">uwsgi</code> package doesn't have it activated by default). Also, we will run the process with the same user as nginx, to allow them to communicate through the <code class="Code-In-Text--PACKT-">/tmp/uwsgi.sock</code> socket. This is added with <code class="Code-In-Text--PACKT-">uid=www-data</code>, with <code class="Code-In-Text--PACKT-">www-data</code> being<a id="_idIndexMarker630"/> the default nginx user.</p>&#13;&#13;
    <h3 id="_idParaDest-186" class="title">nginx configuration</h3>&#13;&#13;
    <p class="normal">The nginx configuration is<a id="_idIndexMarker631"/> also very similar to the one presented in <em class="chapterRef">Chapter 5</em>,<em class="chapterRef"> </em><em class="italic">The Twelve-Factor App Methodology</em>:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">server {&#13;&#13;
    listen 8000 default_server;&#13;&#13;
    listen [::]:8000 default_server;&#13;&#13;
    root /opt/code/;&#13;&#13;
    location /static/ {&#13;&#13;
        autoindex on;&#13;&#13;
        try_files $uri $uri/ =404;&#13;&#13;
    }&#13;&#13;
    location / {&#13;&#13;
        proxy_set_header Host $host;&#13;&#13;
        proxy_set_header X-Real-IP $remote_addr;&#13;&#13;
        uwsgi_pass unix:///tmp/uwsgi.sock;&#13;&#13;
        include uwsgi_params;&#13;&#13;
    }&#13;&#13;
}&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The only difference is the exposed port, which is <code class="Code-In-Text--PACKT-">8000</code>. Note that the root directory is <code class="Code-In-Text--PACKT-">/opt/code</code>, making the static file directory <code class="Code-In-Text--PACKT-">/opt/code/static</code>. This needs to be in sync with the configuration from Django.</p>&#13;&#13;
    <h3 id="_idParaDest-187" class="title">Start script</h3>&#13;&#13;
    <p class="normal">Let's take a look at the script <a id="_idIndexMarker632"/>that starts the service, <code class="Code-In-Text--PACKT-">start_script.sh</code>:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">#!/bin/bash&#13;&#13;
_term() {&#13;&#13;
  # See details in the uwsgi.ini file and&#13;&#13;
  # in http://uwsgi-docs.readthedocs.io/en/latest/MasterFIFO.html&#13;&#13;
  # q means "graceful stop"&#13;&#13;
  echo q &gt; /tmp/uwsgi-fifo&#13;&#13;
}&#13;&#13;
trap _term TERM&#13;&#13;
nginx&#13;&#13;
uwsgi --ini /opt/server/uwsgi.ini &amp;&#13;&#13;
# We need to wait to properly catch the signal, that's why uWSGI is started&#13;&#13;
# in the background. $! is the PID of uWSGI&#13;&#13;
wait $!&#13;&#13;
# The container exits with code 143, which means "exited because SIGTERM"&#13;&#13;
# 128 + 15 (SIGTERM)&#13;&#13;
# http://www.tldp.org/LDP/abs/html/exitcodes.html&#13;&#13;
# http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_12_02.html&#13;&#13;
echo "Exiting, bye!"&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The core of the start is at the center, in these lines, nginx:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">uwsgi --ini /opt/server/uwsgi.ini &amp;&#13;&#13;
wait $!&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This starts both <code class="Code-In-Text--PACKT-">nginx</code> and <code class="Code-In-Text--PACKT-">uwsgi</code>, and waits until the <code class="Code-In-Text--PACKT-">uwsgi</code> process is not running. In Bash, <code class="Code-In-Text--PACKT-">$!</code> is the PID of the last process (the <code class="Code-In-Text--PACKT-">uwsgi</code> process).</p>&#13;&#13;
    <p class="normal">When Docker attempts to stop a container, it will first send a <code class="Code-In-Text--PACKT-">SIGTERM</code> signal to the container. That's why we create a <code class="Code-In-Text--PACKT-">trap</code> command that captures this signal and executes the <code class="Code-In-Text--PACKT-">_term()</code> function. This function sends a graceful stop command to the <code class="Code-In-Text--PACKT-">uwsgi</code> queue, as we described in <em class="chapterRef">Chapter 5</em>,<em class="chapterRef"> </em><em class="italic">The Twelve-Factor App Methodology</em>, which ends the process in a graceful manner:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">_term() {&#13;&#13;
  echo q &gt; /tmp/uwsgi-fifo&#13;&#13;
}&#13;&#13;
trap _term TERM&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">If the initial <code class="Code-In-Text--PACKT-">SIGTERM</code> signal is <a id="_idIndexMarker633"/>not successful, Docker will stop the container killing it following a grace period, but that will risk having a non-graceful end for the process.</p>&#13;&#13;
    <h3 id="_idParaDest-188" class="title">Building and running</h3>&#13;&#13;
    <p class="normal">We can now build the<a id="_idIndexMarker634"/> image and run it. To build the image, we perform a similar command as before:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ docker build -f docker/Dockerfile --tag example .&#13;&#13;
[+] Building 0.2s (19/19) FINISHED&#13;&#13;
 =&gt; [internal] load build definition from Dockerfile&#13;&#13;
 =&gt; =&gt; transferring dockerfile: 85B&#13;&#13;
 =&gt; [internal] load .dockerignore &#13;&#13;
 =&gt; =&gt; transferring context: 2B&#13;&#13;
 =&gt; [internal] load metadata for docker.io/library/ubuntu:latest&#13;&#13;
 =&gt; [ 1/14] FROM docker.io/library/ubuntu&#13;&#13;
 =&gt; [internal] load build context&#13;&#13;
 =&gt; =&gt; transferring context: 4.02kB&#13;&#13;
 =&gt; CACHED [ 2/14] RUN apt-get update &amp;&amp; apt-get install -y python3 nginx uwsgi uwsgi-plugin-pytho  &#13;&#13;
 =&gt; CACHED [ 3/14] RUN apt-get install -y python3-pip&#13;&#13;
 =&gt; CACHED [ 4/14] RUN mkdir -p /opt/server &#13;&#13;
 =&gt; CACHED [ 5/14] ADD ./docker/uwsgi.ini /opt/server&#13;&#13;
 =&gt; CACHED [ 6/14] ADD ./docker/nginx.conf /etc/nginx/conf.d/default.conf&#13;&#13;
 =&gt; CACHED [ 7/14] ADD ./docker/start_server.sh /opt/server&#13;&#13;
 =&gt; CACHED [ 8/14] RUN chmod +x /opt/server/start_server.sh&#13;&#13;
 =&gt; CACHED [ 9/14] ADD requirements.txt /opt/server&#13;&#13;
 =&gt; CACHED [10/14] RUN pip3 install -r /opt/server/requirements.txt&#13;&#13;
 =&gt; CACHED [11/14] RUN mkdir -p /opt/code &#13;&#13;
 =&gt; CACHED [12/14] ADD ./src/ /opt/code&#13;&#13;
 =&gt; CACHED [13/14] WORKDIR /opt/code&#13;&#13;
 =&gt; CACHED [14/14] RUN python3 manage.py collectstatic --noinput&#13;&#13;
 =&gt; exporting to image&#13;&#13;
 =&gt; =&gt; exporting layers&#13;&#13;
 =&gt; =&gt; writing image sha256:7be9ae2ab0e16547480aef6d32a11c2ccaa3da4aa5efbfddedb888681b8e10fa&#13;&#13;
 =&gt; =&gt; naming to docker.io/library/example&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">To run the service, start <a id="_idIndexMarker635"/>the container, mapping its port <code class="Code-In-Text--PACKT-">8000</code> to a local port, for example, <code class="Code-In-Text--PACKT-">local 8000</code>:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ docker run -p 8000:8000 example&#13;&#13;
[uWSGI] getting INI configuration from /opt/server/uwsgi.ini&#13;&#13;
*** Starting uWSGI 2.0.18-debian (64bit) on [Sat Jul 31 20:07:20 2021] ***&#13;&#13;
compiled with version: 10.0.1 20200405 (experimental) [master revision 0be9efad938:fcb98e4978a:705510a708d3642c9c962beb663c476167e4e8a4] on 11 April 2020 11:15:55&#13;&#13;
os: Linux-5.10.25-linuxkit #1 SMP Tue Mar 23 09:27:39 UTC 2021&#13;&#13;
nodename: b01ce0d2a335&#13;&#13;
machine: x86_64&#13;&#13;
clock source: unix&#13;&#13;
pcre jit disabled&#13;&#13;
detected number of CPU cores: 2&#13;&#13;
current working directory: /opt/code&#13;&#13;
detected binary path: /usr/bin/uwsgi-core&#13;&#13;
setuid() to 33&#13;&#13;
chdir() to /opt/code&#13;&#13;
your memory page size is 4096 bytes&#13;&#13;
detected max file descriptor number: 1048576&#13;&#13;
lock engine: pthread robust mutexes&#13;&#13;
thunder lock: disabled (you can enable it with --thunder-lock)&#13;&#13;
uwsgi socket 0 bound to UNIX address /tmp/uwsgi.sock fd 3&#13;&#13;
Python version: 3.8.10 (default, Jun  2 2021, 10:49:15)  [GCC 9.4.0]&#13;&#13;
*** Python threads support is disabled. You can enable it with --enable-threads ***&#13;&#13;
Python main interpreter initialized at 0x55a60f8c2a40&#13;&#13;
your server socket listen backlog is limited to 100 connections&#13;&#13;
your mercy for graceful operations on workers is 60 seconds&#13;&#13;
mapped 145840 bytes (142 KB) for 1 cores&#13;&#13;
*** Operational MODE: single process ***&#13;&#13;
WSGI app 0 (mountpoint='') ready in 1 seconds on interpreter 0x55a60f8c2a40 pid: 11 (default app)&#13;&#13;
*** uWSGI is running in multiple interpreter mode ***&#13;&#13;
spawned uWSGI master process (pid: 11)&#13;&#13;
spawned uWSGI worker 1 (pid: 13, cores: 1)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">After doing this, you can <a id="_idIndexMarker636"/>access your local address, <code class="Code-In-Text--PACKT-">http://localhost:8000</code>, and access the service; for example, accessing the URL <code class="Code-In-Text--PACKT-">http://localhost:8000/api/users/jaime/collection</code>:</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_09_07.png" alt="Graphical user interface, application&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="685"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 9.7: Microposts list</p>&#13;&#13;
    <p class="normal">You'll see the access log in the screen where you started the container:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">[pid: 13|app: 0|req: 2/2] 172.17.0.1 () {42 vars in 769 bytes} [Sat Jul 31 20:28:56 2021] GET /api/users/jaime/collection =&gt; generated 10375 bytes in 173 msecs (HTTP/1.1 200) 8 headers in 391 bytes (1 switches on core 0)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The container can be stopped gracefully using the <code class="Code-In-Text--PACKT-">docker stop</code> command. To do so, you'll need to first discover the container ID using <code class="Code-In-Text--PACKT-">docker ps</code>:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ docker ps&#13;&#13;
CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS                                       NAMES&#13;&#13;
b01ce0d2a335   example   "/bin/bash /opt/serv…"   23 minutes ago   Up 23 minutes   0.0.0.0:8000-&gt;8000/tcp, :::8000-&gt;8000/tcp   hardcore_chaum&#13;&#13;
$ docker stop b01ce0d2a335&#13;&#13;
b01ce0d2a335&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The container log will show the <a id="_idIndexMarker637"/>details when capturing the <code class="Code-In-Text--PACKT-">SIGTERM</code> signal sent by Docker and will then exit:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">Caught SIGTERM signal! Sending graceful stop to uWSGI through the master-fifo&#13;&#13;
Exiting, bye!&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">To be able to set this example, we made some conscious decisions to simplify the operation compared with a typical service.</p>&#13;&#13;
    <h3 id="_idParaDest-189" class="title">Caveats</h3>&#13;&#13;
    <p class="normal">Remember to check <em class="chapterRef">Chapter 5</em>,<em class="chapterRef"> </em><em class="italic">The Twelve-Factor App Methodology</em>, to see the defined API and understand it better.</p>&#13;&#13;
    <p class="normal">The <code class="Code-In-Text--PACKT-">DEBUG</code> mode in the Django <code class="Code-In-Text--PACKT-">settings.py</code> file is set to <code class="Code-In-Text--PACKT-">True</code>, which allows us to see more information when, for example, 404 or 500 errors are triggered. This parameter should be disabled in production as it can give away critical information.</p>&#13;&#13;
    <p class="normal">The <code class="Code-In-Text--PACKT-">STATIC_ROOT</code> and <code class="Code-In-Text--PACKT-">STATIC_URL</code> parameters need to be coordinated between Django and nginx to point to the same place. That way, the <code class="Code-In-Text--PACKT-">collectstatic</code> command will store the data in the same place where nginx will pick it up.</p>&#13;&#13;
    <p class="normal">The most important detail is the use of a SQLite database instead of an internal one. This database is stored in the <code class="Code-In-Text--PACKT-">src/db.sqlite3</code> file, in the filesystem of the container. This means that if the container is stopped and restarted, any changes will be destroyed. </p>&#13;&#13;
    <p class="normal">The <code class="Code-In-Text--PACKT-">db.sqlite3</code> file<a id="_idIndexMarker638"/> in the GitHub repo contains some information that has been stored for convenience, two users, <code class="Code-In-Text--PACKT-">jaime</code> and <code class="Code-In-Text--PACKT-">dana</code>, each with a couple of microposts. The API so far hasn't been defined in such a way to create new users, so it needs to relay into creating them using Django tools or manipulating the SQL directly. These users are added for demonstration purposes.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">As an exercise, create a script that seeds the database with information as part of the build process.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">In general, this database usage is not well suited for production usage, requiring connection to a database external to the container. This obviously requires an available external database, which complicates the setup.</p>&#13;&#13;
    <p class="normal">Now that we know how to use containers, we can perhaps start another Docker container with a database, such as MySQL, for a better configuration.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">A containerized database is not a great idea for production. In general, containers are great for stateless services that change often, as they can be started and stopped easily. Databases tend to be very stable and there are a lot of services that make provisions for managed databases. The advantages that containers bring are simply not relevant for a typical database.</p>&#13;&#13;
      <p class="Tip--PACKT-">That doesn't mean that there are usages out of production. It is a great option for local development, for example, as it allows the creation of a replicable local environment easily.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">If we want to create more than one container and connect them, like a web server and a database that acts as a backend for storing the data, instead of starting all the containers individually, we can use orchestration tools.</p>&#13;&#13;
    <h1 id="_idParaDest-190" class="title">Orchestration and Kubernetes</h1>&#13;&#13;
    <p class="normal">Managing multiple containers and connecting them is known as orchestrating them. Microservices that are deployed in containers will have to orchestrate them to be sure that the multiple microservices are interconnecting.</p>&#13;&#13;
    <p class="normal">This concept includes details such as discovering where the other containers are, dependencies between services, and generating multiple copies of the same container.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Orchestration tools are very powerful, as well as complex, and require that you become familiar with a lot of terms. To explain them fully is beyond the scope of this book, but we will point to some and give a short introduction. Please refer to the linked documentation in the sections below for more information.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">There are several tools that can <a id="_idIndexMarker639"/>perform orchestration, the two most common ones being <code class="Code-In-Text--PACKT-">docker-compose</code> and Kubernetes.</p>&#13;&#13;
    <p class="normal"><code class="Code-In-Text--PACKT-">docker-compose</code> is part of the general offering by Docker. It works very well for small deployments or local development. It defines a single YAML file that contains the definition of the different services, and the name that they can use. It can be used to replace a lot of <code class="Code-In-Text--PACKT-">docker build</code> and <code class="Code-In-Text--PACKT-">docker run</code> commands, as it can define all the parameters in the YAML file.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">You can check the documentation for Docker Compose<a id="_idIndexMarker640"/> here: <a href="https://docs.docker.com/compose/">https://docs.docker.com/compose/</a>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Kubernetes is <a id="_idIndexMarker641"/>aimed at bigger deployments and clusters and allows the generation of a full logical structure for containers to define how they connect to one another, thereby allowing abstraction to the underlying infrastructure.</p>&#13;&#13;
    <p class="normal">Any physical (or virtual) server configured in Kubernetes is called a <strong class="keyword">node</strong>. All nodes define the cluster. Each node is handled by <a id="_idIndexMarker642"/>Kubernetes, and Kubernetes will create a network between the nodes and assign the different containers to each of them, attending to the available space on them. This means that the number, location, or kind of node doesn't need to be handled by the services.</p>&#13;&#13;
    <p class="normal">Instead, the applications in the cluster are distributed in the logical layer. Several elements can be defined:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet"><strong class="keyword">Pod</strong>. A Pod<a id="_idIndexMarker643"/> is the minimal unit defined in Kubernetes, and it is defined as a group of containers that runs as a unit. Normally, Pods will consist of just one container, but in some cases, they may comprise several. Everything in Kubernetes runs in Pods.</li>&#13;&#13;
      <li class="bullet"><strong class="keyword">Deployment</strong>. A collection of Pods. The Deployment<a id="_idIndexMarker644"/> will define the number of replicas that are needed, and create the proper number of Pods. Each Pod for the same deployment can live in different nodes, but that's under the control of Kubernetes.<p class="bullet">Because the Deployment controls the number of Pods, if a Pod crashes, the Deployment will restart it. Also, the Deployment can be manipulated to change the number, for example, by creating autoscalers. If the image to be deployed in the Pods is changed, the Deployment will create new Pods with the right image and remove the old ones accordingly, based on rolling updates or other strategies.</p>&#13;&#13;
      </li>&#13;&#13;
      <li class="bullet"><strong class="keyword">Service</strong>. A label that can be used to route requests to certain Pods, acting as a DNS name. Normally, this will point to the Pods created for deployment. This allows other Pods in the<a id="_idIndexMarker645"/> system to send requests to a known place. The requests will be load-balanced between the different Pods.</li>&#13;&#13;
      <li class="bullet"><strong class="keyword">Ingress</strong>. External access to a <a id="_idIndexMarker646"/>service. This will map an incoming DNS to a service. Ingresses allow applications to be exposed externally. An external request will go through the process of entering through an Ingress, being directed to a Service, and then handled by a specific pod.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Some components can be described in a Kubernetes cluster, such as <code class="Code-In-Text--PACKT-">ConfigMaps</code>, defining key-value pairs that can be used for configuration purposes; <code class="Code-In-Text--PACKT-">Volumes</code> to share storage across Pods; and <code class="Code-In-Text--PACKT-">Secrets</code> to define secret values that can be injected into Pods.</p>&#13;&#13;
    <p class="normal">Kubernetes<a id="_idIndexMarker647"/> is a fantastic tool that can handle pretty big clusters with hundreds of nodes and thousands of Pods. It's also a complex tool that requires you to learn how it can be used and has a significant learning curve. It's pretty popular these days, and there's plenty of <a id="_idIndexMarker648"/>documentation about it. The official documentation can be found here: <a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a>.</p>&#13;&#13;
    <h1 id="_idParaDest-191" class="title">Summary</h1>&#13;&#13;
    <p class="normal">In this chapter, we described both the monolithic and microservices architectures. We started by presenting the monolithic architecture and how it tends to be a "default architecture," generated organically as an application is designed. Monoliths are created as unitary blocks that contain all the code within a single block.</p>&#13;&#13;
    <p class="normal">In comparison, the microservices architecture divides the functionality of the whole application into smaller parts so that they can be worked in parallel. For this strategy to work, it needs to define clear boundaries and document how to interconnect the different services. Compared with the monolithic architecture, microservices aim to generate more structured code and control big code bases by dividing them into smaller, more manageable systems.</p>&#13;&#13;
    <p class="normal">We discussed what the best architecture is and how to choose whether to design a system as a monolith or as microservices. Each approach has its pros and cons, but in general, systems start as monolithic and the move to divide the code base into smaller microservices comes after the code base and the number of developers working on it reaches a certain size.</p>&#13;&#13;
    <p class="normal">The difference between the two architectures is not just technical. It largely involves how developers working on the system need to communicate and divide the teams. We discussed the different aspects to take into account, including the structure and size of the teams.</p>&#13;&#13;
    <p class="normal">Since migrating from an old monolithic architecture to a new microservices one is such a common case, we talked about how to approach the work, analyze it, and perform it, using a four-stage roadmap: Analyze, Design, Plan, and Execute.</p>&#13;&#13;
    <p class="normal">We then discussed how containerizing services (and, in particular, microservices) can be helpful. We explored how to use Docker as a tool to containerize services and its multiple advantages and uses. We included an example of containerizing our example web service, as described in <em class="chapterRef">Chapter 5</em>, <em class="italic">The Twelve-Factor App Methodology</em>.</p>&#13;&#13;
    <p class="normal">Finally, we described briefly the usage of an orchestration tool to coordinate and intercommunicate between multiple containers, and the most popular, Kubernetes. We then covered a brief introduction to Kubernetes.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">You can get more information about microservices and how to perform a migration from a monolithic architecture to a microservices one in the book <em class="italic">Hands-On Docker for Microservices with Python</em>, from the author of this book, which expands on these concepts and goes into greater depth.</p>&#13;&#13;
    </p>&#13;&#13;
  </div>&#13;&#13;
</div></body></html>