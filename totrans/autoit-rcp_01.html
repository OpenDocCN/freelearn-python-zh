<html><head></head><body><div class="chapter" title="Chapter&#xA0;1.&#xA0;Working with the Web"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Working with the Web</h1></div></div></div><p>Can you image a life without the Internet? For almost everything, right from exchanging information to ordering food, we rely heavily on the Internet today. Let's go through the interesting world of the World Wide Web and cover numerous ways with which we can interact with it using Python modules.</p><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Making HTTP requests</li><li class="listitem" style="list-style-type: disc">A brief look at web scraping</li><li class="listitem" style="list-style-type: disc">Parsing and extracting web content</li><li class="listitem" style="list-style-type: disc">Downloading content from the Web</li><li class="listitem" style="list-style-type: disc">Working with third-party REST APIs</li><li class="listitem" style="list-style-type: disc">Asynchronous HTTP server in Python</li><li class="listitem" style="list-style-type: disc">Web automation with selenium bindings</li><li class="listitem" style="list-style-type: disc">Automating lead generation with web scraping</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec8"/>Introduction</h1></div></div></div><p>Internet has made life so easy that sometimes you just don't realize the power of it. Checking out your friend's status, calling your parents, responding to an important business e-mail, or playing a game--we rely on the <span class="strong"><strong>World Wide Web</strong></span> (<span class="strong"><strong>WWW</strong></span>) today for almost everything.</p><p>Thankfully, Python has a rich set of modules that help us perform various tasks on the Web. Phew! Not only could you make simple HTTP requests retrieve data from websites or download pages and images, you could also parse the page content to gather information and analyze it to generate meaningful insights with Python. And wait; did I mention that you could spawn a browser in an automated fashion to perform a daily mundane task?</p><p>The recipes in this chapter will primarily focus on the Python modules that can be treated as the tool of choice while performing the preceding operations on the Web. Specifically, we will focus on the following Python modules in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">requests</code> (<a class="ulink" href="http://docs.python-requests.org/en/master/">http://docs.python-requests.org/en/master/</a>)</li><li class="listitem" style="list-style-type: disc"><code class="literal">urllib2</code> (<a class="ulink" href="https://docs.python.org/2/library/urllib2.html">https://docs.python.org/2/library/urllib2.html</a>)</li><li class="listitem" style="list-style-type: disc"><code class="literal">lxml</code> (<a class="ulink" href="https://pypi.python.org/pypi/lxml">https://pypi.python.org/pypi/lxml</a>)</li><li class="listitem" style="list-style-type: disc"><code class="literal">BeautifulSoup4 </code>(<a class="ulink" href="https://pypi.python.org/pypi/beautifulsoup4">https://pypi.python.org/pypi/beautifulsoup4</a>)</li><li class="listitem" style="list-style-type: disc"><code class="literal">selenium</code> (<a class="ulink" href="http://selenium-python.readthedocs.org/">http://selenium-python.readthedocs.org/</a>)</li></ul></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note3"/>Note</h3><p>While the recipes in this chapter will give you an overview of how to interact with the Web using Python modules, I encourage you to try out and develop code for multiple use cases, which will benefit you as an individual and your project on an organizational scale.</p></div></div></div></div>
<div class="section" title="Making HTTP requests"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec9"/>Making HTTP requests</h1></div></div></div><p>Throughout the following recipes in this chapter, we will use Python v2.7 and the <code class="literal">requests</code> (v2.9.1) module of Python. This recipe will show you how to make HTTP requests to web pages on the Internet.</p><p>But before going there, let's understand the <span class="strong"><strong>Hypertext Transfer Protocol </strong></span>(<span class="strong"><strong>HTTP</strong></span>) in brief. HTTP is a stateless application protocol for data communication on the WWW. A typical HTTP Session involves a sequence of request or response transactions. The client initiates a <span class="strong"><strong>TCP</strong></span> connection to the Server on a dedicated IP and Port; when the Server receives the request, it responds with the response code and text. HTTP defines request methods (HTTP verbs like <code class="literal">GET</code>, <code class="literal">POST</code>), which indicate the desired action to be taken on the given Web URL.</p><p>In this recipe, we'll learn how to make HTTP <code class="literal">GET</code>/<code class="literal">POST</code> requests using Python's <code class="literal">requests</code> module. We'll also learn how to POST <code class="literal">json</code> data and handle HTTP exceptions. Cool, let's jump in.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec11"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need to install Python v2.7. Once installed, you will need to install Python <code class="literal">pip</code>. <span class="strong"><strong>PIP</strong></span> stands for <span class="strong"><strong>Pip Installs Packages</strong></span> and is a program that can be used to download and install the required Python packages on your computer. Lastly, we'll need the <code class="literal">requests</code> module to make HTTP requests.</p><p>We will start by installing the <code class="literal">requests</code> module (I'll leave the Python and <code class="literal">pip</code> installation for you to perform on your machine, based on your operating system). No other prerequisites are required. So, hurry up and let's get going!</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec12"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">On your Linux/Mac computer, go to Terminal and run the following command:<pre class="programlisting">
<span class="strong"><strong>        pip install -U requests</strong></span>
</pre><p>You only need to use <code class="literal">sudo</code> if you don't have permissions to Python site packages, else <code class="literal">sudo</code> is not required.</p></li><li class="listitem">The following code helps you make a HTTP <code class="literal">GET</code> request with Python's <code class="literal">requests</code> module:<pre class="programlisting">        import requests r =&#13;
        requests.get('http://ip.jsontest.com/')&#13;
        print("Response object:", r)&#13;
        print("Response Text:", r.text)</pre><p>
</p></li><li class="listitem">You will observe the following output:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/B05370_01_01.jpg"/></div><p>
</p></li><li class="listitem">Creating a HTTP <code class="literal">GET</code> request with data payload is also trivial with requests. The following code helps you in achieving this. This is how you can also check the URL request that will be sent:<pre class="programlisting">        payload = {'q': 'chetan'} r =&#13;
        requests.get('https://github.com/search', params=payload)&#13;
        print("Request URL:", r.url)</pre><p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_002.jpg"/></div><p>
</p></li><li class="listitem">Let's now make a HTTP <code class="literal">POST</code> call using the <code class="literal">requests</code> module. This is similar to filling up and posting a login or signup form on a website:<pre class="programlisting">        payload = {'key1': 'value1'} r = &#13;
        requests.post("http://httpbin.org/post", data=payload)&#13;
        print("Response text:", r.json())</pre><p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_003.jpg"/></div><p>
</p></li><li class="listitem">Handling errors and exceptions is also very convenient with requests. The following code snippet shows an example of error handling. If you run this code without an Internet connection on your machine, it will result in an exception. The exception handler catches the exception and states that it failed to establish a new connection, as expected:<pre class="programlisting">        try:&#13;
            r = requests.get("http://www.google.com/")&#13;
        except requests.exceptions.RequestException as e:&#13;
            print("Error Response:", e.message)</pre><p>
</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec13"/>How it works...</h2></div></div></div><p>In the this recipe, we looked at how to make different types of HTTP requests with Python's <code class="literal">requests</code> module. Let's look at how this code works:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">In the first example, we made a <code class="literal">GET</code> request to <a class="ulink" href="http://ip.jsontest.com">http://ip.jsontest.com</a>  and got the response code and response text. It returns the current IP address of our computer on the Internet.</li><li class="listitem" style="list-style-type: disc">In the second example, we made a HTTP <code class="literal">GET </code>request with the payload data. Look how the request URL contains <code class="literal">?q=chetan</code>, and it searches all the repositories by the name, Chetan, on GitHub.</li><li class="listitem" style="list-style-type: disc">Next, we made a <code class="literal">POST</code> request with the payload data being <code class="literal">{'key1', 'value1'}</code>. This is like submitting an online form, as we observed in the <span class="emphasis"><em>How to do it</em></span> section.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">requests</code> module has a <code class="literal">Response</code> object, <code class="literal">r</code>, which includes various methods. These methods help in extracting response, status code and other information required while working with the Web:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">r.status_code</code> - Returns the response code</li><li class="listitem" style="list-style-type: disc"><code class="literal">r.json()</code> - Converts the response to <code class="literal">.json</code> format</li><li class="listitem" style="list-style-type: disc"><code class="literal">r.text</code> - Returns the response data for the query</li><li class="listitem" style="list-style-type: disc"><code class="literal">r.content</code> - Includes the HTML and XML tags in the response content</li><li class="listitem" style="list-style-type: disc"><code class="literal">r.url</code> - Defines the Web URL of the request made</li></ul></div><p>
</p></li><li class="listitem" style="list-style-type: disc">We also looked at the exception handling with the <code class="literal">requests</code> module, wherein, if there was no Internet, an exception occurred and the <code class="literal">requests</code> module could easily catch this exception. This was achieved with the <code class="literal">requests.exceptions</code> class of the <code class="literal">requests</code> module.</li></ul></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec14"/>There's more...</h2></div></div></div><p>Cool, that was neat! Making HTTP requests on the Web is just the beginning. There's still more in terms of what we can do with the Web, such as working with page contents. So, let's see what's next.</p></div></div>
<div class="section" title="A brief look at web scraping"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/>A brief look at web scraping</h1></div></div></div><p>Before we learn how to perform <span class="strong"><strong>web scraping</strong></span>, let's understand what scraping means. In the Web world, scraping is a way to sift through the pages of a website with the intention of extracting the required information in the said format with the help of a computer program. For example, if I want to get the title and date of all the articles published on a blog, I could write a program to scrape through the blog, get the required data, and store it in a database or a flat file, based on the requirement.</p><p>Web scraping is often confused with web crawling. The <span class="strong"><strong>web crawler</strong></span> is a bot that systematically browses the Web with the purpose of web indexing and is used by search engines to index web pages so that users can search the Web more effectively.</p><p>But scraping is not easy. The data, which is interesting to us, is available on a blog or website in a particular format, say XML tags or embedded in HTML tags. So, it is important for us to know the format before we begin extracting the data we need. Also, the web scraper should know the format in which the extracted data needs to be stored in order to act on it later. It is also important to understand that the scraping code will fail should the HTML or XML format change, even though the browser display may be the same.</p><div class="section" title="Legality of web scraping"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec15"/>Legality of web scraping</h2></div></div></div><p>Web scraping has always been under the scanner in legal terms. Can you do web scraping? How legal or ethical is it? Can we use the data obtained from scraping for profit?</p><p>This subject has been under a lot of discussion, but at a high level, you may get into issues with web scraping if you scrape the Web for copyright information, violate the Computer Fraud and Abuse Act, or violate a website's terms of service. For instance, if you're scraping the Web to get public data, you should still be fine. However, it is very contextual and you need to be careful about what you're scraping and how you are using the data.</p><p>Here are a few pointers on the Web on data scraping:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://en.wikipedia.org/wiki/Web_scraping#Legal_issues">https://en.wikipedia.org/wiki/Web_scraping#Legal_issues</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://www.quora.com/What-is-the-legality-of-web-scraping">https://www.quora.com/What-is-the-legality-of-web-scraping</a></li></ul></div></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec16"/>Getting ready</h2></div></div></div><p>We take an example of pricing data from the <a class="ulink" href="https://github.com/">https://github.com/</a> website to demonstrate web scraping with Python. This is a really trivial example but gets us up to speed with scraping. Let's get started and scrape some interesting data with this Python recipe. </p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec17"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open the Google Chrome browser on your computer and open the <a class="ulink" href="https://github.com/pricing/">https://github.com/pricing/</a> web page. On this page, you will notice multiple pricing plans namely, <span class="strong"><strong>Personal</strong></span>, <span class="strong"><strong>Organization</strong></span>, and <span class="strong"><strong>Enterprise</strong></span>.</li><li class="listitem">Now, on your browser, right-click on the pricing of the <span class="strong"><strong>Personal</strong></span> plan and click on the <span class="strong"><strong>Inspect</strong></span> element, as shown in the following screenshot:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_004.jpg"/></div><p>
</p></li><li class="listitem">Once you click on <span class="strong"><strong>Inspect</strong></span>, the Chrome browser's console log opens up, which will help you understand the HTML structure of GitHub's pricing page, as follows:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_005.jpg"/></div><p>
</p></li><li class="listitem">If you look at the highlighted HTML <code class="literal">span</code> - <code class="literal">&lt;span class="default-currency"&gt;$7&lt;/span&gt;</code>, you'll know that this web page uses the <code class="literal">default-currency</code> class to list down the pricing of plans. We'll now use this property to extract the prices of multiple GitHub plans.</li><li class="listitem">But before doing that, let's install the Python module, <code class="literal">lxml</code>, which will be needed to extract content from the preceding HTML document. Install the <code class="literal">lxml</code> and <code class="literal">requests</code> modules:<pre class="programlisting">
<span class="strong"><strong>        pip install lxml&#13;
</strong></span>
<span class="strong"><strong>        pip install requests</strong></span>
</pre></li><li class="listitem">Now, open your favorite editor and type this code snippet:<pre class="programlisting">        from lxml import html &#13;
        import requests &#13;
 &#13;
        page = requests.get('https://github.com/pricing/') &#13;
        tree = html.fromstring(page.content) &#13;
        print("Page Object:", tree) &#13;
        plans = tree.xpath('//h2[@class="pricing-card-name &#13;
        alt-h3"]/text()') &#13;
        pricing = tree.xpath('//span[@class="default-&#13;
        currency"]/text()') &#13;
        print("Plans:", plans, "\nPricing:", pricing) &#13;
</pre><p>
</p></li><li class="listitem">If you look at the preceding code, we used the <code class="literal">default-currency</code> class and <code class="literal">pricing-card-name display-heading-3</code> to get the pricing and pricing plan. If you run the code snippet, the output of the program will be as follows:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_006.jpg"/></div><p>
</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note4"/>Note</h3><p>With web scrapping you will see issues when the HTML tags for the web content has changed. For instance, if a CSS class name gets changed or an anchor is replaced with a button, the scraping code may not fetch the data you need. So, make sure you change your Python code accordingly.</p></div></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec18"/>How it works...</h2></div></div></div><p>As we discussed earlier, we need to find out an appropriate way of extracting information. So, in this example, we first got the HTML tree for the <a class="ulink" href="https://github.com/pricing/">https://github.com/pricing/</a> page. We got the tree with the help of the <code class="literal">fromstring()</code> method that converts the contents of the page (string format) to the HTML format.</p><p>Then, using the <code class="literal">lxml</code> module and the <code class="literal">tree_xpath()</code> method, we looked for the <code class="literal">default-currency</code> class and <code class="literal">pricing-card-name display-heading-3</code> to get the pricing and pricing plans.</p><p>See how we used the complete XPath, <code class="literal">h3[@class='class-name']</code>, to locate the pricing plans and the  <code class="literal">//span[@class="default-currency"]</code> XPath to select the actual pricing data. Once the elements were selected, we printed the text data that was returned to us as a Python list.</p><p>That's it; we scraped the GitHub page for the required data. Nice and simple.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec19"/>There's more...</h2></div></div></div><p>You learnt what web scrapers are, and how they go ahead and extract interesting information from the Web. You also understood how they are different from web crawlers. But then, there's always something more!</p><p>Web scraping involves extraction, which cannot happen until we parse the HTML content from the web page to get the data interesting to us. In the next recipe, we'll learn about parsing HTML and XML content in detail.</p></div></div>
<div class="section" title="Parsing and extracting web content"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec11"/>Parsing and extracting web content</h1></div></div></div><p>Well, now we're confident about making HTTP requests to multiple URLs. We also looked at a simple example of web scraping.</p><p>But WWW is made up of pages with multiple data formats. If we want to scrape the Web and make sense of the data, we should also know how to parse different formats in which data is available on the Web.</p><p>In this recipe, we'll discuss how to s.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec20"/>Getting ready</h2></div></div></div><p>Data on the Web is mostly in the HTML or XML format. To understand how to parse web content, we'll take an example of an HTML file. We'll learn how to select certain HTML elements and extract the desired data. For this recipe, you need to install the <code class="literal">BeautifulSoup</code> module of Python. The <code class="literal">BeautifulSoup</code> module is one of the most comprehensive Python modules that will do a good job of parsing HTML content. So, let's get started.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec21"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We start by installing <code class="literal">BeautifulSoup</code> on our Python instance. The following command will help us install the module. We install the latest version, which is <code class="literal">beautifulsoup4</code>:<pre class="programlisting">
<span class="strong"><strong>        pip install beautifulsoup4</strong></span>
</pre><p>
</p></li><li class="listitem">Now, let's take a look at the following HTML file, which will help us learn how to parse the HTML content:<pre class="programlisting">        &lt;html &gt;&#13;
        &lt;head&gt;&#13;
            &lt;title&gt;Enjoy Facebook!&lt;/title&gt; &#13;
        &lt;/head&gt;&#13;
        &lt;body&gt;&#13;
            &lt;p&gt;&#13;
              &lt;span&gt;You know it's easy to get intouch with&#13;
              your &lt;strong&gt;Friends&lt;/strong&gt; on web!&lt;br&gt;&lt;/span&gt;&#13;
              Click here &lt;a href="https://facebook.com"&gt;here&lt;/a&gt;&#13;
              to sign up and enjoy&lt;br&gt;&#13;
            &lt;/p&gt;&#13;
            &lt;p class="wow"&gt; Your gateway to social web! &lt;/p&gt;&#13;
            &lt;div id="inventor"&gt;Mark Zuckerberg&lt;/div&gt;&#13;
            Facebook, a webapp used by millions&#13;
        &lt;/body&gt;&#13;
        &lt;/html&gt;</pre><p>
</p></li><li class="listitem">Let's name this file as <code class="literal">python.html</code>. Our HTML file is hand-crafted so that we can learn the multiple ways of parsing it to get the required data from it. <code class="literal">Python.html</code> has typical HTML tags given as follows:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">&lt;head&gt;</code> - It is the container of all head elements like <code class="literal">&lt;title&gt;</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">&lt;body&gt;</code> - It defines the body of the HTML document.</li><li class="listitem" style="list-style-type: disc"><code class="literal">&lt;p&gt;</code> - This element defines a paragraph in HTML.</li><li class="listitem" style="list-style-type: disc"><code class="literal">&lt;span&gt;</code> - It is used to group inline elements in a document.</li><li class="listitem" style="list-style-type: disc"><code class="literal">&lt;strong&gt;</code> - It is used to apply a bold style to the text present under this tag.</li><li class="listitem" style="list-style-type: disc"><code class="literal">&lt;a&gt;</code> - It represents a hyperlink or anchor and contains <code class="literal">&lt;href&gt;</code> that points to the hyperlink.</li><li class="listitem" style="list-style-type: disc"><code class="literal">&lt;class&gt;</code> - It is an attribute that points to a class in a style sheet.</li><li class="listitem" style="list-style-type: disc"><code class="literal">&lt;div id&gt;</code> - It is a container that encapsulates other page elements and divides the content into sections. Every section can be identified by attribute <code class="literal">id</code>.</li></ul></div><p>
</p></li><li class="listitem">If we open this HTML in a browser, this is how it'll look:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_007.jpg"/></div><p>
</p></li><li class="listitem">Let's now write some Python code to parse this HTML file. We start by creating a <code class="literal">BeautifulSoup</code> object.<div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip5"/>Tip</h3><p>We always need to define the parser. In this case we used <code class="literal">lxml</code> as the parser. The parser helps us read files in a designated format so that querying data becomes easy.</p></div></div><pre class="programlisting">        import bs4&#13;
        myfile = open('python.html')&#13;
        soup = bs4.BeautifulSoup(myfile, "lxml")&#13;
        #Making the soup&#13;
        print "BeautifulSoup Object:", type(soup)</pre><p>The output of the preceding code is seen in the following screenshot:</p><p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_008.jpg"/></div><p>
</p></li><li class="listitem">OK, that's neat, but how do we retrieve data? Before we try to retrieve data, we need to select the HTML elements that contain the data we need.</li><li class="listitem">We can select or find HTML elements in different ways. We could select elements with ID, CSS, or tags. The following code uses <code class="literal">python.html</code> to demonstrate this concept:<pre class="programlisting">        #Find Elements By tags&#13;
        print soup.find_all('a')&#13;
        print soup.find_all('strong')&#13;
        #Find Elements By id&#13;
        print soup.find('div', {"id":"inventor"})&#13;
        print soup.select('#inventor')&#13;
        #Find Elements by css print&#13;
        soup.select('.wow')</pre><p>The output of the preceding code can be viewed in the following screenshot:</p><p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_009.jpg"/></div><p>
</p></li><li class="listitem">Now let's move on and get the actual content from the HTML file. The following are a few ways in which we can extract the data of interest:</li></ol></div><pre class="programlisting">        print "Facebook URL:", soup.find_all('a')[0]['href']&#13;
        print "Inventor:", soup.find('div', {"id":"inventor"}).text &#13;
        print "Span content:", soup.select('span')[0].getText()</pre><p>The output of the preceding code snippet is as follows:</p><p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_010.jpg"/></div><p>
</p><p>Whoopie! See how we got all the text we wanted from the HTML elements.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec22"/>How it works...</h2></div></div></div><p>In this recipe, you learnt the skill of finding or selecting different HTML elements based on ID, CSS, or tags.</p><p>In the second code example of this recipe, we used <code class="literal">find_all(<span class="strong"><strong>'</strong></span>a<span class="strong"><strong>'</strong></span>)</code> to get all the anchor elements from the HTML file. When we used the <code class="literal">find_all()</code> method, we got multiple instances of the match as an array. The <code class="literal">select()</code> method helps you reach the element directly.</p><p>We also used <code class="literal">find(<span class="strong"><strong>'</strong></span>div<span class="strong"><strong>'</strong></span>, &lt;divId&gt;)</code> or <code class="literal">select(&lt;divId&gt;)</code> to select HTML elements by <code class="literal">div Id</code>. Note how we selected the <code class="literal">inventor</code> element with <code class="literal">div</code> ID <code class="literal">#inventor</code> in two ways using the <code class="literal">find()</code> and <code class="literal">select()</code> methods. Actually, the select method can also be used as <code class="literal">select(&lt;class<span class="strong"><strong>-</strong></span>name&gt;)</code> to select HTML elements with a CSS class name. We used this method to select element <code class="literal">wow</code> in our example.</p><p>In the third code example, we searched for all the anchor elements in the HTML page and looked at the first index with <code class="literal">soup.find_all(<span class="strong"><strong>'</strong></span>a<span class="strong"><strong>'</strong></span>)[0]</code>. Note that since we have only one anchor tag, we used the index 0 to select that element, but if we had multiple anchor tags, it could be accessed with index 1. Methods like <code class="literal">getText()</code> and attributes like <code class="literal">text</code> (as seen in the preceding examples) help in extracting the actual content from the elements.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec23"/>There's more...</h2></div></div></div><p>Cool, so we understood how to parse a web page (or an HTML page) with Python. You also learnt how to select or find HTML elements by ID, CSS, or tags. We also looked at examples of how to extract the required content from HTML. What if we want to download the contents of a page or file from the Web? Let's see if we can achieve that in our next recipe.</p></div></div>
<div class="section" title="Downloading content from the Web"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec12"/>Downloading content from the Web</h1></div></div></div><p>So, in the earlier recipe, we saw how to make HTTP requests, and you also learnt how to parse a web response. It's time to move ahead and download content from the Web. You know that the WWW is not just about HTML pages. It contains other resources, such as text files, documents, and images, among many other formats. Here, in this recipe, you'll learn ways to download images in Python with an example.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec24"/>Getting ready</h2></div></div></div><p>To download images, we will need two Python modules, namely <code class="literal">BeautifulSoup</code> and <code class="literal">urllib2</code>. We could use the <code class="literal">requests</code> module instead of <code class="literal">urrlib2</code>, but this will help you learn about <code class="literal">urllib2</code> as an alternative that can be used for HTTP requests, so you can boast about it.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec25"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Before starting this recipe, we need to answer two questions. What kind of images would we like to download? From which location on the Web do I download the images? In this recipe, we download <span class="emphasis"><em>Avatar </em></span>movie images from Google (<a class="ulink" href="https://google.com">https://google.com</a>) images search. We download the top five images that match the search criteria. For doing this, let's import the Python modules and define the variables we'll need:<pre class="programlisting">        from bs4 import BeautifulSoup&#13;
        import re&#13;
        import urllib2&#13;
        import os &#13;
        ## Download paramters&#13;
        image_type = "Project"&#13;
        movie = "Avatar"&#13;
        url = "https://www.google.com/search?q="+movie+"&amp;source=lnms&amp;tbm=isch"</pre><p>
</p></li><li class="listitem">OK then, let's now create a <code class="literal">BeautifulSoup</code> object with URL parameters and appropriate headers. See the use of <code class="literal">User-Agent</code> while making HTTP calls with Python's <code class="literal">urllib</code> module. The <code class="literal">requests</code> module uses its own <code class="literal">User-Agent</code> while making <code class="literal">HTTP</code> calls:<pre class="programlisting">        header = {'User-Agent': 'Mozilla/5.0'}&#13;
        soup = BeautifulSoup(urllib2.urlopen&#13;
        (urllib2.Request(url,headers=header)))</pre><p>
</p></li><li class="listitem">Google images are hosted as static content under the domain name <code class="literal">http://www.gstatic.com/</code>. So, using the <code class="literal">BeautifulSoup</code> object, we now try to find all the images whose source URL contains <code class="literal">http://www.gstatic.com/</code>. The following code does exactly the same thing:<pre class="programlisting">        images = [a['src'] for a in soup.find_all("img", {"src":&#13;
        re.compile("gstatic.com")})][:5]&#13;
        for img in images:&#13;
        print "Image Source:", img</pre><p>The output of the preceding code snippet can be seen in the following screenshot. Note how we get the image source URL on the Web for the top five images:</p><p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_011.jpg"/></div><p>
</p></li><li class="listitem">Now that we have the source URL of all the images, let's download them. The following Python code uses the <code class="literal">urlopen()</code> method to <code class="literal">read()</code> the image and downloads it onto the local file system:<pre class="programlisting">        for img in images:&#13;
          raw_img = urllib2.urlopen(img).read()&#13;
          cntr = len([i for i in os.listdir(".") if image_type in i]) + 1&#13;
        f = open(image_type + "_"+ str(cntr)+".jpg", 'wb') &#13;
        f.write(raw_img)&#13;
        f.close()</pre><p>
</p></li><li class="listitem">When the images get downloaded, we can see them on our editor. The following snapshot shows the top five images we downloaded and <code class="literal">Project_3.jpg</code> looks as follows:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/B05370_01_35.jpg"/></div><p>
</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec26"/>How it works...</h2></div></div></div><p>So, in this recipe, we looked at downloading content from the Web. First, we defined the parameters for download. Parameters are like configurations that define the location where the downloadable resource is available and what kind of content is to be downloaded. In our example, we defined that we have to download <span class="emphasis"><em>Avatar</em></span> movie images and, that too, from <span class="strong"><strong>Google</strong></span>.</p><p>Then we created the <code class="literal">BeautifulSoup</code> object, which will make the URL request using the <code class="literal">urllib2</code> module. Actually, <code class="literal">urllib2.Request()</code> prepares the request with the configuration, such as headers and the URL itself, and <code class="literal">urllib2.urlopen()</code> actually makes the request. We wrapped the HTML response of the <code class="literal">urlopen()</code> method and created a <code class="literal">BeautifulSoup</code> object so that we could parse the HTML response.</p><p>Next, we used the soup object to search for the top five images present in the HTML response. We searched for images based on the <code class="literal">img</code> tag with the <code class="literal">find_all()</code> method. As we know, <code class="literal">find_all()</code> returns a list of image URLs where the picture is available on <span class="strong"><strong>Google</strong></span>.</p><p>Finally, we iterated through all the URLs and again used the <code class="literal">urlopen()</code> method on URLs to <code class="literal">read()</code> the images. <code class="literal">Read()</code> returns the image in a raw format as binary data. We then used this raw image to write to a file on our local file system. We also added a logic to name the image (they actually auto-increment) so that they're uniquely identified in the local file system.</p><p>That's nice! Exactly what we wanted to achieve! Now let's up the ante a bit and see what else we can explore in the next recipe.</p></div></div>
<div class="section" title="Working with third-party REST APIs"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Working with third-party REST APIs</h1></div></div></div><p>Now that we've covered ground on scraping, crawling, and parsing, it's time for another interesting work that we can do with Python, which is working with third-party APIs. I'd assume many of us are aware and might have a basic understanding of <span class="strong"><strong>REST API</strong></span>. So, let's get started!</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec27"/>Getting ready</h2></div></div></div><p>To demonstrate the understanding, we take the case of GitHub gists. Gists in GitHub are the best way to share your work, a small code snippet that helps your colleague or a small app with multiple files that gives an understanding of a concept. GitHub allows the creation, listing, deleting, and updating of gists, and it presents a classical case of working with GitHub REST APIs.</p><p>So, in this section, we use our very own <code class="literal">requests</code> module to make HTTP requests to GitHub REST API to create, update, list, or delete gists.</p><p>The following steps will show you how to work with GitHub REST APIs using Python.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec28"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">To work with GitHub REST APIs, we need to create a <span class="strong"><strong>Personal access token</strong></span>. For doing that, log in to <a class="ulink" href="https://github.com/">https://github.com/</a> and browse to <a class="ulink" href="https://github.com/settings/tokens">https://github.com/settings/tokens</a>  and click on<span class="strong"><strong>  Generate new  token</strong></span>:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_024.jpg"/></div><p>
</p></li><li class="listitem">You'll now be taken to the <span class="strong"><strong>New personal access token</strong></span> page. Enter a description at the top of the page and check the <span class="strong"><strong>gists</strong></span> option among the scopes given out. Note that scope represents the access for your token. For instance, if you just select <span class="strong"><strong>gists</strong></span>, you can use GitHub APIs to work on the <span class="strong"><strong>gists </strong></span>resource but not on other resources such as <span class="strong"><strong>repo</strong></span> or users. For this recipe, the <span class="strong"><strong>gists </strong></span>scope is just what we need:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_025.jpg"/></div><p>
</p></li><li class="listitem">Once you click on <span class="strong"><strong>Generate token</strong></span>, you'd be presented with a screen containing your personal access token. Keep this token confidential with you.</li><li class="listitem">With the access token available, let's start working with APIs and create a new gist. With create, we add a new resource, and for doing this, we make an HTTP <code class="literal">POST</code> request on GitHub APIs, such as in the following code:<pre class="programlisting">        import requests&#13;
        import json&#13;
        BASE_URL = 'https://api.github.com'&#13;
        Link_URL = 'https://gist.github.com'&#13;
        username = '&lt;username&gt;' ## Fill in your github username&#13;
        api_token = '&lt;api_token&gt;'  ## Fill in your token&#13;
        header = {  'X-Github-Username': '%s' % username,&#13;
                    'Content-Type': 'application/json',&#13;
                    'Authorization': 'token %s' % api_token,&#13;
        }&#13;
        url = "/gists" &#13;
        data ={&#13;
          "description": "the description for this gist",&#13;
          "public": True,&#13;
          "files": { &#13;
            "file1.txt": { &#13;
              "content": "String file contents" &#13;
            } &#13;
          }&#13;
        }&#13;
        r = requests.post('%s%s' % (BASE_URL, url), &#13;
            headers=header, &#13;
           data=json.dumps(data))&#13;
       print r.json()['url']</pre><p>
</p></li><li class="listitem">If I now go to my <code class="literal">gists</code> page on GitHub, I should see the newly created gist. And voila, it's available!<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_026.jpg"/></div><p>
</p></li><li class="listitem">Hey, we were successful in creating the gist with the GitHub APIs. That's cool, but can we now view this <code class="literal">gist</code>? In the preceding example, we also printed the URL of the newly created gist. It will be in the format,<code class="literal">https://gist.github.com/&lt;username&gt;/&lt;gist_id&gt;</code>. We now use this <span class="strong"><strong>gist_id</strong></span> to get the details of the gist, which means we make a HTTP <code class="literal">GET</code> request on the <span class="strong"><strong>gist_id</strong></span>:<pre class="programlisting">        import requests&#13;
        import json&#13;
        BASE_URL = 'https://api.github.com'&#13;
        Link_URL =&#13;
        'https://gist.github.com'&#13;
&#13;
        username = '&lt;username&gt;'&#13;
        api_token = '&lt;api_token&gt;'&#13;
        gist_id = '&lt;gist id&gt;' &#13;
&#13;
        header = { 'X-Github-Username': '%s' % username,&#13;
                   'Content-Type': 'application/json',&#13;
                   'Authorization': 'token %s' % api_token,&#13;
        }&#13;
        url = "/gists/%s" % gist_id&#13;
        r = requests.get('%s%s' % (BASE_URL, url),&#13;
                          headers=header)&#13;
        print r.json()</pre><p>
</p></li><li class="listitem">We created a new gist with the HTTP <code class="literal">POST</code> request and got the details of the gist with the HTTP <code class="literal">GET</code> request in the previous steps. Now, let's update this gist with the HTTP <code class="literal">PATCH</code> request.<div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note6"/>Note</h3><p>Many third-party libraries choose to use the <code class="literal">PUT</code> request to update a resource, but HTTP <code class="literal">PATCH</code> can also be used for this operation, as chosen by GitHub.</p></div></div><p>
</p></li><li class="listitem">The following code demonstrates updating the gist:<pre class="programlisting">        import requests&#13;
        import json&#13;
&#13;
        BASE_URL = 'https://api.github.com'&#13;
        Link_URL = 'https://gist.github.com'&#13;
&#13;
        username = '&lt;username&gt;'&#13;
        api_token = '&lt;api_token&gt;'&#13;
        gist_id = '&lt;gist_id&gt;'&#13;
&#13;
        header = { 'X-Github-Username': '%s' % username,&#13;
                   'Content-Type': 'application/json',&#13;
                   'Authorization': 'token %s' % api_token,&#13;
        }&#13;
        data = {   "description": "Updating the description&#13;
                   for this gist",&#13;
                   "files": {&#13;
                     "file1.txt": {&#13;
                       "content": "Updating file contents.."&#13;
                     }&#13;
                   } &#13;
        }&#13;
        url = "/gists/%s" % gist_id&#13;
        r = requests.patch('%s%s' %(BASE_URL, url), &#13;
                           headers=header,&#13;
                           data=json.dumps(data))&#13;
        print r.json()</pre><p>
</p></li><li class="listitem">Now, if I look at my GitHub login and browse to this gist, the contents of the gist have been updated. Awesome! Don't forget to see the <span class="strong"><strong>Revisions</strong></span> in the screenshot--see it got updated to revision <span class="strong"><strong>2</strong></span>:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_027.jpg"/></div><p>
</p></li><li class="listitem">Now comes the most destructive API operation--yes deleting the gist. GitHub provides an API for removing the gist by making use of the HTTP <strong class="userinput"><code>
<code class="literal">DELETE</code>
</code></strong>operation on its <code class="literal">/gists/&lt;gist_id&gt;</code> resource. The following code helps us delete the <code class="literal">gist</code>:<pre class="programlisting">        import requests&#13;
        import json&#13;
        BASE_URL = 'https://api.github.com'&#13;
        Link_URL = 'https://gist.github.com'&#13;
        username = '&lt;username&gt;'&#13;
        api_token = '&lt;api_token&gt;'&#13;
        gist_id = '&lt;gist_id&gt;'&#13;
&#13;
        header = {  'X-Github-Username': '%s' % username,&#13;
                    'Content-Type': 'application/json', &#13;
                    'Authorization': 'token %s' % api_token,&#13;
        }&#13;
        url = "/gists/%s" % gist_id &#13;
        r = requests.delete('%s%s' %(BASE_URL, url),&#13;
                            headers=header, )</pre><p>
</p></li><li class="listitem">Let's quickly find out if the gist is now available on the GitHub website? We can do that by browsing the gist URL on any web browser. And what does the browser say? It says <span class="strong"><strong>404</strong></span> resource not found, so we have successfully deleted the gist! Refer to the following screenshot:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_028.jpg"/></div><p>
</p></li><li class="listitem">Finally, let's list all the gists in your account. For this we make an HTTP <code class="literal">GET</code> API call on the <code class="literal">/users/&lt;username&gt;/gists</code> resource:</li></ol></div><pre class="programlisting">        import requests&#13;
&#13;
        BASE_URL = 'https://api.github.com'&#13;
        Link_URL = 'https://gist.github.com'&#13;
&#13;
        username = '&lt;username&gt;'      ## Fill in your github username &#13;
        api_token = '&lt;api_token&gt;'  ## Fill in your token&#13;
&#13;
        header = {  'X-Github-Username': '%s' % username, &#13;
                    'Content-Type': 'application/json',&#13;
                    'Authorization': 'token %s' % api_token,&#13;
        }&#13;
        url = "/users/%s/gists" % username&#13;
        r = requests.get('%s%s' % (BASE_URL, url),&#13;
                          headers=header)&#13;
        gists = r.json()&#13;
        for gist in gists:&#13;
            data = gist['files'].values()[0]&#13;
            print data['filename'],&#13;
            data['raw_url'], data['language']</pre><p>The output of the preceding code for my account is as follows:</p><p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_029.jpg"/></div><p>
</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec29"/>How it works...</h2></div></div></div><p>Python's <code class="literal">requests</code> module helps in making HTTP <code class="literal">GET</code>/<code class="literal">POST</code>/<code class="literal">PUT</code>/<code class="literal">PATCH</code> and <code class="literal">DELETE</code> API calls on GitHub's resources. These operations, also known as HTTP verbs in the REST terminology, are responsible for taking certain actions on the URL resources.</p><p>As we saw in the examples, the HTTP <code class="literal">GET</code> request helps in listing the gists, <code class="literal">POST</code> creates a new gist, <code class="literal">PATCH</code> updates a gist, and <code class="literal">DELETE</code> completely removes the gist. Thus, in this recipe, you learnt how to work with third-party REST APIs--an essential part of WWW today--using Python.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec30"/>See also</h2></div></div></div><p>There are many third-party applications that are written as REST APIs. You may want to try them out the same way we did for GitHub. For example, both Twitter and Facebook have great APIs and the documents are also easy to understand and use. Of course, they do have Python bindings.</p></div></div>
<div class="section" title="Asynchronous HTTP server in&#xA0;Python"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec14"/>Asynchronous HTTP server in Python</h1></div></div></div><p>If you realize, many web applications that we interact with, are by default synchronous. A client connection gets established for every request made by the client and a callable method gets invoked on the server side. The server performs the business operation and writes the response body to the client socket. Once the response is exhausted, the client connection gets closed. All these operations happen in sequence one after the other--hence, synchronous.</p><p>But the Web today, as we see it, cannot rely on synchronous modes of operations only. Consider the case of a website that queries data from the Web and retrieves the information for you. (For instance, your website allows for integration with <span class="strong"><strong>Facebook</strong></span> and every time a user visits a certain page of your website, you pull data from his <span class="strong"><strong>Facebook</strong></span> account.) Now, if we develop this web application in a synchronous manner, for every request made by the client, the server would make an I/O call to either the database or over the network to retrieve information and then present it back to the client. If these I/O requests take a longer time to respond, the server gets blocked waiting for the response. Typically web servers maintain a thread pool that handles multiple requests from the client. If a server waits long enough to serve requests, the thread pool may get exhausted soon and the server will get stalled.</p><p>Solution? In comes the asynchronous ways of doing things!</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec31"/>Getting ready</h2></div></div></div><p>For this recipe, we will use Tornado, an asynchronous framework developed in Python. It has support for both Python 2 and Python 3 and was originally developed at FriendFeed (<a class="ulink" href="http://blog.friendfeed.com/">http://blog.friendfeed.com/</a>). Tornado uses a non-blocking network I/O and solves the problem of scaling to tens of thousands of live connections (<code class="literal">C10K problem</code>). I like this framework and enjoy developing code with it. I hope you'd too! Before we get into the <span class="emphasis"><em>How to do it</em></span> section, let's first install tornado by executing the following command:</p><pre class="programlisting">
<span class="strong"><strong>    pip install -U tornado</strong></span>
</pre></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec32"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We're now ready to develop our own HTTP server that works on an asynchronous philosophy. The following code represents an asynchronous server developed in the <code class="literal">tornado</code> web framework:<pre class="programlisting">        import tornado.ioloop&#13;
        import tornado.web&#13;
        import httplib2&#13;
&#13;
        class AsyncHandler(tornado.web.RequestHandler):&#13;
            @tornado.web.asynchronous&#13;
            def get(self):&#13;
              http = httplib2.Http()&#13;
              self.response, self.content = &#13;
                http.request("http://ip.jsontest.com/", "GET")&#13;
              self._async_callback(self.response, self.content)&#13;
&#13;
            def _async_callback(self, response, content): &#13;
            print "Content:", content&#13;
            print "Response:\nStatusCode: %s Location: %s"&#13;
              %(response['status'], response['content-location']) &#13;
            self.finish()&#13;
            tornado.ioloop.IOLoop.instance().stop()&#13;
        application = tornado.web.Application([&#13;
              (r"/", AsyncHandler)], debug=True)&#13;
       if __name__ == "__main__":&#13;
         application.listen(8888)&#13;
         tornado.ioloop.IOLoop.instance().start()</pre><p>
</p></li><li class="listitem">Run the server as:<pre class="programlisting">
<span class="strong"><strong>        python tornado_async.py</strong></span>
</pre><p>
</p></li><li class="listitem">The server is now running on port 8888 and ready to receive requests.</li><li class="listitem">Now, launch any browser of your choice and browse to <code class="literal">http://localhost:8888/</code>. On the server, you'll see the following output:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_034.jpg"/></div><p>
</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec33"/>How it works...</h2></div></div></div><p>Our asynchronous web server is now up and running and accepting requests on port 8888. But what is asynchronous about this? In fact, tornado works on the philosophy of a single-threaded event loop. This event loop keeps polling for events and passes it on to the corresponding event handlers.</p><p>In the preceding example, when the app is run, it starts by running the <code class="literal">ioloop</code>. The <code class="literal">ioloop</code> is a single-threaded event loop and is responsible for receiving requests from the clients. We have defined the <code class="literal">get()</code> method, which is decorated with <code class="literal">@tornado.web.asynchronous</code>, which makes it asynchronous. When a user makes a HTTP GET request on <code class="literal">http://localhost:8888/</code>, the <code class="literal">get()</code> method is triggered that internally makes an I/O call to <a class="ulink" href="http://ip.jsontest.com">http://ip.jsontest.com</a>.</p><p>Now, a typical synchronous web server would wait for the response of this I/O call and block the request thread. But tornado being an asynchronous framework, it triggers a task, adds it to a queue, makes the I/O call, and returns the thread of execution back to the event loop.</p><p>The event loop now keeps monitoring the task queue and polls for a response from the I/O call. When the event is available, it executes the event handler, <code class="literal">async<span class="emphasis"><em>_</em></span>callback()</code>, to print the content and its response and then stops the event loop.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec34"/>There's more...</h2></div></div></div><p>Event-driven web servers such as tornado make use of kernel-level libraries to monitor for events. These libraries are <code class="literal">kqueue</code>, <code class="literal">epoll</code>, and so on. If you're really interested, you should do more reading on this. Here are a few resources:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://linux.die.net/man/4/epoll">https://linux.die.net/man/4/epoll</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://www.freebsd.org/cgi/man.cgi?query=kqueue&amp;sektion=2">https://www.freebsd.org/cgi/man.cgi?query=kqueue&amp;sektion=2</a></li></ul></div></div></div>
<div class="section" title="Web automation with selenium bindings"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec15"/>Web automation with selenium bindings</h1></div></div></div><p>In all the recipes so far, we had a dedicated URL to make HTTP requests, be it calling a REST API or downloading content from the Web. But then, there are services that don't have a defined API resource or need to log in to the Web to perform operations. In such cases, you don't have much control over the requests, as it is the same URL that serves multiple different content, based on the user session or cookie. Then what do we do?</p><p>Well, how about controlling the browser itself to achieve tasks in such scenarios? Controlling the browser itself? Interesting, isn't it?</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec35"/>Getting ready</h2></div></div></div><p>For this recipe, we'll use Python's selenium module. Selenium (<a class="ulink" href="http://www.seleniumhq.org/">http://www.seleniumhq.org/</a>) is a portable software framework for web applications and automates browser actions. You could automate mundane tasks with selenium. Selenium spawns a browser and helps you perform tasks as though a human is doing them. Selenium supports some of the most popularly used browsers like Firefox, Chrome, Safari, and Internet Explorer, among others. Let's take an example of logging in to Facebook with Python's selenium in this recipe.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec36"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We start by installing selenium bindings for Python. Installing selenium can be done with the following command:<pre class="programlisting">
<span class="strong"><strong>           pip install selenium</strong></span>
</pre><p>
</p></li><li class="listitem">Let's start by first creating a browser object. We use the Firefox browser for spawning the browser instance:<pre class="programlisting">        from selenium import webdriver browser =&#13;
        webdriver.Firefox()&#13;
        print "WebDriver Object", browser</pre><p>
</p></li><li class="listitem">The following screenshot shows how a selenium web driver object got created. It also has a unique session ID:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_035.jpg"/></div><p>
</p></li><li class="listitem">Next, we ask the browser to browse to the Facebook home page. The following code helps us achieve this:<pre class="programlisting">        browser.maximize_window()&#13;
        browser.get('https://facebook.com')</pre><p>
</p></li><li class="listitem">Once you run the preceding code, you will see a Firefox browser opened, and it connects to the Facebook login page, as in the following screenshot:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_036.jpg"/></div><p>
</p></li><li class="listitem">For the next step, we locate the e-mail and password elements and enter the appropriate data:<pre class="programlisting">        email = browser.find_element_by_name('email')&#13;
        password = browser.find_element_by_name('pass')&#13;
        print "Html elements:"&#13;
        print "Email:", email, "\nPassword:", password</pre><p>The output of the preceding code is as follows:</p><p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_037.jpg"/></div><p>
</p></li><li class="listitem">Once we have selected the <span class="strong"><strong>Email </strong></span>and <span class="strong"><strong>Password</strong></span> text inputs, we now fill them with the correct <span class="strong"><strong>Email </strong></span>and <span class="strong"><strong>Password</strong></span>. The following code will enable entering <span class="strong"><strong>Email </strong></span>and <span class="strong"><strong>Password</strong></span>:<pre class="programlisting">        email.send_keys('abc@gmail.com') <span class="emphasis"><em>#Enter correct email&#13;
        address</em></span>password.send_keys('pass123') <span class="emphasis"><em>#Enter correct password</em></span>
</pre><p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_038.jpg"/></div><p>
</p></li><li class="listitem">Now that we have entered <span class="strong"><strong>Email </strong></span>and <span class="strong"><strong>Password</strong></span>, the last thing to do is submit the form and click on the <span class="strong"><strong>Log In</strong></span> button. We do this by finding the element by ID and clicking on the element:<pre class="programlisting">        browser.find_element_by_id('loginbutton').click()</pre><p>If you have entered the correct e-mail ID and password, you'd have logged in to Facebook!</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec37"/>How it works...</h2></div></div></div><p>For this recipe, we used the selenium WebDriver Python APIs. <span class="strong"><strong>WebDrive</strong></span>r is the latest inclusion in selenium APIs and drives browsers natively like a user. It can drive locally or on a remote machine using the selenium server. In this example, we ran it on the local machine. Basically, the selenium server runs on a local machine on a default port 4444 and selenium WebDriver APIs interact with the selenium server to take actions on the browser.</p><p>In this recipe, we first created a WebDriver instance using the Firefox browser. We then used the WebDriver API to browse to the Facebook homepage. We then parsed the HTML page and located the <span class="strong"><strong>Email </strong></span>and <span class="strong"><strong>Password </strong></span>input elements. How did we find the elements? Yes, similar to what we did in the web scraping example. As we have the developer console in Chrome, we can install the firebug plugin in Firefox. Using this plugin, we can get the HTML elements for <span class="strong"><strong>Email </strong></span>and <span class="strong"><strong>Password</strong></span>. See the following screenshot:</p><p>
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_01_039.jpg"/></div><p>
</p><p>Once we figured the HTML element names, we programmatically created an HTML element object using WebDriver's  <code class="literal">find_element_by_name()</code> method. WebDriver API has a method <code class="literal">send_keys()</code> that can work on element objects and enter the required text (in this case <code class="literal">email</code> and <code class="literal">password</code>). The last operation is to submit the form, and we performed it by finding the <span class="strong"><strong>Log In</strong></span> object and clicking on it.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec38"/>There's more...</h2></div></div></div><p>We looked at a very basic example with the selenium WebDriver Python bindings. Now it's up to your imagination what you can achieve with selenium, automating mundane tasks.</p></div></div>
<div class="section" title="Automating lead generation with web scraping"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec16"/>Automating lead generation with web scraping</h1></div></div></div><p>
<span class="emphasis"><em>Ryan</em></span> is a marketing manager at <span class="emphasis"><em>Dely In</em></span>c. Dely is a food delivery start-up and is trying to establish itself in the city of London. Dely is good at logistics and wants to aggregate restaurants on their platform, so when consumers order food from these restaurants, Dely will be responsible for the actual delivery. Dely is hoping that with every delivery they do, they will get a percentage cut from the restaurants. In return, restaurants have to think about their kitchen and not the logistical aspects. If you carefully think, virtually, every restaurant, big or small, is their probable lead. Dely wants to reach out to these restaurants and hopes to add them to their platform and fulfill their delivery needs.</p><p>Ryan is responsible for getting in touch with restaurants and wants to run a marketing campaign on all the target restaurants. But before he can do this, he needs to create a database of all the restaurants in London. He needs details, such as the name of the restaurant, the street address, and the contact number so that he can reach these restaurants. Ryan knows all his leads are listed on Yelp, but doesn't know where to start. Also, if he starts looking at all restaurants manually, it will take him a huge amount of time. With the knowledge you gained in this chapter, can you help Ryan with lead generation?</p><div class="section" title="Legality of web scraping"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec39"/>Legality of web scraping</h2></div></div></div><p>We covered the legal aspects of web scraping in the initial parts of the chapter. I would like to warn you again on this. The example covered in this chapter, again, is for you to understand how to perform web scraping. Also, here we're scraping Yelp for public data, which is commonly available, as in this case, it is available on the restaurant's website itself.</p></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec40"/>Getting ready</h2></div></div></div><p>Now, if you look at Ryan's problem, he needs an automated way of collecting the database of all the restaurants listed in London. Yes, you got it right. Web scraping can help Ryan build this database. Can it be that easy? Let's see in this recipe.</p><p>For this recipe, we don't need any extra modules. We'll use the <code class="literal">BeautifulSoup </code>and <code class="literal">urllib</code> Python modules that we used in the previous recipes of this chapter.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec41"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We start by going to the Yelp website (<a class="ulink" href="https://yelp.com/">https://yelp.com/</a>) and searching for all the restaurants in the city of London. When you do that, you'll get a list of all the restaurants in London. Observe the URL that displays the search criteria. It is <a class="ulink" href="https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London">https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London</a>. See the following screenshot for reference:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_040.jpg"/></div><p>
</p></li><li class="listitem">Now, if you click on any of the restaurants' link that shows up in the search results, we should get the details that Ryan needs. See the following screenshot, where we get the details of <span class="emphasis"><em>Ffiona's Restaurant</em></span>. Note how every restaurant has a dedicated URL; in this case, it is <a class="ulink" href="https://www.yelp.com/biz/ffionas-restaurant-london?osq=Restaurants">https://www.yelp.com/biz/ffionas-restaurant-london?osq=Restaurants</a>. Also note that on this page, we have the name of the restaurant, the street address, and even the contact number. All the details that Ryan needs for his campaign; that's cool!<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_041.jpg"/></div><p>
</p></li><li class="listitem">OK nice, so we now know how to get the list of restaurants and also fetch the relevant details for a restaurant. But how do we achieve this in an automated way? As we saw in the web scraping example, we need to look for the HTML elements on the web pages from where we can collect this data.</li><li class="listitem">Let's start with the search page. Open the search page (<a class="ulink" href="https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London">https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London</a>) on your Chrome browser. Now, right-click on the first restaurant's URL and click on <span class="strong"><strong>Inspect</strong></span> to get the HTML elements. If you notice, in the following screenshot, all the restaurants that are listed on the search page have a common CSS class name, <code class="literal">biz-name</code>, which indicates the name of the restaurant. It also contains the <code class="literal">href</code> tag, which points to the dedicated URL of the restaurant. In our screenshot, we get the name, <span class="strong"><strong>Ffiona's Restaurant</strong></span>, and the <code class="literal">href</code> points to the restaurant's URL, <a class="ulink" href="https://yelp.com/biz/ffionas-restaurant-london?osq=Resturants">https://yelp.com/biz/ffionas-restaurant-london?osq=Resturants</a>.<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_042.jpg"/></div><p>
</p></li><li class="listitem">Now, let's look at the dedicated page of the restaurant to see how we collect the street address and the contact number of the restaurant with the HTML elements. We perform the same operation, right-click, and <span class="strong"><strong>Inspect </strong></span>to get the HTML elements of street address and contact number. See the following screenshot for reference. Note that for the street address, we have a separate CSS class, <code class="literal">street-address</code>, and the contact number is available under a span with the class name, <span class="strong"><strong>biz-phone</strong></span>.<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_043.jpg"/></div><p>
</p></li><li class="listitem">Awesome! So, we now have all the HTML elements that can be used to scrape the data in an automated way. Let's now look at the implementation. The following Python code performs these operations in an automated way:<pre class="programlisting">        from bs4 import BeautifulSoup &#13;
        from threading import Thread &#13;
        import urllib &#13;
 &#13;
        #Location of restaurants &#13;
        home_url = "https://www.yelp.com" &#13;
        find_what = "Restaurants" &#13;
        location = "London" &#13;
 &#13;
        #Get all restaurants that match the search criteria &#13;
        search_url = "https://www.yelp.com/search?find_desc=" +&#13;
        find_what + "&amp;find_loc=" + location &#13;
        s_html = urllib.urlopen(search_url).read() &#13;
        soup_s = BeautifulSoup(s_html, "lxml") &#13;
 &#13;
        #Get URLs of top 10 Restaurants in London &#13;
        s_urls = soup_s.select('.biz-name')[:10] &#13;
        url = [] &#13;
        for u in range(len(s_urls)): &#13;
        url.append(home_url + s_urls[u]['href']) &#13;
 &#13;
 &#13;
        #Function that will do actual scraping job &#13;
        def scrape(ur): &#13;
                html = urllib.urlopen(ur).read() &#13;
                soup = BeautifulSoup(html, "lxml") &#13;
 &#13;
                title = soup.select('.biz-page-title') &#13;
                saddress = soup.select('.street-address') &#13;
                phone = soup.select('.biz-phone') &#13;
 &#13;
                if title: &#13;
                     print "Title: ", title[0].getText().strip() &#13;
                if saddress: &#13;
                     print "Street Address: ",&#13;
        saddress[0].getText().strip() &#13;
                if phone: &#13;
                     print "Phone Number: ", phone[0].getText().strip() &#13;
                print "-------------------" &#13;
 &#13;
        threadlist = [] &#13;
        i=0 &#13;
        #Making threads to perform scraping &#13;
        while i&lt;len(url): &#13;
                  t = Thread(target=scrape,args=(url[i],)) &#13;
                  t.start() &#13;
                  threadlist.append(t) &#13;
                  i=i+1 &#13;
 &#13;
        for t in threadlist: &#13;
                  t.join() &#13;
 &#13;
</pre><p>
</p></li><li class="listitem">OK, great! Now, if we run the preceding Python code, we get the details of the top 10 restaurants in <span class="emphasis"><em>London,</em></span> along with their names, street addresses and contact numbers. Refer to the following screenshot:<p>
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_01_044.jpg"/></div><p>
</p></li><li class="listitem">In the preceding screenshot, we get the records of 10 restaurants in London provided by Yelp. <span class="strong"><strong>Title </strong></span>is the name of the restaurant and <span class="strong"><strong>Street Address</strong></span> and <span class="strong"><strong>Phone Number</strong></span> are self-explanatory. Awesome! We did it for Ryan.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec42"/>How it works...</h2></div></div></div><p>In the preceding code snippet, we built the search criteria. We searched on <a class="ulink" href="https://yelp.com">https://yelp.com</a> and looked for restaurants in <span class="emphasis"><em>London</em></span>. With these details, we got the search URL on Yelp.</p><p>We then created a <code class="literal">urllib</code> object and used the <code class="literal">urlopen()</code> method on this search URL to <code class="literal">read()</code> the list of all the restaurants provided by Yelp matching the search criteria. The list of all the restaurants is stored as an HTML page, which is stored in the variable, <code class="literal">s_html</code>.</p><p>Using the <code class="literal">BeautifulSoup</code> module, we created a soup instance on the HTML content so that we could start extracting the required data using the CSS elements.</p><p>Initially, we browsed the top 10 results of the search on Yelp and got the URLs of the restaurants. We stored these URLs in the URL Python list. To get the URL, we selected the CSS class name <code class="literal">biz-name</code> using the code <code class="literal">soup_s.select(.biz-name)[:10]</code>.</p><p>We also defined a method, <code class="literal">scrape()</code>, which takes the restaurant URL as a parameter. In this method, we read the details of the restaurant, such as name, street address, and contact number, using the CSS class names <code class="literal">biz-page-title</code>, <code class="literal">street-address</code>, and <code class="literal">biz-phone</code>, respectively. To get the exact data, we selected the HTML elements using <code class="literal">title=soup.select</code>(<code class="literal">.biz-page-title</code>) and got the data with <code class="literal">title[0].getText().strip()</code>. Note that the <code class="literal">select()</code> method returns the found element as an array, so we need to look for index <code class="literal">0</code> to get the actual text.</p><p>We iterated through all the restaurant URLs in a <code class="literal">while </code>loop and scraped the URL using the <code class="literal">scrape()</code> method to get the details for each restaurant. It prints the name, street address, and contact number for each restaurant on your console, as we saw in the preceding screenshot.</p><p>To improve on the performance of our screaping program, we performed data extraction for every restaurant in an independent thread. We created a new thread with <code class="literal">t = Thread(target=scrape,args=(url[i],))</code> and got the results from each of them with the <code class="literal">t.join()</code> call.</p><p>That’s it, folks! Ryan is extremely happy with this effort. In this example, we helped Ryan and automated a critical business task for him. Throughout this book we'll look at various use cases where Python can be leveraged to automate business processes and make them efficient. Interested in more? Well, see you in the next chapter.</p></div></div></body></html>