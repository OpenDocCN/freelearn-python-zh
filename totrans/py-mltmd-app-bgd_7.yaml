- en: Chapter 7. Working with Videos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Photographs capture the moment, but it is the video that helps us relive that
    moment! Video has become a major part of our lives. We preserve our memories by
    capturing the family vacation on a camcorder. When it comes to digitally preserving
    those recorded memories, the digital video processing plays an important role.
    In the previous chapter, to learn various audio processing techniques, the GStreamer
    multimedia framework was used. We will continue to use GStreamer for learning
    the fundamentals of video processing.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this chapter, we shall:'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a simple command-line video player
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform basic video manipulations such as cropping, resizing, and tweaking the
    parameters such as brightness, contrast, and saturation levels of a streaming
    video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add text string on top of a video stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to convert video between different video formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a utility that separates audio and video tracks from an input video file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mix audio and video tracks to create a single video file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save one or more video frames as still images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So let's get on with it.
  prefs: []
  type: TYPE_NORMAL
- en: Installation prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use Python bindings of GStreamer multimedia framework to process video
    data. See the installation instructions in [Chapter 5](ch05.html "Chapter 5. Working
    with Audios"), *Working with Audios* to install GStreamer and other dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: For video processing, we will be using several GStreamer plugins not introduced
    earlier. Make sure that these plugins are available in your GStreamer installation
    by running the `gst-inspect-0.10` command from the console (gst-inspect-0.10.exe
    for Windows XP users). Otherwise, you will need to install these plugins or use
    an alternative if available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a list of additional plugins we will use in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`autoconvert:` Determines an appropriate converter based on the capabilities.
    It will be used extensively used throughout this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`autovideosink:` Automatically selects a video sink to display a streaming
    video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ffmpegcolorspace:` Transforms the color space into a color space format that
    can be displayed by the video sink.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`capsfilter:` It''s the capabilities filter used to restrict the type of media
    data passing down stream, discussed extensively in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`textoverlay:` Overlays a text string on the streaming video. Used in the *Adding
    text and time on a video stream* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeoverlay:` Adds a timestamp on top of the video buffer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clockoverlay:` Puts current clock time on the streaming video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`videobalance:` Used to adjust brightness, contrast, and saturation of the
    images. It is used in the *Video manipulations and effects* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`videobox:` Crops the video frames by specified number of pixels used in the
    *Cropping* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ffmux_mp4:` Provides `muxer` element for MP4 video muxing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ffenc_mpeg4:` Encodes data into MPEG4 format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ffenc_png:` Encodes data in PNG format used in the *Saving video frames as
    images* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing a video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, we saw how to play an audio. Like audio, there are different ways in
    which a video can be streamed. The simplest of these methods is to use the `playbin`
    plugin. Another method is to go by the basics, where we create a conventional
    pipeline and create and link the required pipeline elements. If we only want to
    play the 'video' track of a video file, then the latter technique is very similar
    to the one illustrated for audio playback. However, almost always, one would like
    to hear the audio track for the video being streamed. There is additional work
    involved to accomplish this. The following diagram is a representative GStreamer
    pipeline that shows how the data flows in case of a video playback.
  prefs: []
  type: TYPE_NORMAL
- en: '![Playing a video](img/0165_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this illustration, the `decodebin` uses an appropriate decoder to decode
    the media data from the source element. Depending on the type of data (audio or
    video), it is then further streamed to the audio or video processing elements
    through the `queue` elements. The two `queue` elements, `queue1` and `queue2`,
    act as media data buffer for audio and video data respectively. When the queue
    elements are added and linked in the pipeline, the thread creation within the
    pipeline is handled internally by the GStreamer.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - video player!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's write a simple video player utility. Here we will not use the `playbin`
    plugin. The use of `playbin` will be illustrated in a later sub-section. We will
    develop this utility by constructing a GStreamer pipeline. The key here is to
    use the queue as a data buffer. The audio and video data needs to be directed
    so that this 'flows' through audio or video processing sections of the pipeline
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `PlayingVidio.py` from the Packt website. The file has the
    source code for this video player utility.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following code gives an overview of the Video player class and its methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the overall structure of the code and the main program execution
    code remains the same as in the audio processing examples. The thread module is
    used to create a new thread for playing the video. The method VideoPlayer.play
    is sent on this thread. The gobject.threads_init() is an initialization function
    for facilitating the use of Python threading within the gobject modules. The main
    event loop for executing this program is created using gobject and this loop is
    started by the call evt_loop.run().
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of using `thread` module you can make use of `threading` module as
    well. The code to use it will be something like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`import threading`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`threading.Thread(target=player.play).start()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will need to replace the line thread.start_new_thread(player.play, ()) in
    earlier code snippet with line 2 illustrated in the code snippet within this note.
    Try it yourself!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now let's discuss a few of the important methods, starting with `self.contructPipeline:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In various audio processing applications, we have used several of the elements
    defined in this method. First, the pipeline object, `self.player`, is created.
    The `self.filesrc` element specifies the input video file. This element is connected
    to a `decodebin`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On line 15, `autoconvert` element is created. It is a GStreamer `bin` that automatically
    selects a converter based on the capabilities (caps). It translates the decoded
    data coming out of the `decodebin` in a format playable by the video device. Note
    that before reaching the video sink, this data travels through a `capsfilter`
    and `ffmpegcolorspace` converter. The `capsfilter` element is defined on line
    26\. It is a filter that restricts the allowed capabilities, that is, the type
    of media data that will pass through it. In this case, the `videoCap` object defined
    on line 25 instructs the filter to only allow `video-xraw-yuv` capabilities .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `ffmpegcolorspace` is a plugin that has the ability to convert video frames
    to a different color space format. At this time, it is necessary to explain what
    a color space is. A variety of colors can be created by use of basic colors. Such
    colors form, what we call, a **color space**. A common example is an rgb color
    space where a range of colors can be created using a combination of red, green,
    and blue colors. The color space conversion is a representation of a video frame
    or an image from one color space into the other. The conversion is done in such
    a way that the converted video frame or image is a closer representation of the
    original one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: The video can be streamed even without using the combination of `capsfilter`
    and the `ffmpegcolorspace`. However, the video may appear distorted. So it is
    recommended to use `capsfilter` and `ffmpegcolorspace` converter. Try linking
    the `autoconvert` element directly to the `autovideosink` to see if it makes any
    difference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice that we have created two sinks, one for audio output and the other for
    the video. The two `queue` elements are created on lines 32 and 33\. As mentioned
    earlier, these act as media data buffers and are used to send the data to audio
    and video processing portions of the GStreamer pipeline. The code block 35-45
    adds all the required elements to the pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the various elements in the pipeline are linked. As we already know, the
    `decodebin` is a plugin that determines the right type of decoder to use. This
    element uses dynamic pads. While developing audio processing utilities, we connected
    the `pad-added` signal from `decodebin` to a method `decodebin_pad_added`. We
    will do the same thing here; however, the contents of this method will be different.
    We will discuss that later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On lines 50-52, the video processing portion of the pipeline is linked. The
    `self.videoQueue` receives the video data from the `decodebin`. It is linked to
    an `autoconvert` element discussed earlier. The `capsfilter` allows only `video-xraw-yuv`
    data to stream further. The `capsfilter` is linked to a `ffmpegcolorspace` element,
    which converts the data into a different color space. Finally, the data is streamed
    to the `videosink`, which, in this case, is an `autovideosink` element. This enables
    the 'viewing' of the input video. The audio processing portion of the pipeline
    is very similar to the one used in earlier chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we will review the `decodebin_pad_added` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This method captures the `pad-added` signal, emitted when the `decodebin` creates
    a dynamic pad. In an earlier chapter, we simply linked the `decodebin` pad with
    a compatible pad on the `autoaudioconvert` element. We could do this because the
    `caps` or the type media data being streamed was always the audio data. However,
    here the media data can either represent an audio or video data. Thus, when a
    dynamic pad is created on the `decodebin`, we must check what `caps` this `pad`
    has. The name of the `get_name` method of `caps` object returns the type of media
    data handled. For example, the name can be of the form `video/x-raw-rgb` when
    it is a video data or `audio/x-raw-int` for audio data. We just check the first
    five characters to see if it is video or audio media type. This is done by the
    code block 4-11 in the code snippet. The `decodebin pad` with video media type
    is linked with the compatible pad on `self.videoQueue` element. Similarly, the
    `pad` with audio `caps` is linked with the one on `self.audioQueue`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the rest of the code from the `PlayingVideo.py`. Make sure you specify
    an appropriate video file path for the variable `self.inFileLocation` and then
    run this program from the command prompt as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should open a GUI window where the video will be streamed. The audio output
    will be synchronized with the playing video.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We created a command-line video player utility. We learned how to create a GStreamer
    pipeline that can play synchronized audio and video streams. It explained how
    the `queue` element can be used to process the audio and video data in a pipeline.
    In this example, the use of GStreamer plugins such as `capsfilter` and `ffmpegcolorspace`
    was illustrated. The knowledge gained in this section will be applied in the upcoming
    sections in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Have a go hero add playback controls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 6](ch06.html "Chapter 6. Audio Controls and Effects"), *Audio Controls
    and Effects* we learned different techniques to control the playback of an audio.
    Develop command-line utilities that will allow you to pause the video or directly
    jump to a specified position on the video track.
  prefs: []
  type: TYPE_NORMAL
- en: Playing video using 'playbin'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of the previous section was to introduce you to the fundamental method
    of processing input video streams. We will use that method one way or another
    in the future discussions. If just video playback is all that you want, then the
    simplest way to accomplish this is by means of `playbin` plugin. The video can
    be played just by replacing the `VideoPlayer.constructPipeline` method in file
    `PlayingVideo.py` with the following code. Here, `self.player` is a `playbin`
    element. The `uri` property of `playbin` is set as the input video file path.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the previous section was to introduce you to the fundamental method
    of processing input video streams. We will use that method one way or another
    in the future discussions. If just video playback is all that you want, then the
    simplest way to accomplish this is by means of `playbin` plugin. The video can
    be played just by replacing the `VideoPlayer.constructPipeline` method in file
    `PlayingVideo.py` with the following code. Here, `self.player` is a `playbin`
    element. The `uri` property of `playbin` is set as the input video file path.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Video format conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Saving the video in a different file format is one of the frequently performed
    tasks for example, the task of converting a recorded footage on to your camcorder
    to a format playable on a DVD player. So let's list out the elements we need in
    a pipeline to carry out the video format conversion.
  prefs: []
  type: TYPE_NORMAL
- en: A `filesrc` element to stream the video file and a `decodebin` to decode the
    encoded input media data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, the audio processing elements of the pipeline, such as `audioconvert`,
    an encoder to encode the raw audio data into an appropriate audio format to be
    written.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The video processing elements of the pipeline, such as a video encoder element
    to encode the video data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A multiplexer or a **muxer** that takes the encoded audio and video data streams
    and puts them into a single channel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There needs to be an element that, depending on the media type, can send the
    media data to an appropriate processing unit. This is accomplished by `queue`
    elements that act as data buffers. Depending on whether it is an audio or video
    data, it is streamed to the audio or video processing elements. The queue is also
    needed to stream the encoded data from audio pipeline to the multiplexer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, a `filesink` element to save the converted video file (containing both
    audio and video tracks).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time for action - video format converter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will create a video conversion utility that will convert an input video
    file into a format specified by the user. The file you need to download from the
    Packt website is `VideoConverter.py`. This file can be run from the command line
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Where, the options are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--input_path:` The full path of the video file we wish to convert. The video
    format of the input files. The format should be in a supported list of formats.
    The supported input formats are MP4, OGG, AVI, and MOV.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--output_path:` The full path of the output video file. If not specified,
    it will create a folder `OUTPUT_VIDEOS` within the input directory and save the
    file there with same name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--output_format:` The audio format of the output file. The supported output
    formats are OGG and MP4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: As we will be using a `decodebin` element for decoding the input media data;
    there is actually a wider range of input formats this utility can handle. Modify
    the code in `VideoPlayer.processArguments` or add more formats to dictionary `VideoPlayer.supportedInputFormats`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If not done already, download the file `VideoConverter.py` from the Packt website.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The overall structure of the code is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A new thread is created by calling `thread.start_new_thread`, to run the application.
    The method `VideoConverter.convert` is sent on this thread. It is similar to the
    `VideoPlayer.play` method discussed earlier. Let's review some key methods of
    the class `VideoConverter`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `__init__` method contains the initialization code. It also calls methods
    to process command-line arguments and then build the pipeline. The code is illustrated
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To process the video file, we need audio and video encoders. This utility will
    support the conversion to only MP4 and OGG file formats. This can be easily extended
    to include more formats by adding appropriate encoders and muxer plugins. The
    values of the self.audioEncoders and self.videoEncoders dictionary objects specify
    the encoders to use for the streaming audio and video data respectively. Therefore,
    to store the video data in MP4 format, we use the ffenc_mp4 encoder. The encoders
    illustrated in the code snippet should be a part of the GStreamer installation
    on your computer. If not, visit the GStreamer website to find out how to install
    these plugins. The values of dictionary self.muxers represent the multiplexer
    to use in a specific output format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `constructPipeline` method does the main conversion job. It builds the required
    pipeline, which is then set to playing state in the `convert` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In an earlier section, we covered several of the elements used in the previous
    pipeline. The code on lines 43 to 48 establishes linkage for the audio and video
    processing elements. On line 44, the multiplexer, self.muxer is linked with the
    video encoder element. It puts the separate parts of the stream in this case,
    the video and audio data, into a single file. The data output from audio encoder,
    self.audio_encoder, is streamed to the muxer via a queue element, self.queue3\.
    The muxed data coming out of self.muxer is then streamed to the self.filesink.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's quickly review the `VideoConverter.convert` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: On line 10, the GStreamer pipeline built earlier is set to playing. When the
    conversion is complete, it will generate the End Of Stream (EOS) message. The
    self.is_playing flag is modified in the method self.message_handler. The while
    loop on line 11 is executed until the EOS message is posted on the bus or some
    error occurs. Finally, on line 24, the main execution loop is terminated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: On line 3, we make a call to `time.clock()`. This actually gives the CPU time
    spent on the process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The other methods such as `VideoConverter.decodebin_pad_added` are identical
    to the one developed in the *Playing a video* section. Review the remaining methods
    from the file `VideoConverter.py` and then run this utility by specifying appropriate
    command-line arguments. The following screenshot shows sample output messages
    when the program is run from the console window.![Time for action - video format
    converter](img/0165_07_02.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a sample run of the video conversion utility from the console.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We created another useful utility that can convert video files from one format
    to the other. We learned how to encode the audio and video data into a desired
    output format and then use a multiplexer to put these two data streams into a
    single file.
  prefs: []
  type: TYPE_NORMAL
- en: Have a go hero batch-convert the video files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The video converter developed in previous sections can convert a single video
    file at a time. Can you make it a batch-processing utility? Refer to the code
    for the audio conversion utility developed in the *Working with Audios* chapter.
    The overall structure will be very similar. However, there could be challenges
    in converting multiple video files because of the use of queue elements. For example,
    when it is done converting the first file, the data in the queue may not be flushed
    when we start conversion of the other file. One crude way to address this would
    be to reconstruct the whole pipeline and connect signals for each audio file.
    However, there will be a more efficient way to do this. Think about it!
  prefs: []
  type: TYPE_NORMAL
- en: Video manipulations and effects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you have a video file that needs to be saved with an adjusted default
    brightness level. Alternatively, you may want to save another video with a different
    aspect ratio. In this section, we will learn some of the basic and most frequently
    performed operations on a video. We will develop code using Python and GStreamer
    for tasks such as resizing a video or adjusting its contrast level.
  prefs: []
  type: TYPE_NORMAL
- en: Resizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data that can flow through an element is described by the capabilities (caps)
    of a pad on that element. If a decodebin element is decoding video data, the capabilities
    of its dynamic pad will be described as, for instance, `video/x-raw-yuv`. Resizing
    a video with GStreamer multimedia framework can be accomplished by using a `capsfilter`
    element, that has `width` and `height` parameters specified. As discussed earlier,
    the `capsfilter` element limits the media data type that can be transferred between
    two elements. For example, a `cap` object described by the string, `video/x-raw-yuv,
    width=800, height=600` will set the width of the video to 800 pixels and the height
    to 600 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - resize a video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now see how to resize a streaming video using the `width` and `height`
    parameters described by a GStreamer `cap` object.
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `VideoManipulations.py` from the Packt website. The overall
    class design is identical to the one studied in the *Playing a video* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The methods `self.constructAudioPipeline()` and `self.constructVideoPipeline()`,
    respectively, define and link elements related to audio and video portions of
    the main pipeline object `self.player`. As we have already discussed most of the
    audio/video processing elements in earlier sections, we will only review the `constructVideoPipeline`
    method here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The capsfilter element is defined on line 16\. It is a filter that restricts
    the type of media data that will pass through it. The videocap is a GStreamer
    cap object created on line 10\. This cap specifies the width and height parameters
    of the streaming video. It is set as a property of the capsfilter, self.capsFilter.
    It instructs the filter to only stream video-xraw-yuv data with width and height
    specified by the videocap object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: In the source file, you will see an additional element `self.videobox` linked
    in the pipeline. It is omitted in the above code snippet. We will see what this
    element is used for in the next section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of the code is straightforward. We already covered similar methods
    in earlier discussions. Develop the rest of the code by reviewing the file `VideoManipulations.py`.
    Make sure to specify an appropriate video file path for the variable `self.inFileLocation`
    .Then run this program from the command prompt as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should open a GUI window where the video will be streamed. The default
    size of this window will be controlled by the parameters self.video_width and
    self.video_height specified in the code.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The command-line video player developed earlier was extended in the example
    we just developed. We used `capsfilter` plugin to specify the `width` and `height`
    parameters of the streaming video and thus resize the video.
  prefs: []
  type: TYPE_NORMAL
- en: Cropping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you have a video that has a large 'gutter space' at the bottom or some
    unwanted portion on a side that you would like to trim off. The `videobox` GStreamer
    plugin facilitates cropping the video from left, right, top, or bottom.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - crop a video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's add another video manipulation feature to the command-line video player
    developed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: The file we need here is the one used in the earlier section, `VideoManipulations.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once again, we will focus our attention on the `constructVideoPipeline` method
    of the class `VideoPlayer`. The following code snippet is from this method. The
    rest of the code in this method is identical to the one reviewed in the earlier
    section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code is self-explanatory. The `videobox` element is created on line 1\.
    The properties of `videobox` that crop the streaming video are set on lines 2-5\.
    It receives the media data from the `autoconvert` element. The source pad of `videobox`
    is connected to the sink of either `capsfilter` or directly the `ffmpegcolorspace`
    element.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Develop the rest of the code by reviewing the file `VideoManipulations.py`.
    Make sure to specify an appropriate video file path for the variable `self.inFileLocation`.
    Then run this program from the command prompt as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should open a GUI window where the video will be streamed. The video will
    be cropped from left, right, bottom, and top sides by the parameters `self.crop_left,
    self.crop_right, self.crop_bottom`, and `self.crop_top` respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We extended the video player application further to add a GStreamer element
    that can crop the video frames from sides. The `videobox` plugin was used to accomplish
    this task.
  prefs: []
  type: TYPE_NORMAL
- en: Have a go hero add borders to a video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we used `videobox` element to trim the video from sides.
    The same plugin can be used to add a border around the video. If you set negative
    values for `videobox` properties, such as, bottom, top, left and right, instead
    of cropping the video, it will add black border around the video. Set negative
    values of parameters such as `self.crop_left` to see this effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The video cropping can be accomplished by using `videocrop` plugin. It is similar
    to the `videobox` plugin, but it doesn't support adding a border to the video
    frames. Modify the code and use this plugin to crop the video.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjusting brightness and contrast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We saw how to adjust the brightness and contrast level in [Chapter 3](ch03.html
    "Chapter 3. Enhancing Images"), *Enhancing Images*. If you have a homemade video
    recorded in poor lighting conditions, you would probably adjust its brightness
    level. The contrast-level highlights the difference between the color and brightness
    level of each video frame. The `videobalance` plugin can be used to adjust the
    brightness, contrast, hue, and saturation. The next code snippet creates this
    element and sets the brightness and contrast properties. The brightness property
    can accept values in the range `-1` to `1`, the default (original) brightness
    level is `0`. The contrast can have values in the range `0` to `2` with the default
    value as `1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `videobalance` is then linked in the GStreamer pipeline as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Review the rest of the code from file `VideoEffects.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a gray scale video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The video can be rendered as gray scale by adjusting the saturation property
    of the `videobalance` plugin. The saturation can have a value in the range `0`
    to `2`. The default value is `1`. Setting this value to `0.0` converts the images
    to gray scale. The code is illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You can refer to the file `VideoEffects.py`, which illustrates how to use the
    `videobalance` plugin to adjust saturation and other parameters discussed in earlier
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Adding text and time on a video stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ability to add a text string or a subtitles track to a video is yet another
    desirable feature one needs when processing videos. The GStreamer plugin `textoverlay`
    enables overlaying informative text string, such as the name of the file, on top
    of a video stream. The other useful plugins such as `timeoverlay` and `clockoverlay`
    provide a way to put the video buffer timestamp and the CPU clock time on top
    of the streaming video.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - overlay text on a video track
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how to add a text string on a video track. We will write a simple
    utility, which essentially has the same code structure as the one we developed
    in the *Playing a video* section. This tool will also add the buffer timestamp
    and the current CPU clock time on the top of the video. For this section, it is
    important that you have `textoverlay, timeoverlay`, and `clockoverlay` plugins
    available in your GStreamer installation. Otherwise, you need to install these
    plugins or use some other plugins, such as `cairotextoverlay`, if available.
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `VideoTextOverlay.py` from the Packt website.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `constructVideoPipeline` method of the class `VideoPlayer` is illustrated
    in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the elements for overlaying text, time, or clock can be simply
    added and linked in a GStreamer pipeline like other elements. Let's discuss various
    properties of these elements now. On lines 20-23, the textoverlay element is defined.
    The text property sets the text string that appears on the streaming video. To
    ensure that the text string is clearly visible in the video, we add a background
    contrast to this text. This is done on line 23 by setting the shaded-background
    property to True. The other properties of this plugin help fix the text position
    on the video. Run gst-inspect-0.10 on textoverlay plugin to see what these properties
    are.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, on lines 25-36, the time and clock overlay elements are defined. The properties
    are similar to the ones available in `textoverlay` plugin. The clock time will
    appear on the bottom-left corner of the streaming video. This is accomplished
    by setting the `valign` and `halign` properties. These three elements are then
    linked in the GStreamer pipeline. The internal order in which they are linked
    doesn't matter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Develop the rest of the code by reviewing the file `VideoTextOverlay.py`. Make
    sure you specify an appropriate video file path for the variable `self.inFileLocation`.
    Then run this program from the command prompt as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should open a GUI window where the video will be streamed. The video will
    show a text string "hello" along with the running time and the clock time. This
    is illustrated by the following snapshot of a video frame.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Time for action - overlay text on a video track](img/0165_07_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The screenshot depicts a video frame showing text, time, and clock overlay.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We learned how to use elements such as `textoverlay, timeoverlay`, and `clockoverlay`
    in a GStreamer pipeline to add text string, timestamp, and clock respectively,
    on top of a video buffer. The `textoverlay` element can be used further to add
    a subtitle track to the video file.
  prefs: []
  type: TYPE_NORMAL
- en: Have a go hero add subtitles to a video track!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Extend the code we just developed to add a subtitles track to the video file.
    To add a subtitle track, you will need the `subparse` plugin. Note that this plugin
    is not available by default in the windows installation of GStreamer using the
    GStreamer-WinBuilds binary. Thus, Windows users may need to install this plugin
    separately. Review the `subparse` plugin reference to see how to accomplish this
    task. The following code snippet shows how to create the `subparse` element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Separating audio and video tracks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are times when you would like to separate an audio and a video track.
    Imagine that you have a collection of your favorite video songs. You are going
    on a long drive and the old CD player in your car can only play audio files in
    a specific file format. Let's write a utility that can separate out the audio
    from a video file!
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - audio and video tracks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will develop code that takes a video file as an input and then creates two
    output files, one with only the audio track of the original file and the other
    with the video portion.
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `SeparatingAudio.py` from the Packt website. The structure
    of the class `AudioSeparator` is similar to the one seen in the *Playing a Video*
    section. We will review two methods of this class, `constructPipeline` and `decodebin_pad_added`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's start with the code in the `constructPipeline` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have already used all the necessary elements in various examples. The key
    here is to link them properly. The self.audiosink and self.videoSink elements
    are filesink elements that define audio and video output file locations respectively.
    Note that, in this example, we will save the output audio in MP3 format and video
    in MP4 format. Thus, the lame encoder is used for the audio file whereas we use
    encoder ffenc_mpeg4 and multiplexer ffmux_mp4 for the video output. Note that
    we have not used ffmpegcolorspace element. It just helps to get an appropriate
    color space format for the video sink (in this case, the output video file). In
    this case, it is not needed. You can always link it in the pipeline if the output
    file doesn't appropriately display the video frames. The media data decoded by
    self.decodebin needs to be streamed to the audio and video portions of the pipeline,
    using the queue elements as data buffers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `decodebin` creates dynamic pads to decode the input audio and video data.
    The `decodebin_pad_added` method needs to check the capabilities (caps) on the
    dynamic pad of the `decodebin`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This check is done by the code block 6-12\. If capabilities indicate it's an
    audio data, the `decodebin pad` is linked to the compatible pad on `self.audioQueue`.
    Similarly, a link between to `self.videoQueue` and `self.decodebin` is created
    when `caps` indicate it is the video data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can work through the remaining code in the file `SeparatingAudio.py`. Replace
    the paths represented by `self.inFileLocation, self.audioOutLocation`, and `self.videoOutLocation`
    with appropriate paths on your computer and then run this utility as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should create two output files a file in MP3 format that contains only
    the audio track from the input file and a file in MP4 format containing the video
    track.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We build a GStreamer pipeline that separates audio and video tracks from an
    input video file. Several of the GStreamer elements that we learned about in a
    number of examples earlier were used to develop this utility. We also learned
    how to use the capabilities (caps) on the dynamic pads of `decodebin` to make
    proper linkage between the `decodebin` and the `queue` elements.
  prefs: []
  type: TYPE_NORMAL
- en: Mixing audio and video tracks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you have recorded your friend's wedding on your camcorder. For some
    specific moments, you would like to mute all other sounds and replace those with
    background music. To accomplish this, first you need to save the video track without
    the audio as a separate file. We just learned that technique. Then you need to
    combine this video track with audio track containing the background music you
    wish to play. Let's now learn how to mix audio and video tracks into a single
    video file.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - audio/video track mixer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will develop a program that generates a video output file, by mixing an audio
    and a video track. Think about what change we will need to incorporate when compared
    to the audio/video track separation utility developed earlier. In that application,
    two `filesink` elements were required as two output files were created. Here,
    we need the opposite. We require two `filesrc` elements containing the audio and
    video data and a single `filesink` element that will contain both the audio and
    the video track.
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `AudioVideoMixing.py` from the Packt website. We will review
    some of the important methods of class `AudioVideoMixer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `constructPipeline` method, as usual, builds the GStreamer pipeline with
    all necessary elements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The audio and video file sources are defined by the elements `self.audiosrc`
    and `self.videosrc` respectively. These are connected to two separate `decodebins`
    (see lines 54 and 59). The `pad-added` signals of `self.audio_decodebin` and `self.video_decodebin`
    are connected in the `connectSignals` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The audio and video data then travels through a chain of audio and video processing
    elements respectively. The data is encoded by their respective encoders. The encoded
    data streams are combined so that the output video file contains both audio and
    video tracks. This job is done by the multiplexer, self.muxer. It is linked with
    the video encoder element. The audio data is streamed to the muxer through a queue
    element (line 57). The data is 'muxed' and fed to the filesink element, self.filesink.
    Note that the ffmpegcolorspace element and the capsfilter, self.capsfiter is not
    really required. In this case, the output video should have proper display format.
    You can try running this application by removing those two elements to see if
    it makes any difference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the `decodebin_pad_added` method, we will check a few extra things before
    linking the dynamic pads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It could happen that each of the input files contains audio as well as video
    data. For example, both self.audiosrc and self.videosrc represent different video
    files with both audio and video data. The file self.audiosrc is linked to self.audio_decodebin.
    Thus, we should make sure that when the self.audio_decodebin generates a pad-added
    signal, the dynamic pad is linked only when its caps have audio data. On similar
    lines, the pad on self.video_decodebin is linked only when caps represent video
    data. This is ensured by the code block 6 13.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Develop the rest of the code by reviewing file `AudioVideoMixer.py`. Replace
    the paths represented by, `self.audioInLocation, self.videoInLocation`, and `self.outFileLocation`
    with appropriate paths on your computer and then run this utility as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should create an output video file in MP4 format that contains the audio
    and video tracks from the specified input files!
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We developed a tool that mixes input audio and video tracks and stores them
    into a single output file. To accomplish this task we used most of the audio/video
    processing elements that were used in video conversion utility. We learned how
    to link the dynamic pads on `decodebin` based on the streaming data represented
    by its 'caps'. The multiplexer plugin `ffmux_mp4` element was used to put the
    audio and video data together.
  prefs: []
  type: TYPE_NORMAL
- en: Saving video frames as images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine that you have a wildlife video and it has recorded a very special moment.
    You would like to save this image. Let's learn how this can be achieved using
    the GStreamer framework.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - saving video frames as images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This file can be run from the command line as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the `[options]` are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--input_file:` The full path to input video file from which one or more frames
    need to be captured and saved as images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--start_time:` The position in seconds on the video track. This will be the
    starting position from which one or more video frames will be captured as still
    image(s). The first snapshot will always be at `start_time`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--duration:` The duration (in seconds) of the video track starting from the
    `start_time`. ''N'' number of frames will be captured starting from the `start_time`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--num_of_captures:` Total number of frames that need to be captured from `start_time`
    (including it) up to, `end_time= start_time + duration` (but not including the
    still image at `end_time)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If not already done, download the file `ImagesFromVideo.py` from the Packt website.
    Following is an outline of the code for saving video frames.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The program execution starts by calling the captureImage method. The gnlfilesource
    element discussed in audio processing chapters will be used here to seek a particular
    frame on the streaming video. The capture_single_image does the main job of saving
    a single frame as an image. We will discuss some of these methods next.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's start with the `constructPipeline` method which defines and links various
    elements needed to capture the video frames.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We already know how to create and connect the gnlfilesource element (called
    self.gnlfilesrc). In the examples we have seen so far, the encoder element used
    in a GStreamer pipeline encoded the streaming media data either in an audio or
    a video format. On line 11, we define a new encoder element that enables saving
    a particular frame in the streaming video as an image. In this example, we use
    the encoder ffenc_png to save the video frame as an image file with PNG file format.
    This plugin should be available by default in your GStreamer installation. If
    not, you will need to install it. There are similar plugins available to save
    the image in different file formats. For example, use jpegenc plugin to save it
    as a JPEG image and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The self.gnlfilesrc uses dynamic pad, which is connected to an appropriate pad
    on ffmpegcolorspace discussed earlier. The self.colorspace element converts the
    color space and this video data is then encoded by the ffenc_png element. The
    self.filesink defines the location to save a particular video frame as an image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `captureImage` is the main controlling method. The overall structure is
    very similar to the audio conversion utility developer in [Chapter 5](ch05.html
    "Chapter 5. Working with Audios"), *Working with Audios*. This method runs the
    top-level controlling loop to capture the frames specified as an argument to the
    program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The method capture_single_image does the main job of saving each of these frames.
    The self.media_start_time defines the position on the streaming video from which
    this utility should start saving the video frames as images. This is specified
    as a command-line argument to this utility. The media_end variable defines the
    position on the video track at which the program should 'stop' capturing the still
    images (the video frames). The self.media_start_time is when the first video frame
    will be saved as an image. This is the initial value assigned to the local variable
    start, which is then incremented in the loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The while loop (lines 8-10) calls the capture_single_image method for each
    of the video frames we wish to save as an image. The self.deltaTime variable defines
    the incremental time steps for capturing video frames. Its value is determined
    in the constructor as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, self.numberOfCaptures is specified as an argument. If this argument is
    not specified, it will save only a single frame as an image. It is used to increment
    the variable start.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's see what `ImageCapture.capture_single_image` does. As the name suggests,
    its job is to save a single image corresponding to the video frame at `media_start_time`
    in the streaming video.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The media_duration is set to a very small value (0.01 seconds), just enough
    to play the video frame at media_start_time. The media_start_time and media_duration
    used to set the properties of the gnlfilesource represented by self.gnlfilesrc.
    On line 14, the location of the output image file is specified. Note that the
    filename is appended with a timestamp that represents the time on the timeline
    of the streaming video, at which this snapshot was taken. After setting up the
    necessary parameter, the pipeline is 'started' on line 20 and will be played until
    the EOS message is posted on the bus, that is, when it finishes writing the output
    PNG file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the remaining methods from the file ImagesFromVideo.py and then run this
    utility by specifying appropriate command-line arguments. The following screenshot
    shows sample output messages when the program is run from the console window.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Time for action - saving video frames as images](img/0165_07_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We developed a very useful application that can save specified frames in a streaming
    video as image files. To accomplish this, we re-used several of the GStreamer
    elements/plugins studied earlier. For example, elements such as `gnlfilesource,
    ffmpegcolorspace`, and so on were used to construct the GStreamer pipeline. Additionally,
    we used an image encoder to save the video data in an image format.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned fundamentals of GStreamer API in previous chapters on audio processing.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we moved one step further to develop some useful video processing
    utilities using Python and GStreamer. To accomplish this task, we learned about
    several new GStreamer plugins required for processing videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we covered:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline that handles audio and video: We learned how to build a GStreamer
    pipeline that can handle both audio and video tracks from the input video file.
    This was used to ''play'' a video file and it was also the basic pipeline used
    in several video-processing tools developed in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Separating audio/video: With the help of example, we learned how to save an
    audio/video track of a video file into two different files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mixing audio/video: We wrote a program that can mix an audio and video stream
    into a single video file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Video effects: How to adjust the properties such as brightness, contrast, and
    saturation for a streaming video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text overlay: We developed a utility that can add text, timestamp, and clock
    strings on the streaming video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Still images from video: We learned how to save a video frame of a streaming
    video as an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes our discussion on video processing using Python and GStreamer.
    For the audio as well as video processing, we mostly developed various command-line
    tools. It gave us a good understanding of the use of the underlying components
    of a multimedia framework. There was no user interface component involved in our
    discussion. The default GUI appeared only while playing a video.
  prefs: []
  type: TYPE_NORMAL
- en: The focus of the next chapter will be on GUI-based audio and video applications.
  prefs: []
  type: TYPE_NORMAL
