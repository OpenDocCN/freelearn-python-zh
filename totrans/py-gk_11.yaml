- en: '*Chapter 8*: Scaling out Python Using Clusters'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 8 章*：使用集群扩展 Python'
- en: In the previous chapter, we discussed parallel processing for a single machine
    using threads and processes. In this chapter, we will extend our discussion of
    parallel processing from a single machine to multiple machines in a cluster. A
    cluster is a group of computing devices that work together to perform compute-intensive
    tasks such as data processing. In particular, we will study Python's capabilities
    in the area of data-intensive computing. Data-intensive computing typically uses
    clusters for processing large volumes of data in parallel. Although there are
    quite a few frameworks and tools available for data-intensive computing, we will
    focus on **Apache Spark** as a data processing engine and PySpark as a Python
    library to build such applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了使用线程和进程在单台机器上进行并行处理。在本章中，我们将扩展我们的并行处理讨论，从单台机器扩展到集群中的多台机器。集群是一组协同工作以执行计算密集型任务（如数据处理）的计算设备。特别是，我们将研究
    Python 在数据密集型计算领域的功能。数据密集型计算通常使用集群来并行处理大量数据。尽管有相当多的框架和工具可用于数据密集型计算，但我们将专注于 **Apache
    Spark** 作为数据处理引擎，以及 PySpark 作为构建此类应用的 Python 库。
- en: If Apache Spark with Python is properly configured and implemented, the performance
    of your application can increase manyfold and surpass competitor platforms such
    as **Hadoop MapReduce**. We will also look into how distributed datasets are utilized
    in a clustered environment. This chapter will help you to understand the use of
    cluster computing platforms for large-scale data processing and how to implement
    data processing applications using Python. To illustrate the practical use of
    Python for applications with cluster computing requirements, we will include two
    case studies; the first one is to compute the value of Pi (π) and the second one
    is to generate a word cloud from a data file.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Apache Spark 与 Python 配置和实现得当，您应用程序的性能可以大幅提升，并超越如 **Hadoop MapReduce** 等竞争对手平台。我们还将探讨如何在集群环境中利用分布式数据集。本章将帮助您了解集群计算平台在大型数据处理中的应用，以及如何使用
    Python 实现数据处理应用程序。为了说明 Python 在具有集群计算需求的应用中的实际应用，我们将包括两个案例研究；第一个是计算 π（圆周率）的值，第二个是从数据文件生成词云。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Learning about the cluster options for parallel processing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解并行处理中的集群选项
- en: Introducing **resilient distributed datasets** (**RDD**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 **弹性分布式数据集**（**RDD**）
- en: Using PySpark for parallel data processing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PySpark 进行并行数据处理
- en: Case studies of using Apache Spark and PySpark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Apache Spark 和 PySpark 的案例研究
- en: By the end of this chapter, you will know how to work with Apache Spark and
    how you can write Python applications for data processing that can be executed
    on the worker nodes of an Apache Spark cluster.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解如何使用 Apache Spark，以及您如何编写可以在 Apache Spark 集群的 worker 节点上执行的数据处理 Python
    应用程序。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following are the technical requirements for this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术要求如下：
- en: Python 3.7 or later installed on your computer
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的计算机上安装了 Python 3.7 或更高版本
- en: An Apache Spark single-node cluster
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Apache Spark 单节点集群
- en: PySpark installed on top of Python 3.7 or later for driver program development
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在驱动程序开发上安装了 Python 3.7 或更高版本的 PySpark
- en: Note
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The Python version used with Apache Spark has to match the Python version that
    is used to run the driver program.
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与 Apache Spark 一起使用的 Python 版本必须与运行驱动程序的 Python 版本相匹配。
- en: The sample code for this chapter can be found at [https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter08](https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter08).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例代码可以在 [https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter08](https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter08)
    找到。
- en: We will start our discussion by looking at the cluster options available for
    parallel processing in general.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论可用于并行处理的一般集群选项。
- en: Learning about the cluster options for parallel processing
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解并行处理中的集群选项
- en: When we have a large volume of data to process, it is not efficient and sometimes
    even not feasible to use a single machine with multiple cores to process the data
    efficiently. This is especially a challenge when working with real-time streaming
    data. For such scenarios, we need multiple systems that can process data in a
    distributed manner and perform these tasks on multiple machines in parallel. Using
    multiple machines to process compute-intensive tasks in parallel and in a distributed
    manner is called **cluster computing**. There are several big data distributed
    frameworks available to coordinate the execution of jobs in a cluster, but Hadoop
    MapReduce and Apache Spark are the leading contenders in this race. Both frameworks
    are open source projects from Apache. There are many variants (for example, Databricks)
    of these two platforms available with add-on features as well as maintenance support,
    but the fundamentals remain the same.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理大量数据时，使用单台具有多个核心的机器来高效地处理数据可能并不高效，有时甚至不可行。当处理实时流数据时，这尤其是一个挑战。对于此类场景，我们需要多个系统可以以分布式方式处理数据，并在多台机器上并行执行这些任务。使用多台机器并行且分布式地处理计算密集型任务被称为**集群计算**。有几个大数据分布式框架可用于协调集群中作业的执行，但Hadoop
    MapReduce和Apache Spark是这场竞赛的领先竞争者。这两个框架都是Apache的开源项目。这两个平台有许多变体（例如，Databricks），它们具有附加功能和维护支持，但基本原理保持不变。
- en: If we look at the market, the number of Hadoop MapReduce deployments may be
    higher compared to Apache Spark, but with its increasing popularity, Apache Spark
    is going to turn the tables eventually. Since Hadoop MapReduce is still very relevant
    due to its large install base, it is important to discuss what exactly Hadoop
    MapReduce is and how Apache Spark is becoming a better choice. Let's have a quick
    overview of the two in the next subsections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们观察市场，Hadoop MapReduce的部署数量可能比Apache Spark多，但随着其日益流行，Apache Spark最终会扭转局势。由于Hadoop
    MapReduce由于其庞大的安装基础仍然非常相关，因此讨论Hadoop MapReduce究竟是什么以及Apache Spark如何成为更好的选择是很重要的。让我们在下一个小节中快速概述这两个框架。
- en: Hadoop MapReduce
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop MapReduce
- en: 'Hadoop is a general-purpose distributed processing framework that offers the
    execution of large-scale data processing jobs across hundreds or thousands of
    computing nodes in a Hadoop cluster. The three core components of Hadoop are included
    in the following figure:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop是一个通用的分布式处理框架，它能够在Hadoop集群中的数百或数千个计算节点上执行大规模数据处理作业。Hadoop的三个核心组件如下所示：
- en: '![Figure 8.1 – Apache Hadoop MapReduce ecosystem'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1 – Apache Hadoop MapReduce生态系统'
- en: '](img/B17189_08_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17189_08_01.jpg)'
- en: Figure 8.1 – Apache Hadoop MapReduce ecosystem
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – Apache Hadoop MapReduce生态系统
- en: 'The three core components are described as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是三个核心组件的描述：
- en: '**Hadoop Distributed File System (HDFS)**: This is a Hadoop-native filesystem
    used to stores files such that those files can be parallelized across a cluster.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop分布式文件系统（HDFS）**：这是一个Hadoop原生文件系统，用于存储文件，以便这些文件可以在集群中并行化。'
- en: '**Yet Another Resource Negotiator (YARN)**: This is a system that processes
    data stored in HDFS and schedules the submitted jobs (for data processing) to
    be run by a processing system. The processing systems can be used for graph processing,
    stream processing, or batch processing.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**另一个资源协调器（YARN）**：这是一个处理存储在HDFS中的数据并调度提交的作业（用于数据处理）以由处理系统运行的系统。处理系统可用于图处理、流处理或批量处理。'
- en: '`map`) and reducer (`reduce`) functions are the same as we discussed in [*Chapter
    6*](B17189_06_Final_PG_ePub.xhtml#_idTextAnchor188), *Advanced Tips and Tricks
    in Python*. The key difference is that we use many `map` and `reduce` functions
    in parallel to process several datasets at the same time.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map`（映射）和`reduce`（归约）函数与我们在[*第6章*](B17189_06_Final_PG_ePub.xhtml#_idTextAnchor188)中讨论的相同，即《Python高级技巧与窍门》。关键区别在于我们使用多个`map`和`reduce`函数并行处理多个数据集。'
- en: After breaking the large dataset into small datasets, we can provide the small
    datasets as input to many mapper functions for processing on different nodes of
    a Hadoop cluster. Each mapper function takes one set of data as an input, processes
    the data based on the goal set by the programmer, and produces the output as key-value
    pairs. Once the output of all the small datasets is available, one or multiple
    reducer functions will take the output from the mapper functions and aggregate
    the results as per the goals of the reducer functions.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在将大型数据集分解为小数据集后，我们可以将这些小数据集作为输入提供给多个mapper函数，以便在Hadoop集群的不同节点上处理。每个mapper函数接收一组数据作为输入，根据程序员设定的目标处理数据，并以键值对的形式产生输出。一旦所有小数据集的输出都可用，一个或多个reducer函数将接收来自mapper函数的输出，并根据reducer函数的目标汇总结果。
- en: 'To explain it in a bit more detail, we can take an example of counting particular
    words such as *attack* and *weapon* in a large source of text data. The text data
    can be divided into small datasets, for example, eight datasets. We can have eight
    mapper functions that count the two words within the dataset provided to them.
    Each mapper function provides us with the count of the words *attack* and *weapon*
    as an output for the dataset provided to it. In the next phase, the outputs of
    all the mapper functions are provided to two reducer functions, one for each word.
    Each reducer function aggregates the count for each word and provides the aggregated
    results as an output. The operating of the MapReduce framework for this word count
    example is shown next. Note that the mapper function is typically implemented
    as `map` and the reducer function as `reduce` in Python programming:'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了更详细地解释，我们可以以计数大量文本数据中特定的单词，如*attack*和*weapon*为例。文本数据可以被划分为小数据集，例如，八个数据集。我们可以为每个数据集提供八个mapper函数来计数这两个单词。每个mapper函数为其提供的每个数据集提供*attack*和*weapon*单词的计数作为输出。在下一阶段，所有mapper函数的输出被提供给两个reducer函数，每个单词一个。每个reducer函数汇总每个单词的计数，并将汇总结果作为输出。下面展示了MapReduce框架在此单词计数示例中的操作。请注意，在Python编程中，mapper函数通常实现为`map`，reducer函数实现为`reduce`：
- en: '![Figure 8.2 – Working of the MapReduce framework'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.2 – MapReduce框架的工作原理'
- en: '](img/B17189_08_02.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17189_08_02.jpg)'
- en: Figure 8.2 – Working of the MapReduce framework
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – MapReduce框架的工作原理
- en: We will skip the next levels of Hadoop components as they are not relevant to
    our discussion in this chapter. Hadoop is built mainly in Java, but any programming
    language, such as Python, can be used to write custom mapper and reducer components
    for the MapReduce module.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将跳过Hadoop组件的下一级，因为它们与本章的讨论无关。Hadoop主要用Java编写，但可以使用任何编程语言，如Python，来编写定制的mapper和reducer组件，用于MapReduce模块。
- en: Hadoop MapReduce is good for processing a large chunk of data by breaking it
    into small blocks. The cluster nodes process these blocks separately and then
    the results are aggregated before being sent to the requester. Hadoop MapReduce
    processes the data from a filesystem and thus is not very efficient in terms of
    performance. However, it works very well if the speed of processing is not a critical
    requirement, for instance, if data processing can be scheduled to occur at night.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop MapReduce通过将数据分成小块来处理大量数据，非常适合。集群节点分别处理这些块，然后将结果汇总后发送给请求者。Hadoop MapReduce从文件系统中处理数据，因此在性能方面不是非常高效。然而，如果处理速度不是关键要求，例如，如果数据处理可以安排在夜间进行，它工作得非常好。
- en: Apache Spark
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Spark
- en: 'Apache Spark is an open source cluster computing framework for real-time as
    well as batch data processing. The main feature of Apache Spark is that it is
    an in-memory data processing framework, which makes it efficient in terms of achieving
    low latency and makes it suitable for many real-world scenarios because of the
    following additional factors:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个开源的集群计算框架，适用于实时和批量数据处理。Apache Spark的主要特点是它是一个内存数据处理框架，这使得它在实现低延迟方面非常高效，并且由于以下额外因素，它适合许多现实世界的场景：
- en: It gets results quickly for mission-critical and time-sensitive applications
    such as real-time or near real-time scenarios.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它能够快速为关键任务和时间敏感的应用程序提供结果，例如实时或近实时场景。
- en: It's good for performing tasks repeatedly or iteratively in an efficient way
    due to in-memory processing.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于内存处理，它非常适合以高效的方式重复或迭代地执行任务。
- en: You can utilize out-of-the-box machine learning algorithms.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以利用现成的机器学习算法。
- en: You can leverage the support of additional programming languages such as Java,
    Python, Scala, and R.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以利用Java、Python、Scala和R等额外编程语言的支持。
- en: In fact, Apache Spark covers a wide range of workloads, including batch data,
    iterative processing, and streaming data. The beauty of Apache Spark is that it
    can use Hadoop (via YARN) as a deployment cluster as well, but it has its own
    cluster manager as well.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '事实上，Apache Spark涵盖了广泛的工作负载，包括批量数据、迭代处理和流数据。Apache Spark的美丽之处在于它可以使用Hadoop（通过YARN）作为部署集群，但它也有自己的集群管理器。 '
- en: 'At a high level, the main components of Apache Spark are segregated into three
    layers, as shown in the following figure:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，Apache Spark的主要组件分为三个层次，如下所示：
- en: '![Figure 8.3 – Apache Spark ecosystem'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.3 – Apache Spark生态系统'
- en: '](img/B17189_08_03.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17189_08_03.jpg]'
- en: Figure 8.3 – Apache Spark ecosystem
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – Apache Spark生态系统
- en: These layers are discussed next.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来讨论这些层次。
- en: Support languages
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持的语言
- en: Scala is a native language of Apache Spark, so it is quite popular for development.
    Apache Spark also provides high-level APIs for Java, Python, and R. In Apache
    Spark, multi-language support is provided by using the **Remote Procedure Call**
    (**RPC**) interface. There is an RPC adapter written for each language in Scala
    that transforms the client requests written in a different language to the native
    Scala requests. This makes its adoption easier across the development community.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Scala是Apache Spark的本地语言，因此在开发中非常流行。Apache Spark还提供了Java、Python和R的高级API。在Apache
    Spark中，通过使用**远程过程调用**（**RPC**）接口提供多语言支持。Scala为每种语言编写了一个RPC适配器，将用不同语言编写的客户端请求转换为原生Scala请求。这使得它在开发社区中的采用更加容易。
- en: Core components
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心组件
- en: 'A brief overview of each of the core components is discussed next:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来简要概述每个核心组件：
- en: '**Spark Core and RDDs**: Spark Core is a core engine of Spark and is responsible
    for providing abstraction to RDDs, scheduling and distributing jobs to a cluster,
    interacting with storage systems such as HDFS, Amazon S3, or an RDBMS, and managing
    memory and fault recoveries. An RDD is a resilient distributed dataset that is
    an immutable and distributable collection of data. RDDs are partitioned to be
    executed on the different nodes of a cluster. We will discuss RDDs in more detail
    in the next section.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Core和RDDs**：Spark Core是Spark的核心引擎，负责为RDDs提供抽象，调度和分配作业到集群，与HDFS、Amazon
    S3或RDBMS等存储系统交互，以及管理内存和故障恢复。RDD是一个弹性分布式数据集，是一个不可变且可分发的数据集合。RDD被分区以在集群的不同节点上执行。我们将在下一节中更详细地讨论RDD。'
- en: '**Spark SQL**: This module is for querying data stored both in RDDs and in
    external data sources using abstracted interfaces. Using these common interfaces
    enables the developers to mix the SQL commands with the analytics tools for a
    given application.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**：此模块用于使用抽象接口查询存储在RDDs和外部数据源中的数据。使用这些通用接口使开发者能够将SQL命令与特定应用程序的分析工具混合使用。'
- en: '**Spark Streaming**: This module is used to process real-time data, which is
    critical to analyze live data streams with low latency.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming**：此模块用于处理实时数据，这对于以低延迟分析实时数据流至关重要。'
- en: '**MLlib**: The **Machine Learning Library** (**MLlib**) is used to apply machine
    learning algorithms in Apache Spark.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLlib**：**机器学习库**（**MLlib**）用于在Apache Spark中应用机器学习算法。'
- en: '**GraphX**: This module provides an API for graph-based parallel computing.
    This module comes with a variety of graph algorithms. Note that a graph is a mathematical
    concept based on vertices and edges that represents how a set of objects are related
    or dependent on each other. The objects are represented by vertices and their
    relationships by the edges.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX**：此模块提供基于图并行计算的API。此模块包含各种图算法。请注意，图是一个基于顶点和边的数学概念，它表示一组对象之间的关系或相互依赖。对象由顶点表示，它们之间的关系由边表示。'
- en: Cluster management
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群管理
- en: 'Apache Spark supports a few cluster managers, such as Standalone, Mesos, YARN,
    and Kubernetes. The key function of a cluster manager is to schedule and execute
    the jobs on cluster nodes and manage the resources on cluster nodes. But to interact
    with one or more cluster managers, there is a special object used in the main
    or driver program called `SparkContext` object was considered as an entry point,
    but its API is now wrapped as part of the `SparkSession` object. Conceptually,
    the following figure shows the interaction of a `SparkSession` (**SparkContext**),
    and the **worker nodes** in a cluster:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark支持一些集群管理器，如Standalone、Mesos、YARN和Kubernetes。集群管理器的关键功能是在集群节点上调度和执行作业以及管理集群节点上的资源。但是，为了与一个或多个集群管理器交互，主程序或驱动程序中使用了特殊对象，称为`SparkContext`对象，但它的API现在被封装为`SparkSession`对象的一部分。从概念上讲，以下图显示了`SparkSession`（**SparkContext**）和集群中的**工作节点**之间的交互：
- en: '![Figure 8.4 – Apache Spark ecosystem](img/B17189_08_04.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – Apache Spark生态系统](img/B17189_08_04.jpg)'
- en: Figure 8.4 – Apache Spark ecosystem
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – Apache Spark生态系统
- en: The `SparkSession` object can connect to different types of cluster managers.
    Once connected, the executors are acquired on the cluster nodes through the cluster
    managers. The executors are the Spark processes that run the jobs and store the
    computational job results. The cluster manager on a master node is responsible
    for sending the application code to the executor processes on the worker nodes.
    Once the application code and data (if applicable) are moved to the worker nodes,
    the `SparkSession` object in a driver program interacts directly with executor
    processes for the execution of tasks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`对象可以连接到不同类型的集群管理器。一旦连接，通过集群管理器在集群节点上获取执行器。执行器是运行作业并存储计算作业结果的Spark进程。主节点上的集群管理器负责将应用程序代码发送到工作节点上的执行器进程。一旦应用程序代码和数据（如果适用）移动到工作节点，驱动程序程序中的`SparkSession`对象将直接与执行器进程交互以执行任务。'
- en: 'As per Apache Spark release 3.1, the following cluster managers are supported:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Apache Spark 3.1版本，以下集群管理器得到支持：
- en: '**Standalone**: This is a simple cluster manager that is included as part of
    the Spark Core engine. The Standalone cluster is based on master and worker (or
    slave) processes. A master process is basically a cluster manager, and the worker
    processes host the executors. Although the masters and the workers can be hosted
    on a single machine, this is not the real deployment scenario of a Spark Standalone
    cluster. It is recommended to distribute workers to different machines for the
    best outcome. The Standalone cluster is easy to set up and provides most of the
    features required from a cluster.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Standalone**：这是一个简单的集群管理器，它是Spark Core引擎的一部分。Standalone集群基于主进程和工作进程（或从进程）。主进程基本上是一个集群管理器，工作进程托管执行器。尽管主节点和工作节点可以托管在单个机器上，但这并不是Spark
    Standalone集群的真实部署场景。建议将工作进程分布到不同的机器上以获得最佳效果。Standalone集群易于设置并提供所需的集群的大部分功能。'
- en: '**Apache Mesos**: This is another general-purpose cluster manager that can
    also run Hadoop MapReduce. For large-scale cluster environments, Apache Mesos
    is the preferred option. The idea of this cluster manager is that it aggregates
    the physical resources into a single virtual resource that acts as a cluster and
    provides a node-level abstraction. It is a distributed cluster manager by design.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Mesos**：这是另一个通用集群管理器，也可以运行Hadoop MapReduce。对于大规模集群环境，Apache Mesos是首选选项。这个集群管理器的理念是将物理资源聚合为单个虚拟资源，该资源充当集群并提供节点级抽象。它是一个设计上的分布式集群管理器。'
- en: '**Hadoop YARN**: This cluster manager is specific to Hadoop. This is also a
    distributed framework by nature.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop YARN**：这个集群管理器是针对Hadoop的。它本质上也是一个分布式框架。'
- en: '**Kubernetes**: This is more in the experimental phase. The purpose of this
    cluster manager is to automate the deployment and scaling of the containerized
    applications. The latest release of Apache Spark includes the Kubernetes scheduler.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes**：这更多处于实验阶段。这个集群管理器的目的是自动化容器化应用的部署和扩展。Apache Spark的最新版本包括了Kubernetes调度器。'
- en: Before concluding this section, it is worth mentioning another framework, **Dask**,
    which is an open source library written in Python for parallel computing. The
    Dask framework works directly with distributed hardware platforms such as Hadoop.
    The Dask framework utilizes industry-proven libraries and Python projects such
    as NumPy, pandas, and scikit-learn. Dask is a small and lightweight framework
    compared to Apache Spark and can handle small to medium-sized clusters. In comparison,
    Apache Spark supports multiple languages and is the most appropriate choice for
    large-scale clusters.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本节之前，值得提及其他一个框架，**Dask**，这是一个用 Python 编写的开源库，用于并行计算。Dask 框架可以直接与分布式硬件平台如
    Hadoop 一起工作。与 Apache Spark 相比，Dask 是一个更小、更轻量级的框架，可以处理从小型到中型规模的集群。相比之下，Apache Spark
    支持多种语言，并且是大型集群的最合适选择。
- en: After introducing the cluster options for parallel computing, we will discuss
    in the next section the core data structure of Apache Spark, which is the RDD.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍并行计算的集群选项之后，我们将在下一节讨论 Apache Spark 的核心数据结构，即 RDD。
- en: Introducing RDDs
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 RDD
- en: The RDD is the core data structure in Apache Spark. This data structure is not
    only a distributed collection of objects but is also partitioned in such a way
    that each dataset can be processed and computed on different nodes of a cluster.
    This makes the RDD a core element of distributed data processing. Moreover, an
    RDD object is resilient in the sense that it is fault-tolerant and the framework
    can rebuild the data in the case of a failure. When we create an RDD object, the
    master node replicates the RDD object to multiple executors or worker nodes. If
    any executor process or worker node fails, the master node detects the failure
    and enables an executor process on another node to take over the execution. The
    new executor node will already have a copy of the RDD object, and it can start
    the execution immediately. Any data processed by the original executor node before
    failing will be lost data that will be computed again by the new executor node.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 是 Apache Spark 的核心数据结构。这种数据结构不仅是一个分布式的对象集合，而且是以一种方式分区，使得每个数据集都可以在集群的不同节点上处理和计算。这使得
    RDD 成为分布式数据处理的核心元素。此外，RDD 对象具有容错性，即在发生故障的情况下，框架可以重建数据。当我们创建 RDD 对象时，主节点会将 RDD
    对象复制到多个执行器或工作节点。如果任何执行器进程或工作节点失败，主节点会检测到故障，并在另一个节点上启用执行器进程以接管执行。新的执行器节点将已经拥有 RDD
    对象的副本，并且可以立即开始执行。在原始执行器节点失败之前处理的所有数据将丢失，将由新的执行器节点重新计算。
- en: In the next subsections, we will learn about two key RDD operations and how
    to create RDD objects from different data sources.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将学习两个关键的 RDD 操作以及如何从不同的数据源创建 RDD 对象。
- en: Learning RDD operations
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习 RDD 操作
- en: An RDD is an immutable object, which means once it is created, it cannot be
    altered. But two types of operations can be performed on the data of an RDD. These
    are **transformations** and **actions**. These operations are described next.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 是一个不可变对象，这意味着一旦创建，就不能更改。但可以对 RDD 的数据进行两种类型的操作。这些是**转换**和**动作**。以下将描述这些操作。
- en: Transformations
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换
- en: These operations are applied on an RDD object and result in creating a new RDD
    object. This type of operation takes an RDD as input and produces one or more
    RDDs as an output. We also need to remember that these transformations are lazy
    in nature. This means they will only be executed when an action is triggered on
    them, which is another type of operation. To explain the concept of lazy evaluation,
    we can assume that we are transforming numeric data in an RDD by subtracting 1
    from each element and then adding arithmetically (the action) all elements to
    the output RDD from the transformation step. Because of the lazy evaluation, the
    transformation operation will not happen until we call the action operation (the
    addition, in this case).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作应用于 RDD 对象，并导致创建一个新的 RDD 对象。这种类型的操作以 RDD 作为输入，并产生一个或多个 RDD 作为输出。我们还需要记住，这些转换本质上是惰性的。这意味着它们只有在触发动作操作（另一种类型的操作）时才会执行。为了解释惰性评估的概念，我们可以假设我们通过从
    RDD 中的每个元素减去 1 然后对输出 RDD 中的转换步骤进行算术加法（动作）来转换 RDD 中的数值数据。由于惰性评估，转换操作将不会发生，直到我们调用动作操作（在这种情况下是加法）。
- en: 'There are several built-in transformation functions available with Apache Spark.
    The commonly used transformation functions are as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark提供了几个内置的转换函数。常用的转换函数如下：
- en: '`map`: The `map` function iterates every element or line of an RDD object and
    applies the defined `map` function for each element.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map`：`map`函数遍历RDD对象的每个元素或每行，并对每个元素应用定义的`map`函数。'
- en: '`filter`: This function will filter the data from the original RDD and provide
    a new RDD with the filtered results.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter`：这个函数将从原始RDD中过滤数据，并提供一个包含过滤结果的新RDD。'
- en: '`union`: This function is applied to two RDDs if they are of the same type
    and results in producing another RDD that is a union of the input RDDs.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`union`：如果两个RDD类型相同，则应用此函数，结果生成另一个RDD，它是输入RDD的并集。'
- en: Actions
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 操作
- en: 'Actions are computational operations applied on an RDD and the results of such
    operations are to be returned to the driver program (for example, `SparkSession`).
    There are several built-in action functions available with Apache Spark. The commonly
    used action functions are as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 操作是在RDD上应用的计算操作，这些操作的输出结果将返回给驱动程序（例如，`SparkSession`）。Apache Spark提供了几个内置的操作函数。常用的操作函数如下：
- en: '`count`: The `count` action returns the number of elements in an RDD.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count`：`count`操作返回RDD中的元素数量。'
- en: '`collect`: This action returns the entire RDD to the driver program.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collect`：这个操作将整个RDD返回给驱动程序。'
- en: '`reduce`: This action will reduce the elements from an RDD. A simple example
    is an addition operation on an RDD dataset.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce`：这个操作将从RDD中减少元素。一个简单的例子是对RDD数据集进行加法操作。'
- en: For a complete list of transformation and action functions, we suggest you check
    the official documentation of Apache Spark. Next, we will study how to create
    RDDs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于转换和操作函数的完整列表，我们建议您查看Apache Spark的官方文档。接下来，我们将研究如何创建RDD。
- en: Creating RDD objects
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建RDD对象
- en: There are three main approaches to create RDD objects, which are described next.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 创建RDD对象有三种主要方法，下面将逐一描述。
- en: Parallelizing a collection
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并行化集合
- en: This is one of the more simple approaches used in Apache Spark to create RDDs.
    In this approach, a collection is created or loaded into a program and then passed
    to the `parallelize` method of the `SparkContext` object. This approach is not
    used beyond development and testing. This is because it requires an entire dataset
    to be available on one machine, which is not convenient for a large amount of
    data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Apache Spark创建RDD中使用的一种更简单的方法。在这种方法中，创建或加载一个集合到程序中，然后将其传递给`SparkContext`对象的`parallelize`方法。这种方法仅用于开发和测试，因为它要求整个数据集都存储在一台机器上，这对大量数据来说并不方便。
- en: External datasets
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 外部数据集
- en: Apache Spark supports distributed datasets from a local filesystem, HDFS, HBase,
    or even Amazon S3\. In this approach of creating RDDs, the data is loaded directly
    from an external data source. There are convenient methods available with the
    `SparkContext` object that can be used to load all sorts of data into RDDs. For
    example, the `textFile` method can be used to load text data from local or remote
    resources using an appropriate URL (for example, `file://`, `hdfs://`, or `s3n://`).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark支持从本地文件系统、HDFS、HBase或甚至Amazon S3等分布式数据集。在这种创建RDD的方法中，数据直接从外部数据源加载。`SparkContext`对象提供了方便的方法，可以将各种数据加载到RDD中。例如，可以使用`textFile`方法从本地或远程资源加载文本数据，使用适当的URL（例如，`file://`、`hdfs://`或`s3n://`）。
- en: From existing RDDs
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从现有RDD中
- en: As discussed previously, RDDs can be created using transformation operations.
    This is one of the differentiators of Apache Spark from Hadoop MapReduce. The
    input RDD is not changed as it is an immutable object, but new RDDs can be created
    from existing RDDs. We have already seen some examples of how to create RDDs from
    existing RDDs using the `map` and `filter` functions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，可以使用转换操作来创建RDD。这是Apache Spark与Hadoop MapReduce的不同之处之一。输入RDD不会改变，因为它是一个不可变对象，但可以从现有的RDD创建新的RDD。我们已经看到了一些使用`map`和`filter`函数从现有RDD创建RDD的例子。
- en: This concludes our introduction of RDDs. In the next section, we will provide
    further details with Python code examples using the PySpark library.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对RDD的介绍。在下一节中，我们将使用Python代码示例和PySpark库提供更多细节。
- en: Using PySpark for parallel data processing
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PySpark进行并行数据处理
- en: As discussed previously, Apache Spark is written in Scala language, which means
    there is no native support for Python. There is a large community of data scientists
    and analytics experts who prefer to use Python for data processing because of
    the rich set of libraries available with Python. Hence, it is not convenient to
    switch to using another programming language only for distributed data processing.
    Thus, integrating Python with Apache Spark is not only beneficial for the data
    science community but also opens the doors for many others who would like to adopt
    Apache Spark without learning or switching to a new programming language.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Apache Spark是用Scala语言编写的，这意味着它没有对Python的原生支持。由于Python拥有丰富的库集，许多数据科学家和分析专家更喜欢使用Python进行数据处理。因此，仅为了分布式数据处理而切换到另一种编程语言并不方便。因此，将Python与Apache
    Spark集成不仅对数据科学社区有益，也为那些希望采用Apache Spark而无需学习或切换到新编程语言的人打开了大门。
- en: The Apache Spark community has built a Python library, **PySpark**, to facilitate
    working with Apache Spark using Python. To make the Python code work with Apache
    Spark, which is built on Scala (and Java), a Java library, **Py4J**, has been
    developed. This Py4J library is bundled with PySpark and allows the Python code
    to interact with JVM objects. This is the reason that when we install PySpark,
    we need to have JVM installed on our system first.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark社区构建了一个Python库，**PySpark**，以促进使用Python与Apache Spark协同工作。为了使Python代码与建立在Scala（和Java）之上的Apache
    Spark协同工作，开发了一个Java库，**Py4J**。这个Py4J库与PySpark捆绑在一起，允许Python代码与JVM对象交互。这就是为什么当我们安装PySpark时，我们首先需要在我们的系统上安装JVM。
- en: PySpark offers almost the same features and advantages as Apache Spark. These
    include in-memory computation, the ability to parallelize workloads, the use of
    the lazy evaluation design pattern, and support for multiple cluster managers
    such as Spark, YARN, and Mesos.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark提供了几乎与Apache Spark相同的功能和优势。这包括内存计算、并行化工作负载的能力、使用延迟评估设计模式，以及支持Spark、YARN和Mesos等多达多个集群管理器。
- en: Installing PySpark (and Apache Spark) is beyond the scope of this chapter. The
    focus of this chapter is to discuss the use of PySpark to utilize the power of
    Apache Spark and not how to install Apache Spark and PySpark. But it is worth
    mentioning some installation options and dependencies.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 安装PySpark（以及Apache Spark）超出了本章的范围。本章的重点是讨论如何使用PySpark来利用Apache Spark的力量，而不是如何安装Apache
    Spark和PySpark。但值得提及一些安装选项和依赖关系。
- en: 'There are many installation guides available online for each version of Apache
    Spark/PySpark and the various target platforms (for example Linux, macOS, and
    Windows). PySpark is included in the official release of Apache Spark, which can
    now be downloaded from the Apache Spark website ([https://spark.apache.org/](https://spark.apache.org/)).
    PySpark is also available via the `pip` utility from PyPI, which can be used for
    a local setup or to connect to a remote cluster. Another option when installing
    PySpark is using **Anaconda**, which is another popular package and environment
    management system. If we are installing PySpark along with Apache Spark, we need
    the following to be available or installed on the target machine:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在线有针对Apache Spark/PySpark每个版本以及各种目标平台（例如Linux、macOS和Windows）的许多安装指南。PySpark包含在Apache
    Spark的官方版本中，现在可以从Apache Spark网站下载（[https://spark.apache.org/](https://spark.apache.org/)）。PySpark也可以通过PyPI上的`pip`工具获得，可用于本地设置或连接到远程集群。安装PySpark时的另一个选项是使用**Anaconda**，这是另一个流行的包和环境管理系统。如果我们要在目标机器上安装PySpark和Apache
    Spark，我们需要以下内容可用或已安装：
- en: JVM
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JVM
- en: Scala
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala
- en: Apache Spark
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark
- en: 'For the code examples that will be discussed later, we have installed Apache
    Spark version 3.1.1 on macOS with PySpark included. PySpark comes with the `SparkSession`
    and `SparkContext` objects automatically, which can be used to interact with the
    core Apache Spark engine. The following figure shows the initialization of the
    PySpark shell:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于后面将要讨论的代码示例，我们在macOS上安装了包含PySpark的Apache Spark版本3.1.1。PySpark自带`SparkSession`和`SparkContext`对象，可以用来与Apache
    Spark的核心引擎交互。以下图显示了PySpark shell的初始化：
- en: '![Figure 8.5 – PySpark shell'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.5 – PySpark shell'
- en: '](img/B17189_08_05.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17189_08_05.jpg]'
- en: Figure 8.5 – PySpark shell
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – PySpark shell
- en: 'From the initialization steps of the PySpark shell, we can observe the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从PySpark shell的初始化步骤中，我们可以观察到以下内容：
- en: The `SparkContext` object is already created, and its instance is available
    in the shell as `sc`.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SparkContext`对象已经创建，并且其实例在shell中作为`sc`可用。'
- en: The `SparkSession` object is also created, and its instance is available as
    `spark`. Now, `SparkSession` is an entry point to the PySpark framework to dynamically
    create RDD and DataFrame objects. The SparkSession object can also be created
    programmatically, and we will discuss this later with a code example.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SparkSession`对象也被创建，其实例作为`spark`可用。现在，`SparkSession`是PySpark框架的入口点，可以动态创建RDD和DataFrame对象。`SparkSession`对象也可以通过编程方式创建，我们将在后面的代码示例中讨论这一点。'
- en: Apache Spark comes with a web UI and a web server to host the web UI, and it
    is initiated at `http://192.168.1.110:4040` for our local machine installation.
    Note that the IP address mentioned in this URL is a private address that is specific
    to our machine. Port `4040` is selected as the default port by Apache Spark. If
    this port is in use, Apache Spark will try to host on the next available port,
    such as `4041` or `4042`.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark自带一个Web UI和一个Web服务器来托管Web UI，并且在我们的本地机器安装中，它通过`http://192.168.1.110:4040`启动。请注意，此URL中提到的IP地址是一个特定于我们机器的私有地址。端口`4040`是由Apache
    Spark选定的默认端口。如果此端口已被占用，Apache Spark将尝试在下一个可用的端口上托管，例如`4041`或`4042`。
- en: In the next subsections, we will learn how to create `SparkSession` objects,
    explore PySpark for RDD operations, and learn how to use PySpark DataFrames and
    PySpark SQL. We will start with creating a Spark session using Python.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将学习如何创建`SparkSession`对象，探索PySpark的RDD操作，以及学习如何使用PySpark DataFrame和PySpark
    SQL。我们将从使用Python创建Spark会话开始。
- en: Creating SparkSession and SparkContext programs
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建SparkSession和SparkContext程序
- en: 'Prior to Spark release 2.0, `SparkContext` was used as an entry point to PySpark.
    Since Spark release 2.0, `SparkSession` has been introduced as an entry point
    to the PySpark underlying framework to work with RDDs and DataFrames. `SparkSession`
    also includes all the APIs available in `SparkContext`, `SQLContext`, `StreamingContext`,
    and `HiveContext`. Now, `SparkSession` can also be created using the `SparkSession`
    class by using its `builder` method, which is illustrated in the next code example:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0版本发布之前，`SparkContext`被用作PySpark的入口点。自Spark 2.0版本发布以来，`SparkSession`已被引入作为PySpark底层框架的入口点，用于处理RDD和DataFrame。`SparkSession`还包括`SparkContext`、`SQLContext`、`StreamingContext`和`HiveContext`中可用的所有API。现在，`SparkSession`也可以通过使用其`builder`方法通过`SparkSession`类来创建。这在下一个代码示例中进行了说明：
- en: '[PRE0]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When we run this code in the PySpark shell, which already has a default `SparkSession`
    object created as `spark`, it will return the same session as an output of this
    `builder` method. The following console output shows the location of the two `SparkSession`
    objects (`spark` and `spark1`), which confirms that they are pointing to the same
    `SparkSession` object:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在PySpark shell中运行此代码时，它已经创建了一个默认的`SparkSession`对象作为`spark`，它将返回与`builder`方法输出相同的会话。以下控制台输出显示了两个`SparkSession`对象（`spark`和`spark1`）的位置，这证实它们指向同一个`SparkSession`对象：
- en: '[PRE1]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A few key concepts to understand regarding the `builder` method are as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`builder`方法需要理解的一些关键概念如下：
- en: '`getOrCreate`: This method is the reason that we will get an already created
    session in the case of the PySpark shell. This method will create a new session
    if no session already exists; otherwise, it returns an already existing session.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`getOrCreate`：这是我们将在PySpark shell的情况下获得已创建会话的原因。如果没有已存在的会话，此方法将创建一个新的会话；否则，它将返回一个已存在的会话。'
- en: '`master`: If we want to create a session connected to a cluster, we will provide
    the master name, which can be instance name of the Spark, or YARN, or Mesos cluster
    manager. If we are using a locally deployed Apache Spark option, we can use `local[n]`,
    where `n` is an integer greater than zero. The `n` will determine the number of
    partitions to be created for the RDD and DataFrame. For a local setup, `n` can
    be the number of CPU cores on the system. If we set it to `local[*]`, which is
    a common practice, this will create as many worker threads as there are logical
    cores on the system.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`master`：如果我们想创建一个连接到集群的会话，我们将提供主机的名称，这可以是Spark的实例名称，或者是YARN或Mesos集群管理器。如果我们使用的是本地部署的Apache
    Spark选项，我们可以使用`local[n]`，其中`n`是一个大于零的整数。`n`将确定要为RDD和DataFrame创建的分区数量。对于本地设置，`n`可以是系统上的CPU核心数。如果我们将其设置为`local[*]`，这是一个常见的做法，这将创建与系统上逻辑核心数量相同的工人线程。'
- en: 'If a new `SparkSession` object needs to be created, we can use the `newSession`
    method, which is available at the instance level of an existing `SparkSession`
    object. A code example of creating a new `SparkSession` object is shown next:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要创建一个新的 `SparkSession` 对象，我们可以使用 `newSession` 方法，该方法在现有的 `SparkSession` 对象实例级别上可用。下面是一个创建新的
    `SparkSession` 对象的代码示例：
- en: '[PRE2]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The console output for the `spark2` object confirms that this is a different
    session than the previously created `SparkSession` objects:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark2` 对象的控制台输出确认，这不同于之前创建的 `SparkSession` 对象：'
- en: '[PRE3]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `SparkContext` object can also be created programmatically. The easiest
    way to get a `SparkContext` object from a `SparkSession` instance is by using
    the `sparkContext` attribute. There is also a `SparkConext` class in the PySpark
    library that can also be used to create a `SparkContext` object directly, which
    was a common approach prior to Spark release 2.0\.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkContext` 对象也可以通过编程方式创建。从 `SparkSession` 实例获取 `SparkContext` 对象的最简单方法是使用
    `sparkContext` 属性。PySpark 库中还有一个 `SparkConext` 类，也可以用来直接创建 `SparkContext` 对象，这在
    Spark 发布 2.0 之前是一个常见的方法。'
- en: Note
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We can have multiple `SparkSession` objects but only one `SparkContext` object
    per JVM.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以有多个 `SparkSession` 对象，但每个 JVM 只有一个 `SparkContext` 对象。
- en: 'The `SparkSession` class offers a few more useful methods and attributes that
    are summarized next:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession` 类提供了一些更多有用的方法和属性，以下将进行总结：'
- en: '`getActiveSession`: This method returns an active `SparkSession` under the
    current Spark thread.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`getActiveSession`：此方法返回当前 Spark 线程下的一个活动 `SparkSession`。'
- en: '`createDataFrame`: This method creates a DataFrame object from an RDD, a list
    of objects, or a pandas DataFrame object.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`createDataFrame`：此方法从 RDD、对象列表或 pandas DataFrame 对象创建 DataFrame 对象。'
- en: '`conf`: This attribute returns the configuration interface for a Spark session.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conf`：此属性返回 Spark 会话的配置接口。'
- en: '`catalog`: This attribute provides an interface to create, update, or query
    associated databases, functions, and tables.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`catalog`：此属性提供了一个接口来创建、更新或查询相关的数据库、函数和表。'
- en: A complete list of methods and attributes can be explored using the PySpark
    documentation for the `SparkSession` class at [https://spark.apache.org/docs/latest/api/python/reference/api/](https://spark.apache.org/docs/latest/api/python/reference/api/).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 PySpark 的 `SparkSession` 类文档中的完整方法列表和属性列表进行探索，文档地址为 [https://spark.apache.org/docs/latest/api/python/reference/api/](https://spark.apache.org/docs/latest/api/python/reference/api/)。
- en: Exploring PySpark for RDD operations
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 PySpark 用于 RDD 操作
- en: In the *Introducing RDDs* section, we covered some of the key functions and
    operations of RDDs. In this section, we will extend the discussion in the context
    of PySpark with code examples.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *介绍 RDD* 部分中，我们介绍了一些 RDD 的关键函数和操作。在本节中，我们将通过代码示例扩展 PySpark 上下文中的讨论。
- en: Creating RDDs from a Python collection and from an external file
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 Python 集合和外部文件创建 RDD
- en: 'We discussed a few ways to create RDDs in the previous section. In the following
    code examples, we will discuss how to create RDDs from an in-memory Python collection
    and from an external file resource. These two approaches are described next:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了几种创建 RDD 的方法。在下面的代码示例中，我们将讨论如何从内存中的 Python 集合和外部文件资源创建 RDD。这两种方法如下所述：
- en: To create an RDD from a Python data collection, we have a `parallelize` method
    available under the `sparkContext` instance. This method distributes the collection
    to form an RDD object. The method takes a collection as a parameter. An optional
    second parameter is available with the `parallelize` method to set the number
    of partitions to be created. By default, this method creates the partitions acocording
    to the number of cores available on the local machine or the number of cores set
    at the time of creating the `SparkSession` object.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要从 Python 数据集合创建 RDD，我们可以在 `sparkContext` 实例下使用 `parallelize` 方法。此方法将集合分发以形成一个
    RDD 对象。该方法接受一个集合作为参数。`parallelize` 方法还提供了一个可选的第二个参数，用于设置要创建的分区数。默认情况下，此方法根据本地机器上的核心数或创建
    `SparkSession` 对象时设置的核心数创建分区。
- en: To create an RDD from an external file, we will use the `textFile` method available
    under the `sparkContext` instance. The `textFile` method can load a file as an
    RDD from HDFS or from a local filesystem (to be available on all cluster nodes).
    For local system-based deployment, an absolute and/or relative path can be provided.
    It is possible to set the minimum number of partitions to be created for the RDD
    using this method.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要从外部文件创建 RDD，我们将使用在 `sparkContext` 实例下可用的 `textFile` 方法。`textFile` 方法可以从 HDFS
    或本地文件系统（在所有集群节点上可用）加载文件作为 RDD。对于基于本地系统的部署，可以提供绝对和/或相对路径。可以使用此方法设置要为 RDD 创建的最小分区数。
- en: 'Some quick sample code (`rddcreate.py`) is shown next to illustrate the exact
    syntax of the PySpark statements to be used for the creation of a new RDD:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的快速示例代码（`rddcreate.py`）展示了用于创建新 RDD 的 PySpark 语句的确切语法：
- en: '[PRE4]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that the `sample.txt` file has random text data, and its contents are not
    relevant for this code example.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`sample.txt` 文件包含随机文本数据，其内容与这个代码示例无关。
- en: RDD transformation operations with PySpark
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 PySpark 的 RDD 转换操作
- en: 'There are several built-in transformation operations available with PySpark.
    To illustrate how to implement a transformation operation such as `map` using
    PySpark, we will take a text file as an input and use the `map` function available
    with RDDs to transform it to another RDD. The sample code (`rddtranform1.py`)
    is shown next:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 提供了几个内置的转换操作。为了说明如何使用 PySpark 实现转换操作，例如 `map`，我们将以文本文件作为输入，并使用 RDD 中可用的
    `map` 函数将其转换为另一个 RDD。下面的示例代码（`rddtranform1.py`）展示了：
- en: '[PRE5]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this sample code, we applied two lambda functions with the `map` operation
    to convert the text in the RDD to lowercase and uppercase. In the end, we used
    the `collect` operation to get the contents of the RDD objects.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例代码中，我们使用 `map` 操作应用了两个 lambda 函数，将 RDD 中的文本转换为小写和大写。最后，我们使用 `collect` 操作来获取
    RDD 对象的内容。
- en: 'Another popular transformation operation is `filter`, which can be used to
    filter out some entries of data. Some example code (`rddtranform2.py`) is shown
    next that is developed to filter all the even numbers from an RDD:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的转换操作是 `filter`，它可以用来过滤掉一些数据条目。下面的示例代码（`rddtranform2.py`）展示了如何从一个 RDD 中过滤出所有偶数：
- en: '[PRE6]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When you execute this code, it will provide console output with 3, 7, 7, and
    9 as collection entries. Next, we will explore a few action examples with PySpark.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行此代码时，它将提供包含 3、7、7 和 9 作为集合条目的控制台输出。接下来，我们将探索一些使用 PySpark 的动作示例。
- en: RDD action operations with PySpark
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 PySpark 的 RDD 动作操作
- en: 'To illustrate the implementation of action operations, we will use an RDD created
    from the Python collection and then apply a few built-in action operations that
    come with the PySpark library. The sample code (`rddaction1.py`) is shown next:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明动作操作的实施，我们将使用从 Python 集合创建的 RDD，然后应用 PySpark 库中的一些内置动作操作。下面的示例代码（`rddaction1.py`）展示了：
- en: '[PRE7]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Some of the action operations used in this code example are self-explanatory
    and trivial (`count`, `max`, `min`, `count`, and `sum`). The rest of the action
    operations (non-trivial) are explained next:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中使用的某些动作操作是自解释的且简单（`count`、`max`、`min`、`count` 和 `sum`）。其余的动作操作（非简单）将在下面解释：
- en: '`glom`: This results in an RDD that is created by coalescing all data entries
    with each partition into a list.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glom`：这会创建一个 RDD，它通过将所有数据条目与每个分区合并到一个列表中而创建。'
- en: '`collect`: This method returns all the elements of an RDD as a list.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collect`：此方法返回 RDD 的所有元素作为列表。'
- en: '`reduce`: This is a generic function to apply to the RDD to reduce the number
    of elements in it. In our case, we used a lambda function to combine two elements
    into one, and so on. This results in adding all the elements in the RDD.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce`：这是一个通用的函数，可以应用于 RDD 以减少其中的元素数量。在我们的例子中，我们使用 lambda 函数将两个元素合并为一个，依此类推。这会导致将
    RDD 中的所有元素相加。'
- en: '`top(x)`: This action returns the top `x` elements in the array if the elements
    in the array are ordered.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top(x)`：如果数组中的元素是有序的，这个动作返回数组中的前 `x` 个元素。'
- en: We have covered how to create RDDs using PySpark and how to implement transformation
    and action operations on an RDD. In the next section, we will cover the PySpark
    DataFrame, which is another popular data structure used mainly for analytics.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了如何使用 PySpark 创建 RDD，以及如何在 RDD 上实现转换和动作操作。在下一节中，我们将介绍 PySpark DataFrame，这是另一个主要用于分析的热门数据结构。
- en: Learning about PySpark DataFrames
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解 PySpark DataFrame
- en: The **PySpark DataFrame** is a tabular data structure consisting of rows and
    columns, like the tables we have in a relational database and like the pandas
    DataFrame, which we introduced in [*Chapter 6*](B17189_06_Final_PG_ePub.xhtml#_idTextAnchor188)*,
    Advanced Tips and Tricks in Python*. In comparison to pandas DataFrames, the key
    difference is that PySpark DataFrame objects are distributed in the cluster, which
    means data is stored across different nodes in a cluster. The use of a DataFrame
    is mainly to process a large collection of structured or unstructured data, which
    may reach into the petabytes, in a distributed manner. Like RDDs, PySpark DataFrames
    are immutable and based on lazy evaluation, which means evaluation will be delayed
    until it needs to be done.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**PySpark DataFrame** 是一个由行和列组成的表格数据结构，类似于我们在关系型数据库中拥有的表，以及我们在[*第 6 章*](B17189_06_Final_PG_ePub.xhtml#_idTextAnchor188)“Python
    高级技巧与窍门”中介绍的 pandas DataFrame。与 pandas DataFrame 相比，关键区别在于 PySpark DataFrame 对象是在集群中分布的，这意味着数据存储在集群的不同节点上。DataFrame
    的使用主要是为了以分布式方式处理大量结构化或非结构化数据，这些数据可能达到PB级别。与 RDDs 类似，PySpark DataFrame 是不可变的，并且基于懒加载，这意味着评估将延迟到需要执行时。'
- en: We can store numeric as well as string data types in a DataFrame. The columns
    in a PySpark DataFrame cannot be empty; they must have the same data type and
    must be of the same length. Rows in a DataFrame can have data of different data
    types. Row names in a DataFrame are required to be unique.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 DataFrame 中存储数值型以及字符串数据类型。PySpark DataFrame 中的列不能为空；它们必须具有相同的数据类型，并且长度必须相同。DataFrame
    中的行可以具有不同的数据类型。DataFrame 中的行名必须是唯一的。
- en: In the next subsections, we will learn how to create a DataFrame and cover some
    key operations on DataFrames using PySpark.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将学习如何创建 DataFrame，并介绍使用 PySpark 在 DataFrame 上的一些关键操作。
- en: Creating a DataFrame object
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 DataFrame 对象
- en: 'A PySpark DataFrame can be created using one of the following sources of data:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下数据源之一创建 PySpark DataFrame：
- en: Python collections such as lists, tuples, and dictionaries.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 的集合，如列表、元组和字典。
- en: Files (CSV, XML, JSON, Parquet, and so on).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件（CSV、XML、JSON、Parquet 等）。
- en: RDDs, by using the `toDF` method or the `createDataFrame` method of PySpark.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用 PySpark 的 `toDF` 方法或 `createDataFrame` 方法。
- en: Apache Kafka streaming messages can be converted to PySpark DataFrames by using
    the `readStream` method of the `SparkSession` object.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `SparkSession` 对象的 `readStream` 方法将 Apache Kafka 流消息转换为 PySpark DataFrame。
- en: Database (for example, Hive and HBase) tables can be queried using traditional
    SQL commands and the output will be transformed into a PySpark DataFrame.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用传统的 SQL 命令查询数据库（例如 Hive 和 HBase）表，输出将被转换为 PySpark DataFrame。
- en: 'We will start creating a DataFrame from a Python collection, which is the simplest
    approach, but it is more helpful for illustration purposes. The next bit of sample
    code shows us how to create a PySpark DataFrame from a collection of employees
    data:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 Python 集合创建 DataFrame 开始，这是最简单的方法，但更有助于说明目的。下面的示例代码展示了如何从员工数据集合创建 PySpark
    DataFrame：
- en: '[PRE8]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this code example, we first created the row data as a list of employees
    and then created a schema with column names. When the schema is only a list of
    column names, the data type of each column is determined by the data, and each
    column is marked as nullable by default. A more advanced API (`StructType` or
    `StructField`) can be used to define the DataFrame schema manually, which includes
    setting the data type and marking a column as nullable or not nullable. The console
    output of this sample code is shown next, which shows the schema first and then
    the DataFrame contents as a table:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码示例中，我们首先创建了一个员工行数据列表，然后创建了一个带有列名的模式。当模式仅是一个列名列表时，每个列的数据类型由数据确定，并且每个列默认标记为可空。可以使用更高级的
    API (`StructType` 或 `StructField`) 手动定义 DataFrame 模式，这包括设置数据类型和标记列是否可空。下面是此示例代码的控制台输出，首先显示模式，然后以表格形式显示
    DataFrame 内容：
- en: '[PRE9]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the next code example, we will create a DataFrame from a CSV file. The CSV
    file will have the same entries as we used in the previous code example. In this
    sample code (`dfcreate2.py`), we also defined the schema manually by using the
    `StructType` and `StructField` objects:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个代码示例中，我们将从 CSV 文件创建 DataFrame。CSV 文件将具有与上一个代码示例中相同的条目。在此示例代码 (`dfcreate2.py`)
    中，我们还使用 `StructType` 和 `StructField` 对象手动定义了模式：
- en: '[PRE10]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The console outcome of this code will be the same as shown for the previous
    code example. The importing of JSON, text, or XML files into a DataFrame is supported
    by the `read` method using a similar syntax. The support of other data sources,
    such as RDDs and databases, is left for you to evaluate and implement as an exercise.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的控制台输出将与前面代码示例中显示的相同。使用类似语法，`read`方法支持将JSON、文本或XML文件导入DataFrame。对其他数据源的支持，如RDDs和数据库，留给你作为练习来评估和实现。
- en: Working on a PySpark DataFrame
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在PySpark DataFrame上工作
- en: 'Once we have created a DataFrame from some data, regardless of the source of
    the data, we are ready to analyze it, transform it, and take some actions on it
    to get meaningful results from it. Most of the operations supported by the PySpark
    DataFrame are similar to RDDs and pandas DataFrames. For illustration purposes,
    we will load the same data as in the previous code example into a DataFrame object
    and then perform the following operations:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从某些数据中创建了一个DataFrame，无论数据的来源如何，我们就可以准备分析它、转换它，并对其采取一些操作以从中获得有意义的成果。PySpark
    DataFrame支持的大多数操作与RDDs和pandas DataFrame类似。为了说明目的，我们将与前面的代码示例相同的数据加载到一个DataFrame对象中，然后执行以下操作：
- en: Select one or more columns from the DataFrame object using the `select` method.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`select`方法从DataFrame对象中选择一个或多个列。
- en: Replace the values in a column using a dictionary and the `replace` method.
    There are more options to replace data in a column available in the PySpark library.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用字典和`replace`方法替换列中的值。PySpark库中还有更多选项可用于替换列中的数据。
- en: Add a new column with values based on an existing column's data.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据现有列的数据添加一个新列。
- en: 'The complete sample code (`dfoperations.py`) is shown next:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的示例代码（`dfoperations.py`）如下所示：
- en: '[PRE11]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Following is the output for the preceding code example:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是前面代码示例的结果：
- en: '![Figure 8.6 – Console output of the dfoperations.py program'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.6 – dfoperations.py程序的控制台输出'
- en: '](img/B17189_08_06.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17189_08_06.jpg)'
- en: Figure 8.6 – Console output of the dfoperations.py program
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – dfoperations.py程序的控制台输出
- en: The first table shows the result of the `select` operation. The next table shows
    the result of the `replace` operation on the `gender` column and also a new column,
    `Pay Level`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张表显示了`select`操作的结果。下一张表显示了在`gender`列上执行`replace`操作的结果，以及一个新的列，`Pay Level`。
- en: There are many built-in operations available to work with PySpark DataFrames,
    and many of them are the same as we discussed for pandas DataFrames. The details
    of those operations can be explored by using the Apache Spark official documentation
    for the software release you have.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多内置操作可用于与PySpark DataFrame一起使用，其中许多与我们在pandas DataFrame中讨论的相同。这些操作的详细信息可以通过使用Apache
    Spark官方文档（针对您使用的软件版本）进行探索。
- en: There is one legitimate question that anyone would ask at this point, which
    is, *Why should we use the PySpark DataFrame when we already have pandas DataFrame
    offering the same types of operations?* The answer is very simple. PySpark offers
    distributed DataFrames, and the operations on such DataFrames are meant to be
    executed on a cluster of nodes in parallel. This makes the PySpark DataFrame's
    performance significantly better than the pandas DataFrame's.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，任何人都可能会提出一个合法的问题，那就是，*为什么我们应该使用PySpark DataFrame，当我们已经有了提供相同类型操作的pandas
    DataFrame时？* 答案非常简单。PySpark提供了分布式DataFrame，对这些DataFrame的操作旨在在节点集群上并行执行。这使得PySpark
    DataFrame的性能显著优于pandas DataFrame。
- en: 'We have seen so far that, as programmers, we are not actually having to program
    anything regarding how to delegate distributed RDDs and DataFrames to different
    executors in a standalone or distributed cluster. Our focus is only on the programming
    aspect of the data processing. Coordination and communication with a local or
    remote cluster of nodes is automatically taken care of by `SparkSession` and `SparkContext`.
    This is the beauty of Apache Spark and PySpark: letting programmers focus on solving
    the real problems instead of worrying about how workloads will be executed.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到，作为程序员，我们实际上并不需要编写任何关于如何将分布式RDDs和DataFrames委派给独立或分布式集群中不同执行器的代码。我们的重点只是数据处理方面的编程。与本地或远程节点集群的协调和通信由`SparkSession`和`SparkContext`自动处理。这正是Apache
    Spark和PySpark的美丽之处：让程序员专注于解决实际问题，而不是担心工作负载的执行方式。
- en: Introducing PySpark SQL
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍PySpark SQL
- en: Spark SQL is one of the key modules of Apache Spark; it is used for structured
    data processing and acts as a distributed SQL query engine. As you can imagine,
    Spark SQL is highly scalable, being a distributed processing engine. Usually,
    the data source for Spark SQL is a database, but SQL queries can be applied to
    temporary views, which can be built from RDDs and DataFrames.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 是 Apache Spark 的关键模块之一；它用于结构化数据处理，并充当分布式 SQL 查询引擎。正如你可以想象的那样，Spark
    SQL 具有高度的扩展性，作为一个分布式处理引擎。通常，Spark SQL 的数据源是一个数据库，但 SQL 查询可以应用于临时视图，这些视图可以从 RDDs
    和 DataFrames 中构建。
- en: 'To demonstrate using the PySpark library with Spark SQL, we will use the same
    DataFrame as in the previous sample code, using employees data to build a `TempView`
    instance for SQL queries. In our code example, we will do the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示使用 PySpark 库与 Spark SQL 的结合使用，我们将使用与前面示例代码相同的 DataFrame，使用员工数据构建一个 `TempView`
    实例以进行 SQL 查询。在我们的代码示例中，我们将执行以下操作：
- en: We will create a PySpark DataFrame for the employees data from a Python collection
    as we did for the previous code example.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将创建一个 PySpark DataFrame，用于存储来自 Python 集合的员工数据，就像我们在前面的代码示例中所做的那样。
- en: We will create a `TempView` instance from the PySpark DataFrame using the `createOrReplaceTempView`
    method.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 `createOrReplaceTempView` 方法从 PySpark DataFrame 创建一个 `TempView` 实例。
- en: Using the `sql` method of the Spark Session object, we will execute the conventional
    SQL queries on the `TempView` instance, such as querying all employee records,
    querying employees with salaries higher than 45,000, querying the count of employees
    per gender type, and using the `group by` SQL command for the `gender` column.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Spark Session 对象的 `sql` 方法，我们将在 `TempView` 实例上执行传统的 SQL 查询，例如查询所有员工记录、查询薪资高于
    45,000 的员工、查询按性别类型划分的员工数量，以及使用 `group by` SQL 命令对 `gender` 列进行分组。
- en: 'The complete code example (`sql1.py`) is as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码示例 (`sql1.py`) 如下：
- en: '[PRE12]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The console output will show the results of the three SQL queries:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台输出将显示三个 SQL 查询的结果：
- en: '[PRE13]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Spark SQL is a big topic within Apache Spark. We provided only an introduction
    to Spark SQL to show the power of using SQL commands on top of Spark data structures
    without knowing the source of the data. This concludes our discussion of the use
    of PySpark for data processing and data analysis. In the next section, we will
    discuss a couple of case studies to build some real-world applications.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 是 Apache Spark 中的一个重要主题。我们只提供了 Spark SQL 的简介，以展示在不知道数据源的情况下，在 Spark
    数据结构上使用 SQL 命令的强大功能。这标志着我们使用 PySpark 进行数据处理和数据分析的讨论结束。在下一节中，我们将讨论几个案例研究，以构建一些实际应用。
- en: Case studies of using Apache Spark and PySpark
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Spark 和 PySpark 的案例研究
- en: In previous sections, we covered the fundamental concepts and architecture of
    Apache Spark and PySpark. In this section, we will discuss two case studies for
    implementing two interesting and popular applications for Apache Spark.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们介绍了 Apache Spark 和 PySpark 的基本概念和架构。在本节中，我们将讨论两个案例研究，以实现 Apache Spark
    的两个有趣且流行的应用。
- en: Case study 1 – Pi (π) calculator on Apache Spark
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究 1 – Apache Spark 上的 Pi (π) 计算器
- en: We will calculate Pi (π) using the Apache Spark cluster that is running on our
    local machine. Pi is the area of a circle when its radius is 1\. Before discussing
    the algorithm and the driver program for this application, it is important to
    introduce the Apache Spark setup used for this case study.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用运行在我们本地机器上的 Apache Spark 集群来计算 Pi (π)。当圆的半径为 1 时，Pi 是圆的面积。在讨论此应用的算法和驱动程序之前，介绍用于此案例研究的
    Apache Spark 设置非常重要。
- en: Setting up the Apache Spark cluster
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 Apache Spark 集群
- en: In all previous code examples, we used PySpark locally installed on our machine
    without a cluster. For this case study, we will set up an Apache Spark cluster
    by using multiple virtual machines. There are many virtualization software tools
    available, such as **VirtualBox**, and any of these software tools will work for
    building this kind of setup.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有之前的代码示例中，我们都在没有集群的情况下在我们的机器上使用本地安装的 PySpark。对于这个案例研究，我们将通过使用多个虚拟机来设置一个 Apache
    Spark 集群。有许多虚拟化软件工具可用，例如 **VirtualBox**，并且这些软件工具中的任何一种都可以用于构建这种类型的设置。
- en: 'We used Ubuntu **Multipass** ([https://multipass.run/](https://multipass.run/))
    to build the virtual machines on top of macOS. Multipass works on Linux and on
    Windows as well. Multipass is a lightweight virtualization manager and is designed
    specifically for developers to create virtual machines with a single command.
    Multipass has very few commands, which makes it easier to use. If you decide to
    use Multipass, we recommend that you use the official documentation for installation
    and configuration. In our virtual machines setup, we have the following virtual
    machines created using Multipass:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Ubuntu **Multipass** ([https://multipass.run/](https://multipass.run/))在macOS上构建虚拟机。Multipass在Linux和Windows上也能工作。Multipass是一个轻量级的虚拟化管理器，专为开发者设计，用于通过单个命令创建虚拟机。Multipass命令非常少，这使得它更容易使用。如果您决定使用Multipass，我们建议您使用官方文档进行安装和配置。在我们的虚拟机设置中，我们使用Multipass创建了以下虚拟机：
- en: '![Figure 8.7 – Virtual machines created for our Apache Spark cluster'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.7 – 为我们的Apache Spark集群创建的虚拟机'
- en: '](img/B17189_08_07.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17189_08_07.jpg)'
- en: Figure 8.7 – Virtual machines created for our Apache Spark cluster
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 为我们的Apache Spark集群创建的虚拟机
- en: 'We installed *Apache Spark 3.1.1* on each virtual machine by using the `apt-get`
    utility. We started Apache Spark as the master on `vm1` and then started Apache
    Spark as the worker on `vm2` and `vm3` by providing the master Spark URI, which
    is `Spark://192.168.64.2.7077` in our case. The complete Spark cluster setup will
    look as shown here:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用`apt-get`实用程序在每个虚拟机上安装了*Apache Spark 3.1.1*。我们在`vm1`上以主节点启动Apache Spark，然后通过提供主Spark
    URI（在我们的案例中是`Spark://192.168.64.2.7077`）在`vm2`和`vm3`上以工作节点启动Apache Spark。完整的Spark集群设置将如下所示：
- en: '![Figure 8.8 – Details of the Apache Spark cluster nodes'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.8 – Apache Spark集群节点详细信息'
- en: '](img/B17189_08_08.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17189_08_08.jpg)'
- en: Figure 8.8 – Details of the Apache Spark cluster nodes
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – Apache Spark集群节点详细信息
- en: 'The web UI for the master Spark node will look as shown here:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 主Spark节点的Web UI如下所示：
- en: '![Figure 8.9 – Web UI for the master node in the Apache Spark cluster'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.9 – Apache Spark集群中主节点的Web UI'
- en: '](img/B17189_08_09.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17189_08_09.jpg)'
- en: Figure 8.9 – Web UI for the master node in the Apache Spark cluster
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – Apache Spark集群中主节点的Web UI
- en: 'A summary of the web UI for the master node is given here:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这里给出了主节点的Web UI摘要：
- en: The web UI provides the node name with the Spark URL. In our case, we used the
    IP address as the host name, which is why we have an IP address in the URL.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Web UI提供了节点名称和Spark URL。在我们的案例中，我们使用了IP地址作为主机名，这就是为什么URL中有IP地址的原因。
- en: There are the details of the worker nodes, of which there are two in our case.
    Each worker node uses 1 CPU core and 1 GB of memory.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里是工作节点的详细信息，在我们的案例中有两个。每个工作节点使用1个CPU核心和1GB内存。
- en: The web UI also provides details of the running and completed applications.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Web UI还提供了正在运行和已完成的应用程序的详细信息。
- en: 'The web UI for the worker nodes will look as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点的Web UI将如下所示：
- en: '![Figure 8.10 – Web UI for the worker nodes in the Apache Spark cluster'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.10 – Apache Spark集群中工作节点的Web UI'
- en: '](img/B17189_08_10.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17189_08_10.jpg)'
- en: Figure 8.10 – Web UI for the worker nodes in the Apache Spark cluster
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 – Apache Spark集群中工作节点的Web UI
- en: 'A summary of the web UI for the worker nodes is given here:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这里给出了工作节点的Web UI摘要：
- en: The web UI provides the worker IDs as well as the node names and the ports where
    the workers are listening for requests.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Web UI还提供了工作ID以及节点名称和工人监听请求的端口。
- en: The master node URL is also provided in the web UI.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Web UI还提供了主节点URL。
- en: Details of the CPU core and memory allocated to the worker nodes are also available.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配给工作节点的CPU核心和内存的详细信息也可用。
- en: The web UI provides details of jobs in progress (**Running Executors**) and
    jobs that are already finished.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Web UI提供了正在进行的作业（**运行中的执行器**）和已完成作业的详细信息。
- en: Writing a driver program for Pi calculation
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写Pi计算的驱动程序
- en: To calculate Pi, we are using a commonly used algorithm (the **Monte Carlo**algorithm)
    that assumes a square having an area equal to 4 that is circumscribing a unit
    circle (circle with a radius value equal to 1). The idea is to generate a huge
    amount of random numbers in the domain of a square with sides having a length
    of 2\. We can assume there is a circle inside the square with the same diameter
    value as the length of the side of the square. This means that the circle will
    be inscribed inside the square. The value of Pi is estimated by calculating the
    ratio of the number of points that lie inside the circle to the total number of
    generated points.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算 Pi，我们使用了一个常用的算法（蒙特卡洛算法），该算法假设一个面积为 4 的正方形包围着一个单位圆（半径值为 1 的圆）。想法是在一个边长为
    2 的正方形域内生成大量随机数。我们可以假设有一个直径值与正方形边长相同的圆在正方形内部。这意味着圆将内嵌在正方形内。Pi 的值是通过计算位于圆内的点数与生成的总点数之比来估计的。
- en: 'The complete sample code for the driver program is shown next. In this program,
    we decided to use two partitions as we have two workers available to us. We used
    10,000,000 points for each worker. Another important thing to note is that we
    used the Spark master node URL as a master attribute when creating the Apache
    Spark session:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例展示了驱动程序的完整代码。在这个程序中，我们决定使用两个分区，因为我们有两个可用的工作者。我们为每个工作者使用了 10,000,000 个点。另一个需要注意的重要事项是，在创建
    Apache Spark 会话时，我们使用了 Spark 主节点 URL 作为主属性：
- en: '[PRE14]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The console output is as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台输出如下：
- en: '[PRE15]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The Spark web UI will provide the status of the application when running and
    even after it completes its execution. In the following screenshot, we can see
    that two workers were engaged to complete the job:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 网页用户界面将在应用程序运行时提供其状态，甚至在执行完成后。在下面的屏幕截图中，我们可以看到有两个工作者参与了完成作业：
- en: '![Figure 8.11 – Pi calculator status in the Spark web UI'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.11 – Pi 计算器在 Spark 网页用户界面中的状态'
- en: '](img/B17189_08_11.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17189_08_11.jpg](img/B17189_08_11.jpg)'
- en: Figure 8.11 – Pi calculator status in the Spark web UI
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – Pi 计算器在 Spark 网页用户界面中的状态
- en: 'We can click on the application name to go to the next level of detail for
    the application, as shown in *Figure 8.12*. This screenshot shows which workers
    are involved in completing the tasks and what resources are being used (if things
    are still running):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以点击应用程序名称，以查看应用程序的下一级详细情况，如图 8.12 所示。这个屏幕截图显示了哪些工作者参与了完成任务以及正在使用哪些资源（如果任务仍在运行）：
- en: '![Figure 8.12 – Pi calculator application executor-level details'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.12 – Pi 计算器应用程序执行器级别的细节'
- en: '](img/B17189_08_12.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17189_08_12.jpg](img/B17189_08_12.jpg)'
- en: Figure 8.12 – Pi calculator application executor-level details
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 – Pi 计算器应用程序执行器级别的细节
- en: In this case study, we covered how we can set up an Apache Spark cluster for
    testing and experimentation purposes and how we can build a driver program in
    Python using the PySpark library to connect to Apache Spark and submit our jobs
    to be processed on two different cluster nodes.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们介绍了如何为测试和实验目的设置 Apache Spark 集群，以及如何使用 PySpark 库在 Python 中构建驱动程序程序以连接到
    Apache Spark 并提交我们的作业以在两个不同的集群节点上处理。
- en: In the next case study, we will build a word cloud using the PySpark library.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个案例研究中，我们将使用 PySpark 库构建一个词云。
- en: Case study 2 – Word cloud using PySpark
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究 2 – 使用 PySpark 的词云
- en: A **word cloud** is a visual representation of the frequency of words that appear
    in some text data. Put simply, if a specific word appears more frequently in a
    text, it appears bigger and bolder in the word cloud. These are also known as
    **tag clouds** or **text clouds** and are very useful tools to identify what parts
    of some textual data are more important. A practical use case of this tool is
    the analysis of content on social media, which has many applications for marketing,
    business analytics, and security.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**词云**是某些文本数据中出现单词频率的视觉表示。简单来说，如果一个特定的单词在文本中出现的频率更高，它在词云中就会更大、更粗。这些也被称为**标签云**或**文本云**，是识别某些文本数据哪些部分更重要非常有用的工具。这个工具的一个实际用例是分析社交媒体上的内容，这在市场营销、商业分析和安全方面有许多应用。'
- en: 'For illustration purposes, we have built a simple word cloud application that
    reads a text file from the local filesystem. The text file is imported into an
    RDD object that is then processed to count the number of times each word occurred.
    We process the data further to filter out the words that are repeated fewer than
    two times and also filter out words that are of a length that''s less than four
    letters. The word frequency data is fed to the `WordCloud` library object. To
    display the word cloud, we used the `matplotlib` library. The complete sample
    code is shown next:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明目的，我们构建了一个简单的词云应用程序，该程序从本地文件系统读取文本文件。文本文件被导入到 RDD 对象中，然后进行处理以计算每个单词出现的次数。我们进一步处理数据以过滤掉出现次数少于两次的单词，并过滤掉长度小于四个字母的单词。单词频率数据被输入到
    `WordCloud` 库对象中。为了显示词云，我们使用了 `matplotlib` 库。完整的示例代码如下：
- en: '[PRE16]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output of this program is plotted as a window application and the output
    will look as shown here, based on the sample text (`wordcloud.txt`) provided to
    the application:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序的输出以窗口应用程序的形式展示，输出结果将如以下所示，基于提供给应用程序的样本文本（`wordcloud.txt`）：
- en: '![Figure 8.13 – Word cloud built using PySpark RDDs'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.13 – 使用 PySpark RDDs 构建的词云](img/B17189_08_13.jpg)'
- en: '](img/B17189_08_13.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17189_08_13.jpg)'
- en: Figure 8.13 – Word cloud built using PySpark RDDs
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 – 使用 PySpark RDDs 构建的词云
- en: Note that we have not used a very big sample of textual data for this illustration.
    In the real world, the source data can be extremely large, which justifies processing
    using an Apache Spark cluster.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在这个示例中没有使用一个非常大的文本数据样本。在现实世界中，源数据可以极其庞大，这证明了使用 Apache Spark 集群进行处理的必要性。
- en: These two case studies have provided you with skills in using Apache Spark for
    large-scale data processing. They provide a foundation for those of you who are
    interested in the fields of **Natural Language Processing** (**NLP**), text analysis,
    and sentimental analysis. These skills are important for you if you are a data
    scientist and your day-to-day job requires data processing for analytics and building
    algorithms for NLP.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个案例研究为您提供了使用 Apache Spark 进行大规模数据处理的能力。它们为对自然语言处理（**NLP**）、文本分析和情感分析感兴趣的您提供了一个基础。如果您是数据科学家，并且您的日常工作需要数据分析以及构建
    NLP 算法，这些技能对您来说非常重要。
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored how to execute data-intensive jobs on a cluster
    of machines to achieve parallel processing. Parallel processing is important for
    large-scale data, which is also known as big data. We started by evaluating the
    different cluster options available for data processing. We provided a comparative
    analysis of Hadoop MapReduce and Apache Spark, which are the two main competing
    platforms for clusters. The analysis showed that Apache Spark has more flexibility
    in terms of supported languages and cluster management systems, and it outperforms
    Hadoop MapReduce for real-time data processing because of its in-memory data processing
    model.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何在机器集群上执行数据密集型作业以实现并行处理。并行处理对于大规模数据非常重要，也称为大数据。我们首先评估了可用于数据处理的不同集群选项。我们提供了
    Hadoop MapReduce 和 Apache Spark 的比较分析，这两个是集群的两个主要竞争平台。分析表明，Apache Spark 在支持的编程语言和集群管理系统方面具有更大的灵活性，并且由于其内存数据处理模型，它在实时数据处理方面优于
    Hadoop MapReduce。
- en: Once we had established that Apache Spark is the most appropriate choice for
    a variety of data processing applications, we started looking into its fundamental
    data structure, which is the RDD. We discussed how to create RDDs from different
    sources of data and introduced two types of operations, transformations and actions.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定 Apache Spark 是各种数据处理应用的最合适选择，我们就开始研究其基本数据结构，即 RDD。我们讨论了如何从不同的数据源创建 RDD，并介绍了两种类型的操作：转换和行动。
- en: In the core part of this chapter, we explored using PySpark to create and manage
    RDDs using Python. This included several code examples of transformation and action
    operations. We also introduced PySpark DataFrames for the next level of data processing
    in a distributed manner. We concluded the topic by introducing PySpark SQL with
    a few code examples.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的核心部分，我们探讨了使用 PySpark 通过 Python 创建和管理 RDD。这包括几个转换和行动操作的代码示例。我们还介绍了用于分布式数据处理下一级别的
    PySpark DataFrames。我们通过介绍 PySpark SQL 和一些代码示例来结束这个主题。
- en: Finally, we looked at two case studies using Apache Spark and PySpark. These
    case studies included the calculation of Pi and the building of a word cloud from
    text data. We also covered in the case studies how we can set up a Standalone
    Apache Spark instance on a local machine for testing purposes.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了两个使用Apache Spark和PySpark的案例研究。这些案例研究包括计算π和从文本数据构建词云。在案例研究中，我们还介绍了如何为测试目的在本地机器上设置一个独立的Apache
    Spark实例。
- en: This chapter gave you a lot of experience in setting up Apache Spark locally
    as well as setting up Apache Spark clusters using virtualization. There are plenty
    of code examples provided in this chapter for you to enhance your practical skills.
    This is important for anyone who wants to process their big data problems using
    clusters for efficiency and scale.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为您提供了大量在本地设置Apache Spark以及使用虚拟化设置Apache Spark集群的经验。本章提供了大量的代码示例，供您增强实践技能。这对于任何希望使用集群以提高效率和规模来处理大数据问题的人来说都很重要。
- en: In the next chapter, we will explore options for leveraging frameworks such
    as Apache Beam and extend our discussion of using public clouds for data processing.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨利用Apache Beam等框架的选项，并扩展我们关于使用公共云进行数据处理讨论。
- en: Questions
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How is Apache Spark different from Hadoop MapReduce?
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Apache Spark与Hadoop MapReduce有何不同？
- en: How are transformations different from actions in Apache Spark?
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Apache Spark中，转换与操作有何不同？
- en: What is lazy evaluation in Apache Spark?
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Apache Spark中的懒评估是什么？
- en: What is `SparkSession`?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是`SparkSession`？
- en: How is the PySpark DataFrame different from the pandas DataFrame?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PySpark DataFrame与pandas DataFrame有何不同？
- en: Further reading
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Spark in Action, Second Edition* by Jean-Georges Perrin'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由Jean-Georges Perrin所著的《*Spark in Action, 第二版*》
- en: '*Learning PySpark* by Tomasz Drabas, Denny Lee'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由Tomasz Drabas和Denny Lee所著的《*Learning PySpark*》
- en: '*PySpark Recipes* by Raju Kumar Mishra'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由Raju Kumar Mishra所著的《*PySpark Recipes*》
- en: '*Apache Spark documentation* for the release you are using ([https://spark.apache.org/docs/rel#](https://spark.apache.org/docs/rel#))'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用您正在使用的版本[https://spark.apache.org/docs/rel#](https://spark.apache.org/docs/rel#)的《*Apache
    Spark文档*》
- en: '*Multipass documentation* available at [https://multipass.run/docs](https://multipass.run/docs)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可在[https://multipass.run/docs](https://multipass.run/docs)找到的《*Multipass文档*》
- en: Answers
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: Apache Spark is an in-memory data processing engine, whereas Hadoop MapReduce
    has to read from and write to the filesystem.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Apache Spark是一个内存数据处理引擎，而Hadoop MapReduce则需要从文件系统中读取和写入。
- en: Transformation is applied to convert or translate data from one form to another,
    and the results stay within the cluster. Actions are the functions applied to
    data to get the results that are returned to the driver program.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换应用于将数据从一种形式转换为另一种形式，并且结果保留在集群内。操作是对数据应用以获取返回给驱动程序的函数的结果。
- en: Lazy evaluation is applied mainly for transformation operations, which means
    transformation operations are not executed until an action is triggered on a data
    object.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 懒评估主要应用于转换操作，这意味着转换操作在数据对象上触发操作之前不会执行。
- en: '`SparkSession` is an entry point to the Spark application to connect to one
    or more cluster managers and to work with executors for task execution.'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SparkSession`是Spark应用程序的入口点，用于连接一个或多个集群管理器，并与执行器协同工作以执行任务。'
- en: The PySpark DataFrame is distributed and is meant to be available on multiple
    nodes of an Apache Spark cluster for parallel processing.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PySpark DataFrame是分布式的，旨在在Apache Spark集群的多个节点上可用，以便进行并行处理。
