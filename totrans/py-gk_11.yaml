- en: '*Chapter 8*: Scaling out Python Using Clusters'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed parallel processing for a single machine
    using threads and processes. In this chapter, we will extend our discussion of
    parallel processing from a single machine to multiple machines in a cluster. A
    cluster is a group of computing devices that work together to perform compute-intensive
    tasks such as data processing. In particular, we will study Python's capabilities
    in the area of data-intensive computing. Data-intensive computing typically uses
    clusters for processing large volumes of data in parallel. Although there are
    quite a few frameworks and tools available for data-intensive computing, we will
    focus on **Apache Spark** as a data processing engine and PySpark as a Python
    library to build such applications.
  prefs: []
  type: TYPE_NORMAL
- en: If Apache Spark with Python is properly configured and implemented, the performance
    of your application can increase manyfold and surpass competitor platforms such
    as **Hadoop MapReduce**. We will also look into how distributed datasets are utilized
    in a clustered environment. This chapter will help you to understand the use of
    cluster computing platforms for large-scale data processing and how to implement
    data processing applications using Python. To illustrate the practical use of
    Python for applications with cluster computing requirements, we will include two
    case studies; the first one is to compute the value of Pi (π) and the second one
    is to generate a word cloud from a data file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning about the cluster options for parallel processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing **resilient distributed datasets** (**RDD**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using PySpark for parallel data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case studies of using Apache Spark and PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will know how to work with Apache Spark and
    how you can write Python applications for data processing that can be executed
    on the worker nodes of an Apache Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.7 or later installed on your computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Apache Spark single-node cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PySpark installed on top of Python 3.7 or later for driver program development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Python version used with Apache Spark has to match the Python version that
    is used to run the driver program.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The sample code for this chapter can be found at [https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter08](https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: We will start our discussion by looking at the cluster options available for
    parallel processing in general.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about the cluster options for parallel processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we have a large volume of data to process, it is not efficient and sometimes
    even not feasible to use a single machine with multiple cores to process the data
    efficiently. This is especially a challenge when working with real-time streaming
    data. For such scenarios, we need multiple systems that can process data in a
    distributed manner and perform these tasks on multiple machines in parallel. Using
    multiple machines to process compute-intensive tasks in parallel and in a distributed
    manner is called **cluster computing**. There are several big data distributed
    frameworks available to coordinate the execution of jobs in a cluster, but Hadoop
    MapReduce and Apache Spark are the leading contenders in this race. Both frameworks
    are open source projects from Apache. There are many variants (for example, Databricks)
    of these two platforms available with add-on features as well as maintenance support,
    but the fundamentals remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the market, the number of Hadoop MapReduce deployments may be
    higher compared to Apache Spark, but with its increasing popularity, Apache Spark
    is going to turn the tables eventually. Since Hadoop MapReduce is still very relevant
    due to its large install base, it is important to discuss what exactly Hadoop
    MapReduce is and how Apache Spark is becoming a better choice. Let's have a quick
    overview of the two in the next subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop MapReduce
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hadoop is a general-purpose distributed processing framework that offers the
    execution of large-scale data processing jobs across hundreds or thousands of
    computing nodes in a Hadoop cluster. The three core components of Hadoop are included
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Apache Hadoop MapReduce ecosystem'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_08_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Apache Hadoop MapReduce ecosystem
  prefs: []
  type: TYPE_NORMAL
- en: 'The three core components are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hadoop Distributed File System (HDFS)**: This is a Hadoop-native filesystem
    used to stores files such that those files can be parallelized across a cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Yet Another Resource Negotiator (YARN)**: This is a system that processes
    data stored in HDFS and schedules the submitted jobs (for data processing) to
    be run by a processing system. The processing systems can be used for graph processing,
    stream processing, or batch processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map`) and reducer (`reduce`) functions are the same as we discussed in [*Chapter
    6*](B17189_06_Final_PG_ePub.xhtml#_idTextAnchor188), *Advanced Tips and Tricks
    in Python*. The key difference is that we use many `map` and `reduce` functions
    in parallel to process several datasets at the same time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After breaking the large dataset into small datasets, we can provide the small
    datasets as input to many mapper functions for processing on different nodes of
    a Hadoop cluster. Each mapper function takes one set of data as an input, processes
    the data based on the goal set by the programmer, and produces the output as key-value
    pairs. Once the output of all the small datasets is available, one or multiple
    reducer functions will take the output from the mapper functions and aggregate
    the results as per the goals of the reducer functions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To explain it in a bit more detail, we can take an example of counting particular
    words such as *attack* and *weapon* in a large source of text data. The text data
    can be divided into small datasets, for example, eight datasets. We can have eight
    mapper functions that count the two words within the dataset provided to them.
    Each mapper function provides us with the count of the words *attack* and *weapon*
    as an output for the dataset provided to it. In the next phase, the outputs of
    all the mapper functions are provided to two reducer functions, one for each word.
    Each reducer function aggregates the count for each word and provides the aggregated
    results as an output. The operating of the MapReduce framework for this word count
    example is shown next. Note that the mapper function is typically implemented
    as `map` and the reducer function as `reduce` in Python programming:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Working of the MapReduce framework'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_08_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – Working of the MapReduce framework
  prefs: []
  type: TYPE_NORMAL
- en: We will skip the next levels of Hadoop components as they are not relevant to
    our discussion in this chapter. Hadoop is built mainly in Java, but any programming
    language, such as Python, can be used to write custom mapper and reducer components
    for the MapReduce module.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop MapReduce is good for processing a large chunk of data by breaking it
    into small blocks. The cluster nodes process these blocks separately and then
    the results are aggregated before being sent to the requester. Hadoop MapReduce
    processes the data from a filesystem and thus is not very efficient in terms of
    performance. However, it works very well if the speed of processing is not a critical
    requirement, for instance, if data processing can be scheduled to occur at night.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apache Spark is an open source cluster computing framework for real-time as
    well as batch data processing. The main feature of Apache Spark is that it is
    an in-memory data processing framework, which makes it efficient in terms of achieving
    low latency and makes it suitable for many real-world scenarios because of the
    following additional factors:'
  prefs: []
  type: TYPE_NORMAL
- en: It gets results quickly for mission-critical and time-sensitive applications
    such as real-time or near real-time scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's good for performing tasks repeatedly or iteratively in an efficient way
    due to in-memory processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can utilize out-of-the-box machine learning algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can leverage the support of additional programming languages such as Java,
    Python, Scala, and R.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In fact, Apache Spark covers a wide range of workloads, including batch data,
    iterative processing, and streaming data. The beauty of Apache Spark is that it
    can use Hadoop (via YARN) as a deployment cluster as well, but it has its own
    cluster manager as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, the main components of Apache Spark are segregated into three
    layers, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Apache Spark ecosystem'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_08_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Apache Spark ecosystem
  prefs: []
  type: TYPE_NORMAL
- en: These layers are discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Support languages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scala is a native language of Apache Spark, so it is quite popular for development.
    Apache Spark also provides high-level APIs for Java, Python, and R. In Apache
    Spark, multi-language support is provided by using the **Remote Procedure Call**
    (**RPC**) interface. There is an RPC adapter written for each language in Scala
    that transforms the client requests written in a different language to the native
    Scala requests. This makes its adoption easier across the development community.
  prefs: []
  type: TYPE_NORMAL
- en: Core components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A brief overview of each of the core components is discussed next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark Core and RDDs**: Spark Core is a core engine of Spark and is responsible
    for providing abstraction to RDDs, scheduling and distributing jobs to a cluster,
    interacting with storage systems such as HDFS, Amazon S3, or an RDBMS, and managing
    memory and fault recoveries. An RDD is a resilient distributed dataset that is
    an immutable and distributable collection of data. RDDs are partitioned to be
    executed on the different nodes of a cluster. We will discuss RDDs in more detail
    in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark SQL**: This module is for querying data stored both in RDDs and in
    external data sources using abstracted interfaces. Using these common interfaces
    enables the developers to mix the SQL commands with the analytics tools for a
    given application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark Streaming**: This module is used to process real-time data, which is
    critical to analyze live data streams with low latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLlib**: The **Machine Learning Library** (**MLlib**) is used to apply machine
    learning algorithms in Apache Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GraphX**: This module provides an API for graph-based parallel computing.
    This module comes with a variety of graph algorithms. Note that a graph is a mathematical
    concept based on vertices and edges that represents how a set of objects are related
    or dependent on each other. The objects are represented by vertices and their
    relationships by the edges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Apache Spark supports a few cluster managers, such as Standalone, Mesos, YARN,
    and Kubernetes. The key function of a cluster manager is to schedule and execute
    the jobs on cluster nodes and manage the resources on cluster nodes. But to interact
    with one or more cluster managers, there is a special object used in the main
    or driver program called `SparkContext` object was considered as an entry point,
    but its API is now wrapped as part of the `SparkSession` object. Conceptually,
    the following figure shows the interaction of a `SparkSession` (**SparkContext**),
    and the **worker nodes** in a cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Apache Spark ecosystem](img/B17189_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Apache Spark ecosystem
  prefs: []
  type: TYPE_NORMAL
- en: The `SparkSession` object can connect to different types of cluster managers.
    Once connected, the executors are acquired on the cluster nodes through the cluster
    managers. The executors are the Spark processes that run the jobs and store the
    computational job results. The cluster manager on a master node is responsible
    for sending the application code to the executor processes on the worker nodes.
    Once the application code and data (if applicable) are moved to the worker nodes,
    the `SparkSession` object in a driver program interacts directly with executor
    processes for the execution of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'As per Apache Spark release 3.1, the following cluster managers are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standalone**: This is a simple cluster manager that is included as part of
    the Spark Core engine. The Standalone cluster is based on master and worker (or
    slave) processes. A master process is basically a cluster manager, and the worker
    processes host the executors. Although the masters and the workers can be hosted
    on a single machine, this is not the real deployment scenario of a Spark Standalone
    cluster. It is recommended to distribute workers to different machines for the
    best outcome. The Standalone cluster is easy to set up and provides most of the
    features required from a cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Mesos**: This is another general-purpose cluster manager that can
    also run Hadoop MapReduce. For large-scale cluster environments, Apache Mesos
    is the preferred option. The idea of this cluster manager is that it aggregates
    the physical resources into a single virtual resource that acts as a cluster and
    provides a node-level abstraction. It is a distributed cluster manager by design.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop YARN**: This cluster manager is specific to Hadoop. This is also a
    distributed framework by nature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes**: This is more in the experimental phase. The purpose of this
    cluster manager is to automate the deployment and scaling of the containerized
    applications. The latest release of Apache Spark includes the Kubernetes scheduler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before concluding this section, it is worth mentioning another framework, **Dask**,
    which is an open source library written in Python for parallel computing. The
    Dask framework works directly with distributed hardware platforms such as Hadoop.
    The Dask framework utilizes industry-proven libraries and Python projects such
    as NumPy, pandas, and scikit-learn. Dask is a small and lightweight framework
    compared to Apache Spark and can handle small to medium-sized clusters. In comparison,
    Apache Spark supports multiple languages and is the most appropriate choice for
    large-scale clusters.
  prefs: []
  type: TYPE_NORMAL
- en: After introducing the cluster options for parallel computing, we will discuss
    in the next section the core data structure of Apache Spark, which is the RDD.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The RDD is the core data structure in Apache Spark. This data structure is not
    only a distributed collection of objects but is also partitioned in such a way
    that each dataset can be processed and computed on different nodes of a cluster.
    This makes the RDD a core element of distributed data processing. Moreover, an
    RDD object is resilient in the sense that it is fault-tolerant and the framework
    can rebuild the data in the case of a failure. When we create an RDD object, the
    master node replicates the RDD object to multiple executors or worker nodes. If
    any executor process or worker node fails, the master node detects the failure
    and enables an executor process on another node to take over the execution. The
    new executor node will already have a copy of the RDD object, and it can start
    the execution immediately. Any data processed by the original executor node before
    failing will be lost data that will be computed again by the new executor node.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsections, we will learn about two key RDD operations and how
    to create RDD objects from different data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Learning RDD operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An RDD is an immutable object, which means once it is created, it cannot be
    altered. But two types of operations can be performed on the data of an RDD. These
    are **transformations** and **actions**. These operations are described next.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These operations are applied on an RDD object and result in creating a new RDD
    object. This type of operation takes an RDD as input and produces one or more
    RDDs as an output. We also need to remember that these transformations are lazy
    in nature. This means they will only be executed when an action is triggered on
    them, which is another type of operation. To explain the concept of lazy evaluation,
    we can assume that we are transforming numeric data in an RDD by subtracting 1
    from each element and then adding arithmetically (the action) all elements to
    the output RDD from the transformation step. Because of the lazy evaluation, the
    transformation operation will not happen until we call the action operation (the
    addition, in this case).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several built-in transformation functions available with Apache Spark.
    The commonly used transformation functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`map`: The `map` function iterates every element or line of an RDD object and
    applies the defined `map` function for each element.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter`: This function will filter the data from the original RDD and provide
    a new RDD with the filtered results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`union`: This function is applied to two RDDs if they are of the same type
    and results in producing another RDD that is a union of the input RDDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Actions are computational operations applied on an RDD and the results of such
    operations are to be returned to the driver program (for example, `SparkSession`).
    There are several built-in action functions available with Apache Spark. The commonly
    used action functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`count`: The `count` action returns the number of elements in an RDD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collect`: This action returns the entire RDD to the driver program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduce`: This action will reduce the elements from an RDD. A simple example
    is an addition operation on an RDD dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a complete list of transformation and action functions, we suggest you check
    the official documentation of Apache Spark. Next, we will study how to create
    RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDD objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are three main approaches to create RDD objects, which are described next.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing a collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is one of the more simple approaches used in Apache Spark to create RDDs.
    In this approach, a collection is created or loaded into a program and then passed
    to the `parallelize` method of the `SparkContext` object. This approach is not
    used beyond development and testing. This is because it requires an entire dataset
    to be available on one machine, which is not convenient for a large amount of
    data.
  prefs: []
  type: TYPE_NORMAL
- en: External datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache Spark supports distributed datasets from a local filesystem, HDFS, HBase,
    or even Amazon S3\. In this approach of creating RDDs, the data is loaded directly
    from an external data source. There are convenient methods available with the
    `SparkContext` object that can be used to load all sorts of data into RDDs. For
    example, the `textFile` method can be used to load text data from local or remote
    resources using an appropriate URL (for example, `file://`, `hdfs://`, or `s3n://`).
  prefs: []
  type: TYPE_NORMAL
- en: From existing RDDs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed previously, RDDs can be created using transformation operations.
    This is one of the differentiators of Apache Spark from Hadoop MapReduce. The
    input RDD is not changed as it is an immutable object, but new RDDs can be created
    from existing RDDs. We have already seen some examples of how to create RDDs from
    existing RDDs using the `map` and `filter` functions.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our introduction of RDDs. In the next section, we will provide
    further details with Python code examples using the PySpark library.
  prefs: []
  type: TYPE_NORMAL
- en: Using PySpark for parallel data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed previously, Apache Spark is written in Scala language, which means
    there is no native support for Python. There is a large community of data scientists
    and analytics experts who prefer to use Python for data processing because of
    the rich set of libraries available with Python. Hence, it is not convenient to
    switch to using another programming language only for distributed data processing.
    Thus, integrating Python with Apache Spark is not only beneficial for the data
    science community but also opens the doors for many others who would like to adopt
    Apache Spark without learning or switching to a new programming language.
  prefs: []
  type: TYPE_NORMAL
- en: The Apache Spark community has built a Python library, **PySpark**, to facilitate
    working with Apache Spark using Python. To make the Python code work with Apache
    Spark, which is built on Scala (and Java), a Java library, **Py4J**, has been
    developed. This Py4J library is bundled with PySpark and allows the Python code
    to interact with JVM objects. This is the reason that when we install PySpark,
    we need to have JVM installed on our system first.
  prefs: []
  type: TYPE_NORMAL
- en: PySpark offers almost the same features and advantages as Apache Spark. These
    include in-memory computation, the ability to parallelize workloads, the use of
    the lazy evaluation design pattern, and support for multiple cluster managers
    such as Spark, YARN, and Mesos.
  prefs: []
  type: TYPE_NORMAL
- en: Installing PySpark (and Apache Spark) is beyond the scope of this chapter. The
    focus of this chapter is to discuss the use of PySpark to utilize the power of
    Apache Spark and not how to install Apache Spark and PySpark. But it is worth
    mentioning some installation options and dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many installation guides available online for each version of Apache
    Spark/PySpark and the various target platforms (for example Linux, macOS, and
    Windows). PySpark is included in the official release of Apache Spark, which can
    now be downloaded from the Apache Spark website ([https://spark.apache.org/](https://spark.apache.org/)).
    PySpark is also available via the `pip` utility from PyPI, which can be used for
    a local setup or to connect to a remote cluster. Another option when installing
    PySpark is using **Anaconda**, which is another popular package and environment
    management system. If we are installing PySpark along with Apache Spark, we need
    the following to be available or installed on the target machine:'
  prefs: []
  type: TYPE_NORMAL
- en: JVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the code examples that will be discussed later, we have installed Apache
    Spark version 3.1.1 on macOS with PySpark included. PySpark comes with the `SparkSession`
    and `SparkContext` objects automatically, which can be used to interact with the
    core Apache Spark engine. The following figure shows the initialization of the
    PySpark shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – PySpark shell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_08_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – PySpark shell
  prefs: []
  type: TYPE_NORMAL
- en: 'From the initialization steps of the PySpark shell, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `SparkContext` object is already created, and its instance is available
    in the shell as `sc`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SparkSession` object is also created, and its instance is available as
    `spark`. Now, `SparkSession` is an entry point to the PySpark framework to dynamically
    create RDD and DataFrame objects. The SparkSession object can also be created
    programmatically, and we will discuss this later with a code example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark comes with a web UI and a web server to host the web UI, and it
    is initiated at `http://192.168.1.110:4040` for our local machine installation.
    Note that the IP address mentioned in this URL is a private address that is specific
    to our machine. Port `4040` is selected as the default port by Apache Spark. If
    this port is in use, Apache Spark will try to host on the next available port,
    such as `4041` or `4042`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next subsections, we will learn how to create `SparkSession` objects,
    explore PySpark for RDD operations, and learn how to use PySpark DataFrames and
    PySpark SQL. We will start with creating a Spark session using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Creating SparkSession and SparkContext programs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Prior to Spark release 2.0, `SparkContext` was used as an entry point to PySpark.
    Since Spark release 2.0, `SparkSession` has been introduced as an entry point
    to the PySpark underlying framework to work with RDDs and DataFrames. `SparkSession`
    also includes all the APIs available in `SparkContext`, `SQLContext`, `StreamingContext`,
    and `HiveContext`. Now, `SparkSession` can also be created using the `SparkSession`
    class by using its `builder` method, which is illustrated in the next code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this code in the PySpark shell, which already has a default `SparkSession`
    object created as `spark`, it will return the same session as an output of this
    `builder` method. The following console output shows the location of the two `SparkSession`
    objects (`spark` and `spark1`), which confirms that they are pointing to the same
    `SparkSession` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A few key concepts to understand regarding the `builder` method are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`getOrCreate`: This method is the reason that we will get an already created
    session in the case of the PySpark shell. This method will create a new session
    if no session already exists; otherwise, it returns an already existing session.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`master`: If we want to create a session connected to a cluster, we will provide
    the master name, which can be instance name of the Spark, or YARN, or Mesos cluster
    manager. If we are using a locally deployed Apache Spark option, we can use `local[n]`,
    where `n` is an integer greater than zero. The `n` will determine the number of
    partitions to be created for the RDD and DataFrame. For a local setup, `n` can
    be the number of CPU cores on the system. If we set it to `local[*]`, which is
    a common practice, this will create as many worker threads as there are logical
    cores on the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If a new `SparkSession` object needs to be created, we can use the `newSession`
    method, which is available at the instance level of an existing `SparkSession`
    object. A code example of creating a new `SparkSession` object is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output for the `spark2` object confirms that this is a different
    session than the previously created `SparkSession` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `SparkContext` object can also be created programmatically. The easiest
    way to get a `SparkContext` object from a `SparkSession` instance is by using
    the `sparkContext` attribute. There is also a `SparkConext` class in the PySpark
    library that can also be used to create a `SparkContext` object directly, which
    was a common approach prior to Spark release 2.0\.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can have multiple `SparkSession` objects but only one `SparkContext` object
    per JVM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SparkSession` class offers a few more useful methods and attributes that
    are summarized next:'
  prefs: []
  type: TYPE_NORMAL
- en: '`getActiveSession`: This method returns an active `SparkSession` under the
    current Spark thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`createDataFrame`: This method creates a DataFrame object from an RDD, a list
    of objects, or a pandas DataFrame object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conf`: This attribute returns the configuration interface for a Spark session.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`catalog`: This attribute provides an interface to create, update, or query
    associated databases, functions, and tables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A complete list of methods and attributes can be explored using the PySpark
    documentation for the `SparkSession` class at [https://spark.apache.org/docs/latest/api/python/reference/api/](https://spark.apache.org/docs/latest/api/python/reference/api/).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring PySpark for RDD operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *Introducing RDDs* section, we covered some of the key functions and
    operations of RDDs. In this section, we will extend the discussion in the context
    of PySpark with code examples.
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDDs from a Python collection and from an external file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We discussed a few ways to create RDDs in the previous section. In the following
    code examples, we will discuss how to create RDDs from an in-memory Python collection
    and from an external file resource. These two approaches are described next:'
  prefs: []
  type: TYPE_NORMAL
- en: To create an RDD from a Python data collection, we have a `parallelize` method
    available under the `sparkContext` instance. This method distributes the collection
    to form an RDD object. The method takes a collection as a parameter. An optional
    second parameter is available with the `parallelize` method to set the number
    of partitions to be created. By default, this method creates the partitions acocording
    to the number of cores available on the local machine or the number of cores set
    at the time of creating the `SparkSession` object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To create an RDD from an external file, we will use the `textFile` method available
    under the `sparkContext` instance. The `textFile` method can load a file as an
    RDD from HDFS or from a local filesystem (to be available on all cluster nodes).
    For local system-based deployment, an absolute and/or relative path can be provided.
    It is possible to set the minimum number of partitions to be created for the RDD
    using this method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some quick sample code (`rddcreate.py`) is shown next to illustrate the exact
    syntax of the PySpark statements to be used for the creation of a new RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `sample.txt` file has random text data, and its contents are not
    relevant for this code example.
  prefs: []
  type: TYPE_NORMAL
- en: RDD transformation operations with PySpark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several built-in transformation operations available with PySpark.
    To illustrate how to implement a transformation operation such as `map` using
    PySpark, we will take a text file as an input and use the `map` function available
    with RDDs to transform it to another RDD. The sample code (`rddtranform1.py`)
    is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this sample code, we applied two lambda functions with the `map` operation
    to convert the text in the RDD to lowercase and uppercase. In the end, we used
    the `collect` operation to get the contents of the RDD objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular transformation operation is `filter`, which can be used to
    filter out some entries of data. Some example code (`rddtranform2.py`) is shown
    next that is developed to filter all the even numbers from an RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When you execute this code, it will provide console output with 3, 7, 7, and
    9 as collection entries. Next, we will explore a few action examples with PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: RDD action operations with PySpark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To illustrate the implementation of action operations, we will use an RDD created
    from the Python collection and then apply a few built-in action operations that
    come with the PySpark library. The sample code (`rddaction1.py`) is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Some of the action operations used in this code example are self-explanatory
    and trivial (`count`, `max`, `min`, `count`, and `sum`). The rest of the action
    operations (non-trivial) are explained next:'
  prefs: []
  type: TYPE_NORMAL
- en: '`glom`: This results in an RDD that is created by coalescing all data entries
    with each partition into a list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collect`: This method returns all the elements of an RDD as a list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduce`: This is a generic function to apply to the RDD to reduce the number
    of elements in it. In our case, we used a lambda function to combine two elements
    into one, and so on. This results in adding all the elements in the RDD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top(x)`: This action returns the top `x` elements in the array if the elements
    in the array are ordered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have covered how to create RDDs using PySpark and how to implement transformation
    and action operations on an RDD. In the next section, we will cover the PySpark
    DataFrame, which is another popular data structure used mainly for analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about PySpark DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **PySpark DataFrame** is a tabular data structure consisting of rows and
    columns, like the tables we have in a relational database and like the pandas
    DataFrame, which we introduced in [*Chapter 6*](B17189_06_Final_PG_ePub.xhtml#_idTextAnchor188)*,
    Advanced Tips and Tricks in Python*. In comparison to pandas DataFrames, the key
    difference is that PySpark DataFrame objects are distributed in the cluster, which
    means data is stored across different nodes in a cluster. The use of a DataFrame
    is mainly to process a large collection of structured or unstructured data, which
    may reach into the petabytes, in a distributed manner. Like RDDs, PySpark DataFrames
    are immutable and based on lazy evaluation, which means evaluation will be delayed
    until it needs to be done.
  prefs: []
  type: TYPE_NORMAL
- en: We can store numeric as well as string data types in a DataFrame. The columns
    in a PySpark DataFrame cannot be empty; they must have the same data type and
    must be of the same length. Rows in a DataFrame can have data of different data
    types. Row names in a DataFrame are required to be unique.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsections, we will learn how to create a DataFrame and cover some
    key operations on DataFrames using PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a DataFrame object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A PySpark DataFrame can be created using one of the following sources of data:'
  prefs: []
  type: TYPE_NORMAL
- en: Python collections such as lists, tuples, and dictionaries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Files (CSV, XML, JSON, Parquet, and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RDDs, by using the `toDF` method or the `createDataFrame` method of PySpark.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Kafka streaming messages can be converted to PySpark DataFrames by using
    the `readStream` method of the `SparkSession` object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database (for example, Hive and HBase) tables can be queried using traditional
    SQL commands and the output will be transformed into a PySpark DataFrame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will start creating a DataFrame from a Python collection, which is the simplest
    approach, but it is more helpful for illustration purposes. The next bit of sample
    code shows us how to create a PySpark DataFrame from a collection of employees
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code example, we first created the row data as a list of employees
    and then created a schema with column names. When the schema is only a list of
    column names, the data type of each column is determined by the data, and each
    column is marked as nullable by default. A more advanced API (`StructType` or
    `StructField`) can be used to define the DataFrame schema manually, which includes
    setting the data type and marking a column as nullable or not nullable. The console
    output of this sample code is shown next, which shows the schema first and then
    the DataFrame contents as a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next code example, we will create a DataFrame from a CSV file. The CSV
    file will have the same entries as we used in the previous code example. In this
    sample code (`dfcreate2.py`), we also defined the schema manually by using the
    `StructType` and `StructField` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The console outcome of this code will be the same as shown for the previous
    code example. The importing of JSON, text, or XML files into a DataFrame is supported
    by the `read` method using a similar syntax. The support of other data sources,
    such as RDDs and databases, is left for you to evaluate and implement as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Working on a PySpark DataFrame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we have created a DataFrame from some data, regardless of the source of
    the data, we are ready to analyze it, transform it, and take some actions on it
    to get meaningful results from it. Most of the operations supported by the PySpark
    DataFrame are similar to RDDs and pandas DataFrames. For illustration purposes,
    we will load the same data as in the previous code example into a DataFrame object
    and then perform the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Select one or more columns from the DataFrame object using the `select` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace the values in a column using a dictionary and the `replace` method.
    There are more options to replace data in a column available in the PySpark library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a new column with values based on an existing column's data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The complete sample code (`dfoperations.py`) is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the output for the preceding code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Console output of the dfoperations.py program'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_08_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – Console output of the dfoperations.py program
  prefs: []
  type: TYPE_NORMAL
- en: The first table shows the result of the `select` operation. The next table shows
    the result of the `replace` operation on the `gender` column and also a new column,
    `Pay Level`.
  prefs: []
  type: TYPE_NORMAL
- en: There are many built-in operations available to work with PySpark DataFrames,
    and many of them are the same as we discussed for pandas DataFrames. The details
    of those operations can be explored by using the Apache Spark official documentation
    for the software release you have.
  prefs: []
  type: TYPE_NORMAL
- en: There is one legitimate question that anyone would ask at this point, which
    is, *Why should we use the PySpark DataFrame when we already have pandas DataFrame
    offering the same types of operations?* The answer is very simple. PySpark offers
    distributed DataFrames, and the operations on such DataFrames are meant to be
    executed on a cluster of nodes in parallel. This makes the PySpark DataFrame's
    performance significantly better than the pandas DataFrame's.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen so far that, as programmers, we are not actually having to program
    anything regarding how to delegate distributed RDDs and DataFrames to different
    executors in a standalone or distributed cluster. Our focus is only on the programming
    aspect of the data processing. Coordination and communication with a local or
    remote cluster of nodes is automatically taken care of by `SparkSession` and `SparkContext`.
    This is the beauty of Apache Spark and PySpark: letting programmers focus on solving
    the real problems instead of worrying about how workloads will be executed.'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing PySpark SQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark SQL is one of the key modules of Apache Spark; it is used for structured
    data processing and acts as a distributed SQL query engine. As you can imagine,
    Spark SQL is highly scalable, being a distributed processing engine. Usually,
    the data source for Spark SQL is a database, but SQL queries can be applied to
    temporary views, which can be built from RDDs and DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate using the PySpark library with Spark SQL, we will use the same
    DataFrame as in the previous sample code, using employees data to build a `TempView`
    instance for SQL queries. In our code example, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We will create a PySpark DataFrame for the employees data from a Python collection
    as we did for the previous code example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will create a `TempView` instance from the PySpark DataFrame using the `createOrReplaceTempView`
    method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the `sql` method of the Spark Session object, we will execute the conventional
    SQL queries on the `TempView` instance, such as querying all employee records,
    querying employees with salaries higher than 45,000, querying the count of employees
    per gender type, and using the `group by` SQL command for the `gender` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The complete code example (`sql1.py`) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output will show the results of the three SQL queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Spark SQL is a big topic within Apache Spark. We provided only an introduction
    to Spark SQL to show the power of using SQL commands on top of Spark data structures
    without knowing the source of the data. This concludes our discussion of the use
    of PySpark for data processing and data analysis. In the next section, we will
    discuss a couple of case studies to build some real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Case studies of using Apache Spark and PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous sections, we covered the fundamental concepts and architecture of
    Apache Spark and PySpark. In this section, we will discuss two case studies for
    implementing two interesting and popular applications for Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Case study 1 – Pi (π) calculator on Apache Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will calculate Pi (π) using the Apache Spark cluster that is running on our
    local machine. Pi is the area of a circle when its radius is 1\. Before discussing
    the algorithm and the driver program for this application, it is important to
    introduce the Apache Spark setup used for this case study.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Apache Spark cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In all previous code examples, we used PySpark locally installed on our machine
    without a cluster. For this case study, we will set up an Apache Spark cluster
    by using multiple virtual machines. There are many virtualization software tools
    available, such as **VirtualBox**, and any of these software tools will work for
    building this kind of setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used Ubuntu **Multipass** ([https://multipass.run/](https://multipass.run/))
    to build the virtual machines on top of macOS. Multipass works on Linux and on
    Windows as well. Multipass is a lightweight virtualization manager and is designed
    specifically for developers to create virtual machines with a single command.
    Multipass has very few commands, which makes it easier to use. If you decide to
    use Multipass, we recommend that you use the official documentation for installation
    and configuration. In our virtual machines setup, we have the following virtual
    machines created using Multipass:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Virtual machines created for our Apache Spark cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_08_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.7 – Virtual machines created for our Apache Spark cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'We installed *Apache Spark 3.1.1* on each virtual machine by using the `apt-get`
    utility. We started Apache Spark as the master on `vm1` and then started Apache
    Spark as the worker on `vm2` and `vm3` by providing the master Spark URI, which
    is `Spark://192.168.64.2.7077` in our case. The complete Spark cluster setup will
    look as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Details of the Apache Spark cluster nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_08_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 – Details of the Apache Spark cluster nodes
  prefs: []
  type: TYPE_NORMAL
- en: 'The web UI for the master Spark node will look as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Web UI for the master node in the Apache Spark cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_08_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.9 – Web UI for the master node in the Apache Spark cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'A summary of the web UI for the master node is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: The web UI provides the node name with the Spark URL. In our case, we used the
    IP address as the host name, which is why we have an IP address in the URL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are the details of the worker nodes, of which there are two in our case.
    Each worker node uses 1 CPU core and 1 GB of memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The web UI also provides details of the running and completed applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The web UI for the worker nodes will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Web UI for the worker nodes in the Apache Spark cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_08_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.10 – Web UI for the worker nodes in the Apache Spark cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'A summary of the web UI for the worker nodes is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: The web UI provides the worker IDs as well as the node names and the ports where
    the workers are listening for requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The master node URL is also provided in the web UI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Details of the CPU core and memory allocated to the worker nodes are also available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The web UI provides details of jobs in progress (**Running Executors**) and
    jobs that are already finished.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a driver program for Pi calculation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To calculate Pi, we are using a commonly used algorithm (the **Monte Carlo**algorithm)
    that assumes a square having an area equal to 4 that is circumscribing a unit
    circle (circle with a radius value equal to 1). The idea is to generate a huge
    amount of random numbers in the domain of a square with sides having a length
    of 2\. We can assume there is a circle inside the square with the same diameter
    value as the length of the side of the square. This means that the circle will
    be inscribed inside the square. The value of Pi is estimated by calculating the
    ratio of the number of points that lie inside the circle to the total number of
    generated points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete sample code for the driver program is shown next. In this program,
    we decided to use two partitions as we have two workers available to us. We used
    10,000,000 points for each worker. Another important thing to note is that we
    used the Spark master node URL as a master attribute when creating the Apache
    Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The Spark web UI will provide the status of the application when running and
    even after it completes its execution. In the following screenshot, we can see
    that two workers were engaged to complete the job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Pi calculator status in the Spark web UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_08_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.11 – Pi calculator status in the Spark web UI
  prefs: []
  type: TYPE_NORMAL
- en: 'We can click on the application name to go to the next level of detail for
    the application, as shown in *Figure 8.12*. This screenshot shows which workers
    are involved in completing the tasks and what resources are being used (if things
    are still running):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Pi calculator application executor-level details'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_08_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.12 – Pi calculator application executor-level details
  prefs: []
  type: TYPE_NORMAL
- en: In this case study, we covered how we can set up an Apache Spark cluster for
    testing and experimentation purposes and how we can build a driver program in
    Python using the PySpark library to connect to Apache Spark and submit our jobs
    to be processed on two different cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next case study, we will build a word cloud using the PySpark library.
  prefs: []
  type: TYPE_NORMAL
- en: Case study 2 – Word cloud using PySpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **word cloud** is a visual representation of the frequency of words that appear
    in some text data. Put simply, if a specific word appears more frequently in a
    text, it appears bigger and bolder in the word cloud. These are also known as
    **tag clouds** or **text clouds** and are very useful tools to identify what parts
    of some textual data are more important. A practical use case of this tool is
    the analysis of content on social media, which has many applications for marketing,
    business analytics, and security.
  prefs: []
  type: TYPE_NORMAL
- en: 'For illustration purposes, we have built a simple word cloud application that
    reads a text file from the local filesystem. The text file is imported into an
    RDD object that is then processed to count the number of times each word occurred.
    We process the data further to filter out the words that are repeated fewer than
    two times and also filter out words that are of a length that''s less than four
    letters. The word frequency data is fed to the `WordCloud` library object. To
    display the word cloud, we used the `matplotlib` library. The complete sample
    code is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this program is plotted as a window application and the output
    will look as shown here, based on the sample text (`wordcloud.txt`) provided to
    the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Word cloud built using PySpark RDDs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_08_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.13 – Word cloud built using PySpark RDDs
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have not used a very big sample of textual data for this illustration.
    In the real world, the source data can be extremely large, which justifies processing
    using an Apache Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: These two case studies have provided you with skills in using Apache Spark for
    large-scale data processing. They provide a foundation for those of you who are
    interested in the fields of **Natural Language Processing** (**NLP**), text analysis,
    and sentimental analysis. These skills are important for you if you are a data
    scientist and your day-to-day job requires data processing for analytics and building
    algorithms for NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored how to execute data-intensive jobs on a cluster
    of machines to achieve parallel processing. Parallel processing is important for
    large-scale data, which is also known as big data. We started by evaluating the
    different cluster options available for data processing. We provided a comparative
    analysis of Hadoop MapReduce and Apache Spark, which are the two main competing
    platforms for clusters. The analysis showed that Apache Spark has more flexibility
    in terms of supported languages and cluster management systems, and it outperforms
    Hadoop MapReduce for real-time data processing because of its in-memory data processing
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Once we had established that Apache Spark is the most appropriate choice for
    a variety of data processing applications, we started looking into its fundamental
    data structure, which is the RDD. We discussed how to create RDDs from different
    sources of data and introduced two types of operations, transformations and actions.
  prefs: []
  type: TYPE_NORMAL
- en: In the core part of this chapter, we explored using PySpark to create and manage
    RDDs using Python. This included several code examples of transformation and action
    operations. We also introduced PySpark DataFrames for the next level of data processing
    in a distributed manner. We concluded the topic by introducing PySpark SQL with
    a few code examples.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at two case studies using Apache Spark and PySpark. These
    case studies included the calculation of Pi and the building of a word cloud from
    text data. We also covered in the case studies how we can set up a Standalone
    Apache Spark instance on a local machine for testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter gave you a lot of experience in setting up Apache Spark locally
    as well as setting up Apache Spark clusters using virtualization. There are plenty
    of code examples provided in this chapter for you to enhance your practical skills.
    This is important for anyone who wants to process their big data problems using
    clusters for efficiency and scale.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore options for leveraging frameworks such
    as Apache Beam and extend our discussion of using public clouds for data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How is Apache Spark different from Hadoop MapReduce?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How are transformations different from actions in Apache Spark?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is lazy evaluation in Apache Spark?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `SparkSession`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is the PySpark DataFrame different from the pandas DataFrame?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Spark in Action, Second Edition* by Jean-Georges Perrin'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning PySpark* by Tomasz Drabas, Denny Lee'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PySpark Recipes* by Raju Kumar Mishra'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Apache Spark documentation* for the release you are using ([https://spark.apache.org/docs/rel#](https://spark.apache.org/docs/rel#))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multipass documentation* available at [https://multipass.run/docs](https://multipass.run/docs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is an in-memory data processing engine, whereas Hadoop MapReduce
    has to read from and write to the filesystem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformation is applied to convert or translate data from one form to another,
    and the results stay within the cluster. Actions are the functions applied to
    data to get the results that are returned to the driver program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lazy evaluation is applied mainly for transformation operations, which means
    transformation operations are not executed until an action is triggered on a data
    object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`SparkSession` is an entry point to the Spark application to connect to one
    or more cluster managers and to work with executors for task execution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PySpark DataFrame is distributed and is meant to be available on multiple
    nodes of an Apache Spark cluster for parallel processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
