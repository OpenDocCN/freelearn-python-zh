- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we ran our different microservices directly in the
    host operating system, as it is sometimes the quickest way to get started, while
    also being a useful approach in general—especially for smaller installations or
    development where everything can be contained in a virtual environment. However,
    if the application requires a database or a compiled extension, then things start
    to be tightly coupled to the operating system and version. Other developers with
    slightly different systems will start to run into problems, and the more differences
    between a development environment and a production one, the more trouble you will
    have when releasing your software.
  prefs: []
  type: TYPE_NORMAL
- en: '**Virtual Machines** (**VMs**) can be a good solution, as they provide an isolated
    environment in which to run your code. A VM is essentially a piece of software
    pretending to be a real computer, in which there is a real operating system running
    in the pretend computer. If you''ve ever used an Amazon EC2 instance or a Google
    Compute Engine instance, then you have used a virtual machine. It''s possible
    to run them locally using tools such as VMware or VirtualBox.'
  prefs: []
  type: TYPE_NORMAL
- en: However, VMs are heavyweights, precisely because they emulate a full computer.
    Using one from scratch involves installing an operating system or using a tool
    such as HashiCorp's Packer ([https://www.packer.io/](https://www.packer.io/))
    to build a disk image—the sort of thing that comes prebuilt for you when selecting
    an AWS or GCP instance.
  prefs: []
  type: TYPE_NORMAL
- en: The big revolution came with **Docker**, an open-source virtualization tool
    first released in 2013\. Docker allows the use of isolated environments called
    *containers* to run applications in a very portable way.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing providers, such as Amazon Web Services (AWS), Google Cloud,
    and Microsoft Azure, allow people to rent space on their computers and make creating
    virtual machines and containers much easier. Provisioning these cloud resources,
    along with attached storage and databases, can be done with a few mouse clicks,
    or a few commands typed in a terminal. They can also be configured using configuration
    files to describe resources, using Infrastructure-as-Code tools, such as HashiCorp's
    **Terraform**.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we present Docker and explain how to run Quart-based microservices
    with it. We then cover deploying a container-based application using some common
    orchestration tools, such as Docker Compose, Docker Swarm, and, briefly, Kubernetes.
    Many of these topics could fill entire books by themselves, so this chapter will
    be an overview that relies on the installation instructions provided by the tools
    themselves to get started.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the cloud computing providers will also have their own versions of these
    tools, modified to better integrate with the other services that they offer. If
    you are already using a particular company's services, it is worth investigating
    their tools. At the same time, it is also worth knowing the cloud-agnostic versions,
    as taking a more independent approach can make migrating from one provider to
    another much easier.
  prefs: []
  type: TYPE_NORMAL
- en: Note that some of the instructions in this chapter may result in incurring a
    charge from AWS. While we will keep those costs to a minimum, it is important
    to understand what costs may be incurred by checking with AWS and also to unsubscribe
    from any unused resources after trying things out.
  prefs: []
  type: TYPE_NORMAL
- en: What is Docker?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Docker** ([https://www.docker.com/](https://www.docker.com/)) project
    is a *container* platform, which lets you run your applications in isolated environments.
    Using the Linux feature called `cgroups` ([https://en.wikipedia.org/wiki/Cgroups](https://en.wikipedia.org/wiki/Cgroups)),
    Docker creates isolated environments called containers that run on Linux without
    a VM. On macOS and Windows, installing Docker will create a lightweight VM for
    you to run containers in, although this is a seamless process. This means that
    macOS, Windows, and Linux users can all develop container-based applications without
    worrying about any interoperability trouble and deploy them to a Linux server
    where they will run natively.
  prefs: []
  type: TYPE_NORMAL
- en: Today, Docker is almost synonymous with containers, but there are other container
    runtimes, such as **CRI-O** ([https://cri-o.io/](https://cri-o.io/)), and historical
    projects such as **rkt** and **CoreOS** that, together with Docker, helped shape
    the standardized ecosystem that we have today.
  prefs: []
  type: TYPE_NORMAL
- en: Because containers do not rely on emulation when running on Linux, there is
    little performance difference between running code inside a container and outside.
    As there is an emulation layer on macOS and Windows, while it is possible to run
    containers in production on these platforms, there is little benefit to doing
    so. It is possible to package up everything needed to run an application inside
    a container image and distribute it for use anywhere that can run a container.
  prefs: []
  type: TYPE_NORMAL
- en: As a Docker user, you just need to choose which image you want to run, and Docker
    does all the heavy lifting by interacting with the Linux kernel. An image in this
    context is the sum of all the instructions required to create a set of running
    processes on top of a Linux kernel, to run one container. An image includes all
    the resources necessary to run a Linux distribution. For instance, you can run
    whatever version of Ubuntu you want in a Docker container even if the host OS
    is of a different distribution.
  prefs: []
  type: TYPE_NORMAL
- en: As containers operate best on a Linux-based system, the rest of this chapter
    assumes that everything is installed under a Linux distribution, such as Ubuntu.
  prefs: []
  type: TYPE_NORMAL
- en: We used Docker in *Chapter 5*, *Splitting the Monolith*, when discussing metrics
    and monitoring, so you may already have Docker installed. With some older Linux
    distributions, you may have a very old version of Docker available. Installing
    a newer one directly from Docker itself is a good idea, to get the latest features
    and security patches. If you have a Docker installation, feel free to jump directly
    to the next section of this chapter, *Introduction to Docker*. If not, you can
    visit [https://www.docker.com/get-docker](https://www.docker.com/get-docker) to
    download it and find the installation instructions. The community edition is good
    enough for building, running, and installing containers. Installing Docker on
    Linux is straightforward—you can probably find a package for your Linux distribution.
  prefs: []
  type: TYPE_NORMAL
- en: For macOS, if you have Homebrew ([https://brew.sh](https://brew.sh)) installed,
    then you can simply use `brew install docker`. Otherwise, follow the instructions
    on Docker's website. Under Windows, Docker can either use the **Windows Subsystem
    for Linux** (**WSL2**), or the built-in Hyper-V to run a virtual machine. We recommend
    WSL, as it is the most straightforward to get working.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the installation was successful, you should be able to run the `docker`
    command in your shell. Try the `version` command to verify your installation,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A Docker installation is composed of a Docker Engine, which controls the running
    containers and a command-line interface. It also includes Docker Compose, which
    is a way of arranging multiple containers that will work together, as well as
    Kubernetes, an orchestration tool for deploying and managing container-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: The engine provides an HTTP API, which can be reached locally through a UNIX
    socket (usually, `/var/run/docker.sock`) or through the network. This means it
    is possible to control a Docker Engine that is running on a different computer
    to the Docker client, or orchestration tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Now that Docker is installed on your system, let's discover how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s experiment with Docker containers. Running a container that you can
    enter commands in is as simple as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With this command, we are telling Docker to run the Ubuntu image, which will
    be fetched from Docker Hub, a central registry of public images. We are providing
    a tag of `20.04` after the image name so that we download the container image
    that represents the Ubuntu 20.04 operating system. This won't contain everything
    that a regular Ubuntu installation has, but anything that's missing is installable.
  prefs: []
  type: TYPE_NORMAL
- en: We also tell Docker to run interactively—the `-i` argument—and to assign a `tty`
    with the `-t` argument, so that we can type commands inside the container. By
    default, Docker assumes that you want to start a container that runs in the background,
    serving requests. By using these two options and asking that the command `bash`
    is run inside the container, we can get a shell that we can use just like a Linux
    shell, outside the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every existing Linux distribution out there provides a base image, not just
    Ubuntu. There are also pared-down base images for running Python, Ruby, or other
    environments, and base Linux images, such as Alpine, which aim to be even smaller.
    The size of the image is important because every time you want to update it or
    run it in a new place, it must be downloaded. Alpine is a little over 5MB in size,
    whereas the `ubuntu:20.04` image is nearly 73MB. You can compare sizes and manage
    the images your Docker Engine knows about with the following commands – the second
    command will remove any local copy of the `ubuntu:20.04` image, so if you run
    that, you will need to download that image again to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You might think that the size means the Ubuntu image is always a better choice
    than the Python base image, but the Ubuntu image doesn't contain Python, and so
    to use it we must build an image that contains everything we need and install
    our own software on top of that. Rather than do all of this set up by hand, we
    can use a **Dockerfile** ([https://docs.docker.com/engine/reference/builder/](https://docs.docker.com/engine/reference/builder/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard name for these Docker configuration files is a Dockerfile, and
    the following is a basic example of one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'A Dockerfile is a text file with a set of instructions. Each line starts with
    the instruction in uppercase, followed by its arguments. In our example, there
    are these three instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FROM`: Points to the base image to use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RUN`: Runs the commands in the container once the base image is installed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CMD`: The command to run when the container is executed by Docker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we should build our image and give it a useful name so that we can refer
    to it later on. Here we will run docker build and tag the new image with the name
    `ubuntu-with-python`, while using the current directory for a build environment
    – by default, this is also where `docker` `build` looks for a Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can run our new image in the same way we ran the Ubuntu image earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When Docker creates images, it creates a cache that has every instruction from
    the Dockerfile. If you run the `build` command a second time, without changing
    the file, it should be done within seconds. Permuting or changing instructions
    rebuilds the image, starting at the first change. For this reason, a good strategy
    when writing these files is to sort instructions so that the most stable ones
    (the ones you rarely change) are at the top.
  prefs: []
  type: TYPE_NORMAL
- en: Another good piece of advice is to clean up each instruction. For example, when
    we run `apt-get update` and `apt-get install` above, this downloads a lot of package
    index files, and the `.deb` packages that, once installed, we no longer need.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can make our resulting image smaller by cleaning up after ourselves, which
    must be done in the same `RUN` command so that the data we are removing is not
    written out as part of the container''s image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: One great feature that Docker offers is the ability to share, publish, and reuse
    images with other developers. Docker Hub ([https://hub.docker.com](https://hub.docker.com))
    is to Docker containers what PyPI is to Python packages.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, the Ubuntu base image was pulled from the Hub by Docker, and
    there are numerous pre-existing images you can use. For instance, if you want
    to launch a Linux distribution that is tweaked for Python, you can look at the
    Python page on the official Docker Hub website and pick one ([https://hub.docker.com/_/python/](https://hub.docker.com/_/python/)).
  prefs: []
  type: TYPE_NORMAL
- en: The `python:version` images are Debian-based, and are an excellent starting
    point for any Python project.
  prefs: []
  type: TYPE_NORMAL
- en: The Python images based on **Alpine Linux** are also quite popular, because
    they produce the smallest images to run Python. They are more than ten times smaller
    than other images, which means they are much faster to download and set up for
    people wanting to run your project in Docker (refer to [http://gliderlabs.viewdocs.io/docker-alpine/](http://gliderlabs.viewdocs.io/docker-alpine/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Python 3.9 from the Alpine base image, you can create a Dockerfile like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Building and running this Dockerfile places you in a Python 3.9 shell. The Alpine
    set is great if you run a Python application that does not require a lot of system-level
    dependencies nor any compilation. It is important to note, however, that Alpine
    has a specific set of compilation tools that are sometimes incompatible with some
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: For a Quart-based microservice project, the slightly larger Debian-based Python
    images are probably a simpler choice, because of its standard compilation environment
    and stability. Moreover, once the base image is downloaded, it is cached and reused,
    so you do not need to download everything again.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is important to use images from trusted people and organizations
    on Docker Hub, since anyone can upload an image. Beyond the risk of running malicious
    code, there's also the problem of using a Linux image that is not up to date with
    the latest security patches. Docker also supports digitally signing images to
    help verify that an image is the one you expect, with no modifications.
  prefs: []
  type: TYPE_NORMAL
- en: Running Quart in Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To run a Quart application in Docker, we can use the base Python image. From
    there, installing the app and its dependencies can be done via pip, which is already
    installed in the Python image.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming your project has a `requirements.txt` file for its pinned dependencies,
    and a `setup.py` file that installs the project, creating an image for your project
    can be done by instructing Docker on how to use the `pip` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we introduce the `COPY` command, which will recursively
    copy files and directories from outside the container into the image. We also
    add the `EXPOSE` directive to indicate to anyone running the container that this
    port should be exposed to the outside world. We still need to connect that exposed
    port when we run the container with the `-p` option. Any process inside the container
    can listen to any ports that it wants to, and communicate with itself using localhost,
    but anything outside the container won''t be able to reach the inside unless that
    port has been exposed. It''s also worth noting that localhost inside the container
    only refers to the container, not the computer that''s hosting the running containers;
    so, if you need to communicate with other services, you will need to use its real
    IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `3.9` tag here will get the latest Python 3.9 image that was uploaded to
    Docker Hub. Now we can run our new container, exposing the port it needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Press Ctrl + C to stop the container, or from another terminal window, find
    the container and tell it to stop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `COPY` command automatically creates the top-level `app` directory in the
    container and copies everything from "`.`" in it. One important detail to remember
    with the `COPY` command is that any change to the local directory ("`.`") invalidates
    the Docker cache, and builds from that step. To tweak this mechanism, you can
    create a `.dockerignore` file where you can list files and directories that should
    be ignored by Docker, such as the `.git` directory that stores all the history
    and metadata about your version control.
  prefs: []
  type: TYPE_NORMAL
- en: We are not using a virtual environment inside the container, as we are already
    in an isolated environment. We also run our Quart application using Hypercorn,
    a good practice for production use as we discussed in *Chapter 9*, *Packaging
    and Running Python*.
  prefs: []
  type: TYPE_NORMAL
- en: That is why the `CMD` instruction, which tells the container what command to
    run when it starts, uses **Hypercorn**. `CMD` can take a normal shell command
    as an argument, but this does get interpreted by the shell inside the container,
    meaning that it could go wrong if there are symbols the shell interprets differently,
    such as `*` and `?`. It's much safer to provide a list, in a format you may be
    familiar with, if you have ever used the Python subprocess module ([https://docs.python.org/3/library/subprocess.html](https://docs.python.org/3/library/subprocess.html))
    or used exec system calls.
  prefs: []
  type: TYPE_NORMAL
- en: The next thing we need to do is orchestrate different containers so that they
    can work together. Let's see in the next section how we can do that.
  prefs: []
  type: TYPE_NORMAL
- en: Docker-based deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying a microservice at scale can be done by running several containers
    spread across either one or several instances. When we are developing our application
    locally, we are limited to what our one desktop or laptop computer can provide;
    but for a production service, it may run on dozens or hundreds of servers, with
    each one running a container that is providing different parts of the application.
    Each of the options for deploying your application in the cloud, or scaling it
    up to meet your needs, will involve running more instances, to run more containers.
  prefs: []
  type: TYPE_NORMAL
- en: The first to examine is Docker Compose, which is aimed at smaller-scale deployments,
    mostly contained in a single instance, but running multiple containers. This is
    ideal for a development environment, a staging environment, or a prototype. Other
    options we will look at are Docker Swarm and Kubernetes, which provide different
    levels of complexity for someone deploying an application, but also increasing
    levels of flexibility and power. Both options will also need someone to run cloud
    instances or bare-metal servers on which to run the containers.
  prefs: []
  type: TYPE_NORMAL
- en: Once your Docker image is created, every host that runs a Docker daemon can
    be used to run as many containers as you want within the limits of the physical
    resources. We will examine several different options, to gain a broad overview
    of the features and complexity involved.
  prefs: []
  type: TYPE_NORMAL
- en: There is no need to over-complicate your initial application. It might be tempting
    to go with a large Kubernetes cluster, but if your application does not need to
    scale that way, it's a wasted effort. Use the metrics collected about your application
    and the knowledge of upcoming business changes to adjust to plan for what you
    need, not what you might want.
  prefs: []
  type: TYPE_NORMAL
- en: To experiment with the **Terraform**, **Docker Swarm**, and **Kubernetes** examples
    in this book and on [https://github.com/PacktPublishing/Python-Microservices-Development-2nd-Edition/tree/main/CodeSamples](https://github.com/PacktPublishing/Python-Microservices-Development-2nd-Edition/tree/main/CodeSample),
    you will need to create an account on AWS by visiting [https://aws.amazon.com/](https://aws.amazon.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have set up the account, visit the **Identity and Access Management**
    (**IAM**) page to create a service user that can create and change resources.
    You could use your root—or main—account to do all of the work, but it is better
    to create service accounts for this purpose, as it means that any leaked access
    keys or secrets can be easily revoked—and new ones created—without causing major
    trouble for accessing the account in general. We should follow the principle of
    least privilege, as we discussed in *Chapter 7*, *Securing Your Services*.
  prefs: []
  type: TYPE_NORMAL
- en: Once on the IAM page, click **Add User** and request **Programmatic Access**
    so that you can obtain API keys to use this account in a program.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17108_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: IAM Add user page in AWS'
  prefs: []
  type: TYPE_NORMAL
- en: Create a group to control the user's permissions more easily. Grant this new
    group the permissions to modify EC2 instances, since it covers most of what we
    will be changing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17108_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Naming the group and setting permissions'
  prefs: []
  type: TYPE_NORMAL
- en: Once the group is created, you will get a chance to download the new **Access
    Key ID** and **Secret Access Key**. These will be used to grant access to any
    programs that we use to create instances and other cloud resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of these tools are Infrastructure-as-Code. That is to say, you will have
    a configuration file, or set of files, that describe what your running services
    will look like, and what resources they require. This configuration should also
    be kept in version control so that any changes can be managed. Whether it is in
    the same source control repository as your code will depend on how you need to
    deploy the software: If you are continuously deploying new versions, then it can
    be helpful to keep the configuration alongside the application, but in many cases,
    it is much clearer to keep it separate, especially if the CI pipelines will be
    difficult to coordinate between the deployments and the code''s own test and packaging
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here we are defining a resource called `swarm_cluster`, in which will create
    three new instances, using an `Ubuntu Focal` base image. We set the instance size
    to `t3.micro` because we are trying things out and want to minimize the cost.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Using Terraform, we can create and destroy our cloud resources in a CI/CD pipeline
    in a similar way that we test and deploy our application code. The following has
    in-depth tutorials and worked examples, and there are many community-provided
    modules to perform common tasks: [https://learn.hashicorp.com/terraform](https://learn.hashicorp.com/terraform).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Terraform''s `plan` command will show you what changes will be made to your
    cloud infrastructure when you run `terraform apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Once you are done with any experiments, you can run `terraform destroy` to clear
    up any resources managed by Terraform—although this is a dangerous command for
    a production service!
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While Docker tries to provide all the tools to deal with clusters of containers,
    managing them can become quite complex. When done properly, it requires sharing
    some configuration across hosts, and to make sure that bringing containers up
    and down is partially automated.
  prefs: []
  type: TYPE_NORMAL
- en: We very quickly come across scenarios that complicate a static configuration.
    If we need to move a microservice to a new AWS region, or a different cloud provider
    entirely, then how do we tell all the other microservices that use it? If we add
    a new feature that's controlled by a feature flag, how do we quickly turn it on
    and off? On a smaller scale, how does a load balancer know about all the containers
    that should receive traffic?
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery is an orchestration method that aims to solve these problems.
    Tools such as **Consul** ([https://www.consul.io/](https://www.consul.io/)) and
    **etcd** ([https://etcd.io/](https://etcd.io/)) allow values to be stored behind
    well-known keys and updated dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of deploying your service with full knowledge of all the URLs it might
    connect to, you provide it with the address of a service discovery tool, and the
    list of keys it should look up for each element. When a microservice starts up,
    and at regular intervals, it can check where it should be sending traffic, or
    whether a feature should be turned on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use `etcd` as an example, with a basic Quart service, while also utilizing
    the `etcd3` Python library. Assuming you have `etcd` running with the default
    options after following the instructions on their website, we can add some configuration-updating
    code to our service, and have an endpoint that returns the URL we would contact,
    if the application was more complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we load the keys in the `settings_map` when the application
    starts, including `/services/dataservice/url`, which we can then validate and
    use. Any time that value changes in `etcd`, the `watch_callback` function will
    be run in its own thread, and the app''s configuration updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Updating the live configuration is a simple command!
  prefs: []
  type: TYPE_NORMAL
- en: If your application has configuration options that depend on each other, such
    as pairs of access tokens, it is best to encode them in a single option so that
    they are updated in a single operation. If something fails and only one of a co-dependent
    set of configuration settings is updated, your application will behave in unwanted
    and unexpected ways.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The commands required to run several containers on the same host can be quite
    long once you need to add names and networks and bind several sockets. Docker
    Compose ([https://docs.docker.com/compose/](https://docs.docker.com/compose/))
    simplifies the task by letting you define multiple containers' configuration in
    a single configuration file, as well as how those containers depend on each other.
    This utility is installed on macOS and Windows alongside Docker. For Linux distributions,
    there should be a system package available to install it, or you can obtain an
    installation script by following the instructions at [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/).
  prefs: []
  type: TYPE_NORMAL
- en: Once the script is installed on your system, create a `yaml` file containing
    the information about services and networks that you want to run. The default
    filename is `docker-compose.yml`, and so we will use that name for our examples
    to make the commands simpler.
  prefs: []
  type: TYPE_NORMAL
- en: 'The compose configuration file has many options that let you define every aspect
    of the deployment of several containers. It''s like a Makefile for a group of
    containers. This URL lists all options: [https://docs.docker.com/compose/compose-file/](https://docs.docker.com/compose/compose-file/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, the `.yaml` file is placed one directory above two
    of our Jeeves microservices and defines three services: the `dataservice` and
    the `tokendealer`, which are built locally from their Dockerfile; the third is
    RabbitMQ, and we use an image published on Docker Hub to run that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Compose` file also creates networks with its `networks` sections, allowing
    the containers to communicate with each other. They will get private DNS entries
    so they can be referred to with the image names, such as `dataservice`, `tokendealer`,
    and `rabbitmq` in the example above. To build and run those three containers,
    you can use the `up` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The first time that command is executed, the two local container images will
    be built. These will either be static, or you can assign volumes to them to mount
    in the source code and continue developing on them.
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker Compose is great when you want to provide a full working stack
    for your microservices, which includes every piece of software needed to run it.
    For instance, if you are using a Postgres database, you can use the Postgres image
    ([https://hub.docker.com/_/postgres/](https://hub.docker.com/_/postgres/)) and
    link it to your service in a Docker Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: Containerizing everything, even the databases, is a great way to showcase your
    software, or simply a good option for development purposes. However, as we stated
    earlier, a Docker container should be seen as an ephemeral filesystem. So if you
    use a container for your database, make sure that the directory where the data
    is written is mounted on the host filesystem. In most cases, however, the database
    service is usually its dedicated server on a production deployment. Using a container
    does not make much sense and adds only a little bit of overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker has a built-in cluster functionality called **swarm** mode ([https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/)).
    This mode has an impressive list of features, which lets you manage all your container
    clusters from a single utility. This makes it ideal for smaller deployments or
    ones that do not need to scale up and down as flexibly to meet changing demands.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have deployed a cluster, you need to set up a load balancer so that
    all the instances of your cluster are sharing the workload. The load balancer
    is commonly software such as nginx, OpenResty, or HAProxy, and is the entry point
    to distribute the incoming requests on clusters.
  prefs: []
  type: TYPE_NORMAL
- en: To set up a swarm, all we really need are three EC2 instances, provided we can
    connect to them using port `22` for SSH access to configure them, and port `2377`
    for Docker's own communication. We should also allow any ports that our application
    needs, such as port `443` for HTTPS connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a swarm, we must create a manager node that will organize the rest.
    Using one of the nodes you have just created, connect to it using SSH, and convert
    it to a Docker Swarm manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To add a worker to this swarm, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To add a manager to this swarm, run `docker swarm join-token manager` and follow
    the instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the `docker` `swarm` command provided, and paste it into an SSH session
    on the other instances you have created. You may need to run `sudo` to gain root
    access before the commands will work. On the manager node, we can now see all
    our workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now all we need to do is create our services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we can scale our service up and down as we need to. To create five
    copies of our dataservice, we would issue a scale command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As long as our manager node remains available, and some of the worker nodes
    are up, then our container service will remain active. We can terminate one of
    the cloud instances and watch things rebalance to the remaining instances with
    `docker service ps`. Adding more nodes is as easy as adjusting a variable in the
    Terraform configuration and re-running `terraform` `apply`, before then joining
    them to the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Looking after the suite of cloud instances is still work, but this environment
    provides a neat way of providing a resilient container deployment, especially
    early on in an application's life.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Originally designed by Google, but now maintained by an independent foundation,
    **Kubernetes** ([https://kubernetes.io/](https://kubernetes.io/), also known as
    k8s) provides a platform-independent way of automating work with containerized
    systems, allowing you to describe the system in terms of different components,
    and issuing commands to a controller to adjust settings.
  prefs: []
  type: TYPE_NORMAL
- en: Like Docker Swarm, Kubernetes also runs on a cluster of servers. It's possible
    to run this cluster yourself, although some cloud providers do have a service
    that makes managing the fleet of instances much easier. A good example of this
    is the **eksctl** utility for AWS ([https://eksctl.io/](https://eksctl.io/)).
    While not created by Amazon, it is an officially supported client for creating
    clusters in Amazon's Elastic Kubernetes Service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than create all the AWS resources yourself, or create Terraform configuration
    to do so, `eksctl` performs all the work for you, with sensible defaults for experimenting
    with Kubernetes. To get started, it is best to use the AWS credentials we created
    for earlier examples and to install both `eksctl` and `kubectl`—the Kubernetes
    command line. The AWS credentials will be used by `eksctl` to create the cluster
    and other necessary resources, and once done, `kubectl` can be used to deploy
    services and software. Unlike Docker Swarm, kubectl''s administrative commands
    are designed to be run from your own computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'It will take a few minutes to create the cluster, but once done, it will write
    the credentials `kubectl` needs to the correct file, so no further setup should
    be needed. We told `eksctl` to create four nodes, and that''s exactly what it
    has done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'For the moment, we have nothing running on our `k8s` cluster, so we shall create
    some work for it to do. The fundamental unit of work for `k8s` is a `pod`, which
    describes a set of running containers on the cluster. We have not created any
    of our own yet, but there are some running in a different namespace to help k8s
    do its own work of managing the rest of the tasks we set it. Namespaces like this
    can be useful for grouping sets of tasks together, making it easier to understand
    what is important when looking at the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: A Pod is a low-level description of some work for the cluster, and to help make
    life easier, there are higher-level abstractions for different types of work,
    such as a `Deployment` for a stateless application, such as a web interface or
    proxy, a `StatefulSet` for when your workload needs storage attached rather than
    keeping its data in a different service, as well as Jobs and CronJobs for one-off
    tasks and scheduled repeating tasks, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes accepts manifests of instructions that it should apply. A good starting
    point is to set up nginx, with a manifest such as this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We include some metadata about the type of resource we are requesting—a `Deployment`—and
    its name, and then dive into the specification for the service. Down at the bottom
    of the file, we can see that we've asked for a container based on the `nginx:1.21.0
    image`, and that it should have port `80` open. One layer up, we describe this
    container specification as a template that we use to create three different copies
    and run them on our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Using kubectl''s `describe` subcommand, we get even more information about
    what was created for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If we decide we need more nginx containers, we can update the manifest. Change
    the number of replicas in our `yaml` file from three to eight, and re-apply the
    manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: A similar change could be performed to upgrade the version of nginx, and Kubernetes
    has several strategies to perform updates of a service so that end users are unlikely
    to notice it happening. For example, it is possible to create an entirely new
    Pod of containers and redirect traffic to it, but it's also possible to do rolling
    updates inside a Pod, where a container is only destroyed when its replacement
    has successfully started. How can you tell the container was successfully started?
    Kubernetes allows you to describe what it should look for to check whether a container
    can do its work, and how long it should wait for a container to start, with its
    liveness and readiness checks.
  prefs: []
  type: TYPE_NORMAL
- en: If you have been following along with the examples, remember to delete the cloud
    resources when you are done, as they cost. To remove just the nginx-deeployment
    we created, use `kubectl`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'But to destroy the entire cluster, return to using `eksctl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This is a very brief overview of an enormously powerful tool, as the topic could
    cover an entire book by itself. For those who need it, the time spent learning
    Kubernetes is well spent, but as ever, you must assess the needs of your own application,
    and whether something simpler will get the job done.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we looked at how microservices can be containerized with containers,
    and how you can create a deployment entirely based on Docker images. Containers
    are a well-established technology that is widely used to run internet services.
    The most important thing to keep in mind is that a containerized application is
    ephemeral: it is designed to be destroyed and recreated on demand, and any data
    that is not externalized using a mount point is lost.'
  prefs: []
  type: TYPE_NORMAL
- en: For provisioning and clustering your services, there is no generic solution,
    as the tools you use will depend on your needs. From a simple Docker Compose setup
    to a full Kubernetes cluster, each option provides different complexity and benefits.
    The best choice often depends on where you are to deploy your services, how your
    teams work, and how large your application needs to be in the present—there is
    no sense in planning for an unknowable future.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to tackle this problem is to take baby steps by first deploying
    everything manually, then automating where it makes sense. Automation is great,
    but can rapidly become difficult if you use a toolset you do not fully understand,
    or that is too complex for your needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a guide, consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose when you need to deploy multiple containers in a small environment,
    and do not need to manage a large infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm when you need flexibility in how many containers are deployed,
    to respond to a changing situation, and are happy to manage a larger cloud infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes when automation and flexibility are paramount, and you have people
    and time available to manage the infrastructure and handle the complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will not be locked into one orchestration tool once you choose it, as the
    containers you build can be used in any of them, but moving to a different orchestration
    tool can be hard work, depending on how complex your configuration is.
  prefs: []
  type: TYPE_NORMAL
- en: In that vein, to make their services easier to use and more appealing, cloud
    providers have built-in features to handle deployments. The three largest cloud
    providers are currently AWS, Google Cloud, and Microsoft Azure, although many
    other good options exist.
  prefs: []
  type: TYPE_NORMAL
