<html><head></head><body>
        

            
                <h1 class="header-title">Fast Array Operations with NumPy and Pandas</h1>
            

            
                
<p>NumPy is the <em>de facto</em> standard for scientific computing in Python. It extends Python with a flexible multidimensional array that allows fast and concise mathematical calculations.</p>
<p>NumPy provides common data structures and algorithms designed to express complex mathematical operations using a concise syntax. The multidimensional array, <kbd>numpy.ndarray</kbd>, is internally based on C arrays. Apart from the performance benefits, this choice allows NumPy code to easily interface with the existing C and FORTRAN routines; NumPy is helpful in bridging the gap between Python and the legacy code written using those languages.</p>
<p>In this chapter, we will learn how to create and manipulate NumPy arrays. We will also explore the NumPy broadcasting feature used to rewrite complex mathematical expressions in an efficient and succinct manner.</p>
<p>Pandas is a tool that relies heavily on NumPy and provides additional data structures and algorithms targeted toward data analysis. We will introduce the main Pandas features and its usage.  We will also learn how to achieve high performance from Pandas data structures and vectorized operations. </p>
<p>The topics covered in this chapter are as follows:</p>
<ul>
<li>Creating and manipulating NumPy arrays</li>
<li>Mastering NumPy's broadcasting feature for fast and succinct vectorized operations</li>
<li>Improving our particle simulator with NumPy</li>
<li>Reaching optimal performance with <kbd>numexpr</kbd></li>
<li>Pandas fundamentals</li>
<li>Database-style operations with Pandas</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Getting started with NumPy</h1>
            

            
                
<p>The NumPy library revolves around its multidimensional array object, <kbd>numpy.ndarray</kbd>. NumPy arrays are collections of elements of the same data type; this fundamental restriction allows NumPy to pack the data in a way that allows for high-performance mathematical operations.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Creating arrays</h1>
            

            
                
<p>You can create NumPy arrays using the <kbd>numpy.array</kbd> function. It takes a list-like object (or another array) as input and, optionally, a string expressing its data type. You can interactively test array creation using an IPython shell, as follows:</p>
<pre>
    import numpy as np <br/>    a = np.array([0, 1, 2]) 
</pre>
<p>Every NumPy array has an associated data type that can be accessed using the <kbd>dtype</kbd> attribute. If we inspect the <kbd>a</kbd> array, we find that its  <kbd>dtype</kbd> is <kbd>int64</kbd>, which stands for 64-bit integer:</p>
<pre>
    a.dtype <br/>    # Result: <br/>    # dtype('int64') 
</pre>
<p>We may decide to convert those integer numbers to <kbd>float</kbd> type. To do this, we can either pass the <kbd>dtype</kbd> argument at array initialization or cast the array to another data type using the <kbd>astype</kbd> method. The two ways to select a data type are shown in the following code:</p>
<pre>
    a = np.array([1, 2, 3], dtype='float32') <br/>    a.astype('float32') <br/>    # Result:<br/>    # array([ 0.,  1.,  2.], dtype=float32) 
</pre>
<p>To create an array with two dimensions (an array of arrays), we can perform the initialization using a nested sequence, as follows:</p>
<pre>
    a = np.array([[0, 1, 2], [3, 4, 5]]) <br/>    print(a) <br/>    # Output:<br/>    # [[0 1 2]<br/>    #  [3 4 5]] 
</pre>
<p>The array created in this way has two dimensions, which are called <strong>axes</strong> in NumPy's jargon. An array formed in this way is like a table that contains two rows and three columns. We can access the axes using the <kbd>ndarray.shape</kbd> attribute:</p>
<pre>
    a.shape <br/>    # Result:<br/>    # (2, 3) 
</pre>
<p>Arrays can also be reshaped as long as the product of the shape dimensions is equal to the total number of elements in the array (that is, the total number of elements is conserved). For example, we can reshape an array containing 16 elements in the following ways: <kbd>(2, 8)</kbd>, <kbd>(4, 4)</kbd>, or <kbd>(2, 2, 4)</kbd>. To reshape an array, we can either use the <kbd>ndarray.reshape</kbd> method or assign a new value to the <kbd>ndarray.shape</kbd> tuple. The following code illustrates the use of the <kbd>ndarray.reshape</kbd> method:</p>
<pre>
    a = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, <br/>                  9, 10, 11, 12, 13, 14, 15]) <br/>    a.shape <br/>    # Output:<br/>    # (16,)<br/><br/>    a.reshape(4, 4) # Equivalent: a.shape = (4, 4) <br/>    # Output: <br/>    # array([[ 0,  1,  2,  3],<br/>    #        [ 4,  5,  6,  7],<br/>    #        [ 8,  9, 10, 11],<br/>    #        [12, 13, 14, 15]]) 
</pre>
<p>Thanks to this property, you can freely add dimensions of size one. You can reshape an array with 16 elements to <kbd>(16, 1)</kbd>, <kbd>(1, 16)</kbd>, <kbd>(16, 1, 1)</kbd>, and so on. In the next section, we will extensively use this feature to implement complex operations through <em>broadcasting</em>. </p>
<p>NumPy provides convenience functions, shown in the following code, to create arrays filled with zeros, ones, or with no initial value (in this case, their actual value is meaningless and depends on the memory state). Those functions take the array shape as a tuple and, optionally, its <kbd>dtype</kbd>:</p>
<pre>
    np.zeros((3, 3)) <br/>    np.empty((3, 3)) <br/>    np.ones((3, 3), dtype='float32') 
</pre>
<p>In our examples, we will use the <kbd>numpy.random</kbd> module to generate random floating point numbers in the <kbd>(0, 1)</kbd> interval. The <kbd>numpy.random.rand</kbd> will take a shape and return an array of random numbers with that shape:</p>
<pre>
    np.random.rand(3, 3) 
</pre>
<p>Sometimes it is convenient to initialize arrays that have the same shape as that of some other array. For that purpose, NumPy provides some handy functions, such as <kbd>zeros_like</kbd>, <kbd>empty_like</kbd>, and <kbd>ones_like</kbd>. These functions can be used as follows:</p>
<pre>
    np.zeros_like(a) <br/>    np.empty_like(a) <br/>    np.ones_like(a) 
</pre>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Accessing arrays</h1>
            

            
                
<p>The NumPy array interface is, on a shallow level, similar to that of Python lists. NumPy arrays can be indexed using integers and iterated using a <kbd>for</kbd> loop:</p>
<pre>
    A = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8]) <br/>    A[0] <br/>    # Result:<br/>    # 0 <br/><br/>    [a for a in A] <br/>    # Result:<br/>    # [0, 1, 2, 3, 4, 5, 6, 7, 8] 
</pre>
<p>In NumPy, array elements and sub-arrays can be conveniently accessed by using multiple values separated by commas inside the subscript operator, <kbd>[]</kbd>. If we take a <kbd>(3,3)</kbd> array (an array containing three triplets), and we access the element with index <kbd>0</kbd>, we obtain the first row, as follows:</p>
<pre>
    A = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) <br/>    A[0] <br/>    # Result:<br/>    # array([0, 1, 2]) 
</pre>
<p>We can index the row again by adding another index separated by a comma. To get the second element of the first row, we can use the <kbd>(0, 1)</kbd> index. An important observation is that the <kbd>A[0, 1]</kbd> notation is actually a shorthand for <kbd>A[(0, 1)]</kbd>, that is, we are actually indexing using a <em>tuple</em>! Both the versions are shown in the following snippet:</p>
<pre>
    A[0, 1] <br/>    # Result:<br/>    # 1<br/><br/>    # Equivalent version using tuple<br/>    A[(0, 1)]
</pre>
<p>NumPy allows you to slice arrays into multiple dimensions. If we slice on the first dimension, we can obtain a collection of triplets, shown as follows:</p>
<pre>
    A[0:2] <br/>    # Result:<br/>    # array([[0, 1, 2], <br/>    #        [3, 4, 5]]) 
</pre>
<p>If we slice the array again on the second dimension with <kbd>0:2</kbd>, we are basically extracting the first two elements from the collection of triplets shown earlier. This results in an array of shape <kbd>(2, 2)</kbd>, shown in the following code:</p>
<pre>
    A[0:2, 0:2] <br/>    # Result:<br/>    # array([[0, 1], <br/>    #        [3, 4]]) 
</pre>
<p>Intuitively, you can update the values in the array using both numerical indexes and slices. An example is illustrated in the following code snippet:</p>
<pre>
    A[0, 1] = 8 <br/>    A[0:2, 0:2] = [[1, 1], [1, 1]]
</pre>
<p>Indexing with the slicing syntax is very fast because, unlike lists, it doesn't produce a copy of the array. In NumPy's terminology, it returns a <em>view</em> of the same memory area. If we take a slice of the original array, and then we change one of its values, the original array will be updated as well. The following code illustrates an example of this feature:</p>
<pre>
    a= np.array([1, 1, 1, 1]) <br/>    a_view = a[0:2] <br/>    a_view[0] = 2 <br/>    print(a) <br/>    # Output:<br/>    # [2 1 1 1] 
</pre>
<p>It is important to be extra careful when mutating NumPy arrays. Since views share data, changing the values of a view can result in hard-to-find bugs. To prevent side effects, you can set the <kbd>a.flags.writeable = False</kbd> flag, which will prevent accidental mutation of the array or any of its views.</p>
<p>We can take a look at another example that shows how the slicing syntax can be used in a real-world setting. We define an <kbd>r_i</kbd> array, shown in the following line of code, which contains a set of 10 coordinates (<em>x</em>, <em>y</em>). Its shape will be <kbd>(10, 2)</kbd>:</p>
<pre>
    r_i = np.random.rand(10, 2)
</pre>
<p>If you have a hard time distinguishing arrays that differ in the axes order, for example between an a array of shape <kbd>(10, 2)</kbd> and <kbd>(2, 10)</kbd>, it is useful to think that every time you say the word <em>of</em>, you should introduce a new dimension. An array with ten elements <em>of</em> size two will be <kbd>(10, 2)</kbd>. Conversely, an array with two elements <em>of</em> size ten will be <kbd>(2, 10)</kbd>.</p>
<p>A typical operation we may be interested in is the extraction of the <em>x</em> component from each coordinate. In other words, you want to extract the <kbd>(0, 0)</kbd>, <kbd>(1, 0)</kbd>, <kbd>(2, 0)</kbd>, and so on items, resulting in an array with shape <kbd>(10,)</kbd>. It is helpful to think that the first index is <em>moving</em> while the second one is <em>fixed</em> (at <kbd>0</kbd>). With this in mind, we will slice every index on the first axis (the moving one) and take the first element (the fixed one) on the second axis, as shown in the following line of code:</p>
<pre>
    x_i = r_i[:, 0] 
</pre>
<p>On the other hand, the following expression will keep the first index fixed and the second index moving, returning the first (<em>x</em>, <em>y</em>) coordinate:</p>
<pre>
    r_0 = r_i[0, :] 
</pre>
<p>Slicing all the indexes over the last axis is optional; using <kbd>r_i[0]</kbd> has the same effect as <kbd>r_i[0, :]</kbd>.</p>
<p>NumPy allows you to index an array using another NumPy array made of either integer or Boolean values--a feature called <em>fancy indexing</em>.</p>
<p>If you index an array (say, <kbd>a</kbd>) with another array of integers (say, <kbd>idx</kbd>), NumPy will interpret the integers as indexes and will return an array containing their corresponding values. If we index an array containing 10 elements with <kbd>np.array([0, 2, 3])</kbd>, we obtain an array of shape <kbd>(3,)</kbd> containing the elements at positions <kbd>0</kbd>, <kbd>2</kbd>, and <kbd>3</kbd>. The following code gives us an illustration of this concept:</p>
<pre>
    a = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) <br/>    idx = np.array([0, 2, 3]) <br/>    a[idx] <br/>    # Result:<br/>    # array([9, 7, 6]) 
</pre>
<p>You can use fancy indexing on multiple dimensions by passing an array for each dimension. If we want to extract the <kbd>(0, 2)</kbd> and <kbd>(1, 3)</kbd> elements, we have to pack all the indexes acting on the first axis in one array, and the ones acting on the second axis in another. This can be seen in the following code:</p>
<pre>
    a = np.array([[0, 1, 2], [3, 4, 5], <br/>                  [6, 7, 8], [9, 10, 11]]) <br/>    idx1 = np.array([0, 1]) <br/>    idx2 = np.array([2, 3]) <br/>    a[idx1, idx2]
</pre>
<p>You can also use normal lists as index arrays, but not tuples. For example, the following two statements are equivalent:</p>
<pre>
    a[np.array([0, 1])] # is equivalent to<br/>    a[[0, 1]]
</pre>
<p>However, if you use a tuple, NumPy will interpret the following statement as an index on multiple dimensions:</p>
<pre>
    a[(0, 1)] # is equivalent to<br/>    a[0, 1] 
</pre>
<p>The index arrays are not required to be one-dimensional; we can extract elements from the original array in any shape. For example, we can select elements from the original array to form a <kbd>(2,2)</kbd> array, as shown:</p>
<pre>
    idx1 = [[0, 1], [3, 2]] <br/>    idx2 = [[0, 2], [1, 1]] <br/>    a[idx1, idx2] <br/>    # Output: <br/>    # array([[ 0,  5],<br/>    #        [10,  7]]) 
</pre>
<p>The array slicing and fancy-indexing features can be combined. This is useful, for instance, when we want to swap the <em>x</em> and <em>y</em> columns in a coordinate array. In the following code, the first index will be running over all the elements (a slice) and, for each of those, we extract the element in position <kbd>1</kbd> (the <em>y</em>) first and then the one in position <kbd>0</kbd> (the <em>x</em>):</p>
<pre>
    r_i = np.random(10, 2) <br/>    r_i[:, [0, 1]] = r_i[:, [1, 0]] 
</pre>
<p>When the index array is of the <kbd>bool</kbd> type, the rules are slightly different. The <kbd>bool</kbd> array will act like a <em>mask</em>; every element corresponding to <kbd>True</kbd> will be extracted and put in the output array. This procedure is shown in the following code:</p>
<pre>
    a = np.array([0, 1, 2, 3, 4, 5]) <br/>    mask = np.array([True, False, True, False, False, False]) <br/>    a[mask] <br/>    # Output:<br/>    # array([0, 2]) 
</pre>
<p>The same rules apply when dealing with multiple dimensions. Furthermore, if the index array has the same shape as the original array, the elements corresponding to <kbd>True</kbd> will be selected and put in the resulting array.</p>
<p>Indexing in NumPy is a reasonably fast operation. Anyway, when speed is critical, you can use the slightly faster <kbd>numpy.take</kbd> and <kbd>numpy.compress</kbd> functions to squeeze out a little more performance. The first argument of <kbd>numpy.take</kbd> is the array we want to operate on, and the second is the list of indexes we want to extract. The last argument is <kbd>axis</kbd>; if not provided, the indexes will act on the flattened array; otherwise, they will act along the specified axis:</p>
<pre>
    r_i = np.random(100, 2) <br/>    idx = np.arange(50) # integers 0 to 50 <br/><br/>    %timeit np.take(r_i, idx, axis=0) <br/>    1000000 loops, best of 3: 962 ns per loop <br/><br/>    %timeit r_i[idx] <br/>    100000 loops, best of 3: 3.09 us per loop 
</pre>
<p>The similar, but faster version for Boolean arrays is <kbd>numpy.compress</kbd>, which works in the same way. The use of <kbd>numpy.compress</kbd> is shown as follows:</p>
<pre>
    In [51]: idx = np.ones(100, dtype='bool') # all True values <br/>    In [52]: %timeit np.compress(idx, r_i, axis=0) <br/>    1000000 loops, best of 3: 1.65 us per loop <br/>    In [53]: %timeit r_i[idx] <br/>    100000 loops, best of 3: 5.47 us per loop 
</pre>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Broadcasting</h1>
            

            
                
<p>The true power of NumPy lies in its fast mathematical operations. The approach used by NumPy is to avoid stepping into the Python interpreter by performing element-wise calculation using optimized C code. <strong>Broadcasting</strong> is a clever set of rules that enables fast array calculations for arrays of similar (but not equal!) shape.</p>
<p>Whenever you do an arithmetic operation on two arrays (like a product), if the two operands have the same shape, the operation will be applied in an element-wise fashion. For example, upon multiplying two shape <kbd>(2,2)</kbd> arrays, the operation will be done between pairs of corresponding elements, producing another <kbd>(2, 2)</kbd> array, as shown in the following code:</p>
<pre>
    A = np.array([[1, 2], [3, 4]]) <br/>    B = np.array([[5, 6], [7, 8]]) <br/>    A * B <br/>    # Output:<br/>    # array([[ 5, 12],           <br/>    #        [21, 32]]) 
</pre>
<p>If the shapes of the operands don't match, NumPy will attempt to match them using broadcasting rules. If one of the operands is a <em>scalar</em> (for example, a number), it will be applied to every element of the array, as the following code illustrates:</p>
<pre>
    A * 2 <br/>    # Output: <br/>    # array([[2, 4], <br/>    #        [6, 8]]) 
</pre>
<p>If the operand is another array, NumPy will try to match the shapes starting from the last axis. For example, if we want to combine an array of shape <kbd>(3, 2)</kbd> with one of shape <kbd>(2,)</kbd>, the second array will be repeated three times to generate a <kbd>(3, 2)</kbd> array. In other words, the array is <em>broadcasted</em> along a dimension to match the shape of the other operand, as shown in the following figure:</p>
<div><img class="aligncenter size-full wp-image-2058 image-border" height="194" src="img/B06440_03CHPNO_01-1.png" width="352"/></div>
<p>If the shapes mismatch, for example, when combining a <kbd>(3, 2)</kbd> array with a <kbd>(2, 2)</kbd> array, NumPy will throw an exception.</p>
<p>If one of the axis's size is 1, the array will be repeated over this axis until the shapes match. To illustrate that point, consider that we have an array of the following shape:</p>
<pre>
    5, 10, 2 
</pre>
<p>Now, consider that we want to broadcast it with an array of shape <kbd>(5, 1, 2)</kbd>; the array will be repeated on the second axis 10 times, which is shown as follows:</p>
<pre>
    5, 10, 2 <br/>    5,  1, 2 → repeated <br/>    - - - - <br/>    5, 10, 2 
</pre>
<p>Earlier, we saw that it is possible to freely reshape arrays to add axes of size 1. Using the <kbd>numpy.newaxis</kbd> constant while indexing will introduce an extra dimension. For instance, if we have a <kbd>(5, 2)</kbd> array and we want to combine it with one of shape <kbd>(5, 10, 2)</kbd>, we can add an extra axis in the middle, as shown in the following code, to obtain a compatible <kbd>(5, 1, 2)</kbd> array:</p>
<pre>
    A = np.random.rand(5, 10, 2) <br/>    B = np.random.rand(5, 2) <br/>    A * B[:, np.newaxis, :] 
</pre>
<p>This feature can be used, for example, to operate on all possible combinations of the two arrays. One of these applications is the <em>outer product</em>. Consider that we have the following two arrays:</p>
<pre>
    a = [a1, a2, a3] <br/>    b = [b1, b2, b3] 
</pre>
<p>The outer product is a matrix containing the product of all the possible combinations (i, j) of the two array elements, as shown in the following snippet:</p>
<pre>
    a x b = a1*b1, a1*b2, a1*b3 <br/>            a2*b1, a2*b2, a2*b3 <br/>            a3*b1, a3*b2, a3*b3 
</pre>
<p>To calculate this using NumPy, we will repeat the <kbd>[a1, a2, a3]</kbd> elements in one dimension, the <kbd>[b1, b2, b3]</kbd> elements in another dimension, and then take their element-wise product, as shown in the following figure:</p>
<div><img class="aligncenter size-full image-border" height="189" src="img/image_03_002.png" width="473"/></div>
<p>Using code, our strategy will be to transform the <kbd>a</kbd> array from shape <kbd>(3,)</kbd> to shape <kbd>(3, 1)</kbd>, and the <kbd>b</kbd> array from shape <kbd>(3,)</kbd> to shape <kbd>(1, 3)</kbd>. The two arrays are broadcasted in the two dimensions and get multiplied together using the following code:</p>
<pre>
    AB = a[:, np.newaxis] * b[np.newaxis, :] 
</pre>
<p>This operation is very fast and extremely effective as it avoids Python loops and is able to process a high number of elements at speeds comparable with pure C or FORTRAN code.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Mathematical operations</h1>
            

            
                
<p>NumPy includes the most common mathematical operations available for broadcasting, by default, ranging from simple algebra to trigonometry, rounding, and logic. For instance, to take the square root of every element in the array, we can use <kbd>numpy.sqrt</kbd>, as shown in the following code:</p>
<pre>
    np.sqrt(np.array([4, 9, 16])) <br/>    # Result:<br/>    # array([2., 3., 4.]) 
</pre>
<p>The comparison operators are useful when trying to filter certain elements based on a condition. Imagine that we have an array of random numbers from <kbd>0</kbd> to <kbd>1</kbd>, and we want to extract all the numbers greater than <kbd>0.5</kbd>. We can use the <kbd>&gt;</kbd> operator on the array to obtain a <kbd>bool</kbd> array, as follows:</p>
<pre>
    a = np.random.rand(5, 3) <br/>    a &gt; 0.3 <br/>    # Result:<br/>    # array([[ True, False,  True],<br/>    #        [ True,  True,  True],<br/>    #        [False,  True,  True],<br/>    #        [ True,  True, False],<br/>    #        [ True,  True, False]], dtype=bool) 
</pre>
<p>The resulting <kbd>bool</kbd> array can then be reused as an index to retrieve the elements greater than <kbd>0.5</kbd>:</p>
<pre>
    a[a &gt; 0.5] <br/>    print(a[a&gt;0.5]) <br/>    # Output:<br/>    # [ 0.9755  0.5977  0.8287  0.6214  0.5669  0.9553  0.5894   <br/>    0.7196  0.9200  0.5781  0.8281 ] 
</pre>
<p>NumPy also implements methods such as <kbd>ndarray.sum</kbd>, which takes the sum of all the elements on an axis. If we have an array of shape <kbd>(5, 3)</kbd>, we can use the <kbd>ndarray.sum</kbd> method to sum the elements on the first axis, the second axis, or over all the elements of the array, as illustrated in the following snippet:</p>
<pre>
    a = np.random.rand(5, 3) <br/>    a.sum(axis=0) <br/>    # Result:<br/>    # array([ 2.7454,  2.5517,  2.0303]) <br/><br/>    a.sum(axis=1) <br/>    # Result:<br/>    # array([ 1.7498,  1.2491,  1.8151,  1.9320,  0.5814]) <br/><br/>    a.sum() # With no argument operates on flattened array <br/>    # Result:<br/>    # 7.3275 
</pre>
<p>Note that by summing the elements over an axis, we eliminate that axis. From the preceding example, the sum on the axis <kbd>0</kbd> produces an array of shape <kbd>(3,)</kbd>, while the sum on the axis <kbd>1</kbd> produces an array of shape <kbd>(5,)</kbd>.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Calculating the norm</h1>
            

            
                
<p>We can review the basic concepts illustrated in this section by calculating the <em>norm</em> of a set of coordinates. For a two-dimensional vector, the norm is defined as follows:</p>
<pre>
    norm = sqrt(x**2 + y**2) 
</pre>
<p>Given an array of 10 coordinates (<em>x</em>, <em>y</em>), we want to find the norm of each coordinate. We can calculate the norm by taking these steps:</p>
<ol>
<li>Square the coordinates, obtaining an array that contains <kbd>(x**2, y**2)</kbd> elements.</li>
<li>Sum those with <kbd>numpy.sum</kbd> over the last axis.</li>
<li>Take the square root, element-wise, with <kbd>numpy.sqrt</kbd>.</li>
</ol>
<p>The final expression can be compressed in a single line:</p>
<pre>
    r_i = np.random.rand(10, 2) <br/>    norm = np.sqrt((r_i ** 2).sum(axis=1)) <br/>    print(norm)<br/>    # Output:<br/>    # [ 0.7314  0.9050  0.5063  0.2553  0.0778   0.9143   1.3245  <br/>    0.9486  1.010   1.0212] 
</pre>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Rewriting the particle simulator in NumPy</h1>
            

            
                
<p>In this section, we will optimize our particle simulator by rewriting some parts of it in NumPy. We found, from the profiling we did in <a href="4db2c3e6-3485-41a5-8450-07220f6d80ec.xhtml">Chapter 1</a>, <em>Benchmarking and Profiling</em>, that the slowest part of our program is the following loop contained in the <kbd>ParticleSimulator.evolve</kbd> method:</p>
<pre>
    for i in range(nsteps): <br/>      for p in self.particles: <br/><br/>        norm = (p.x**2 + p.y**2)**0.5 <br/>        v_x = (-p.y)/norm <br/>        v_y = p.x/norm <br/><br/>        d_x = timestep * p.ang_vel * v_x <br/>        d_y = timestep * p.ang_vel * v_y <br/><br/>        p.x += d_x <br/>        p.y += d_y 
</pre>
<p>You may have noticed that the body of the loop acts solely on the current particle. If we had an array containing the particle positions and angular speed, we could rewrite the loop using a broadcasted operation. In contrast, the loop's steps depend on the previous step and cannot be parallelized in this way.</p>
<p>It is natural then, to store all the array coordinates in an array of shape <kbd>(nparticles, 2)</kbd> and the angular speed in an array of shape <kbd>(nparticles,)</kbd>, where <kbd>nparticles</kbd> is the number of particles. We'll call those arrays <kbd>r_i</kbd> and <kbd>ang_vel_i</kbd>:</p>
<pre>
    r_i = np.array([[p.x, p.y] for p in self.particles]) <br/>    ang_vel_i = np.array([p.ang_vel for p in self.particles]) 
</pre>
<p>The velocity direction, perpendicular to the vector (<em>x</em>, <em>y</em>), was defined as follows:</p>
<pre>
    v_x = -y / norm <br/>    v_y = x / norm 
</pre>
<p>The norm can be calculated using the strategy illustrated in the <em>Calculating the norm</em> section under the <em>Getting started with NumPy</em> heading:</p>
<pre>
    norm_i = ((r_i ** 2).sum(axis=1))**0.5 
</pre>
<p>For the (<em>-y</em>, <em>x</em>) components, we first need to swap the x and y columns in <kbd>r_i</kbd> and then multiply the first column by -1, as shown in the following code:</p>
<pre>
    v_i = r_i[:, [1, 0]] / norm_i <br/>    v_i[:, 0] *= -1 
</pre>
<p>To calculate the displacement, we need to compute the product of <kbd>v_i</kbd>, <kbd>ang_vel_i</kbd>, and <kbd>timestep</kbd>. Since <kbd>ang_vel_i</kbd> is of shape <kbd>(nparticles,)</kbd>, it needs a new axis in order to operate with <kbd>v_i</kbd> of shape <kbd>(nparticles, 2)</kbd>. We will do that using <kbd>numpy.newaxis</kbd>, as follows:</p>
<pre>
    d_i = timestep * ang_vel_i[:, np.newaxis] * v_i <br/>    r_i += d_i 
</pre>
<p>Outside the loop, we have to update the particle instances with the new coordinates, <em>x</em> and <em>y</em>, as follows:</p>
<pre>
    for i, p in enumerate(self.particles): <br/>      p.x, p.y = r_i[i] 
</pre>
<p>To summarize, we will implement a method called <kbd>ParticleSimulator.evolve_numpy</kbd> and benchmark it against the pure Python version, renamed as <kbd>ParticleSimulator.evolve_python</kbd>:</p>
<pre>
    def evolve_numpy(self, dt): <br/>      timestep = 0.00001 <br/>      nsteps = int(dt/timestep) <br/><br/>      r_i = np.array([[p.x, p.y] for p in self.particles]) <br/>      ang_vel_i = np.array([p.ang_vel for p in self.particles]) <br/><br/>      for i in range(nsteps): <br/><br/>        norm_i = np.sqrt((r_i ** 2).sum(axis=1)) <br/>        v_i = r_i[:, [1, 0]] <br/>        v_i[:, 0] *= -1 <br/>        v_i /= norm_i[:, np.newaxis] <br/>        d_i = timestep * ang_vel_i[:, np.newaxis] * v_i <br/>        r_i += d_i <br/><br/>        for i, p in enumerate(self.particles): <br/>          p.x, p.y = r_i[i] 
</pre>
<p>We also update the benchmark to conveniently change the number of particles and the simulation method, as follows:</p>
<pre>
    def benchmark(npart=100, method='python'): <br/>      particles = [Particle(uniform(-1.0, 1.0),     <br/>                            uniform(-1.0, 1.0),<br/>                            uniform(-1.0, 1.0))  <br/>                            for i in range(npart)] <br/><br/>      simulator = ParticleSimulator(particles) <br/><br/>      if method=='python': <br/>        simulator.evolve_python(0.1) <br/><br/>      elif method == 'numpy': <br/>        simulator.evolve_numpy(0.1) 
</pre>
<p>Let's run the benchmark in an IPython session:</p>
<pre>
    from simul import benchmark <br/>    %timeit benchmark(100, 'python') <br/>    1 loops, best of 3: 614 ms per loop <br/>    %timeit benchmark(100, 'numpy') <br/>    1 loops, best of 3: 415 ms per loop 
</pre>
<p>We have some improvement, but it doesn't look like a huge speed boost. The power of NumPy is revealed when handling big arrays. If we increase the number of particles, we will note a more significant performance boost:</p>
<pre>
    %timeit benchmark(1000, 'python') <br/>    1 loops, best of 3: 6.13 s per loop <br/>    %timeit benchmark(1000, 'numpy') <br/>    1 loops, best of 3: 852 ms per loop 
</pre>
<p>The plot in the following figure was produced by running the benchmark with different particle numbers:</p>
<div><img class="aligncenter size-full image-border" height="257" src="img/image_03_003.png" width="376"/></div>
<p>The plot shows that both the implementations scale linearly with particle size, but the runtime in the pure Python version grows much faster than the NumPy version; at greater sizes, we have a greater NumPy advantage. In general, when using NumPy, you should try to pack things into large arrays and group the calculations using the broadcasting feature.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Reaching optimal performance with numexpr</h1>
            

            
                
<p>When handling complex expressions, NumPy stores intermediate results in memory. David M. Cooke wrote a package called <kbd>numexpr</kbd>, which optimizes and compiles array expressions on the fly. It works by optimizing the usage of the CPU cache and by taking advantage of multiple processors.</p>
<p>Its usage is generally straightforward and is based on a single function--<kbd>numexpr.evaluate</kbd>. The function takes a string containing an array expression as its first argument. The syntax is basically identical to that of NumPy. For example, we can calculate a simple <kbd>a + b * c</kbd> expression in the following way:</p>
<pre>
    a = np.random.rand(10000) <br/>    b = np.random.rand(10000) <br/>    c = np.random.rand(10000) <br/>    d = ne.evaluate('a + b * c') 
</pre>
<p>The <kbd>numexpr</kbd> package increases the performances in almost all cases, but to get a substantial advantage, you should use it with large arrays. An application that involves a large array is the calculation of a <em>distance matrix</em>. In a particle system, a distance matrix contains all the possible distances between the particles. To calculate it, we should first calculate all the vectors connecting any two particles <kbd>(i,j)</kbd>, as follows:</p>
<pre>
    x_ij = x_j - x_i <br/>    y_ij = y_j - y_i. 
</pre>
<p>Then, we calculate the length of this vector by taking its norm, as in the following code:</p>
<pre>
    d_ij = sqrt(x_ij**2 + y_ij**2) 
</pre>
<p>We can write this in NumPy by employing the usual broadcasting rules (the operation is similar to the outer product):</p>
<pre>
    r = np.random.rand(10000, 2) <br/>    r_i = r[:, np.newaxis] <br/>    r_j = r[np.newaxis, :] <br/>    d_ij = r_j - r_i 
</pre>
<p>Finally, we calculate the norm over the last axis using the following line of code:</p>
<pre>
    d_ij = np.sqrt((d_ij ** 2).sum(axis=2)) 
</pre>
<p>Rewriting the same expression using the <kbd>numexpr</kbd> syntax is extremely easy. The <kbd>numexpr</kbd> package doesn't support slicing in its array expression; therefore, we first need to prepare the operands for broadcasting by adding an extra dimension, as follows:</p>
<pre>
    r = np.random(10000, 2) <br/>    r_i = r[:, np.newaxis] <br/>    r_j = r[np.newaxis, :] 
</pre>
<p>At that point, we should try to pack as many operations as possible in a single expression to allow a significant optimization.</p>
<p>Most of the NumPy mathematical functions are also available in <kbd>numexpr</kbd>. However, there is a limitation--the reduction operations (the ones that reduce an axis, such as sum) have to happen last. Therefore, we have to first calculate the sum, then step out of <kbd>numexpr</kbd>, and finally calculate the square root in another expression:</p>
<pre>
    d_ij = ne.evaluate('sum((r_j - r_i)**2, 2)') <br/>    d_ij = ne.evaluate('sqrt(d_ij)') 
</pre>
<p>The <kbd>numexpr</kbd> compiler will avoid redundant memory allocation by not storing intermediate results. When possible, it will also distribute the operations over multiple processors. In the <kbd>distance_matrix.py</kbd> file, you will find two functions that implement the two versions: <kbd>distance_matrix_numpy</kbd> and <kbd>distance_matrix_numexpr</kbd>:</p>
<pre>
    from distance_matrix import (distance_matrix_numpy, <br/>                                 distance_matrix_numexpr) <br/>    %timeit distance_matrix_numpy(10000) <br/>    1 loops, best of 3: 3.56 s per loop <br/>    %timeit distance_matrix_numexpr(10000) <br/>    1 loops, best of 3: 858 ms per loop 
</pre>
<p>By simply converting the expressions to use <kbd>numexpr</kbd>, we were able to obtain a 4.5x increase in performance over standard NumPy. The <kbd>numexpr</kbd> package can be used every time you need to optimize a NumPy expression that involves large arrays and complex operations, and you can do so with minimal changes in the code.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Pandas</h1>
            

            
                
<p>Pandas is a library originally developed by Wes McKinney, which was designed to analyze datasets in a seamless and performant way. In recent years, this powerful library has seen an incredible growth and huge adoption by the Python community. In this section, we will introduce the main concepts and tools provided in this library, and we will use it to increase performance of various usecases that can't otherwise be addressed with NumPy's vectorized operations and broadcasting.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Pandas fundamentals</h1>
            

            
                
<p>While NumPy deals mostly with arrays, Pandas main data structures are <kbd>pandas.Series</kbd>, <kbd>pandas.DataFrame</kbd>, and <kbd>pandas.Panel</kbd>. In the rest of this chapter, we will abbreviate <kbd>pandas</kbd> with <kbd>pd</kbd>.</p>
<p>The main difference between a <kbd>pd.Series</kbd> object and an <kbd>np.array</kbd> is that a <kbd>pd.Series</kbd> object associates a specific <em>key</em> to each element of an array. Let’s see how this works in practice with an example.</p>
<p>Let's assume that we are trying to test a new blood pressure drug, and we want to store, for each patient, whether the patient's blood pressure improved after administering the drug. We can encode this information by associating to each subject ID (represented by an integer),  <kbd>True</kbd> if the drug was effective, and <kbd>False</kbd> otherwise.</p>
<p>We can create a <kbd>pd.Series</kbd> object by associating an array of keys, the patients, to the array of values that represent the drug effectiveness. The array of keys can be passed to the <kbd>Series</kbd> constructor using the <kbd>index</kbd> argument, as shown in the following snippet:</p>
<pre>
    import pandas as pd<br/>    patients = [0, 1, 2, 3]<br/>    effective = [True, True, False, False]<br/><br/>    effective_series = pd.Series(effective, index=patients)
</pre>
<p>Associating a set of integers from 0 to <em>N</em> to a set of values can technically be implemented with <kbd>np.array</kbd>, since, in this case, the key will simply be the position of the element in the array. In Pandas, keys are not limited to integers but can also be strings, floating point numbers, and also generic (hashable) Python objects. For example, we can easily turn our IDs into strings with little effort, as shown in the following code:</p>
<pre>
    patients = ["a", "b", "c", "d"]<br/>    effective = [True, True, False, False]<br/><br/>    effective_series = pd.Series(effective, index=patients)
</pre>
<p>An interesting observation is that, while NumPy arrays can be thought of as a contiguous collection of values similar to Python lists, the Pandas <kbd>pd.Series</kbd> object can be thought of as a structure that maps keys to values, similar to Python dictionaries.</p>
<p>What if you want to store the initial and final blood pressure for each patient? In Pandas, one can use a <kbd>pd.DataFrame</kbd> object to associate multiple data to each key.</p>
<p><kbd>pd.DataFrame</kbd> can be initialized, similarly to a <kbd>pd.Series</kbd> object, by passing a dictionary of columns and an index. In the following example, we will see how to create a <kbd>pd.DataFrame</kbd> containing four columns that represent the initial and final measurements of systolic and dyastolic blood pressure for our patients:</p>
<pre>
    patients = ["a", "b", "c", "d"]<br/><br/>    columns = {<br/>      "sys_initial": [120, 126, 130, 115],<br/>      "dia_initial": [75, 85, 90, 87],<br/>      "sys_final": [115, 123, 130, 118],<br/>      "dia_final": [70, 82, 92, 87]<br/>    }<br/>    <br/>    df = pd.DataFrame(columns, index=patients)
</pre>
<p>Equivalently, you can think of a <kbd>pd.DataFrame</kbd> as a collection of <kbd>pd.Series</kbd>. In fact, it is possible to directly initialize a <kbd>pd.DataFrame</kbd>, using a dictionary of <kbd>pd.Series</kbd> instances:</p>
<pre>
    columns = {<br/>      "sys_initial": pd.Series([120, 126, 130, 115], index=patients),<br/>      "dia_initial": pd.Series([75, 85, 90, 87], index=patients),<br/>      "sys_final": pd.Series([115, 123, 130, 118], index=patients),<br/>      "dia_final": pd.Series([70, 82, 92, 87], index=patients)<br/>    }<br/>    df = pd.DataFrame(columns)
</pre>
<p>To inspect the content of a <kbd>pd.DataFrame</kbd> or <kbd>pd.Series</kbd> object, you can use the <kbd>pd.Series.head</kbd> and <kbd>pd.DataFrame.head</kbd> methods, which print the first few rows of the dataset:</p>
<pre>
    effective_series.head()<br/>    # Output:<br/>    # a True<br/>    # b True<br/>    # c False<br/>    # d False<br/>    # dtype: bool<br/><br/>    df.head()<br/>    # Output:<br/>    #    dia_final  dia_initial  sys_final  sys_initial<br/>    # a         70           75        115          120<br/>    # b         82           85        123          126<br/>    # c         92           90        130          130<br/>    # d         87           87        118          115
</pre>
<p>Just like a <kbd>pd.DataFrame</kbd> can be used to store a collection of <kbd>pd.Series</kbd>, you can use a <kbd>pd.Panel</kbd> to store a collection of <kbd>pd.DataFrames</kbd>. We will not cover the usage of <kbd>pd.Panel</kbd> as it is not used as often as <kbd>pd.Series</kbd> and <kbd>pd.DataFrame</kbd>. To learn more about <kbd>pd.Panel</kbd>, ensure that you refer to the excellent documentation at <a href="http://pandas.pydata.org/pandas-docs/stable/dsintro.html#panel">http://pandas.pydata.org/pandas-docs/stable/dsintro.html#panel</a>.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Indexing Series and DataFrame objects</h1>
            

            
                
<p>Retrieving data from a <kbd>pd.Series</kbd>, given its <em>key</em>, can be done intuitively by indexing the <kbd>pd.Series.loc</kbd> attribute:</p>
<pre>
    effective_series.loc["a"]<br/>    # Result:<br/>    # True
</pre>
<p>It is also possible to access the elements, given its <em>position</em> in the underlying array, using the <kbd>pd.Series.iloc</kbd> attribute:</p>
<pre>
    effective_series.iloc[0]<br/>    # Result:<br/>    # True
</pre>
<p>You can also use the <kbd>pd.Series.ix</kbd> attribute for mixed access. If the key is not an integer, it will try to match by key, otherwise it will extract the element at the position indicated by the integer. A similar behavior will take place when you access the <kbd>pd.Series</kbd> directly. The following example demonstrates these concepts:</p>
<pre>
    effective_series.ix["a"] # By key<br/>    effective_series.ix[0]   # By position<br/><br/>    # Equivalent<br/>    effective_series["a"] # By key<br/>    effective_series[0]   # By position
</pre>
<p>Note that if the index is made of integers, this method will fall back to the key-only method (like <kbd>loc</kbd>). To index by position in this scenario, the <kbd>iloc</kbd> method is your only option.</p>
<p>Indexing <kbd>pd.DataFrame</kbd> works in a similar way. For example, you can use <kbd>pd.DataFrame.loc</kbd> to extract a row by key, and you can use <kbd>pd.DataFrame.iloc</kbd> to extract a row by position:</p>
<pre>
    df.loc["a"]<br/>    df.iloc[0]<br/>    # Result:<br/>    # dia_final 70<br/>    # dia_initial 75<br/>    # sys_final 115<br/>    # sys_initial 120<br/>    # Name: a, dtype: int64
</pre>
<p>An important aspect is that the return type in this case is a <kbd>pd.Series</kbd>, where each column is a new key. In order to retrieve a specific row and column, you can use the following code. The <kbd>loc</kbd> attribute will index both row and column by key, while the <kbd>iloc</kbd> version will index row and column by an integer:</p>
<pre>
    df.loc["a", "sys_initial"] # is equivalent to<br/>    df.loc["a"].loc["sys_initial"]<br/><br/>    df.iloc[0, 1] # is equivalent to<br/>    df.iloc[0].iloc[1]
</pre>
<p>Indexing a <kbd>pd.DataFrame</kbd> using the <kbd>ix</kbd> attribute is convenient to mix and match index and location-based indexing. For example, retrieving the <kbd>"sys_initial"</kbd> column for the row at position 0 can be accomplished as follows:</p>
<pre>
    df.ix[0, "sys_initial"] 
</pre>
<p>Retrieving a column from a <kbd>pd.DataFrame</kbd> by name can be achieved by regular indexing or attribute access.  To retrieve a column by position, you can either use <kbd>iloc</kbd> or use the <kbd>pd.DataFrame.column</kbd> attribute to retrieve the name of the column:</p>
<pre>
    # Retrieve column by name<br/>    df["sys_initial"] # Equivalent to<br/>    df.sys_initial<br/><br/>    # Retrieve column by position<br/>    df[df.columns[2]] # Equivalent to<br/>    df.iloc[:, 2]
</pre>
<p>The mentioned methods also support more advanced indexing similar to those of NumPy, such as <kbd>bool</kbd>, lists, and <kbd>int</kbd> arrays.</p>
<p>Now it's time for some performance considerations. There are some differences between an index in Pandas and a dictionary. For example, while the keys of a dictionary cannot contain duplicates, Pandas indexes can contain repeated elements. This flexibility, however, comes at a cost--if we try to access an element in a non-unique index, we may incur substantial performance loss--the access will be <em>O</em>(<em>N</em>), like a linear search, rather than <em>O</em>(1), like a dictionary.</p>
<p>A way to mitigate this effect is to sort the index; this will allow Pandas to use a binary search algorithm with a computational complexity of <em>O</em>(<em>log</em>(<em>N</em>)), which is much better. This can be accomplished using the <kbd>pd.Series.sort_index</kbd> function, as in the following code (the same applies for <kbd>pd.DataFrame</kbd>):</p>
<pre>
    # Create a series with duplicate index<br/>    index = list(range(1000)) + list(range(1000))<br/><br/>    # Accessing a normal series is a O(N) operation<br/>    series = pd.Series(range(2000), index=index)<br/><br/>    # Sorting the will improve look-up scaling to O(log(N))<br/>    series.sort_index(inplace=True)
</pre>
<p>The timings for the different versions are summarized in the following table:</p>
<table>
<tbody>
<tr>
<td><strong>Index type</strong></td>
<td><strong>N=10000</strong></td>
<td><strong>N=20000</strong></td>
<td><strong>N=30000</strong></td>
<td><strong>Time</strong></td>
</tr>
<tr>
<td>Unique</td>
<td>12.30</td>
<td>12.58</td>
<td>13.30</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td>Non unique</td>
<td>494.95</td>
<td>814.10</td>
<td>1129.95</td>
<td><em>O</em>(N)</td>
</tr>
<tr>
<td>Non unique (sorted)</td>
<td>145.93</td>
<td>145.81</td>
<td>145.66</td>
<td><em>O</em>(<em>log</em>(<em>N</em>))</td>
</tr>
</tbody>
</table>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Database-style operations with Pandas</h1>
            

            
                
<p>You may have noted that the “tabular” data is similar to what is usually stored in a database. A database is usually indexed using a primary key, and the various columns can have different data types, just like in a <kbd>pd.DataFrame</kbd>. </p>
<p>The efficiency of the index operations in Pandas makes it suitable for database style manipulations, such as counting, joining, grouping, and aggregations.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Mapping</h1>
            

            
                
<p>Pandas supports element-wise operations just like NumPy (after all, <kbd>pd.Series</kbd> stores their data using <kbd>np.array</kbd>). For example, it is possible to apply transformation very easily on both <kbd>pd.Series</kbd> and <kbd>pd.DataFrame</kbd>:</p>
<pre>
    np.log(df.sys_initial) # Logarithm of a series<br/>    df.sys_initial ** 2    # Square a series<br/>    np.log(df)             # Logarithm of a dataframe<br/>    df ** 2                # Square of a dataframe
</pre>
<p>You can also perform element-wise operations between two <kbd>pd.Series</kbd> in a way similar to NumPy. An important difference is that the operands will be matched by key, rather than by position; if there is a mismatch in the index, the resulting value will be set to <kbd>NaN</kbd>. Both the scenarios are exemplified in the following example:</p>
<pre>
    # Matching index<br/>    a = pd.Series([1, 2, 3], index=["a", "b", "c"])<br/>    b = pd.Series([4, 5, 6], index=["a", "b", "c"])<br/>    a + b<br/>    # Result: <br/>    # a 5<br/>    # b 7<br/>    # c 9<br/>    # dtype: int64<br/><br/>    # Mismatching index<br/>    b = pd.Series([4, 5, 6], index=["a", "b", "d"])<br/>    # Result:<br/>    # a 5.0<br/>    # b 7.0<br/>    # c NaN<br/>    # d NaN<br/>    # dtype: float64
</pre>
<p>For added flexibility, Pandas exposes the <kbd>map</kbd>, <kbd>apply</kbd>, and <kbd>applymap</kbd> methods that can be used to apply specific transformations.</p>
<p>The <kbd>pd.Series.map</kbd> method can be used to execute a function to each value and return a <kbd>pd.Series</kbd> containing each result. In the following example, we show how to apply the <kbd>superstar</kbd> function to each element of a <kbd>pd.Series</kbd>:</p>
<pre>
    a = pd.Series([1, 2, 3], index=["a", "b", "c"])<br/>    def superstar(x):<br/>        return '*' + str(x) + '*'<br/>    a.map(superstar)<br/><br/>    # Result:<br/>    # a *1*<br/>    # b *2*<br/>    # c *3*<br/>    # dtype: object
</pre>
<p>The <kbd>pd.DataFrame.applymap</kbd> function is the equivalent of <kbd>pd.Series.map</kbd>, but for <kbd>DataFrames</kbd>:</p>
<pre>
    df.applymap(superstar)<br/>    # Result:<br/>    #    dia_final  dia_initial  sys_final  sys_initial<br/>    # a       *70*         *75*      *115*        *120*<br/>    # b       *82*         *85*      *123*        *126*<br/>    # c       *92*         *90*      *130*        *130*<br/>    # d       *87*         *87*      *118*        *115*
</pre>
<p>Finally, the <kbd>pd.DataFrame.apply</kbd> function can apply the passed function to each column or each row, rather than element-wise. The selection can be performed with the argument axis, where a value of <kbd>0</kbd> (the default) corresponds to columns, and <kbd>1</kbd> corresponds to rows. Also, note that the return value of <kbd>apply</kbd> is a <kbd>pd.Series</kbd>:</p>
<pre>
    df.apply(superstar, axis=0)<br/>    # Result:<br/>    # dia_final *a 70nb 82nc 92nd 87nName: dia...<br/>    # dia_initial *a 75nb 85nc 90nd 87nName: dia...<br/>    # sys_final *a 115nb 123nc 130nd 118nName:...<br/>    # sys_initial *a 120nb 126nc 130nd 115nName:...<br/>    # dtype: object<br/><br/>    df.apply(superstar, axis=1)<br/>    # Result:<br/>    # a *dia_final 70ndia_initial 75nsys_f...<br/>    # b *dia_final 82ndia_initial 85nsys_f...<br/>    # c *dia_final 92ndia_initial 90nsys_f...<br/>    # d *dia_final 87ndia_initial 87nsys_f...<br/>    # dtype: object
</pre>
<p>Pandas also supports efficient <kbd>numexpr</kbd>-style expressions with the convenient <kbd>eval</kbd> method. For example, if we want to calculate the difference in the final and initial blood pressure, we can write the expression as a string, as shown in the following code:</p>
<pre>
    df.eval("sys_final - sys_initial")<br/>    # Result:<br/>    # a -5<br/>    # b -3<br/>    # c 0<br/>    # d 3<br/>    # dtype: int64<br/><br/>
</pre>
<p>It is also possible to create new columns using the assignment operator in the <kbd>pd.DataFrame.eval</kbd> expression. Note that, if the <kbd>inplace=True</kbd> argument is used, the operation will be applied directly on the original <kbd>pd.DataFrame</kbd>; otherwise, the function will return a new dataframe. In the next example, we compute the difference between <kbd>sys_final</kbd> and <kbd>sys_initial</kbd>, and we store it in the <kbd>sys_delta</kbd> column:</p>
<pre>
<strong>df.eval("sys_delta = sys_final - sys_initial", inplace=False)</strong><br/><strong># Result:</strong><br/><strong>#     dia_final   dia_initial   sys_final   sys_initial   sys_delta</strong><br/><strong># a          70            75         115           120          -5</strong><br/><strong># b          82            85         123           126          -3</strong><br/><strong># c          92            90         130           130           0</strong><br/><strong># d          87            87         118           115           3</strong>
</pre>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Grouping, aggregations, and transforms</h1>
            

            
                
<p>One of the most appreciated features of Pandas is the simple and concise expression of data analysis pipelines that requires grouping, transforming, and aggregating the data. To demonstrate this concept, let's extend our dataset by adding two new patients to whom we didn't administer the treatment (this is usually called a <em>control group</em>). We also include a column, <kbd>drug_admst</kbd>, which records whether the patient was administered the treatment:</p>
<pre>
    patients = ["a", "b", "c", "d", "e", "f"]<br/><br/>    columns = {<br/>      "sys_initial": [120, 126, 130, 115, 150, 117],<br/>      "dia_initial": [75, 85, 90, 87, 90, 74],<br/>      "sys_final": [115, 123, 130, 118, 130, 121],<br/>      "dia_final": [70, 82, 92, 87, 85, 74],<br/>      "drug_admst": [True, True, True, True, False, False]<br/>    }<br/><br/>    df = pd.DataFrame(columns, index=patients)
</pre>
<p>At this point, we may be interested to know how the blood pressure changed between the two groups. You can group the patients according to <kbd>drug_amst</kbd> using the <kbd>pd.DataFrame.groupby</kbd> function. The return value will be the <kbd>DataFrameGroupBy</kbd> object, which can be iterated to obtain a new <kbd>pd.DataFrame</kbd> for each value of the <kbd>drug_admst</kbd> column:</p>
<pre>
    df.groupby('drug_admst')<br/>    for value, group in df.groupby('drug_admst'):<br/>        print("Value: {}".format(value))<br/>        print("Group DataFrame:")<br/>        print(group)<br/><strong># Output:</strong><br/><strong># Value: False</strong><br/><strong># Group DataFrame:</strong><br/><strong>#    dia_final   dia_initial   drug_admst   sys_final   sys_initial</strong><br/><strong># e         85            90        False         130           150</strong><br/><strong># f         74            74        False         121           117</strong><br/><strong># Value: True</strong><br/><strong># Group DataFrame:</strong><br/><strong>#    dia_final   dia_initial   drug_admst   sys_final   sys_initial</strong><br/><strong># a         70            75         True         115           120</strong><br/><strong># b         82            85         True         123           126</strong><br/><strong># c         92            90         True         130           130</strong><br/><strong># d         87            87         True         118           115</strong>
</pre>
<p>Iterating on the <kbd>DataFrameGroupBy</kbd> object is almost never necessary, because, thanks to method chaining, it is possible to calculate group-related properties directly. For example, we may want to calculate mean, max, or standard deviation for each group. All those operations that summarize the data in some way are called aggregations and can be performed using the <kbd>agg</kbd> method. The result of <kbd>agg</kbd> is another <kbd>pd.DataFrame</kbd> that relates the grouping variables and the result of the aggregation, as illustrated in the following code:</p>
<pre>
<strong>df.groupby('drug_admst').agg(np.mean)</strong><br/><strong>#              dia_final   dia_initial   sys_final   sys_initial</strong><br/><strong># drug_admst </strong><br/><strong># False            79.50         82.00       125.5        133.50</strong><br/><strong># True             82.75         84.25       121.5        122.75</strong>
</pre>
<p><strong>It is also possible to perform processing on the DataFrame groups that do not represent a summarization. One common example of such an operation is filling in missing values. Those intermediate steps are called <em>transforms</em>.</strong></p>
<p>We can illustrate this concept with an example. Let's assume that we have a few missing values in our dataset, and we want to replace those values with the average of the other values in the same group. This can be accomplished using a transform, as follows:</p>
<pre>
<strong>df.loc['a','sys_initial'] = None</strong><br/><strong>df.groupby('drug_admst').transform(lambda df: df.fillna(df.mean())) </strong><br/><strong>#     dia_final    dia_initial   sys_final   sys_initial</strong><br/><strong># a          70             75         115    123.666667</strong><br/><strong># b          82             85         123    126.000000</strong><br/><strong># c          92             90         130    130.000000</strong><br/><strong># d          87             87         118    115.000000</strong><br/><strong># e          85             90         130    150.000000</strong><br/><strong># f          74             74         121    117.000000</strong>
</pre>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Joining</h1>
            

            
                
<p>Joins are useful to aggregate data that is scattered among different tables. Let’s say that we want to include the location of the hospital in which patient measurements were taken in our dataset. We can reference the location for each patient using the <kbd>H1</kbd>, <kbd>H2</kbd>, and <kbd>H3</kbd> labels, and we can store the address and identifier of the hospital in a <kbd>hospital</kbd> table:</p>
<pre>
    hospitals = pd.DataFrame(<br/>      { "name" : ["City 1", "City 2", "City 3"],<br/>        "address" : ["Address 1", "Address 2", "Address 3"],<br/>        "city": ["City 1", "City 2", "City 3"] },<br/>      index=["H1", "H2", "H3"])<br/><br/>    hospital_id = ["H1", "H2", "H2", "H3", "H3", "H3"]<br/>    df['hospital_id'] = hospital_id
</pre>
<p>Now, we want to find the city where the measure was taken for each patient. We need to <em>map</em> the keys from the <kbd>hospital_id</kbd> column to the city stored in the <kbd>hospitals</kbd> table.</p>
<p>This can surely be implemented in Python using dictionaries:</p>
<pre>
    hospital_dict = {<br/>     "H1": ("City 1", "Name 1", "Address 1"),<br/>     "H2": ("City 2", "Name 2", "Address 2"),<br/>     "H3": ("City 3", "Name 3", "Address 3")<br/>    }<br/>    cities = [hospital_dict[key][0] <br/>               for key in hospital_id]
</pre>
<p>This algorithm runs efficiently with an <em>O</em>(<em>N</em>) time complexity, where <em>N</em> is the size of <kbd>hospital_id</kbd>. Pandas allows you to encode the same operation using simple indexing; the advantage is that the join will be performed in heavily optimized Cython and with efficient hashing algorithms. The preceding simple Python expression can be easily converted to Pandas in this way:</p>
<pre>
    cities = hospitals.loc[hospital_id, "city"]
</pre>
<p>More advanced joins can also be performed with the <kbd>pd.DataFrame.join</kbd> method, which will produce a new <kbd>pd.DataFrame</kbd> that will attach the hospital information for each patient:</p>
<pre>
    result = df.join(hospitals, on='hospital_id')<br/>    result.columns<br/>    # Result:<br/>    # Index(['dia_final', 'dia_initial', 'drug_admst', <br/>    # 'sys_final', 'sys_initial',<br/>    # 'hospital_id', 'address', 'city', 'name'],<br/>    # dtype='object')
</pre>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Summary</h1>
            

            
                
<p>In this chapter, we learned how to manipulate NumPy arrays and how to write fast mathematical expressions using array broadcasting. This knowledge will help you write more concise, expressive code and, at the same time, to obtain substantial performance gains. We also introduced the <kbd>numexpr</kbd> library to further speed up NumPy calculations with minimal effort.</p>
<p>Pandas implements efficient data structures that are useful when analyzing large datasets. In particular, Pandas shines when the data is indexed by non-integer keys and provides very fast hashing algorithms.</p>
<p>NumPy and Pandas work well when handling large, homogenous inputs, but they are not suitable when the expressions grow complex and the operations cannot be expressed using the tools provided by these libraries. In such cases, we can leverage Python capabilities as a glue language by interfacing it with C using the Cython package.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    </body></html>