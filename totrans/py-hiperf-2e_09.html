<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Designing for High Performance</h1>
            </header>

            <article>
                
<p>In the earlier chapters, we learned how to use the vast array of tools available in Python's standard library and third-party packages to assess and improve the performance of Python applications. In this chapter, we will provide some general guidelines on how to approach different kinds of applications as well as illustrate some good practices that are commonly adopted by several Python projects.</p>
<p>In this chapter, we will learn the following:</p>
<ul>
<li>Picking the right performance technique for generic, number crunching, and big data applications</li>
<li>Structuring a Python project</li>
<li>Isolating Python installations with virtual environments and containerization</li>
<li>Setting up continuous integration with Travis CI</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Choosing a suitable strategy</h1>
            </header>

            <article>
                
<p>Many packages are available for improving the performance of programs, but how do we determine the best optimization strategy for our program? A variety of factors dictate the decision on which method to use. In this section, we will try to answer this question as comprehensively as possible, based on broad application categories. </p>
<p>The first aspect to take into consideration is the type of application. Python is a language that serves multiple and very diverse communities that span web services, system scripting, games, machine learning, and much more. Those different applications will require optimization efforts for different parts of the program.</p>
<p>For example, a web service can be optimized to have a very short response time. Also, it has to be able to answer as many requests as possible using as little resources as possible (that is, it will try to achieve lower latency), while numerical code may require weeks to run. It's important to improve the amount of data the system may process, even if there's a significant start up overhead (in this case, we are interested in throughput). </p>
<p>Another aspect is the platform and architecture we are developing for. While Python has support for a lot of platforms and architectures, many of the third-party libraries may have limited support for certain platforms, especially when dealing with packages that bind into C extensions. For this reason, it's necessary to check the availability of libraries for the target platforms and architectures.</p>
<p>Also, some architectures, such as embedded systems and small devices, may have severe CPU and memory restrictions. This is an important factor to take into consideration as, for instance, some techniques (such as multiprocessing) may consume too much memory or require the execution of additional software.</p>
<p>Finally, the business requirements are equally important. Many times, software products require fast iterations and the ability to change the code quickly. Generally speaking, you want to keep your software stack as minimal as possible so that modification, testing, deployment, and introducing additional platform support becomes easy and feasible in a short period of time. This also applies to the team--installing the software stack and starting the development should be as smooth as possible. For this reason, one should generally prefer pure Python libraries over extensions, with the possible exception of solid, battle-tested libraries, such as NumPy. Additionally, various business aspects will help determine which operations need to be optimized first (always remember that <em>premature optimization is the root of all evil</em>).</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Generic applications</h1>
            </header>

            <article>
                
<p>Generic applications, such as web apps or mobile app backends, usually involve calls to remote services and databases. For such cases, it can be useful to take advantage of asynchronous frameworks, such as the ones presented in <a href="2b46e5c0-5308-4073-b1c6-4232a881b39f.xhtml">Chapter 6</a>, <em>Implementing Concurrency</em>; this will improve application logic, system design, responsiveness and, also, it will simplify the handling of network failures.</p>
<p>Use of asynchronous programming also makes it easier to implement and use microservices. A <strong>microservice</strong>, although there is no standard definition, can be thought of as a remote service that focuses on a specific aspect of the application (for example, authentication).</p>
<p>The idea behind microservices is that you can build an application by composing different microservices that communicate through a simple protocol (such as gRPC, REST calls, or through a dedicated message queue). This architecture is in contrast with a monolithic application where all the services are handled by the same Python process.</p>
<p>Advantages of microservices include strong decoupling of different parts of the application. Small, simple services can be implemented and maintained by different teams as well as be updated and deployed at different times. This also allows microservices to be easily replicated so that they can handle more users. Additionally, since the communication is done through a simple protocol, microservices can be implemented in a different language that can be more appropriate than Python for the specific application.</p>
<p>If the performance of a service is not satisfactory, the application can often be executed on a different Python interpreter, such as PyPy (provided that all the third-party extensions are compatible) to achieve sufficient speed gains. Otherwise, algorithmic strategies as well as porting bottlenecks to Cython is generally sufficient to achieve satisfactory performance.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Numerical code</h1>
            </header>

            <article>
                
<p>If your goal is to write numerical code, an excellent strategy is to start directly with a NumPy implementation. Using NumPy is a safe bet because it is available and tested on many platforms and, as we have seen in the earlier chapters, many other packages treat NumPy arrays as first-class citizens.</p>
<p>When properly written (such as by taking advantage of broadcasting and other techniques we learned in <a href="68a7c14e-5270-49a5-862e-96cf59cddf60.xhtml">Chapter 2</a>, <em>Pure Python Optimizations</em>), NumPy performance is already quite close to the native performance achievable by C code, and won't require further optimization. That said, certain algorithms are hard to express efficiently using NumPy's data structures and methods. When this happens, two very good options can be, for example, Numba or Cython.</p>
<p>Cython is a very mature tool used intensely by many important projects, such as <kbd>scipy</kbd> and <kbd>scikit-learn</kbd>. Cython code, with its explicit, static type declarations, makes it very understandable, and most Python programmers will have no problem picking up its familiar syntax. Additionally, the absence of "magic" and good inspection tools make it easy for the programmer to predict its performance and have educated guesses as to what to change to achieve maximum performance.</p>
<p>Cython, however, has some drawbacks. Cython code needs to be compiled before it can be executed, thus breaking the convenience of the Python edit-run cycle. This also requires the availability of a compatible C compiler for the target platform.  This also complicates distribution and deployment, as multiple platforms, architectures, configurations, and compilers need to be tested for every target.</p>
<p>On the other hand, Numba API requires only the definition of pure-Python functions, which get compiled on the fly, maintaining the fast Python edit-run cycle. In general, Numba requires a LLVM toolchain installation to be available on the target platform. Note that, as of version 0.30, there is some limited support for <strong>Ahead-Of-Time</strong> (<strong>AOT</strong>) compilation of Numba functions so that Numba-compiled functions can be packaged and deployed without requiring a Numba and <span>LLVM</span> installation.</p>
<p>Note that both Numba and Cython are usually available pre-packaged with all of their dependencies (including compilers) on the default channels of the conda package manager. Therefore, deployment of Cython can be greatly simplified on the platforms where the conda package manager is available.</p>
<div class="packt_infobox">What if Cython and Numba are still not enough? While this is generally not required, an additional strategy would be to implement a pure C module (which can be further optimized using compiler flags or hand-tuning) and use it from a Python module using either the <kbd>cffi</kbd> package (<a href="https://cffi.readthedocs.io/en/latest/">https://cffi.readthedocs.io/en/latest/</a>) or Cython. </div>
<p>Using NumPy, Numba, and Cython is a very effective strategy to obtain near-optimal performance on serial codes. For many applications, serial codes are certainly enough and, even if the ultimate plan is to have a parallel algorithm, it’s still very worthy working on a serial reference implementation for debugging purposes and because a serial implementation is likely to be faster on small datasets.</p>
<p>Parallel implementations vary considerably in complexity depending on the particular application. In many cases, the program can be easily expressed as a series of independent calculations followed by some sort of <em>aggregation</em> and is parallelizable using simple process-based interfaces, such as <kbd>multiprocessing.Pool</kbd> or <kbd>ProcessPoolExecutor</kbd>, which have the advantage of being able to execute generic Python code in parallel without much trouble.</p>
<p>To avoid the time and memory overhead of starting multiple processes, one can use threads. NumPy functions typically release the GIL and are good candidates for thread-base parallelization. Additionally, Cython and Numba provide special <kbd>nogil</kbd> statements as well as automatic parallelization, which makes them suitable for simple, lightweight parallelization.</p>
<p>For more complex use cases, you may have to change the algorithm significantly. In those cases, Dask arrays are a decent option, which provide an almost-drop-in replacement for standard NumPy. Dask has the further advantage of operating very transparently and is easy to tweak.</p>
<p>Specialized applications that make intensive use of linear algebra routines (such as deep learning and computer graphics) may benefit from packages such as Theano and Tensorflow, which are capable of highly performant and automatic parallelization with built-in GPU support.</p>
<p>Finally, <kbd>mpi4py</kbd> usage can be used for deploying parallel python scripts on a MPI-based supercomputer (commonly available for researchers in universities).</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Big data</h1>
            </header>

            <article>
                
<p>Large datasets (typically larger than 1 TB) are becoming increasingly common and a lot of resources have been invested in developing technologies capable of collecting, storing, and analyzing them. Typically, the choice of which framework to use is bound to how the data is stored in the first place.</p>
<p>Many times, even if the complete dataset doesn't fit in a single machine, it is still possible to devise strategies to extract the answers without having to probe the whole dataset. For example, it is quite often possible to answer questions by extracting a small, interesting subset of data that can be easily loaded in memory and analyzed with highly convenient and performant libraries, such as Pandas. By filtering or randomly sampling data points, one can often find a good enough answer to a business question without having to resort to big data tools.</p>
<p>If the bulk of the company's software is written in Python, and you have the freedom to decide your software stack, it would make sense to use Dask distributed. The software package has a very simple setup and is tightly integrated with the Python ecosystem. Using something such as Dask <kbd>array</kbd> and <kbd>DataFrame</kbd>, it's very easy to scale your already-existing Python algorithms by adapting NumPy and Pandas code.</p>
<p>Quite often, some companies may have already set up a Spark cluster. In this case, PySpark is the optimal choice, and the use of SparkSQL is encouraged for higher performance. One of the Spark advantages is that it allows the use of other languages, such as Scala and Java.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Organizing your source code</h1>
            </header>

            <article>
                
<p>The repository structure of a typical Python project consists, at a minimum, of a directory containing a <kbd>README.md</kbd> file, a Python module or package containing the source code for the application or library, and a <kbd>setup.py</kbd> file. Projects may also adopt different conventions to comply with company policies or specific frameworks in use. In this section, we will illustrate some common practices that are commonly found in community-driven Python projects which can include some of the tools we illustrated in the earlier chapters.</p>
<p>A typical directory structure for a Python project named  <kbd>myapp</kbd> can look like this. Now, we will elucidate the role of each file and directory:</p>
<pre>
    myapp/ <br/>      README.md<br/>      LICENSE<br/>      setup.py<br/>      myapp/<br/>        __init__.py<br/>        module1.py<br/>        cmodule1.pyx<br/>        module2/<br/>           __init__.py<br/>      src/<br/>        module.c<br/>        module.h<br/>      tests/<br/>        __init__.py<br/>        test_module1.py<br/>        test_module2.py<br/>      benchmarks/<br/>        __init__.py<br/>        test_module1.py<br/>        test_module2.py<br/>      docs/<br/>      tools/
</pre>
<p><kbd>README.md</kbd> is a text file that contains general information about the software, such as project scope, installation, a quick start, and useful links. If the software is released to the public, a <kbd>LICENSE</kbd> file is used to specify terms and conditions for its usage.</p>
<p>Python software is commonly packaged using the <kbd>setuptools</kbd> library in a <kbd>setup.py</kbd> file. As we have seen in the earlier chapters, <kbd>setup.py</kbd> is also an effective way to compile and distribute Cython code.</p>
<p>The <kbd>myapp</kbd> package contains the source code for the application, including Cython modules. Sometimes, it's convenient to maintain pure-Python implementations besides their Cython-optimized counterparts. Commonly, the Cython version of a module is named with a c prefix (such as <kbd>cmodule1.pyx</kbd> in the preceding example).</p>
<p>If the external <kbd>.c</kbd> and <kbd>.h</kbd> files are needed, those are usually stored under an additional <kbd>src/</kbd> directory placed in the top-level (<kbd>myapp</kbd>) project directory.</p>
<p>The <kbd>tests/</kbd> directory contains testing code for application (usually in the form of unit tests), which can be run using a test runner, such as <kbd>unittest</kbd> or <kbd>pytest</kbd>. However, some projects prefer to place the <kbd>tests/</kbd> directory inside the <kbd>myapp</kbd> package. Since high-performance code is tweaked and rewritten continuously, having a solid test suite is crucial to spot bugs as early as possible and to improve the developer experience by shortening the test-edit-run cycle. </p>
<p>Benchmarks can be placed in the <kbd>benchmarks</kbd> directory; the advantage of having benchmarks separated from tests is that benchmarks can potentially take more time to execute. Benchmarks can also be run on a build server (see the <em>Continuous integration</em> section) as a simple mean to compare the performance of versions. While benchmarks usually take longer to run than unit tests, it's best to keep their execution as short as possible to avoid waste of resources.</p>
<p>Finally, the <kbd>docs/</kbd> directory contains user and developer documentation and API references. This usually also includes configuration files for documentation tools, such as <kbd>sphinx</kbd>. Other tools and scripts can be placed in the <kbd>tools/</kbd> directory.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Isolation, virtual environments, and containers</h1>
            </header>

            <article>
                
<p>The importance of having isolated environments for code testing and execution becomes quite apparent by noticing what happens when you ask a friend to run one of your Python scripts. What happens is that you provide instructions to install Python version X and dependent packages <kbd>Y</kbd>, <kbd>X</kbd>, and ask them to copy and execute the script on their machine.</p>
<p>In many cases, your friend will proceed and download Python for its platform as well as the dependent libraries and try to execute the script. However, it can happen (more often than not) that the script will fail because either their computer has a different operating system than yours, or the installed libraries are not the same version as the one you installed on your machine. At other times, there can be previous installations that are improperly removed and will cause hard-to-debug conflicts and a lot of frustration.</p>
<p>A very easy way to avoid this scenario is to use virtual environments. Virtual environments are used to create and manage several Python installations by isolating Python, related executables, and third-party packages. Since Python's 3.3 version, the standard library includes the <kbd>venv</kbd> module (previously known as <strong>virtualenv</strong>), which is a tool designed to create and manage simple isolated environments. Python packages in <kbd>venv</kbd>-based virtual environments can be installed using <kbd>setup.py</kbd> files or through <kbd>pip</kbd>.</p>
<p>Providing exact and specific library versions is crucial when dealing with high-performance code. Libraries evolve all the time between releases and changes in the algorithms may dramatically affect the performance. For instance, popular libraries, such as <kbd>scipy</kbd> and <kbd>scikit-learn</kbd>, often port some of their codes and data structures to Cython, so it's really important that the user installs the correct version for optimal performance.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using conda environments</h1>
            </header>

            <article>
                
<p>Most of the time, using <kbd>venv</kbd> is a fine choice. However, when writing high-performance code, it often happens that some high-performance libraries also require non-Python software to be installed. This typically involves additional setting up of compilers and high-performance native libraries (in C, C++, or Fortran) to which Python packages link. As <kbd>venv</kbd> and <kbd>pip</kbd> are designed to deal with Python packages only, this scenario is poorly supported by these tools.</p>
<p>The <kbd>conda</kbd> package manager was created specifically to handle such cases. Creating a virtual environment with conda can be done using the <kbd>conda create</kbd> command. The command takes a <kbd>-n</kbd> argument (<kbd>-n</kbd> stands for <kbd>--name</kbd>) that specifies an identifier for the newly created environment and the packages we intend to install. If we wish to create an environment that uses python version <kbd>3.5</kbd> and the latest version of NumPy, we use the following command:</p>
<pre>
<strong>$ conda create -n myenv Python=3.5 numpy</strong>
</pre>
<p>Conda will take care of fetching the relative packages from their repositories and placing them in an isolated Python installation. To enable the virtual environment, you can use the <kbd>source activate</kbd> command:</p>
<pre>
<strong>$ source activate myenv</strong>
</pre>
<p>After executing this command, the default Python interpreter will be switched to the version we specified earlier. You can easily verify the location of your Python executable using the <kbd>which</kbd> command, which returns the full path of the executable:</p>
<pre>
<strong>(myenv) $ which python</strong><br/><strong>/home/gabriele/anaconda/envs/myenv/bin/python</strong>
</pre>
<p>At this point, you are free to add, remove, and modify packages in the virtual environment without affecting the global Python installation. Further packages can be installed using the <kbd>conda install &lt;package name&gt;</kbd> command or through <kbd>pip</kbd>.</p>
<p>The beauty of virtual environments is that you can install or compile any software you want in a well-isolated fashion. This means that if, for some reason, your environment gets corrupted, you can scratch it and start from zero.</p>
<p>To remove the <kbd>myenv</kbd> environment, you first need to deactivate it, and then use the <kbd>conda env remove</kbd> command, as follows:</p>
<pre>
<strong>(myenv) $ source deactivate</strong><br/><strong>$ conda env remove -n myenv</strong>
</pre>
<p>What if a package is not available on the standard <kbd>conda</kbd> repositories? One option is to look whether it is available in the <kbd>conda-forge</kbd> community channel. To search for a package in <kbd>conda-forge</kbd>, you can add the <kbd>-c</kbd> option (which stands for <kbd>--channel</kbd>) to the <kbd>conda search</kbd> command:</p>
<pre>
<strong>$ conda search -c conda-forge scipy</strong>
</pre>
<p>The command will list a series of packages and versions available that match the <kbd>scipy</kbd> query string. Another option is to search for the package in the public channels hosted on <strong>Anaconda Cloud</strong>. The command-line client for Anaconda Cloud can be downloaded by installing the <kbd>anaconda-client</kbd> package:</p>
<pre>
<strong>$ conda install anaconda-client</strong>
</pre>
<p>Once the client is installed, you can use the <kbd>anaconda</kbd> command-line client to search for packages. In the following example, we demonstrate how to look for the <kbd>chemview</kbd> package:</p>
<pre>
<strong>$ anaconda search chemview <br/>Using Anaconda API: https://api.anaconda.org</strong><br/><strong>Run 'anaconda show &lt;USER/PACKAGE&gt;' to get more details:</strong><br/><strong>Packages:</strong><br/><strong> Name                      | Version | Package Types   | Platforms </strong><br/><strong> ------------------------- | ------  | --------------- | ---------------</strong><br/><strong> cjs14/chemview            | 0.3     | conda           | linux-64, win-64, osx-64</strong><br/><strong>                                     : WebGL Molecular Viewer for IPython notebook.</strong><br/><strong> gabrielelanaro/chemview   | 0.7     | conda           | linux-64, osx-64</strong><br/><strong>                                     : WebGL Molecular Viewer for IPython notebook.<br/></strong>
</pre>
<p>Installation can then be easily performed by specifying the appropriate channel with the <kbd>-c</kbd> option:</p>
<pre>
<strong>$ conda install -c gabrielelanaro chemlab</strong>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Virtualization and Containers</h1>
            </header>

            <article>
                
<p>Virtualization has been around for a long time as a way to run multiple operating systems on the same machine in order to better utilize physical resources.</p>
<p>One way to achieve virtualization is to employ a <em>virtual machine</em>. Virtual machines work by creating virtual hardware resources, such as CPU, memory, and devices, and use those to install and run multiple operating systems on the same machine. Virtualization can be accomplished by installing a hypervisor application on top of an operating system (called <em>host</em>). The hypervisor is capable of creating, managing, and monitoring virtual machines and their respective operating systems (called <em>guests</em>).</p>
<div class="packt_infobox">It's important to note that virtual environments, despite their name, have nothing to do with virtual machines. A virtual environment is Python-specific and works by setting up different Python interpreters through shell scripts.</div>
<p>Containers are a way to isolate an application by creating an environment separated from the host operating system and contain only the necessary dependencies. Containers are an operating system feature that allows you to share the hardware resources (provided by the operating system kernel) for multiple instances. A container is different from a virtual machine because it does not abstract hardware resources, but merely shares the operating system's kernel.</p>
<p>Containers are very efficient at utilizing hardware resources as those are accessed natively through the kernel. For this reason, they are an excellent solution for high-performance applications. They are also fast to create and destroy and can be used to quickly test an application in isolation. Containers are also used to simplify deployments (especially microservices) and to develop build servers, such as the ones we mentioned in the preceding section.</p>
<p>In <a href="95fc6212-c0ee-4dee-8d8a-56dc57fb6c97.xhtml">Chapter 8</a>, <em>Distributed Processing</em>, we used <strong>docker</strong> to easily set up a PySpark installation. Docker is one of the most popular containerization solutions available today. The best way to install docker is by following the instructions on the official website (<a href="https://www.docker.com/">https://www.docker.com/</a>). After installation, it is possible to easily create and manage containers using the docker command-line interface.</p>
<p>You can start a new container by using the <kbd>docker run</kbd> command. In the following example, we will demonstrate how to use <kbd>docker run</kbd> to execute a shell session in an Ubuntu 16.04 container. To do this, we will need to specify the following arguments:</p>
<ul>
<li><kbd>-i</kbd> specifies that we are trying to start an interactive session. It is also possible to execute individual docker commands without interactivity (for example, when starting a web server).</li>
<li><kbd>-t &lt;image name&gt;</kbd> specifies which system image to use. In the following example, we use the <kbd>ubuntu:16.04</kbd> image.</li>
<li><kbd> /bin/bash</kbd>, which is the command to run inside the container, demonstrated as follows:</li>
</ul>
<pre>
<strong>      $ docker run -i -t ubuntu:16.04 /bin/bash</strong><br/><strong>      root@585f53e77ce9:/#</strong>
</pre>
<p>This command will immediately take us into a separate, isolated shell where we can play around with the system and install software without touching the host operating system. Using a container is a very good way to test installations and deployments on different Linux flavors. After we are done with the interactive shell, we can type the <kbd>exit</kbd> command to return to the host system.</p>
<p>In the last chapter, we also made use of the port and detach options, <kbd>-p</kbd> and <kbd>-d</kbd>, to run the executable <kbd>pyspark</kbd>. The <kbd>-d</kbd> option simply asks Docker to run the command in the background. The <kbd>-p &lt;host_port&gt;:&lt;guest_port&gt;</kbd> option was, instead, necessary to map a network port of the host operating system to the guest system; without this option, the Jupyter Notebook would not have been reachable from a browser running in the host system.</p>
<p>We can monitor the status of the containers with <kbd>docker ps</kbd>, as shown in the following snippet. The <kbd>-a</kbd> option (which stands for <em>all</em>) serves to output information about all the containers, whether they are currently running or not:</p>
<pre>
<strong>$ docker ps -a</strong><br/><strong>CONTAINER ID IMAGE        COMMAND     CREATED       STATUS     PORTS NAMES</strong><br/><strong>585f53e77ce9 ubuntu:16.04 "/bin/bash" 2 minutes ago Exited (0)       2 minutes ago pensive_hamilton</strong>
</pre>
<p>The information provided by <kbd>docker ps</kbd> includes a hexadecimal identifier, <kbd>585f53e77ce9</kbd>, as well as a human readable name, <kbd>pensive_hamilton</kbd>, both of which can be used to specify the container in other docker commands. It also includes additional information about the command executed, creation time, and the execution's current status.</p>
<p>You can resume the execution of an exited container using the <kbd>docker start</kbd> command. To gain shell access to the container, you can use <kbd>docker attach</kbd>. Both these commands can be followed by either the container ID or its human readable name:</p>
<pre>
<strong>$ docker start pensive_hamilton</strong> <br/><strong>pensive_hamilton</strong><br/><strong>$ docker attach pensive_hamilton </strong><br/><strong>root@585f53e77ce9:/#</strong>
</pre>
<p>A container can be easily removed using the <kbd>docker run</kbd> command followed by a container identifier:</p>
<pre>
<strong>$ docker rm pensive_hamilton</strong>
</pre>
<p>As you can see, you are free to execute commands, run, stop, and resume containers as needed, in less than a second. Using docker containers interactively is a great way to test things out and play with new packages without disturbing the host operating system. Since you can run many containers at the same time, docker can also be used to simulate a distributed system (for testing and learning purposes) without having to own an expensive computing cluster.</p>
<p>Docker also allows you to create your own system images, which is useful for distribution, testing, deployment, and documentation purposes. This will be the topic of the next subsection.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating docker images</h1>
            </header>

            <article>
                
<p>Docker images are ready-to-use, pre-configured systems. The <kbd>docker run</kbd> command can be used to access and install the docker images available on the <strong>DockerHub</strong> (<a href="https://hub.docker.com/">https://hub.docker.com/</a>), a web service where package maintainers upload ready-to-use images to test and deploy various applications.</p>
<p>One way to create a docker image is by using the <kbd>docker commit</kbd> command on an existing container. The docker commit command takes a container reference and the output image names as arguments:</p>
<pre>
<strong>$ docker commit &lt;container_id&gt; &lt;new_image_name&gt;</strong>
</pre>
<p>Using this method is useful to save snapshots of a certain container but, if the image is removed from the system, the steps to recreate the image are lost as well.</p>
<p>A better way to create an image is to build it using a <strong>Dockerfile</strong>. A Dockerfile is a text file that provides instructions on how to build an image starting from another image. As an example, we will illustrate the contents of the Dockerfile we used in the last chapter to set up PySpark with Jupyter notebook support. The complete file is reported here.</p>
<p>Each Dockerfile needs a starting image, which can be declared with the <kbd>FROM</kbd> command. In our case, the starting image is <kbd>jupyter/scipy-notebook</kbd>, which is available through DockerHub (<a href="https://hub.docker.com/r/jupyter/scipy-notebook/">https://hub.docker.com/r/jupyter/scipy-notebook/</a>).</p>
<p>Once we have defined our starting image, we can start issuing shell commands to install packages and perform other configurations using a series of <kbd>RUN</kbd> and <kbd>ENV</kbd> commands. In the following example, you can recognize installation of Java Runtime Environment (<kbd>openjdk-7-jre-headless</kbd>) as well as downloading Spark and setting up relevant environment variables. The <kbd>USER</kbd> instructions can be used to specify which user executes the subsequent commands:</p>
<pre>
<strong>    FROM</strong> jupyter/scipy-notebook<br/><strong>    MAINTAINER</strong> Jupyter Project &lt;jupyter@googlegroups.com&gt;<br/><strong>    USER</strong> root<br/><br/>    # Spark dependencies<br/><strong>    ENV</strong> APACHE_SPARK_VERSION 2.0.2<br/><strong>    RUN</strong> apt-get -y update &amp;&amp; <br/>        apt-get install -y --no-install-recommends <br/>        openjdk-7-jre-headless &amp;&amp; <br/>        apt-get clean &amp;&amp; <br/>        rm -rf /var/lib/apt/lists/*<br/><strong>    RUN</strong> cd /tmp &amp;&amp; <br/>        wget -q http://d3kbcqa49mib13.cloudfront.net/spark-<br/>        ${APACHE_SPARK_VERSION}-bin-hadoop2.6.tgz    &amp;&amp; <br/>        echo "ca39ac3edd216a4d568b316c3af00199<br/>              b77a52d05ecf4f9698da2bae37be998a <br/>              *spark-${APACHE_SPARK_VERSION}-bin-hadoop2.6.tgz" | <br/>        sha256sum -c - &amp;&amp; <br/>        tar xzf spark-${APACHE_SPARK_VERSION}<br/>        -bin-hadoop2.6.tgz -C /usr/local &amp;&amp; <br/>        rm spark-${APACHE_SPARK_VERSION}-bin-hadoop2.6.tgz<br/><strong>    RUN</strong> cd /usr/local &amp;&amp; ln -s spark-${APACHE_SPARK_VERSION}<br/>        -bin-hadoop2.6 spark<br/><br/>    # Spark and Mesos config<br/><strong>    ENV</strong> SPARK_HOME /usr/local/spark<br/><strong>    ENV</strong> PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/<br/>        py4j-0.10.3-src.zip<br/><strong>    ENV</strong> SPARK_OPTS --driver-java-options=-Xms1024M <br/>        --driver-java-options=-<br/>        Xmx4096M --driver-java-options=-Dlog4j.logLevel=info<br/><br/><strong>    USER</strong> $NB_USER
</pre>
<p>Dockerfiles can be used to create images using the following command from the directory where the Dockerfile is located. The <kbd>-t</kbd> option can be used to specify the tag that will be used to store the image. With the following line, we can create the image named <kbd>pyspark</kbd> from the preceding Dockerfile:</p>
<pre>
<strong>$ docker build -t pyspark .</strong>
</pre>
<p>The command will automatically retrieve the starting image, <kbd>jupyter/scipy-notebook</kbd>, and produce a new image, named <kbd>pyspark</kbd>. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Continuous integration</h1>
            </header>

            <article>
                
<p>Continuous integration is a great way to ensure that the application stays bug-free at every development iteration. The main idea behind continuous integration is to run the test suite for the project very frequently, usually on a separate build server that pulls the code directly from the main project repository.</p>
<p>Setting up a build server can be accomplished by manually setting up software such as Jenkins (<a href="https://jenkins.io/">https://jenkins.io/</a>), Buildbot (<a href="http://buildbot.net/">http://buildbot.net/</a>), and Drone (<a href="https://github.com/drone/drone">https://github.com/drone/drone</a>) on a machine. This a convenient and cheap solution, especially for small teams and private projects.</p>
<p>Most open source projects take advantage of Travis CI (<a href="https://travis-ci.org/">https://travis-ci.org/</a>), a service capable of building and testing your code automatically from your repository because it's tightly integrated with GitHub. As of today, Travis CI provides a free plan for open source projects. Many open source Python projects take advantage of Travis CI to ensure that the programs run correctly on multiple Python versions and platforms.</p>
<p>Travis CI can be set up easily from a GitHub repository by including a <kbd>.travis.yml</kbd> file containing the build instruction for the project and activating the builds on the Travis CI website (<a href="https://travis-ci.org/">https://travis-ci.org/</a>) after registering an account.</p>
<p>An example <kbd>.travis.yml</kbd> for a high performance application is illustrated here. The file contains instructions to build and run the software that are specified using a few sections written in YAML syntax.</p>
<p>The <kbd>python</kbd> section specifies which Python versions to use. The <kbd>install</kbd> section will download and set up conda for testing, installing dependencies, and setting up the project. While this step is not necessary (one can use <kbd>pip</kbd> instead), conda is a great package manager for high-performance applications as it contains useful native packages.</p>
<p>The <kbd>script</kbd> section contains the code needed to test the code. In this example, we limit ourselves to run our tests and benchmarks:</p>
<pre>
<strong><span class="l l-Scalar l-Scalar-Plain">    language</span><span class="p p-Indicator">:</span></strong> <span class="l l-Scalar l-Scalar-Plain">python</span><br/><strong><span class="l l-Scalar l-Scalar-Plain">    python</span><span class="p p-Indicator">:</span></strong><br/>      <span class="p p-Indicator">-</span> <span class="s">"2.7"</span><br/>      <span class="p p-Indicator">-</span> <span class="s">"3.5"</span><br/><strong><span class="l l-Scalar l-Scalar-Plain">    install</span><span class="p p-Indicator">:<br/></span></strong>      <strong># Setup miniconda</strong><br/>      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">sudo apt-get update</span><br/>      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">if [[ "$TRAVIS_PYTHON_VERSION" == "2.7" ]]; then</span><br/>          <span class="l l-Scalar l-Scalar-Plain">wget https://repo.continuum.io/miniconda/<br/>          Miniconda2-latest-Linux-x86_64.sh -O miniconda.sh;</span><br/>        <span class="l l-Scalar l-Scalar-Plain">else</span><br/>          <span class="l l-Scalar l-Scalar-Plain">wget https://repo.continuum.io/miniconda/<br/>          Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;</span><br/>        <span class="l l-Scalar l-Scalar-Plain">fi</span><br/>      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">bash miniconda.sh -b -p $HOME/miniconda</span><br/>      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">export PATH="$HOME/miniconda/bin:$PATH"</span><br/>      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">hash -r</span><br/>      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">conda config --set always_yes yes --set changeps1 no</span><br/>      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">conda update -q conda</span><br/>      <strong># Installing conda dependencies</strong><br/>      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">conda create -q -n test-environment python=<br/>        $TRAVIS_PYTHON_VERSION numpy pandas cython pytest</span><br/>      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">source activate test-environment<br/>      <strong># Installing pip dependencies</strong><br/></span>      - pip install pytest-benchmark<br/>      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">python setup.py install</span><br/><br/><strong><span class="l l-Scalar l-Scalar-Plain">    script</span><span class="p p-Indicator">:</span></strong><br/>      pytest tests/<br/>      pytest benchmarks/
</pre>
<p>Every time new code is pushed (as well as other configurable events) to the GitHub repository, Travis CI will spin up a container, install dependencies, and run the test suite. Using Travis CI in open source projects is a great practice as it is a form of constant feedback about the status of the project and also provides up-to-date installation instructions through a continuously tested <kbd>.travis.yml</kbd> file.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>Deciding on a strategy to optimize your software is a complex and delicate task that depends on the application type, target platforms, and business requirements. In this chapter, we provided some guidelines to help you think and choose an appropriate software stack for your own applications.</p>
<p>High-performance numerical applications sometimes require managing installation and deployment of third-party packages that may require handling of external tools and native extensions. In this chapter, we saw how to structure your Python project, including tests, benchmarks, documentation, Cython modules, and C extensions. Also, we introduced the continuous integration service Travis CI, which can be used to enable continuous testing for your projects hosted on GitHub.</p>
<p>Finally, we also learned about virtual environments and docker containers that can be used to test applications in isolation and to greatly simplify deployments and ensure that multiple developers have access to the same platform.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>