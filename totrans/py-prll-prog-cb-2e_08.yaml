- en: Heterogeneous Computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will help us to explore the **Graphics Processing Unit** (**GPU**) programming
    techniques through the Python language. The continuous evolution of GPUs is revealing
    how these architectures can bring great benefits to performing complex calculations.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs certainly cannot replace CPUs. However, they are a well-structured and
    heterogeneous code that is able to exploit the strengths of both types of processors
    that can, in fact, bring considerable advantages.
  prefs: []
  type: TYPE_NORMAL
- en: We will examine the main development environments for heterogeneous programming,
    namely, the **PyCUDA** and **Numba** environments for **Compute Unified Device
    Architecture** (**CUDA**) and **PyOpenCL** environments, which are for**Open Computing
    Language** (**OpenCL**) frameworks in their Python version.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding heterogeneous computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the GPU architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding GPU programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with PyCUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heterogeneous programming with PyCUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing memory management with PyCUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing PyOpenCL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building applications with PyOpenCL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Element-wise expressions with PyOpenCL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating PyOpenCL applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU programming with Numba
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with understanding heterogeneous computing in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding heterogeneous computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the years, the search for better performance for increasingly complex calculations
    has led to the adoption of new techniques in the use of computers. One of these
    techniques is called *heterogeneous computing*, which aims to cooperate with different
    (or heterogeneous) processors in such a way as to have advantages (in particular)
    in terms of temporal computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, the processor on which the main program is run (generally the
    CPU) is called the *h**ost*, while the coprocessors (for example, the GPUs) are
    called *d**evices*. The latter are generally physically separated from the host and
    manage their own memory space, which is also separated from the host's memory.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, following significant market demand, the GPU has evolved into
    a highly parallel processor, transforming the GPU from devices for graphics rendering to
    devices for parallelizable and computationally intensive general-purpose calculations.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the use of GPU for tasks other than rendering graphics on the screen
    is called heterogeneous computing.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the task of good GPU programming is to make the most of the great level
    of parallelism and mathematical capabilities offered by the graphics card, minimizing
    all the disadvantages presented by it, such as the delay of the physical connection
    between the host and device.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the GPU architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A GPU is a specialized CPU/core for vector processing of graphical data to render
    images from polygonal primitives. The task of a good GPU program is to make the
    most of the great level of parallelism and mathematical capabilities offered by
    the graphics card and minimize all the disadvantages presented by it, such as
    the delay in the physical connection between the host and device.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs are characterized by a highly parallel structure that allows you to manipulate
    large datasets in an efficient manner. This feature is combined with rapid improvements
    in hardware performance programs, bringing the attention of the scientific world
    to the possibility of using GPUs for purposes other than just rendering images.
  prefs: []
  type: TYPE_NORMAL
- en: 'A GPU (refer to the following diagram) is composed of several processing units
    called **Streaming Multiprocessors **(**SMs**), which represent the first logic
    level of parallelism. In fact, each SM works simultaneously and independently
    from the others:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12715105-d093-49e4-8e05-cdb976bc755c.png)'
  prefs: []
  type: TYPE_IMG
- en: GPU architecture
  prefs: []
  type: TYPE_NORMAL
- en: Each SM is divided into a group of **Streaming Processors** (**SPs**), which
    have a core that can run a thread sequentially. The SP represents the smallest
    unit of execution logic and the level of finer parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: In order to best program this type of architecture, we need to introduce GPU
    programming, which is described in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GPU programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GPUs have become increasingly programmable. In fact, their set of instructions
    has been extended to allow the execution of a greater number of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Today, on a GPU, it is possible to execute classic CPU programming instructions,
    such as cycles and conditions, memory access, and floating-point calculations.
    The two major discrete video card manufacturers—**NVIDIA** and **AMD**—have developed
    their GPU architectures, providing developers with related development environments
    that allow programming in different programming languages, including Python.
  prefs: []
  type: TYPE_NORMAL
- en: At present, developers have valuable tools for programming software that uses
    GPUs in contexts that aren't purely graphics-related. Among the main development
    environments for heterogeneous computing, we have CUDA and OpenCL.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now have a look at them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA is a proprietary hardware architecture of NVIDIA, which also gives its
    name to the related development environment. Currently, CUDA has a pool of hundreds
    of thousands of active developers, which demonstrates the growing interest that
    is developing around this technology in the parallel programming environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'CUDA offers extensions for the most commonly used programming languages, including
    Python. The most well known CUDA Python extensions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: PyCUDA ([https://mathema.tician.de/software/PyCUDA/](https://mathema.tician.de/software/pycuda/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numba ([http://numba.pydata.org](http://numba.pydata.org))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll use these extensions in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second protagonist in parallel computing is OpenCL, which (unlike its NVIDIA
    counterpart) is open standard and can be used not only with GPUs of different
    manufacturers but also with microprocessors of different types.
  prefs: []
  type: TYPE_NORMAL
- en: However, OpenCL is a more complete and versatile solution as it does not boast
    the maturity and simplicity of use that CUDA has.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenCL Python extension is PyOpenCL ([https://mathema.tician.de/software/pyopencl/](https://mathema.tician.de/software/pyopencl/)).
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, the CUDA and OpenCL programming models will be analyzed
    in their Python extension and will be accompanied by some interesting application
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyCUDA is a binding library that provides access to CUDA's Python API by Andreas
    Klöckner. The main features include automatic cleanup, which is tied to an object's
    lifetime, thus preventing leaks, convenient abstraction over modules and buffers,
    full access to the driver, and built-in error handling. It is also very light.
  prefs: []
  type: TYPE_NORMAL
- en: The project is open source under the MIT license, the documentation is very
    clear, and many different sources found online can provide help and support. The
    main purpose of PyCUDA is to let a developer invoke CUDA with minimal abstraction
    from Python, and it also supports CUDA metaprogramming and templatization.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please follow the instructions on the Andreas Klöckner home page ([https://mathema.tician.de/software/pycuda/](https://mathema.tician.de/software/pycuda/))
    to install PyCUDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next programming example has a dual function:'
  prefs: []
  type: TYPE_NORMAL
- en: The first is to verify that PyCUDA is properly installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second is to read and to print the characteristics of the GPU cards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the steps, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the first instruction, we import the Python driver (that is, `pycuda.driver`)
    to the CUDA library installed on our PC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize CUDA. Note also that the following instruction must be called before
    any other instruction in the `pycuda.driver` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Enumerate the number of GPU cards on the PC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For each of the GPU cards present, print the model name, the computing capability,
    and the total amount of memory on the device in kilobytes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The execution is pretty simple. In the first line of code, `pycuda.driver`
    is imported and then initialized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `pycuda.driver` module exposes the driver level to the programming interface
    of CUDA, which is more flexible than the CUDA C runtime-level programming interface,
    and it has a few features that are not present in the runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, it cycles into the `drv.Device.count()` function and, for each GPU card,
    the name of the card and its main characteristics (computing capability and total
    memory) are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When you''ve done so, the installed GPU will be shown on the screen, as in
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CUDA programming model (and consequently PyCUDA, which is a Python wrapper)
    is implemented through specific extensions to the standard library of the C language.
    These extensions have been created just like function calls in the standard C
    library, allowing a simple approach to a heterogeneous programming model that
    includes the host and device code. The management of the two logical parts is
    done by the `nvcc` compiler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a brief description of how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Separate* device code from the host code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Invoke* a default compiler (for example, GCC) to compile the host code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Build* the device code in binary form (`.cubin` objects) or in assembly form
    (`PTX` objects):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6c16c259-1075-4eb4-bf70-ad0b6ac78a12.png)'
  prefs: []
  type: TYPE_IMG
- en: PyCUDA execution model
  prefs: []
  type: TYPE_NORMAL
- en: All the preceding steps are performed by PyCUDA during execution, with an increase
    in the application loading time compared to a CUDA application.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CUDA programming guide is available here: [https://docs.nvidia.com/CUDA/CUDA-c-programming-guide/](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PyCUDA documentation is available here: [https://documen.tician.de/PyCUDA/](https://documen.tician.de/pycuda/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heterogeneous programming with PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CUDA programming model (and, hence, that of PyCUDA) is designed for the
    joint execution of a software application on a CPU and GPU, in order to perform
    the sequential parts of the application on the CPU and those that can be parallelized
    on the GPU. Unfortunately, the computer is not smart enough to understand how
    to distribute the code autonomously, so it is up to the developer to indicate
    which parts should be run by the CPU and by the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, a CUDA application is composed of serial components, which are executed
    by the system CPU or host, or by parallel components called kernels, which are
    executed by the GPU or by the device instead.
  prefs: []
  type: TYPE_NORMAL
- en: A kernel is defined as a *grid* and can, in turn, be decomposed into blocks
    that are sequentially assigned to the various multiprocessors, thus implementing *coarse-grained
    parallelism*. Inside the blocks, there is the fundamental computational unit,
    the thread, with a very *fine parallel granularity*. A thread can belong to only
    one block and is identified by a unique index for the whole kernel. For convenience,
    there is the possibility of using two-dimensional indexes for blocks and three-dimensional
    indexes for threads. The kernels are executed sequentially between them. Blocks
    and threads, on the other hand, are executed in parallel. The number of threads
    running (in parallel) depends on their organization in blocks and on their requests
    in terms of resources, with respect to the resources available in the device.
  prefs: []
  type: TYPE_NORMAL
- en: To visualize the concepts expressed previously, please refer to (*Figure 5*)
    at [https://sites.google.com/site/computationvisualization/programming/cuda/article1](https://sites.google.com/site/computationvisualization/programming/cuda/article1).
  prefs: []
  type: TYPE_NORMAL
- en: The blocks are designed to guarantee scalability. In fact, if you have an architecture
    with two multiprocessors and another with four, then, a GPU application can be
    performed on both architectures, obviously with different times and levels of
    parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'The execution of a heterogeneous program according to the PyCUDA programming
    model is thus structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Allocate* memory on the host.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Transfer* data from the hostmemory to the device memory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Run* the device through the invocation of the kernel functions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Transfer* the results from the device memory to the host memory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Release* the memory allocated on the device.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following diagram shows the execution flow of a program according to the
    PyCUDA programming model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d58c37c0-992d-4f12-b1e3-b152038611bc.png)'
  prefs: []
  type: TYPE_IMG
- en: PyCUDA programming model
  prefs: []
  type: TYPE_NORMAL
- en: In the next example, we will go through a concrete example of the programming
    methodology to follow in order to build PyCUDA applications.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to show the PyCUDA programming model, we consider the task of having
    to double all the elements of a 5 × 5 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the libraries needed for the task we want to perform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `numpy` library, which we imported, allows us to construct the input to
    our problem, that is, a 5 × 5 matrix whose values are chosen randomly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The matrix, thus built, must be copied from the memory of the host to the memory
    of the device. For this, we allocate a memory space (`a*_*gpu`) on the device that
    is necessary to contain matrix `a`. For this purpose, we use the `mem_alloc` function,
    which has the allocated memory space as its subject. In particular, the number
    of bytes of matrix `a`, as expressed by the `a.nbytes` parameter, is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we can transfer the matrix from the host to the memory area, created
    specifically on the device by using the `memcpy_htod` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the device, the `doubleMatrix` kernel function will operate. Its purpose
    will be to multiply each element of the input matrix by `2`. As you can see, the
    syntax of the `doubleMatrix` function is C-like, while the `SourceModule` statement
    is a real directive for the NVIDIA compiler (the `nvcc` compiler), which creates
    a module that, in this case, consists of the `doubleMatrix` function only:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `func` parameter, we identify the `doubleMatrix` function, which is
    contained in the `mod` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we run the kernel function. In order to successfully execute a kernel
    function on the device, the CUDA user must specify the input for the kernel and
    the size of the execution thread block. In the following case, the input is the
    `a_gpu` matrix that was previously copied to the device, while the dimension of
    the thread block is `(5,5,1)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, we allocate an area of memory of size equal to that of the input
    matrix `a`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we copy the contents of the memory area allocated to the device—that
    is, the `a_gpu` matrix—to the previously defined memory area, `a_doubled`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print the contents of the input matrix `a` and the output matrix
    in order to verify the quality of the implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with looking at which libraries are imported for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In particular, the `autoinit` import automatically identifies which GPU on our
    system is available for execution, while `SourceModule` is the directive for the
    compiler of NVIDIA (`nvcc`) that allows us to identify the objects that must be
    compiled and uploaded to the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we build the 5 × 5 input matrix by using the `numpy` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the elements in the matrix are converted to single-precision
    mode (since the graphics card on which this example is executed only supports
    single precision):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we copy the array from the host to the device, using the following two
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that the device and host memory may never communicate during the execution
    of a kernel function. For this reason, in order to parallel execute the kernel
    function on the device, all input data relating to the kernel function must also
    be present in the memory of the device.
  prefs: []
  type: TYPE_NORMAL
- en: It should also be noted that the `a_gpu` matrix is linearized, that is, it is
    one-dimensional, and therefore we must manage it as such.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, all these operations do not require kernel invocation. This means
    that they are made directly by the host.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SourceModule` entity allows the definition of the `doubleMatrix` kernel
    function. `__global__`, which is an `nvcc` directive, indicates that the `doubleMatrix`
    function will be processed by the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s consider the kernel''s body. The `idx` parameter is the matrix index,
    which is identified by the `threadIdx.x` and `threadIdx.y` thread coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, `mod.get_function("doubleMatrix")` returns an identifier to the `func`
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to execute the kernel, we need to configure the execution context.
    This means setting the three-dimensional structure of the threads that belong
    to the block grid by using the block parameter inside the `func` call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`block = (5, 5, 1)` tells us that we are calling a kernel function with the
    `a_gpu` linearized input matrix and a single thread block of size `5` (that is, `5`
    threads) in the *x*-direction, `*5*` threads in the *y*-direction, and 1 thread
    in the *z*-direction, which makes *16* threads in total. Note that each thread
    executes the same kernel code (25 threads in total).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the computation in the GPU device, we use an array to store the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the example, type the following on Command Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key feature of CUDA that makes this programming model substantially different
    from other parallel models (normally used on CPUs) is that in order to be efficient,
    it requires thousands of threads to be active. This is made possible by the typical
    structure of GPUs, which use light threads and also allow the creation and modification
    of execution contexts in a very fast and efficient way.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the scheduling of threads is directly linked to the GPU architecture
    and its intrinsic parallelism. In fact, a block of threads is assigned to a single
    SM. Here, the threads are further divided into groups, called warps. The threads
    that belong to the same warp are managed by the *warp scheduler*. To take full
    advantage of the inherent parallelism of the SM, the threads of the same warp
    must execute the same instruction. If this condition does not occur, then we speak
    of *threads divergence.*
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The complete tutorial on using PyCUDA is available at the following site: [https://documen.tician.de/pycuda/tutorial.html](https://documen.tician.de/pycuda/tutorial.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To install PyCUDA on Windows 10, take a look at the following link: [https://github.com/kdkoadd/Win10-PyCUDA-Install](https://github.com/kdkoadd/Win10-PyCUDA-Install).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing memory management with PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyCUDA programs should respect the rules dictated by the structure and the
    internal organization of SM that impose constraints on thread performances. In
    fact, the knowledge and the correct use of various types of memory that the GPU
    makes available are fundamental in order to achieve maximum efficiency. In those
    GPU cards, enabled for CUDA use, there are four types of memory, which are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Registers**: Each thread is assigned a memory register which only the assigned
    thread can access, even if the threads belong to the same block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shared memory**: Each block has its own shared memory between the threads
    that belong to it. Even this memory is extremely fast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constant memory**: All threads in a grid have constant access to the memory,
    but can only be accessed in reading. The data present in it persists for the entire
    duration of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global memory**: All the threads of the grid, and therefore all the kernels,
    have access to the global memory. Moreover, data persistence is exactly like a
    constant memory:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b43f49cb-fad9-4e64-8752-610540f62d30.png)'
  prefs: []
  type: TYPE_IMG
- en: GPU memory model
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For best performance, a PyCUDA program must, therefore, make the most of every
    type of memory. In particular, it must make the most of shared memory, minimizing
    access to memory on a global level.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, the problem domain is typically subdivided so that a single block
    of threads is able to execute its processing in a closed subset of data. In this
    way, the threads operating on the single block will all work together on the same
    shared memory area, optimizing access.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic steps for each thread are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Load* data from global memory to shared memory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Synchronize* all threads of the block so that everyone can read safety positions
    and shared memory filled by other threads.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Process* the data of the shared memory. Making a new synchronization is necessary
    to ensure that the shared memory has been updated with the results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Write* the results in global memory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To clarify this approach, in the following section, we will present an example
    based on the calculation of the product of two matrices.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code fragment shows the calculation of the product of two matrices,
    *M×N*, in the standard method, which is based on a sequential approach. Each element
    of the output matrix, `P`, is obtained by taking a row element from matrix `M`,
    and a column element from matrix `N`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In this case, if each thread had been given the task of calculating each element
    of the matrix, then access to the memory would have dominated the execution time
    of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we can do is rely on a block of threads to calculate one output submatrix
    at a time. In this way, the threads that access the same memory block cooperate
    to optimize accesses, thereby minimizing the total calculation time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to load all the necessary modules to implement the algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, initialize the GPU device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We implement `kernel_code_template`, which implements the product of two matrices
    that are respectively indicated with `a` and `b`, while the resulting matrix is
    indicated with the parameter `c`. Note that the `MATRIX_SIZE` parameter will be
    defined in the next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following parameter will be used to set the dimensions of the matrices.
    In this case, the size is 5 × 5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the two input matrices, `a_cpu` and `b_cpu`, that will contain random
    floating-point values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we calculate the product of the two matrices, `a` and `b`, on the host
    device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We allocate memory areas on the device (GPU), equal in size to the input matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We allocate a memory area on the GPU, equal in size to the output matrix resulting
    from the product of the two matrices. In this case, the resulting matrix, `c_gpu`,
    will have a size of 5 × 5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `kernel_code` redefines `kernel_code_template`, but with the
    `matrix_size` parameter set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The `SourceModule` directive tells `nvcc` (*NVIDIA CUDA Compiler)* that it
    will have to create a module—that is, a collection of functions—containing the previously
    defined `kernel_code`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we take the `MatrixMulKernel` function from the module, `mod`, to
    which we give the name `matrixmul`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We execute the product between two matrices, `a_gpu` and `b_gpu`, resulting
    in the `c_gpu` matrix. The size of the thread block is defined as `MATRIX_SIZE,
    MATRIX_SIZE, 1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the input matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'To check the validity of the calculation performed on the GPU, we compare the
    results of the two implementations, which are the one performed on the host device
    (CPU) and the one performed on the device (GPU). To do this, we use the `numpy
    allclose` directive, which verifies that two element-wise arrays are equal within
    a tolerance equal to `1e-05`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider the PyCUDA programming workflow. Let''s prepare the input matrix,
    the output matrix, and where to store the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we transfer these matrices to the GPU device by using the `gpuarray.to_gpu()`
    PyCUDA function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The core of the algorithm is the following kernel function. Let''s remark that
    the `__global__` keyword specifies that this function is a kernel function, which
    means that it will be executed by the device (GPU) following a call from the host
    code (CPU):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '`threadIdx.x` and `threadIdy.y` are coordinates that allow the identification
    of the threads in the grid of two-dimensional blocks. Note that the threads within
    the grid block execute the same kernel code but on different pieces of data. If
    we compare the parallel version with the sequential one, then we immediately notice
    that the cycle indexes, *i* and *j*, have been replaced by the `threadIdx.x` and
    `threadIdx.y` indexes.'
  prefs: []
  type: TYPE_NORMAL
- en: This means that in the parallel version, we will have only one iteration of
    the cycle. In fact, the `MatrixMulKernel` kernel will be executed on a grid of
    dimensions of 5 × 5 parallel threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'This condition is expressed in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65aa99d1-d699-4329-883c-543cb7ef15de.png)'
  prefs: []
  type: TYPE_IMG
- en: Grid and block of thread organization for the example
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we verify the product computation just by comparing the two resulting
    matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data allocated in shared memory has limited visibility in the single-threaded
    block. It is easy to see that the PyCUDA programming model adapts to specific
    classes of applications.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, the features that these applications must present concern the
    presence of many mathematical operations, with a high degree of data parallelism
    (that is, the same sequence of operations being repeated on large amounts of data).
  prefs: []
  type: TYPE_NORMAL
- en: 'The application fields that possess these characteristics all belong to the
    following sciences: cryptography, computational chemistry, and image and signal
    analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More examples of using PyCUDA can be found at [https://github.com/zamorays/miniCursoPycuda](https://github.com/zamorays/miniCursoPycuda).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing PyOpenCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyOpenCL is a sister project to PyCUDA. It is a binding library that provides
    full access to OpenCL's API from Python and is also by Andreas Klöckner. It features
    many of the same concepts as PyCUDA, including cleanup for out-of-scope objects,
    partial abstraction over data structures, and error handling, all with minimal
    overhead. The project is available under the MIT license; its documentation is
    very good and plenty of guides and tutorials can be found online.
  prefs: []
  type: TYPE_NORMAL
- en: The main focus of PyOpenCL is to provide a lightweight connection between Python
    and OpenCL, but it also includes support for templates and metaprograms. The flow
    of a PyOpenCL program is almost exactly the same as a C or C++ program for OpenCL.
    The host program prepares the call of the device program, launches it, and then
    waits for the result.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main reference for the PyOpenCL installation is the Andreas Klöckner home
    page: [https://mathema.tician.de/software/pyopencl/](https://mathema.tician.de/software/pyopencl/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using Anaconda, then it is advisable to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the latest Anaconda distribution with Python 3.7 from the following
    link: [https://www.anaconda.com/distribution/#download-section](https://www.anaconda.com/distribution/#download-section). For
    this section, the Anaconda 2019.07 for Windows Installer has been installed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the PyOpenCL prebuilt binary from Christoph Gohlke from this link: [https://www.lfd.uci.edu/~gohlke/pythonlibs/](https://www.lfd.uci.edu/~gohlke/pythonlibs/).
    Select the right combination of OS and CPython versions. Here, we use `pyopencl-2019.1+cl12-cp37-cp37m-win_amd64.whl`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use `pip` to install the previous package. Simply type this in your Anaconda
    Prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '`<directory>` is the folder where the PyOpenCL package is located.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, the following notation indicates that we are operating on the Anaconda
    Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following example, we will use a function of PyOpenCL that allows us
    to enumerate the features of the GPU on which it will operate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code we implement is very simple and logical:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first step, we import the `pyopencl` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We build a function whose output will provide us with the characteristics of
    the GPU hardware in use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we implement the `main` function, which calls the previously implemented `print_device_info` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following command is used to import the `pyopencl` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This allows us to use the **`get_platforms` **method, which returns a list
    of platform instances, that is, a list of devices in the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for each device found, the following main features are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: Name and device type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max clock speed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute units
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local/constant/global memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output for this example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenCL is currently managed by the Khronos Group, a non-profit consortium of
    companies that collaborate in defining the specifications of this (and many other)
    standards and compliance parameters for the creation of OpenCL-specific drivers for
    each type of platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'These drivers also provide functions for compiling programs that are written
    in the kernel language: these are converted into programs in some form of intermediate
    language that is usually vendor-specific, and then executed on the reference architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More info on OpenCL can be found at the following link: [https://www.khronos.org/registry/OpenCL/](https://www.khronos.org/registry/OpenCL/).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyOpenCL documentation is available here: [https://documen.tician.de/pyopencl/](https://documen.tician.de/pyopencl/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the best introductions to PyOpenCL, even if somewhat dated, can be found
    at the following link: [http://www.drdobbs.com/open-source/easy-opencl-with-python/240162614](http://www.drdobbs.com/open-source/easy-opencl-with-python/240162614).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building applications with PyOpenCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in the construction of a program for PyOpenCL is the coding of
    the host application. This is performed on the CPU and has the task of managing
    the possible execution of the kernel on the GPU card (that is, the device).
  prefs: []
  type: TYPE_NORMAL
- en: A *kernel* is a basic unit of executable code, similar to a C function. It can
    be data-parallel or task-parallel. However, the cornerstone of PyOpenCL is the
    exploitation of parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: A fundamental concept is a *program*, which is a collection of kernels and other
    functions, analogous to dynamic libraries. So, we can group instructions in a
    kernel and group different kernels into a program.
  prefs: []
  type: TYPE_NORMAL
- en: Programs can be called from applications. We have the execution queues that
    indicate the order in which the kernels are executed. However, in some cases,
    these can be launched without following the original order.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can finally list the fundamental elements for developing an application
    with PyOpenCL:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Device**: This identifies the hardware in which the kernel code is to be
    executed. Note that the PyOpenCL application can be run on both CPU and GPU boards
    (as well as PyCUDA) but also on embedded devices such as**Field-Programmable Gate
    Arrays**(**FPGAs**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Program**: This is a group of kernels that has the task of selecting which
    kernel must be run on the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel**: This is the code to execute on the device. A kernel is a C-like
    function, which means it can be compiled on any device that supports PyOpenCL
    drivers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Command queue**: This orders the execution of kernels on the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context**: This is a group of devices that allows devices to receive kernels
    and transfer data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows how this data structure can work in a host application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60b38941-28e9-4b2f-a920-d59c4f426b1e.png)'
  prefs: []
  type: TYPE_IMG
- en: PyOpenCL programming model
  prefs: []
  type: TYPE_NORMAL
- en: Again, we observe that a program can contain more functions to run on the device
    and that each kernel encapsulates only a single function from the program.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we show the basic steps to build an application with
    PyOpenCL: the task to be performed is the sum of two vectors. In order to have
    a readable output, we''ll consider two vectors that each have 100 elements: each
    *i-th* element of the resulting vector will be equal to the sum of the *i-th*
    element of **`vector_a`**, plus the *i-th* element of **`vector_b`**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing all the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the size of the vectors to be added, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the input vectors, `vector_a` and `vector_b`, are defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'In sequence, we define **`platform`**, **`device`**, **`context`**, and **`queue`**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to organize the memory areas that will contain the input vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we build the application kernel by using the `Program` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we allocate the memory of the resulting matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we call the kernel function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The memory space used to store the result is allocated in the host memory area
    *(*`res_np`*)*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the result of the computation into the memory area created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we perform a simple check in order to verify that the sum operation is
    correct:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following lines, after the relevant import, we define the input vectors*:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Each vector contains 100 integer items, which are randomly selected through
    the `numpy` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we select the platform to achieve the computation by using the `get_platform()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, select the corresponding device. Here, `platform.get_devices()[0]` corresponds
    to the Intel(R) HD Graphics 5500 graphics card:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following steps, the context and the queue are defined; PyOpenCL provides
    the method context (device selected) and queue (context selected):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to perform the computation in the selected device, the input vector
    is copied to the device''s memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we prepare the buffer for the resulting vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the kernel code is defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '`vectorSum` is the name of the kernel, and the parameter list defines the data
    types of the input arguments and output data type (both are integer vectors).
    Inside the kernel body, the sum of two vectors is defined in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Initialize* the vector''s index: `int gid = get_global_id(0)`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Sum* the vector''s components: `res_g[gid] = a_g[gid] + b_g[gid]`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In OpenCL (hence, in PyOpenCL), the buffers are attached to a context ([https://documen.tician.de/pyopencl/runtime.html#pyopencl.Context](https://documen.tician.de/pyopencl/runtime.html#pyopencl.Context)),
    which are moved to a device once the buffer is used on that device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we execute `vectorSum` in the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'To check the result, we use the `assert` statement. This tests the result and
    triggers an error if the condition is false:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we have seen that the PyOpenCL execution model, like PyCUDA,
    involves a host processor that manages one or more heterogeneous devices. In particular,
    each PyOpenCL command is sent to the devices from the host in the form of source
    code that is defined through the kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: The source code is then loaded into a program object for the reference architecture,
    the program is compiled into the reference architecture, and the kernel object that
    is relative to the program is created.
  prefs: []
  type: TYPE_NORMAL
- en: A kernel object can be executed in a variable number of workgroups, creating
    an *n*-dimensional computation matrix that allows it to effectively subdivide
    the workload for a problem in *n*-dimensions (1, 2, or 3) in each workgroup. In
    turn, they are composed of a number of work items that work in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing the workload for each workgroup based on the parallel computing capability
    of a device is one of the critical parameters for achieving good application performance.
  prefs: []
  type: TYPE_NORMAL
- en: A wrong balancing of the workload, together with the specific characteristics
    of each device (such as transfer latency, throughput, and bandwidth), can lead
    to a substantial loss of performance or compromise the portability of the code
    when executed without considering any system of dynamic acquisition of information
    in terms of device calculation capacities.
  prefs: []
  type: TYPE_NORMAL
- en: However, the accurate use of these technologies allows us to reach high levels
    of performance by combining the results of the calculation of different computational
    units.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More on PyOpenCL programming can be found at [https://pydanny-event-notes.readthedocs.io/en/latest/PyConPL2012/async_via_pyopencl.html](https://pydanny-event-notes.readthedocs.io/en/latest/PyConPL2012/async_via_pyopencl.html).
  prefs: []
  type: TYPE_NORMAL
- en: Element-wise expressions with PyOpenCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The element-wise functionality allows us to evaluate kernels on complex expressions
    (which are made of more operands) into a single computational pass.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `ElementwiseKernel (context, argument, operation, name, optional_parameters)` method is
    implemented in PyOpenCL to handle element-wise expressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`context` is the device or the group of devices to which the element-wise operation
    will be executed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`argument` is a C-like argument list of all the parameters involved in the
    computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`operation` is a string that represents the operation to perform on the argument
    list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name` is the kernel''s name that is associated with `Elementwisekernel`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optional_parameters` is not important in this recipe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we consider the task of adding two integer vectors again:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start importing the relevant libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the context element (`context`) and the command queue (`queue`) :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we set the vector dimension and the space allocation for the input and
    output vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'We set `elementwiseSum` as the application of `ElementwiseKernel`, and then
    set it to a set of arguments that define the operations to be applied to the input
    vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first lines of the script, we import all the requested modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to initialize the context, we use the `cl.create_some_context()` method.
    This asks the user which context must be used to perform the calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to instantiate the queue that will receive `ElementwiseKernel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Input and output vectors are instantiated. The input vectors, `vector_a` and
    `vector_b`, are integer vectors of random values obtained using the `random.randint`
    NumPy function. These vectors are then copied into the device by using the PyOpenCL
    statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'In `ElementwiseKernel`, an object is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Note that all the arguments are in the form of a string formatted as a C argument
    list (they are all integers).
  prefs: []
  type: TYPE_NORMAL
- en: The operation is a C-like code snippet that carries out the operation, that
    is, the sum of the input vector elements.
  prefs: []
  type: TYPE_NORMAL
- en: The name of the function with which the kernel will be compiled is `sum`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we call the `elementwiseSum` function with the arguments defined previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'The example ends by printing the input vectors and the result obtained. The
    output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyCUDA also has element-wise functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'This feature has pretty much the same arguments as the function built for PyOpenCL,
    except for the context parameter. The same example this section, which is implemented
    through PyCUDA, has the following listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following link, you'll find interesting examples of PyOpenCL applications: [https://github.com/romanarranz/PyOpenCL](https://github.com/romanarranz/PyOpenCL).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating PyOpenCL applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are doing a comparative test of performance between CPU
    and GPU by using the PyOpenCL library.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, before studying the performance of the algorithms to be implemented,
    it is also important to understand the computational advantages offered by the
    computing platform you have.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The specific characteristics of a computing system interfere with the computational
    time, and hence they represent an aspect of primary importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will perform a test in order to monitor performance
    on such a system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPU: GeForce 840 M'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CPU: Intel Core i7 – 2.40 GHz'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RAM: 8 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following test, the calculation time of a mathematical operation, as
    the sum of two vectors with floating-point elements, will be evaluated and compared.
    To make the comparison, the same operation will be performed on two separate functions.
  prefs: []
  type: TYPE_NORMAL
- en: The first function is computed by the CPU only, while the second function is
    written by using the PyOpenCL library to use the GPU card. The test is performed
    on vectors with a size of 10,000 elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant libraries. Note the import of the `time` library to calculate
    the computation times, and the `linalg` library, which is a tool of linear algebra
    tools of the `numpy` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the input vectors. They both contain `10000` random elements
    of floating-point numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function computes the sum of the two vectors working on the CPU
    (host):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function computes the sum of the two vectors working on the GPU
    (device):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Within the `test_gpu_vector_sum` function, we prepare the memory buffers to
    contain the input vectors and the output vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Still, within the `test_gpu_vector_sum` function, we define the kernel that
    will computerize the sum of the two vectors on the device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we reset the `gpu_start_time` variable before starting the calculation.
    After this, we calculate the sum of two vectors and then we evaluate the calculation
    time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we perform the test, recalling the two functions defined previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained previously, the test consists of executing the calculation task,
    both on the CPU via the `test_cpu_vector_sum` function, and then on the GPU via
    the `test_gpu_vector_sum` function.
  prefs: []
  type: TYPE_NORMAL
- en: Both functions report the execution time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the testing function on the CPU, `test_cpu_vector_sum`, it consists
    of a double calculation loop on `10000` vector elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'The total CPU time is the difference between the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'As for the `test_gpu_vector_sum` function, you can see the following by looking
    at the execution kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: The sum of the two vectors is performed through a single calculation loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result, as can be imagined, is a substantial reduction in the execution
    time for the `test_gpu_vector_sum` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: Even if the test is not computationally expansive, it provides useful indications
    of the potential of a GPU card.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenCL is a standardized cross-platform API for developing applications that
    exploit parallel computing in heterogeneous systems. The similarities with CUDA
    are remarkable, including everything from the memory hierarchy to the direct correspondence
    between threads and work items.
  prefs: []
  type: TYPE_NORMAL
- en: Even at the programming level, there are many similar aspects and extensions
    with the same functionality.
  prefs: []
  type: TYPE_NORMAL
- en: However, OpenCL has a much more complex device management model due to its ability
    to support a wide variety of hardware. On the other hand, OpenCL is designed to
    have code portability between products from different manufacturers.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA, thanks to its greater maturity and dedicated hardware, offers simplified
    device management and higher-level APIs that make it preferable, but only if you
    are dealing with specific architectures (that is, NVIDIA graphic cards).
  prefs: []
  type: TYPE_NORMAL
- en: The pros and cons of the CUDA and OpenCL libraries, as well as the PyCUDA and
    PyOpenCL libraries, are explained in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Pros of OpenCL and PyOpenCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The pros are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: They allow the use of heterogeneous systems with different types of microprocessors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same code runs on different systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons of OpenCL and PyOpenCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The cons are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Complex device management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: APIs not fully stable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pros of CUDA and PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The pros are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: APIs with very high abstraction levels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extensions for many programming languages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huge documentation and a very large community
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons of CUDA and PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The cons are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Supports only the latest NVIDIA GPUs as devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduces heterogeneity to CPUs and GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Andreas Klöckner has made a series of lectures on GPU programming with PyCuda
    and PyOpenCL available at [https://www.bu.edu/pasi/courses/gpu-programming-with-pyopencl-and-pycuda/](https://www.bu.edu/pasi/courses/gpu-programming-with-pyopencl-and-pycuda/)
    and [https://www.youtube.com/results?search_query=pyopenCL+and+pycuda](https://www.youtube.com/results?search_query=pyopenCL+and+pycuda).
  prefs: []
  type: TYPE_NORMAL
- en: GPU programming with Numba
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Numba is a Python compiler that provides CUDA-based APIs. It has been designed
    primarily for numerical computing tasks, just like the NumPy library. In particular,
    the `numba` library manages and processes the array data types provided by NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the exploitation of data parallelism, which is inherent in numerical
    computation involving arrays, is a natural choice for GPU accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: The Numba compiler works by specifying the signature types (or decorators) for
    Python functions and enabling the compilation at runtime (this type of compilation
    is also called *Just In Time*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important decorators are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`jit`: This allows the developer to write CUDA-like functions. When encountered,
    the compiler translates the code under the decorator into the pseudo-assembly
    PTX language, so that it can be executed by the GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`autojit`: This annotates a function for a *deferred compilation* procedure,
    which means that the function with this signature is compiled exactly once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vectorize`: This creates a so-called** NumPy Universal Function** (**ufunc**)
    that takes a function and executes it in parallel with vector arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guvectorize`: This builds a so-called **NumPy Generalized Universal Function**
    (**gufunc**). A `gufunc` object may operate on entire sub-arrays.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Numba (release 0.45) is compatible with Python 2.7 and 3.5 or later, as well
    as NumPy versions 1.7 to 1.16.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `numba`, it is recommended as per `pyopencl` to use the Anaconda
    framework, so, from the Anaconda Prompt, just type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, to use the full potential of `numba`, the `cudatoolkit` library
    must be installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: After that, it's possible to verify whether the CUDA library and GPU are properly
    detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the Python interpreter from the Anaconda Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'The first test entails checking whether the CUDA library (`cudatoolkit`) is
    properly installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows the quality of the installation, where all the checks
    returned a positive result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'In the second test, we verify the presence of a graphics card:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the graphic card found and whether it is supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we provide a demonstration of the Numba compiler using the `@guvectorize` annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task to execute is matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `guvectorize` from the `numba` library and the `numpy` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `@guvectorize` decorator, we define the `matmul` function, which
    will perform the matrix multiplication task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'The input matrices are 10 × 10 in size, while the elements are integers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we call the `matmul` function on the previously defined input matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'We print the input matrices and the resulting matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `@guvectorize` decorator works on array arguments, taking four arguments in
    order to specify the `gufunc` signature:'
  prefs: []
  type: TYPE_NORMAL
- en: The first three arguments specify the types of data to be managed and arrays
    of integers: `void(int64[:,:], int64[:,:], int64[:,:])`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last argument of `@guvectorize` specifies how to manipulate the matrix dimensions: `(m,n),(n,p)->(m,p)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, the matrix multiplication operation is defined, where `A` and `B` are
    the input matrices and `C` is the output matrix: *A(m,n)* B(n,p) = C(m,p)*, where *m*, *n*,
    and *p* are the matrix dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The matrix product is performed through three `for` loops along with the matrix
    indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'The `randint` NumPy function is used here to build the input matrices of 10
    × 10 dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `matmul` function is called with these matrices as arguments,
    and the resultant `C` matrix is printed out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'To execute this example, type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'The result shows the two matrices given as input and the matrix resulting from
    their product:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Writing an algorithm for a reduction operation using PyCUDA can be quite complex.
    For this purpose, Numba provides the `@reduce` decorator for converting simple
    binary operations into *reduction kernels*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reduction operations reduce a set of values to a single value. A typical example
    of a reduction operation is to calculate the sum of all the elements of an array.
    As an example, consider the following array of elements: 1, 2, 3, 4, 5, 6, 7,
    8.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sequential algorithm operates in the way shown in the diagram, that is,
    adding the elements of the array one after the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e5ea317-7653-4c24-96f3-8ea106d866df.png)'
  prefs: []
  type: TYPE_IMG
- en: Sequential sum
  prefs: []
  type: TYPE_NORMAL
- en: 'A parallel algorithm operates according to the following schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3704575d-6b42-4dc7-b4f9-01093cb44870.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallel sum
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that the latter has the advantage of shorter execution time.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using Numba and the `@reduce` decorator, we can write an algorithm, in a
    few lines of code, for the parallel sum on an array of integers ranging from 1
    to 10,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous example can be performed by typing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following repository, you can find many examples of Numba: [https://github.com/numba/numba-examples](https://github.com/numba/numba-examples). 
    An interesting introduction to Numba and CUDA programming can be found at [https://nyu-cds.github.io/python-numba/05-cuda/](https://nyu-cds.github.io/python-numba/).
  prefs: []
  type: TYPE_NORMAL
