- en: '*Chapter 7*: Implementing Concurrency'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have explored how to measure and improve the performance of programs
    by reducing the number of operations performed by the **central processing unit**
    (**CPU**) through clever algorithms and more efficient machine code. In this chapter,
    we will shift our focus to programs where most of the time is spent waiting for
    resources that are much slower than the CPU, such as persistent storage and network
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: '**Asynchronous programming** is a programming paradigm that helps to deal with
    slow and unpredictable resources (such as users) and is widely used to build responsive
    services and **user interfaces** (**UIs**). In this chapter, we will show you
    how to program asynchronously in Python using techniques such as coroutines and
    reactive programming. As we will see, the successful application of these techniques
    will allow us to speed up our programs without the use of specialized data structures
    or algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `asyncio` framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reactive programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter07](https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter07).
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asynchronous programming is a way of dealing with slow and unpredictable resources.
    Rather than waiting idly for resources to become available, asynchronous programs
    can handle multiple resources concurrently and efficiently. Programming in an
    asynchronous way can be challenging because it is necessary to deal with external
    requests that can arrive in any order, may take a variable amount of time, or
    may fail unpredictably. In this section, we will introduce the topic by explaining
    the main concepts and terminology as well as by giving an idea of how asynchronous
    programs work.
  prefs: []
  type: TYPE_NORMAL
- en: Waiting for input/output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A modern computer employs different kinds of memory to store data and perform
    operations. In general, a computer possesses a combination of expensive memory
    that is capable of operating efficiently and cheaply and more abundant memory
    that is slower and is used to store a larger amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory hierarchy is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Illustration of the memory hierarchy ](img/Figure_7.1_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Illustration of the memory hierarchy
  prefs: []
  type: TYPE_NORMAL
- en: At the top of the memory hierarchy, are the CPU **registers**. Those are integrated
    with the CPU and are used to store and execute machine instructions. Accessing
    data in a register generally takes one clock cycle. This means that if the CPU
    operates at 3 **gigahertz** (**GHz**), the time it takes to access one element
    in a CPU register is in the order of 0.3 nanoseconds.
  prefs: []
  type: TYPE_NORMAL
- en: At the layer just below the registers, you can find the CPU **cache**, which
    comprise multiple levels and is integrated with the processor. The cache operates
    at a slightly slower speed than the registers but within the same **order of magnitude**
    (**OOM**).
  prefs: []
  type: TYPE_NORMAL
- en: The next item in the hierarchy is the main memory (**random-access memory**,
    or **RAM**), which holds much more data but is slower than the cache. Fetching
    an item from memory can take a few hundred clock cycles.
  prefs: []
  type: TYPE_NORMAL
- en: At the bottom layer, you can find persistent **storage**, such as rotating disks
    (**hard disk drives** (**HDDs**)) and **solid-state drives** (**SSDs**). These
    devices hold the most data and are OOMs slower than the main memory. An HDD may
    take a few milliseconds to seek and retrieve an item, while an SSD is substantially
    faster and takes only a fraction of a millisecond.
  prefs: []
  type: TYPE_NORMAL
- en: To put the relative speed of each memory type into perspective, if you were
    to have the CPU with a clock speed of about 1 second, a register access would
    be equivalent to picking up a pen from a table. A cache access would be equivalent
    to picking up a book from a shelf. Moving higher up the hierarchy, a RAM access
    would be equivalent to loading up the laundry (about 20 times slower than the
    cache). When we move to persistent storage, things are quite different. Retrieving
    an element from an SSD will be equivalent to going on a 4-day road trip while
    retrieving an element from an HDD can take up to 6 months! The duration can stretch
    even further if we move on to access resources over the network.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, accessing data from storage and other **input/output** (**I/O**) devices
    is much slower compared to the CPU; therefore, it is very important to handle
    those resources so that the CPU is never stuck waiting aimlessly. This can be
    accomplished by carefully designed software capable of managing multiple ongoing
    requests at the same time. This is the idea of concurrency or concurrent programming.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Concurrency** is a way to implement a system that can deal with multiple
    requests at the same time. The idea is that we can move on and start handling
    other resources while we wait for a resource to become available. Concurrency
    works by splitting a task into smaller subtasks that can be executed out of order
    so that multiple tasks can be partially advanced without waiting for the previous
    tasks to finish.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first example, we will describe how to implement concurrent access to
    a slow network resource; the code for this example is included in `Chapter07/example1.py`.
    Let''s say we have a web service that takes the square of a number, and the time
    between our request and the response will be approximately 1 second. We can implement
    the `network_request` function that takes a number and returns a dictionary that
    contains information about the success of the operation and the result. We can
    simulate such services using the `time.sleep` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also write some additional code that performs the request, verifies
    that the request was successful, and prints the result. In the following code
    snippet, we define the `fetch_square` function and use it to calculate the square
    of the number 2 using a call to `network_request`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Fetching a number from the network will take 1 second because of the slow network.
    What if we want to calculate the square of multiple numbers? We can call `fetch_square`,
    which will start a network request as soon as the previous one is done. Its use
    is illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This code as-is will take roughly 3 seconds to run, but it's not the best we
    can do. Notice that the calculation of the square of 2 is independent of that
    of the square of 3, and both are in turn independent of calculating the square
    of 4\. As such, waiting for a previous result to finish before moving on to the
    next number is unnecessary, if we can technically submit multiple requests and
    wait for them at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, the three tasks are represented as boxes. The time
    spent by the CPU processing and submitting the request is in orange, while the
    waiting times are in blue. You can see how most of the time is spent waiting for
    the resources while our machine sits idle without doing anything else:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Illustration of the execution time of independent calculations
    ](img/Figure_7.2_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Illustration of the execution time of independent calculations
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, we would like to start another new task while we are waiting for the
    already submitted tasks to finish. In the following screenshot, you can see that
    as soon as we submit our request in `fetch_square(2)`, we can start preparing
    for `fetch_square(3)`, and so on. This allows us to reduce the CPU waiting time
    and to start processing the results as soon as they become available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – A more efficient way of performing independent calculations
    ](img/Figure_7.3_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – A more efficient way of performing independent calculations
  prefs: []
  type: TYPE_NORMAL
- en: Again, this strategy is made possible by the fact that the three requests are
    completely independent, and we don't need to wait for the completion of a previous
    task to start the next one. Also, note how a single CPU can comfortably handle
    this scenario. While distributing the work on multiple CPUs can further speed
    up the execution, if the waiting time is large compared to the processing times,
    the speedup will be minimal.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement concurrency, it is necessary to think about our programs and their
    design differently; in the following sections, we''ll demonstrate techniques and
    best practices to implement robust concurrent applications, starting with a new
    concept: callbacks.'
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code we have seen so far blocks the execution of the program until the resource
    is available. The call responsible for the waiting is `time.sleep`. To make the
    code start working on other tasks, we need to find a way to avoid blocking the
    program flow so that the rest of the program can move on to these other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: One of the simplest ways to accomplish this behavior is through callbacks. The
    strategy is quite similar to what we do when we request a cab. Imagine that you
    are at a restaurant and you've had a few drinks. It's raining outside, and you'd
    rather not take the bus; therefore, you request a taxi and ask them to call when
    they're outside so that you can come out and you don't have to wait in the rain.
    What you did, in this case, is request a taxi (that is, the slow resource), but
    instead of waiting outside until the taxi arrives, you provide your number and
    instructions (callback) so that you can come outside when they're ready and go
    home.
  prefs: []
  type: TYPE_NORMAL
- en: We will now show how this mechanism can work in code. We will compare the blocking
    code of `time.sleep` with the equivalent non-blocking code of `threading.Timer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, which is implemented in `example2.py`, we will write a function,
    `wait_and_print`, that will block the program execution for 1 second and then
    print a message, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to write the same function in a non-blocking way, we can use the
    `threading.Timer` class. We can initialize a `threading.Timer` instance by passing
    the amount of time we want to wait and a `Timer.start` method to activate the
    timer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: An important feature of the `wait_and_print_async` function is that none of
    the statements is blocking the execution flow of the program.
  prefs: []
  type: TYPE_NORMAL
- en: How Is threading.Timer Capable of Waiting without Blocking?
  prefs: []
  type: TYPE_NORMAL
- en: The strategy used by `threading.Timer` involves starting a new thread that can
    execute code in parallel. If this is confusing, don't worry, as we will explore
    threading and parallel programming in detail in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: This technique of registering callbacks for execution in response to certain
    events is commonly called the *Hollywood principle*. This is because, after auditioning
    for a movie or TV role at Hollywood, you may be told *Don't call us, we'll call
    you*, meaning that they won't tell you if they chose you for the role immediately,
    but they'll call you if they do.
  prefs: []
  type: TYPE_NORMAL
- en: 'To highlight the difference between the blocking and non-blocking versions
    of `wait_and_print`, we can test and compare the execution of the two versions.
    In the output comments, the waiting periods are indicated by `<wait...>`, as illustrated
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The synchronous version behaves in a very familiar, expected way. The code waits
    for a second, prints `First call`, waits for another second, and then prints `Second
    call` and `After call` messages.
  prefs: []
  type: TYPE_NORMAL
- en: In the asynchronous version, `wait_and_print_async` *submits* (rather than *executes*)
    those calls and moves on *immediately*. You can see this mechanism in action by
    noticing that the `"After submission"` message is printed immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, we can explore a slightly more complex situation by rewriting
    our `network_request` function using callbacks. In `example3.py`, we define a
    `network_request_async` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The biggest difference between `network_request_async` and its blocking counterpart
    is that `network_request_async` *doesn't return anything*. This is because we
    are merely submitting the request when `network_request_async` is called, but
    the value is available only when the request is completed.
  prefs: []
  type: TYPE_NORMAL
- en: If we can't return anything, how do we pass the result of the request? Rather
    than returning the value, we will pass the result as an argument to the `on_done`
    callback. The rest of the function consists of submitting a callback (called `timer_done`)
    to the `threading.Timer` class that will call `on_done` when it's ready.
  prefs: []
  type: TYPE_NORMAL
- en: 'The usage of `network_request_async` is quite similar to `threading.Timer`;
    all we have to do is pass the number we want to square and a callback that will
    receive the result *when it''s ready*. This is demonstrated in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we submit multiple network requests, we note that the calls get executed
    concurrently and do not block the code, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to use `network_request_async` in `fetch_square`, we need to adapt
    the code to take advantage of asynchronous constructs. In the following code snippet,
    we modify `fetch_square` by defining and passing the `on_done` callback to `network_request_async`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You may have noted that the asynchronous code is significantly more convoluted
    than its synchronous counterpart. This is due to the fact that we are required
    to write and pass a callback every time we need to retrieve a certain result,
    causing the code to become nested and hard to follow.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, a concept that is essential to concurrent programming that we are
    examining next, futures, will help simplify matters.
  prefs: []
  type: TYPE_NORMAL
- en: Futures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Futures are a more convenient pattern that can be used to keep track of the
    results of asynchronous calls. In the preceding code snippet, we saw that rather
    than returning values, we accept callbacks and pass the results when they are
    ready. It is interesting to note that, so far, there is no easy way to track the
    status of the resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `concurrent.futures.Future` class. A `Future` instance can be created by
    calling its constructor with no arguments, as in the following IPython snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'A future represents a value that is not yet available. You can see that its
    string representation reports the current status of the result, which, in our
    case, is still pending. In order to make a result available, we can use the `fut.set_result`
    method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that once we set the result, the `Future` instance will report
    that the task is finished and can be accessed using the `fut.result` method. It
    is also possible to subscribe a callback to a future so that, as soon as the result
    is available, the callback is executed. To attach a callback, it is sufficient
    to pass a function to the `fut.add_done_callback` method. When the task is completed,
    the function will be called with the `Future` instance as its first argument and
    the result can be retrieved using the `future.result()` method, as illustrated
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To understand how futures can be used in practice, we will adapt the `network_request_async`
    function to use futures in `example4.py`. The idea is that this time, instead
    of returning nothing, we return a `Future` instance that will keep track of the
    result for us. Note the following two things:'
  prefs: []
  type: TYPE_NORMAL
- en: We don't need to accept an `on_done callback` as callbacks can be connected
    later using the `fut.add_done_callback` method. Also, we pass the generic `fut.set_result`
    method as the callback for `threading.Timer`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This time, we are able to return a value, thus making the code a bit more similar
    to the blocking version we saw in the preceding section, as illustrated here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Even though we instantiate and manage futures directly in these examples, in
    practical applications, futures are handled by frameworks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you execute the preceding code, nothing will happen as the code only consists
    of preparing and returning a `Future` instance. To enable further operation of
    the future results, we need to use the `fut.add_done_callback` method. In the
    following code snippet, we adapt the `fetch_square` function to use futures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the code still looks quite similar to the callback version,
    but when `fetch_square` is called, the corresponding future will be processed,
    and the result string will be printed out.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, futures are a different and slightly more convenient way of working
    with callbacks. Futures are also advantageous in the sense that they can keep
    track of the resource status, cancel (unschedule) scheduled tasks, and handle
    exceptions more naturally.
  prefs: []
  type: TYPE_NORMAL
- en: Event loops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have implemented parallelism using **operating system** (**OS**)
    threads. However, in many asynchronous frameworks, the coordination of concurrent
    tasks is managed by an **event loop**.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind an event loop is to continuously monitor the status of the various
    resources (for example, network connections and database queries) and trigger
    the execution of callbacks when specific events take place (for example, when
    a resource is ready or when a timer expires).
  prefs: []
  type: TYPE_NORMAL
- en: Why Not Just Stick to Threading?
  prefs: []
  type: TYPE_NORMAL
- en: Events loops are sometimes preferred as every unit of execution never runs at
    the same time as another, and this can simplify dealing with shared variables,
    data structures, and resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the first example in `example5.py`, we will implement a thread-free version
    of `threading.Timer`. We can define a `Timer` class that will take a timeout and
    implement the `Timer.done` method, which returns `True` if the timer has expired.
    The code is illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To determine whether the timer has expired, we can write a loop that continuously
    checks the timer status by calling the `Timer.done` method. When the timer expires,
    we can print a message and exit the cycle. The code is illustrated in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: By implementing the timer in this way, the flow of execution is never blocked,
    and we can, in principle, do other work inside the `while` loop.
  prefs: []
  type: TYPE_NORMAL
- en: Busy-Waiting
  prefs: []
  type: TYPE_NORMAL
- en: Waiting for events to happen by continuously polling using a loop is commonly
    termed **busy-waiting**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, we would like to attach a custom function that executes when the timer
    goes off, just as we did in `threading.Timer`. To do this, we can implement a
    `Timer.on_timer_done` method that will accept a callback to be executed when the
    timer goes off in `example6.py`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `on_timer_done` merely stores a reference to the callback. The entity
    that monitors the event and executes the callback is the loop. This concept is
    demonstrated as follows. Rather than using the `print` function, the loop will
    call `timer.callback` when appropriate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, an asynchronous framework is starting to take place. All we
    did outside the loop was define the timer and the callback, while the loop took
    care of monitoring the timer and executing the associated callback. We can further
    extend our code by implementing support for multiple timers.
  prefs: []
  type: TYPE_NORMAL
- en: 'A natural way to implement multiple timers is to add a few `Timer` instances
    to a list and modify our event loop to periodically check all the timers and dispatch
    the callbacks when required. In the following code snippet, we define two timers
    and attach a callback to each of them. Those timers are added to a list, `timers`,
    that is continuously monitored by our event loop. As soon as a timer is done,
    we execute the callback and remove the event from the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The main restriction of an event loop is, since the flow of execution is managed
    by a continuously running loop, that it `time.sleep`) inside the loop, you can
    imagine how the event monitoring and callback dispatching will stop until the
    blocking call is done.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this, rather than using a blocking call such as `time.sleep`, we let
    the event loop detect and execute the callback when the resource is ready. By
    not blocking the execution flow, the event loop is free to monitor multiple resources
    in a concurrent way.
  prefs: []
  type: TYPE_NORMAL
- en: How Is the Event Loop Notified of Events?
  prefs: []
  type: TYPE_NORMAL
- en: Events notification is usually implemented through OS calls (such as the `select`
    Unix tool) that will resume the execution of the program whenever an event is
    ready (in contrast to busy-waiting).
  prefs: []
  type: TYPE_NORMAL
- en: The Python standard libraries include a very convenient event loop-based concurrency
    framework, `asyncio`, which will be the topic of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The asyncio framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we have seen how concurrency works and how to use callbacks and
    futures. We can now move on and learn how to use the `asyncio` package, which
    has been present in the standard Python library since version 3.4\. We will also
    explore the `async`/`await` syntax to deal with asynchronous programming in a
    very natural way.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first example, we will see how to retrieve and execute a simple callback
    using `asyncio`. The `asyncio` loop can be retrieved by calling the `asyncio.get_event_loop()`
    function. We can schedule a callback for execution using `loop.call_later`, which
    takes a delay in seconds and a callback. We can also use the `loop.stop` method
    to halt the loop and exit the program. To start processing the scheduled call,
    it is necessary to start the loop, which can be done using `loop.run_forever`.
    The following example in `example7.py` demonstrates the usage of these basic methods
    by scheduling a callback that will print a message and halt the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This code schedules a callback that will print out a message and then halt the
    loop. One of the main problems with callbacks is that they require you to break
    the program execution into small functions that will be invoked when a certain
    event takes place. As we saw in the earlier sections, callbacks can quickly become
    cumbersome. In the next section, we will see how to work with coroutines to, as
    with futures, simplify many aspects of concurrent programming.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Coroutines** are another, perhaps more natural, way to break up the program
    execution into chunks. They allow the programmer to write code that resembles
    synchronous code but will execute asynchronously. You may think of a coroutine
    as a function that can be stopped and resumed. A basic example of coroutines is
    generators.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generators can be defined in Python using the `yield` statement inside a function.
    In the following code example in `example8.py`, we implement the `range_generator`
    function, which produces and returns values from `0` to `n`. We also add a `print`
    statement to log the internal state of the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'When we call the `range_generator` function, the code is not executed immediately.
    Note that nothing is printed to output when the following snippet is executed.
    Instead, a `generator` object is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to start pulling values from a generator, it is necessary to use the
    `next` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that every time we invoke `next`, the code runs until it encounters the
    next `yield` statement, and it is necessary to issue another `next` statement
    to resume the generator execution. You can think of a `yield` statement as a breakpoint
    where we can stop and resume execution (while also maintaining the internal state
    of the generator). This ability to stop and resume execution can be leveraged
    by the event loop to allow for and implement concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to *inject* (rather than *extract*) values in the generator
    through the `yield` statement. In the following code example in `example9.py`,
    we declare a `parrot` function that will repeat each message that we send:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: To allow a generator to receive a value, you can assign `yield` to a variable
    (in our case, it is `message = yield`). To insert values in the generator, we
    can use the `send` method. In the Python world, a generator that can also receive
    values is called a **generator-based coroutine**.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we also need to issue a `generator.send(None)` request before we can
    start sending messages; this is to bootstrap the function execution and bring
    us to the first `yield` statement. Also, note that there is an infinite loop inside
    `parrot`; if we implement this without using generators, we will get stuck running
    the loop forever!
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, you can imagine how an event loop can partially progress
    several of these generators without blocking the execution of the whole program,
    as well as how a generator can be advanced only when some resource is ready, therefore
    eliminating the need for a callback.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to implement coroutines in `asyncio` using the `yield` statement.
    However, Python supports the definition of powerful coroutines using a more intuitive
    syntax. To define a coroutine with `asyncio`, you can use the `async def` statement,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, if we call the `hello` function, the function body is not executed
    immediately, but a `coroutine object` instance is returned. The `asyncio` coroutines
    do not support `next`, but they can be easily run in the `asyncio` event loop
    using the `run_until_complete` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Native Coroutines
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines defined with the `async def` statement are also called *native coroutines*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `asyncio` module provides resources (called `await` syntax. For example,
    in `example10.py`, if we want to wait for a certain time and then execute a statement,
    we can use the `asyncio.sleep` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The result is beautiful, clean code. We are writing perfectly functional asynchronous
    code without all the ugliness of callbacks!
  prefs: []
  type: TYPE_NORMAL
- en: Breakpoints for Event Loops
  prefs: []
  type: TYPE_NORMAL
- en: You may have noted how `await` provides a breakpoint for the event loop so that,
    as it waits for the resource, the event loop can move on and concurrently manage
    other coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even better, coroutines are also `awaitable`, and we can use the `await` statement
    to chain coroutines asynchronously. In the following example, we rewrite the `network_request`
    function, which we defined earlier, by replacing the call to `time.sleep` with
    `asyncio.sleep`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can follow up by reimplementing `fetch_square`, as illustrated in the following
    code snippet. As you can see, we await `network_request` directly without needing
    additional futures or callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The coroutines can be executed individually using `loop.run_until_complete`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Running tasks using `run_until_complete` serves as a demonstration for testing
    and debugging. However, our program will be started with `loop.run_forever` most
    of the time, and we will need to submit our tasks while the loop is already running.
  prefs: []
  type: TYPE_NORMAL
- en: '`asyncio` provides the `ensure_future` function, which schedules coroutines
    (as well as futures) for execution. `ensure_future` can be used by simply passing
    the coroutine we want to schedule. The following code snippet in `example11.py`
    will schedule multiple calls to `fetch_square` that will be executed concurrently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As a bonus, when passing a coroutine, the `asyncio.ensure_future` function will
    return a `Task` instance (which is a subclass of `Future`) so that we can take
    advantage of the `await` syntax without having to give up the resource-tracking
    capabilities of regular futures.
  prefs: []
  type: TYPE_NORMAL
- en: Converting blocking code into non-blocking code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While `asyncio` supports connecting to resources in an asynchronous way, it
    is required to use blocking calls in certain cases. This happens, for example,
    when third-party `asyncio`. Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: An effective strategy for dealing with blocking code is to run it in a separate
    thread. Threads are implemented at the OS level and allow parallel execution of
    blocking code. For this purpose, Python provides the `Executor` interface designed
    to run tasks in a separate thread and to monitor their progress using futures.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can initialize a `ThreadPoolExecutor` instance by importing it from the
    `concurrent.futures` module. The executor will spawn a collection of threads (called
    workers) that will wait to execute whichever task we throw at them. Once a function
    is submitted, the executor will take care of dispatching its execution to an available
    worker thread and keep track of the result. The `max_workers` argument can be
    used to select the number of threads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the executor will not destroy a thread once a task is completed. By
    doing so, it reduces the cost associated with the creation and destruction of
    threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code example in `example12.py`, we create a `ThreadPoolExecutor`
    instance with three workers, and we submit a `wait_and_return` function that will
    block the program execution for 1 second and return a message string. We then
    use the `submit` method to schedule its execution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `executor.submit` method immediately schedules the function and returns
    a future. It is possible to manage the execution of tasks in `asyncio` using the
    `loop.run_in_executor` method, which works quite similarly to `executor.submit`.
    The code is illustrated in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `run_in_executor` method will also return an `asyncio.Future` instance
    that can be awaited from other code, the main difference being that the future
    will not be run until we start the loop. We can run and obtain the response using
    `loop.run_until_complete`, as illustrated in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As a practical example, we can use this technique to implement concurrent fetching
    of several web pages. To do this, we will import the popular (blocking) `requests`
    library and run the `requests.get` function in the executor in `example13.py`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This version of `fetch_url` will not block the execution and allow other coroutines
    in `asyncio` to run; however, it is not optimal as the function will not fetch
    a `asyncio.ensure_future` or employ the `asyncio.gather` convenience function
    that will submit all the coroutines at once and gather the results as they come.
    The usage of `asyncio.gather` is demonstrated in `example14.py`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Upper Bound for the Number of Threads
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The number of URLs you can fetch in parallel with this method will be dependent
    on the number of worker threads you have. To avoid this limitation, you should
    use a natively non-blocking library, such as `aiohttp`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So far, we have seen how to work with concurrent programs in Python using core
    concepts such as callbacks, futures, and coroutines. For the remaining portion
    of this chapter, we will discuss a more streamlined programming paradigm for implementing
    concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Reactive programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reactive programming** is a paradigm that aims at building better concurrent
    systems. Reactive applications are designed to comply with the following requirements
    exemplified by the reactive manifesto:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Responsive**: The system responds immediately to the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elastic**: The system is capable of handling different levels of load and
    can adapt to accommodate increasing demands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resilient**: The system deals with failure gracefully. This is achieved by
    modularity and avoiding having a **single point of failure** (**SPOF**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Message-driven**: The system should not block and take advantage of events
    and messages. A message-driven application helps achieve all the previous requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The requirements for reactive systems are quite reasonable but abstract, which
    leads us to a natural question: how exactly does reactive programming work? In
    this section, we will learn about the principles of reactive programming using
    the **Reactive Extensions for Python** (**RxPY**) library.'
  prefs: []
  type: TYPE_NORMAL
- en: Additional Information
  prefs: []
  type: TYPE_NORMAL
- en: The `RxPY` library is part of ReactiveX ([http://reactivex.io/](http://reactivex.io/)),
    which is a project that implements reactive programming tools for a large variety
    of languages.
  prefs: []
  type: TYPE_NORMAL
- en: To install the library, simply run `pip install rx`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the following code uses `RxPY` v3, the syntax of which is quite different
    from `RxPY` v1\. If you are familiar with `RxPY` v1 and the discussion from this
    book's previous version, watch out for changes in syntax!
  prefs: []
  type: TYPE_NORMAL
- en: Observables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name implies, the main idea of reactive programming is to *react* to
    events. In the preceding section, we saw some examples of this idea with callbacks;
    you subscribe to them and the callback is executed as soon as the event takes
    place.
  prefs: []
  type: TYPE_NORMAL
- en: 'In reactive programming, this idea is expanded if we think of events as streams
    of data. This can be exemplified by showing examples of such streams in `RxPY`.
    A data stream can be created from an iterator using the `from_iterable` method
    in IPython, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to receive data from `obs`, we can use the `Observable.subscribe`
    method, which will execute the function we pass for each value that the data source
    emits. This method is shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: You may have noticed that observables are ordered collections of items just
    like lists or, more generally, iterators. This is not a coincidence.
  prefs: []
  type: TYPE_NORMAL
- en: The term *observable* comes from the combination of observer and iterable. An
    *observer* is an object that reacts to changes of the variable it observes, while
    an *iterable* is an object that can produce and keep track of an iterator.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, iterators are objects that define the `__next__` method, and whose
    elements can be extracted by calling `next`. An iterator can generally be obtained
    by a collection using `iter`; then, we can extract elements using `next` or a
    `for` loop. Once an element is consumed from the iterator, we can''t go back.
    We can demonstrate its usage by creating an iterator from a list, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: You can see how, every time we call `next` or we iterate, the iterator produces
    a value and advances. In a sense, we are *pulling* results from the iterator.
  prefs: []
  type: TYPE_NORMAL
- en: Iterators versus Generators
  prefs: []
  type: TYPE_NORMAL
- en: Iterators sound a lot like generators; however, they are more general. In Python,
    generators are returned by functions that use `yield` expressions. As we saw,
    generators support `next`; therefore, they are a special class of iterators.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you can appreciate the contrast between an iterator and an observable.
    An observable *pushes* a stream of data to us whenever it's ready, but that's
    not everything. An observable can also tell us when there is an error and where
    there is no more data. In fact, it is possible to register further callbacks to
    the `Observable.subscribe` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example in IPython, we create an observable and register callbacks
    to be called using `on_next` whenever the next item is available and using the
    `on_completed` argument when there is no more data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: With that said, the similarity between observables and iterators is more important,
    because we can use the same techniques that can be used with iterators to handle
    streams of events.
  prefs: []
  type: TYPE_NORMAL
- en: '`RxPy` provides operators that can be used to create, transform, filter, and
    group observables. The power of reactive programming lies in the fact that those
    operations return other observables that can be conveniently chained and composed
    together. For a quick demonstration, we will examine the usage of the `take` operator
    next.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given an observable, `take` will return a new observable that will stop after
    `n` items. Its usage is straightforward, as we can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The collection of operations implemented in `RxPy` is varied and rich and can
    be used to build complex applications using these operators as building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Useful operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we will explore operators that transform the elements of
    a source observable in some way. The most prominent member of this family of operators
    is the familiar `map` operator, which emits the elements of the source observable
    after applying a function to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we may use `map` to calculate the square of a sequence of numbers,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Operators can be represented with marble diagrams that help us better understand
    how the operator works, especially when taking into account the fact that elements
    can be emitted over a region of time. In a marble diagram, a data stream (in our
    case, an observable) is represented by a solid line. A circle (or another shape)
    identifies a value emitted by the observable, an *X* symbol represents an error,
    and a vertical line represents the end of the stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see a marble diagram of `map`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Marble diagram illustrating the procedure of squaring numbers
    ](img/Figure_7.4_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Marble diagram illustrating the procedure of squaring numbers
  prefs: []
  type: TYPE_NORMAL
- en: The source observable is placed at the top of the diagram, the transformation
    is placed in the middle, and the resulting observable is placed at the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: Another example of a transformation is `group_by`, which sorts items into groups
    based on a key. The `group_by` operator takes a function that extracts a key when
    given an element and produces an observable for each key with the elements associated
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `group_by` operation can be expressed more clearly using a marble diagram.
    In the following diagram, you can see how `group_by` emits two observables. Additionally,
    the items are dynamically sorted into groups *as soon as they are emitted*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Marble diagram illustrating grouping ](img/Figure_7.5_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Marble diagram illustrating grouping
  prefs: []
  type: TYPE_NORMAL
- en: 'We can further understand how `group_by` works with a simple example. Let''s
    say that we want to group a number according to the fact that it''s even or odd.
    We can implement this using `group_by` by passing the `lambda x: x % 2` expression
    as a key function, which will return `0` if the number is even and `1` if the
    number is odd, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, if we subscribe and print the content of `obs`, two observables
    are actually printed, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'You can determine the group key using the `key` attribute. To extract all the
    even numbers, we can take the first observable (corresponding to a key equal to
    0) and subscribe to it. In the following code snippet, we show how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: With `group_by`, we introduced an observable that emits other observables. This
    turns out to be quite a common pattern in reactive programming, and there are
    functions that allow you to combine different observables.
  prefs: []
  type: TYPE_NORMAL
- en: 'A useful tool for combining observables is `merge_all` which takes multiple
    observables and produces a single observable that contains the element of the
    two observables in the order they are emitted. This is better illustrated using
    a marble diagram, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Marble diagram illustrating merging ](img/Figure_7.6_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Marble diagram illustrating merging
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate its usage, we can apply the operation to the observable of observables
    returned by `group_by`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: With `merge_all`, the items are returned in the same order as they were initially
    (remember that `group_by` emits elements in the two groups as they come).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: '`RxPy` also provides the `merge` operation, which can be used to combine individual
    observables.'
  prefs: []
  type: TYPE_NORMAL
- en: Hot and cold observables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding section, we learned how to create an observable using the `from_iterable`
    method. `RxPy` provides many other tools to create more interesting event sources.
  prefs: []
  type: TYPE_NORMAL
- en: '`interval` takes a time interval in seconds, `period`, and creates an observable
    that emits a value every time the period has passed. The following code can be
    used to define an observable, `obs`, that will emit a number, starting from zero,
    every second. We use the `take` operator to limit the timer to four events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'A very important fact about `interval` is that the timer doesn''t start until
    we subscribe. We can observe this by printing both the index and the delay from
    when the timer starts definition using `time.time()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the first element (corresponding to a `0` index) is produced
    after 3 seconds, which means that the timer started when we issue the `subscribe(print)`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Observables such as `interval` are called *lazy* because they start producing
    values only when requested (think of them as vending machines that won''t dispense
    food unless we press the button). In Rx jargon, these kinds of observables are
    called **cold**. A property of cold observables is that, if we attach two subscribers,
    the interval timer will be started multiple times. This is quite evident from
    the following example. Here, we add a new subscription 0.5 seconds after the first,
    and you can see how the output of the two subscriptions comes at different times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Sometimes, we may not want this behavior as we may want multiple subscribers
    to subscribe to the same data source. To make the observable produce the same
    data, we can delay the data production and ensure that all the subscribers will
    get the same data using the `publish` method.
  prefs: []
  type: TYPE_NORMAL
- en: '`publish` will transform our observable into `ConnectableObservable`, which
    won''t start pushing data immediately, but only when we call the `connect` method.
    The usage of `publish` and `connect` is demonstrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: In this example, you can see how we first issue `publish`, then we subscribe
    the first subscriber, and finally, we issue `connect`. When `connect` is issued,
    the timer will start producing data. The second subscriber joins the party late
    and, in fact, won't receive the first two messages but will start receiving data
    from the third, and so on. Note that, this time around, the subscribers share
    the exact same data. This kind of data source, where data is produced independently
    of the subscribers, is called **hot**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to `publish`, you can use the `replay` method that will produce the
    data *from the beginning* for each new subscriber. This is illustrated in the
    following example, which is identical to the preceding one except that we replaced
    `publish` with `replay`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Notice that even though the second subscriber arrives late to the party, it
    is still given all the items that have been given out so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way of creating hot observables is through the `Subject` class. `Subject`
    is interesting because it''s capable of both receiving and pushing data, and thus
    it can be used to manually *push* items to an observable. Using `Subject` is very
    intuitive; in the following code snippet, we create a `Subject` instance and subscribe
    to it. Later, we push values to it using the `on_next` method; as soon as we do
    that, the subscriber is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Note that `Subject` is another example of a hot observable.
  prefs: []
  type: TYPE_NORMAL
- en: Building a CPU monitor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a grasp of the main reactive programming concepts, we can
    implement an example application: a monitor that will give us real-time information
    about our CPU usage and that can detect spikes.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The complete code for the CPU monitor can be found in the `cpu_monitor.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, let''s implement a data source. We will use the `psutil` module
    that provides a function, `psutil.cpu_percent`, that returns the latest available
    CPU usage as a percentage (and doesn''t block). The code is illustrated in the
    following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are developing a monitor, we would like to sample this information
    over a few time intervals. To accomplish, this we can use the familiar `interval`
    observable, followed by `map`, just as we did in the previous section. Also, we
    would like to make this observable *hot* as, for this application, all subscribers
    should receive a single source of data; to make `interval` hot, we can use the
    `publish` and `connect` methods. The full code for the creation of a `cpu_data`
    observable is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test our monitor by printing a sample of four items, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our main data source is in place, we can implement a monitor visualization
    using `matplotlib`. The idea is to create a plot that contains a fixed number
    of measurements and, as new data arrives, we include the newest measurement and
    remove the oldest one. This is commonly referred to as a *moving window* and is
    better understood with an illustration. In the following diagram, our `cpu_data`
    stream is represented as a list of numbers. The first plot is produced as soon
    as we have the first four numbers and, each time a new number arrives, we shift
    the window by one position and update the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Illustration of a moving window ](img/Figure_7.7_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Illustration of a moving window
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this algorithm, we can write a function called `monitor_cpu` that
    will create and update our plotting window. The function will do the following
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize an empty plot and set up the correct plot limits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform our `cpu_data` observable to return a moving window over the data.
    This can be accomplished using the `buffer_with_count` operator, which will take
    the number of points in our window, `npoints`, as parameters and the shift as
    `1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subscribe to this new data stream and update the plot with the incoming data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complete code for the function is implemented in `cpu_monitor.py` and shown
    here and, as you can see, is extremely compact. Go ahead and install `matplotlib`
    if you don't have it in your environment already, using `pip install matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take some time to run the function and play with the parameters. You can view
    the code here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Another feature we may want to develop is an alert that triggers when the CPU
    has been high for a certain amount of time, as this may indicate that some of
    the processes in our machine are working very hard. This can be accomplished by
    combining `buffer_with_count` and `map`. We can take the CPU stream and a window,
    and test whether all items have a value higher than 20% usage (in a quad-core
    CPU, that corresponds to about one processor working at 100%) in the `map` function.
    If all the points in the window have a higher-than-20% usage, we display a warning
    in our plot window.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of the new observable can be written as follows and will
    produce an observable that emits `True` if the CPU has high usage, and `False`
    otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the `high_cpu` observable is ready, we can create a `matplotlib` label
    and subscribe to it for updates, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the program from your terminal, and an interactive window will open to
    display your CPU usage data in real time. Here is a screenshot showing the output
    of our program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Monitoring CPU usage ](img/Figure_7.8_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Monitoring CPU usage
  prefs: []
  type: TYPE_NORMAL
- en: Here, the blue curve denotes the CPU usage, which stays below the normal threshold.
  prefs: []
  type: TYPE_NORMAL
- en: While this example is relatively simple, it possesses the core components of
    a reactive program, and more complex applications that fully utilize the power
    of `RxPY` may be built using this as a blueprint.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asynchronous programming is useful when our code deals with slow and unpredictable
    resources, such as I/O devices and networks. In this chapter, we explored the
    fundamental concepts of concurrency and asynchronous programming and how to write
    concurrent code with the `asyncio` and `RxPy` libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '`asyncio` coroutines are an excellent choice when dealing with multiple, interconnected
    resources as they greatly simplify the code logic by cleverly avoiding callbacks.
    Reactive programming is also very good in these situations, but it truly shines
    when dealing with streams of data that are common in real-time applications and
    UIs.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does asynchronous programming help programs run at higher speed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the main differences between callbacks and futures in asynchronous
    programming?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the core characteristics/requirements of a reactive application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Migrating to `RxPY` v3: [https://rxpy.readthedocs.io/en/latests/migration.html](https://rxpy.readthedocs.io/en/latests/migration.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
