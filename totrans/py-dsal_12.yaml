- en: Design Techniques and Strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will take a step back and look into the broader topics in
    computer algorithm design. As your experience with programming grows, certain
    patterns begin to become apparent to you. And just like with any other skilled
    trade, you cannot do without some techniques and principles to achieve the means.
    In the world of algorithms, there are a plethora of these techniques and design
    principles. A working knowledge and mastery of these techniques is required to
    tackle harder problems in the field.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at the ways in which algorithms are generally classified. Other
    design techniques will be treated alongside implementation of some of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this chapter is not to make you a pro at algorithm design and strategy
    but to unveil the large expanse of algorithms in a few pages.
  prefs: []
  type: TYPE_NORMAL
- en: Classification of algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There exist a number of classification schemes that are based on the goal that
    an algorithm has to achieve. In the previous chapters, we implemented a number
    of algorithms. One question that may arise is, do these algorithms share the same
    form? If yes, what are the similarities and characteristics being used as the
    basis? If no, can the algorithms be grouped into classes?
  prefs: []
  type: TYPE_NORMAL
- en: These are the questions we will examine as we tackle the major modes of classifying
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Classification by implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When translating a series of steps or processes into a working algorithm, there
    are a number of forms that it may take. The heart of the algorithm may employ some
    assets, described further in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Recursion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recursive algorithms are the ones that make calls to themselves until a certain
    condition is satisfied. Some problems are more easily expressed by implementing
    their solution through recursion. One classic example is the Towers of Hanoi.
    There are also different types of recursive algorithms, some of which include
    single and multiple recursion, indirect recursion, anonymous recursion, and generative
    recursion. An iterative algorithm, on the other hand, uses a series of steps or
    a repetitive construct to formulate a solution. This repetitive construct could
    be a simple `while` loop or any other kind of loop. Iterative solutions also come
    to mind more easily than their recursive implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Logical
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One implementation of an algorithm is expressing it as a controlled logical
    deduction. This logic component is comprised of the axioms that will be used in
    the computation. The control component determines the manner in which deduction
    is applied to the axioms. This is expressed in the form a*lgorithm = logic + control*.
    This forms the basis of the logic programming paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: The logic component determines the meaning of the algorithm. The control component
    only affects its efficiency. Without modifying the logic, the efficiency can be
    improved by improving the control component.
  prefs: []
  type: TYPE_NORMAL
- en: Serial or parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The RAM model of most computers allows for the assumption that computing is
    done one instruction at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Serial algorithms, also known as **sequential algorithms**, are algorithms that
    are executed sequentially. Execution commences from start to finish without any
    other execution procedure.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to process several instructions at once, a different model or computing
    technique is required. Parallel algorithms perform more than one operation at
    a time. In the PRAM model, there are serial processors that share a global memory.
    The processors can also perform various arithmetic and logical operations in parallel.
    This enables the execution of several instructions at one time.
  prefs: []
  type: TYPE_NORMAL
- en: The parallel/distributed algorithms divide a problem into subproblems among
    its processors to collect the results. Some sorting algorithms can be efficiently
    parallelized, while iterative algorithms are generally parallelizable.
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic versus nondeterministic algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deterministic algorithms will produce the same output without fail every time
    the algorithm is run with the same input. There are some sets of problems that
    are so complex in the design of their solutions that expressing their solution
    in a deterministic way can be a challenge. Nondeterministic algorithms can change
    the order of execution or some internal subprocess that leads to a change in the
    final result any time the algorithm is run. As such, with every run of a nondeterministic
    algorithm, the output of the algorithm is different. For instance, an algorithm
    that makes use of a probabilistic value will yield different outputs on successive
    execution depending on the value of the random number generated.
  prefs: []
  type: TYPE_NORMAL
- en: Classification by complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To determine the complexity of an algorithm is to try to estimate how much space
    (memory) and time is used overall during the computation or program execution.
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 3](a98d7333-0a20-49b1-8bf1-3e007ddb9793.xhtml), *Principles of Algorithm
    Design*, presents more comprehensive coverage of the subject matter on complexity.
    We will summarize what we learned there here.'
  prefs: []
  type: TYPE_NORMAL
- en: Complexity curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now consider a problem of magnitude *n*. To determine the time complexity of
    an algorithm, we denote it with **T**(n). The value may fall under **O**(*1*),
    **O**(*log n*), **O**(*n*), **O**(*n log(n)*), **O**(*n²*), **O**(*n³*), or **O**(*2^n*).
    Depending on the steps an algorithm performs, the time complexity may or may not
    be affected. The notation **O**(*n*) captures the growth rate of an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now examine a practical scenario. By which means do we arrive at the conclusion
    that the bubble sort algorithm is slower than the quick sort algorithm? Or in
    general, how do we measure the efficiency of one algorithm against the other?
    Well, we can compare the Big O of any number of algorithms to determine their
    efficiency. It is this approach that gives us a time measure or the growth rate,
    which charts the behavior of the algorithm as *n* gets bigger.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a graph of the different runtimes that an algorithm''s performance
    may fall under:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In ascending order, the list of runtimes from better to worse is given as **O**(1),
    **O**(log n), **O**(*n*), **O**(*n log n*), **O**(*n²*), **O**(*n³*), and **O**(*2^n*).
  prefs: []
  type: TYPE_NORMAL
- en: Classification by design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will present the categories of algorithms based on the design
    of the various algorithms used in solving problems.
  prefs: []
  type: TYPE_NORMAL
- en: A given problem may have a number of solutions. When the algorithms of these
    solutions are analyzed, it becomes evident that some implement a certain technique
    or pattern. It is these techniques that we will discuss here, and in a later section,
    in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: Divide and conquer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This approach to problem-solving is just as its name suggests. To solve (conquer)
    certain problems, this algorithm divides the problem into subproblems identical
    to the original problem that can easily be solved. Solutions to the subproblems
    are combined in such a way that the final solution is the solution of the origin
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: The way in which the problems are broken down into smaller chunks is mostly
    by recursion. We will examine this technique in detail in the upcoming sections.
    Some algorithms that use this technique include merge sort, quick sort, and binary
    search.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This technique is similar to divide and conquer, in that a problem is broken
    down into smaller problems. In divide and conquer, each subproblem has to be solved
    before its results can be used to solve bigger problems. By contrast, dynamic
    programming does not compute the solution to an already encountered subproblem.
    Rather, it uses a remembering technique to avoid the recomputation.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming problems have two characteristics: **optimal substructure**
    and **overlapping subproblem**. We will talk more on this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Greedy algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a certain category of problems, determining the best solution is really
    difficult. To make up for the lack of optimal solution, we resort to an approach
    where we select out of a bunch of options or choices the closest solution that
    is the most promising in obtaining a solution.
  prefs: []
  type: TYPE_NORMAL
- en: Greedy algorithms are much easier to conceive because the guiding rule is for
    one to always select the solution that yields the most benefit and continue doing
    that, hoping to reach a perfect solution.
  prefs: []
  type: TYPE_NORMAL
- en: This technique aims to find a global optimal final solution by making a series
    of local optimal choices. The local optimal choice seems to lead to the solution.
    In real life, most of those local optimal choices made are suboptimal. As such,
    most greedy algorithms have a poor asymptotic time complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Technical implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's dig into the implementation of some of the theoretical programming techniques
    that we discussed previously in this chapter. We will start with dynamic programming.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have already described, in this approach, we divide a problem into smaller
    subproblems. In finding the solutions to the subprograms, care is taken not to
    recompute any of the previously encountered subproblems.
  prefs: []
  type: TYPE_NORMAL
- en: This sounds a bit like recursion, but things are a little broader here. A problem
    may lend itself to being solved by using dynamic programming but will not necessarily
    take the form of making recursive calls.
  prefs: []
  type: TYPE_NORMAL
- en: A property of a problem that will make it an ideal candidate for being solved
    with dynamic programming is that it should have an overlapping set of subproblems.
  prefs: []
  type: TYPE_NORMAL
- en: Once we realize that the form of subproblems has repeated itself during computation,
    we need not compute it again. Instead, we return the result of a pre-computed
    value of that subproblem previously encountered.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid a situation where we never have to re-evaluate a subproblem, we need
    an efficient way in which we can store the results of each subproblem. The following
    two techniques are readily available.
  prefs: []
  type: TYPE_NORMAL
- en: Memoization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This technique starts from the initial problem set and divides it into small
    subproblems. After the solution to a subprogram has been determined, we store
    the result to that particular subproblem. In the future, when this subproblem
    is encountered, we only return its pre-computed result.
  prefs: []
  type: TYPE_NORMAL
- en: Tabulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In tabulation, we settle on an approach where we fill a table of solutions to
    subproblems and then combine them to solve bigger problems.
  prefs: []
  type: TYPE_NORMAL
- en: The Fibonacci series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the Fibonacci series to illustrate both memoization and tabulation
    techniques of generating the series.
  prefs: []
  type: TYPE_NORMAL
- en: The Memoization technique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s generate the Fibonacci series to the fifth term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A recursive style of a program to generate the sequence is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The code is very simple but a little tricky to read because of the recursive
    calls being made that end up solving the problem.
  prefs: []
  type: TYPE_NORMAL
- en: When the base case is met, the `fib()` function returns 1\. If `n` is equal
    to or less than 2, the base case is met.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the base case is not met, we will call the `fib()` function again and this
    time supply the first call with `n-1` and the second with `n-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The layout of the strategy to solve the i^(th) term in the Fibonacci sequence
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A careful observation of the preceding tree shows some interesting patterns.
    The call to `f(1)` happens twice. The call to `f(1)` happens thrice. Also, the
    call to `f(3)` happens twice.
  prefs: []
  type: TYPE_NORMAL
- en: The return values of the function calls to all the times that `fib(2)` was called
    never changes. The same goes for `fib(1)` and `fib(3)`. The computational time
    is wasted since the same result is returned for the function calls with the same
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: These repeated calls to a function with the same parameters and output suggest
    that there is an overlap. Certain computations are reoccurring down in the smaller
    subproblems.
  prefs: []
  type: TYPE_NORMAL
- en: A better approach would be to store the results of the computation of `fib(1)`
    the first time it is encountered. This also applies to `fib(2)` and `fib(3)`.
    Later, anytime we encounter a call to `fib(1)`, `fib(2)`, or `fib(3)`, we simply
    return their respective results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram of our `fib` calls will now look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have now completely eliminated the need to compute `fib(3)`, `fib(2)`, and
    `fib(1)`. This typifies the memoization technique wherein breaking a problem into
    its subproblems, there is no recomputation of overlapping calls to functions.
    The overlapping function calls in our Fibonacci example are `fib(1)`, `fib(2)`,
    and `fib(3)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a list of 1,000 elements, we do the following and pass it to the
    lookup parameter of the `dyna_fib` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This list will store the value of the computation of the various calls to the `dyna_fib()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Any call to the `dyna_fib()` with `n` being less than or equal to 2 will return
    1\. When `dyna_fib(1)` is evaluated, we store the value at index 1 of `map_set`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Write the condition for `lookup[n]`, as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We pass lookup so that it can be referenced when evaluating the subproblems.
    The calls to `dyna_fib(n-1, lookup)` and `dyna_fib(n-2, lookup)` are stored in
    `lookup[n]`. When we run our updated implementation of the function to find the
    i^(th) term of the Fibonacci series, we realize that there is considerable improvement.
    This implementation runs much faster than our initial implementation. Supply the
    value 20 to both implementations and witness the difference in the execution speed.
    The algorithm sacrificed space complexity for time because of the use of memory
    in storing the result to the function calls.
  prefs: []
  type: TYPE_NORMAL
- en: The tabulation technique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a second technique in dynamic programming, which involves the use of
    a table of results or matrix in some cases to store results of computations for
    later use.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach solves the bigger problem by first working out a route to the
    final solution. In the case of the `fib()` function, we will develop a table with
    the values of `fib(1)` and `fib(2)` predetermined. Based on these two values,
    we will work our way up to `fib(n)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `results` variable is at index 0, and 1 the values, 1 and 1\. This represents
    the return values of `fib(1)` and `fib(2)`. To calculate the values of the `fib()`
    function for higher than 2, we simply call the `for` loop appends the sum of the
    `results[i-1] + results[i-2]` to the list of results.
  prefs: []
  type: TYPE_NORMAL
- en: Divide and conquer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This programming approach to problem-solving emphasizes the need to break down
    a problem into smaller problems of the same type or form of the original problem.
    These subproblems are solved and combined to solve the original problem. The following
    three steps are associated with this kind of programming.
  prefs: []
  type: TYPE_NORMAL
- en: Divide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To divide means to break down an entity or problem. Here, we devise the means
    to break down the original problem into subproblems. We can achieve this through
    iterative or recursive calls.
  prefs: []
  type: TYPE_NORMAL
- en: Conquer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is impossible to continue to break the problems into subproblems indefinitely.
    At some point, the smallest indivisible problem will return a solution. Once this
    happens, we can reverse our thought process and say that if we know the solution
    to the smallest problem possible, we can obtain the final solution to the original
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Merge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To obtain the final solution, we need to combine the smaller solutions to the
    smaller problems in order to solve the bigger problem.
  prefs: []
  type: TYPE_NORMAL
- en: There are other variants to the divide and conquer algorithm, such as merge
    and combine, and conquer and solve.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms that make use of the divide and conquer principle include merge sorting,
    quick sort, and Strassen's matrix multiplication. We will go through an implementation
    of the merge sort as we started earlier in [Chapter 3](a98d7333-0a20-49b1-8bf1-3e007ddb9793.xhtml),
    *Principles of Algorithm Design*.
  prefs: []
  type: TYPE_NORMAL
- en: Merge sort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The merge sort algorithm is based on the divide and conquer rule. Given a list
    of unsorted elements, we split the list into approximately two halves. We continue
    to divide the two halves recursively. After a while, the sublists created as a
    result of the recursive call will contain only one element. At that point, we
    begin to merge the solutions in the conquer or merge step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Our implementation starts by accepting the list of unsorted elements into the
    `merge_sort` function. The `if` statement is used to establish the base case,
    where if there is only one element in the `unsorted_list`, we simply return that
    list again. If there is more than one element in the list, we find the approximate
    middle using `mid_point = int((len(unsorted_list))/2)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this `mid_point`, we divide the list into two sublists, namely `first_half`
    and `second_half`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'A recursive call is made by passing the two sublists to the `merge_sort` function
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Enter the merge step. When `half_a` and `half_b` have been passed their values,
    we call the merge function that will merge or combine the two solutions stored
    in `half_a` and `half_b`, which are lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The merge function takes the two lists we want to merge together, `first_sublist`
    and `second_sublist`. The `i` and `j` variables are initialized to `0` and are
    used as pointers to tell us where in the two lists we are with respect to the
    merging process. The final `merged_list` will contain the merged list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `while` loop starts comparing the elements in `first_sublist` and `second_sublist`.
    The `if` statement selects the smaller of the two, `first_sublist[i]` or `second_sublist[j]`,
    and appends it to `merged_list`. The `i` or `j` index is incremented to reflect
    the point we are at with the merging step. The `while` loop only when either sublist is
    empty.
  prefs: []
  type: TYPE_NORMAL
- en: There may be elements left behind in either `first_sublist` or `second_sublist`.
    The last two `while` loops make sure that those elements are added to the `merged_list`
    before it is returned.
  prefs: []
  type: TYPE_NORMAL
- en: The last call to `merge(half_a, half_b)` will return the sorted list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s give the algorithm a dry run by playing the last step of merging the
    two sublists `[4, 6, 8]` and `[5, 7, 11, 40]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Step** | `first_sublist` | `second_sublist` | `merged_list` |'
  prefs: []
  type: TYPE_TB
- en: '| Step 0 | [**4** 6 8] | [**5** 7 11 40] | [] |'
  prefs: []
  type: TYPE_TB
- en: '| Step 1 | [ **6** 8] | [**5** 7 11 40] | [4] |'
  prefs: []
  type: TYPE_TB
- en: '| Step 2 | [ **6** 8] | [ **7** 11 40] | [4 5] |'
  prefs: []
  type: TYPE_TB
- en: '| Step 3 | [ **8**] | [ **7** 11 40] | [4 5 6] |'
  prefs: []
  type: TYPE_TB
- en: '| Step 4 | [ **8**] | [ **11** 40] | [4 5 6 7] |'
  prefs: []
  type: TYPE_TB
- en: '| Step 5 | [ ] | [ **11** 40] | [4 5 6 7 8] |'
  prefs: []
  type: TYPE_TB
- en: Note that the text in bold represents the current item referenced in the loops
    `first_sublist` (which uses the index `i`) and `second_sublist` (which uses the
    index `j`).
  prefs: []
  type: TYPE_NORMAL
- en: At this point in the execution, the third `while` loop in the merge function
    kicks in to move 11 and 40 into the `merged_list`. The returned `merged_list`
    will contain the fully sorted list.
  prefs: []
  type: TYPE_NORMAL
- en: Greedy algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we said earlier, greedy algorithms make decisions that yield the largest
    benefit in the interim. It is the hope of this technique that by making these
    high yielding benefit choices, the total path will lead to an overall good solution
    or end.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of greedy algorithms include **Prim's algorithm** for finding the minimum
    spanning trees, the **Knapsack problem**, and the **Travelling Salesman problem**,
    just to mention a few.
  prefs: []
  type: TYPE_NORMAL
- en: Coin-counting problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's examine a very simple use of this greedy technique. In some arbitrary
    country, we have the denominations 1 GHC, 5 GHC, and 8 GHC. Given an amount such
    as 12 GHC, we may want to find the least possible number of denominations needed
    to provide change. Using the greedy approach, we pick the largest value from our
    denomination to divide 12 GHC. We use 8 because it yields the best possible means
    by which we can reduce the amount 12 GHC into lower denominations.
  prefs: []
  type: TYPE_NORMAL
- en: The remainder, 4 GHC, cannot be divided by 5, so we try the 1 GHC denomination
    and realize that we can multiply it by 4 to obtain 4 GHC. At the end of the day,
    the least possible number of denominations to create 12 GHC is to get a one 8
    GHC and four 1 GHC notes.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, our greedy algorithm seems to be doing pretty well. A function that
    returns the respective denominations is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This greedy algorithm always starts by using the largest denomination possible.
    `denom` is a list of denominations. `sorted(denom, reverse=True)` will sort the
    list in reverse so that we can obtain the largest denomination at index 0\. Now,
    starting from index 0 of the sorted list of denominations, `sorted_denominations`,
    we iterate and apply the greedy technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The loop will run through the list of denominations. Each time the loop runs,
    it obtains the quotient, `div`, by dividing the `total_amount` by the current
    denomination, `i`. `total_amount` is updated to store the remainder for further
    processing. If the quotient is greater than 0, we store it in `number_of_denoms`.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there are instances where our algorithm fails. For instance,
    when passed 14 GHS, our algorithm returns one 8 GHC and four 1 GHS. This output
    is, however, not the optimal solution. The right solution will be to use two 5
    GHC and two 1 GHC denominations.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better greedy algorithm is presented here. This time, the function returns
    a list of tuples that allow us to investigate the better results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The outer `for` loop enables us to limit the denominations from which we find
    our solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Assuming that we have the list [5, 4, 3] in `sorted_denominations`, slicing
    it with `[j:]` helps us obtain the sublists [5, 4, 3], [4, 3], and [3], from which
    we try to get the right combination to create the change.
  prefs: []
  type: TYPE_NORMAL
- en: Dijkstra's shortest path algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduce and study Dijkstra's algorithm. This algorithm is an example of
    a greedy algorithm. It finds the shortest distance from a source to all other
    nodes or vertices in a graph. By the end of this section, you will come to understand
    why it is classified as a greedy algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/CH_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: By inspection, the first answer to the question of finding the shortest path
    between node **A** and node **D** that comes to mind is the edge with value or
    distance 9\. From the diagram, it would seem that the straight path from node
    **A** to **D** would also yield the shortest route between the two nodes. But
    the assumption that the edge connecting the two nodes is the shortest route does
    not always hold true.
  prefs: []
  type: TYPE_NORMAL
- en: This shortsighted approach of selecting the first option when solving a problem
    is what gives the algorithm its name and class. Having found the supposed shortest
    route or distance, the algorithm continues to refine and improve its results.
  prefs: []
  type: TYPE_NORMAL
- en: Other paths from node **A** to node **D** prove to be shorter than our initial
    pick. For instance, travelling from node **A** to node **B** to node **C** will
    incur a total distance of 10\. But the route through node **A** to **E**, **F**,
    and **D** is even shorter.
  prefs: []
  type: TYPE_NORMAL
- en: We will implement the shortest path algorithm with a single source. Our result
    should help us determine the shortest path from the origin, which in this case
    is **A**, to any other node in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: The shortest path from node **A** to node **C** is 7 through node **B**. Likewise,
    the shortest path to **F** is through node **E** with a total distance of 5.
  prefs: []
  type: TYPE_NORMAL
- en: In order to come up with an algorithm to help us find the shortest path in a
    graph, let's solve the problem by hand. Thereafter, we will present the working
    solution in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the chapter on graphs, we saw how we could represent a graph with an adjacency
    list. We will use it with a slight modification to enable us capture the distance
    on every edge. A table will be used to also keep track of the shortest distance
    from the source in the graph to any other node. A Python dictionary will be used
    to implement this table. Here is one such table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Shortest distance from source** | **Previous node** |'
  prefs: []
  type: TYPE_TB
- en: '| A | 0 | None |'
  prefs: []
  type: TYPE_TB
- en: '| B | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: '| C | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: '| D | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: '| E | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: '| F | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: 'The adjacency list for the diagram and table is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The nested dictionary holds the distance and adjacent nodes.
  prefs: []
  type: TYPE_NORMAL
- en: This table forms the basis for our effort as we try to solve the problem at
    hand. When the algorithm starts, we have no idea what the shortest distance from
    the source (**A**) to any of the nodes is. To play it safe, we set the values
    in that column to infinity with the exception of node **A**. From the starting
    node, the distance covered from node **A** to node **A** is 0\. So we can safely
    use this value as the shortest distance from node **A** to itself. No prior nodes
    have been visited when the algorithm begins. We therefore mark the previous node
    column of node as `None`.
  prefs: []
  type: TYPE_NORMAL
- en: In step 1 of the algorithm, we start by examining the adjacent nodes of node
    **A**. To find the shortest distance from node **A** to node **B**, we need to
    find the distance from the start node to the previous node of node B, which happens
    to be node **A**, and add it to the distance from node **A** to node **B**. We
    do this for other adjacent nodes of **A**, which are **B**, **E**, and **D**.
  prefs: []
  type: TYPE_NORMAL
- en: Using the adjacent node **B** as an example, the distance from the start node
    to the previous node is 0\. The distance from the previous node to the current
    node (**B**) is 5\. This sum is compared with the data in the shortest distance
    column of node B. Since 5 is less than infinity(**∞**), we replace **∞** with
    the smallest of the two, which is 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any time the shortest distance of a node is replaced by a lesser value, we
    need to update the previous node column too. At the end of the first step, our
    table looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Shortest distance from source** | **Previous node** |'
  prefs: []
  type: TYPE_TB
- en: '| **A*** | 0 | None |'
  prefs: []
  type: TYPE_TB
- en: '| B | 5 | A |'
  prefs: []
  type: TYPE_TB
- en: '| C | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: '| D | 9 | A |'
  prefs: []
  type: TYPE_TB
- en: '| E | 2 | A |'
  prefs: []
  type: TYPE_TB
- en: '| F | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: At this point, node **A** is considered visited. As such, we add node **A**
    to the list of visited nodes. In the table, we show that node **A** has been visited
    by making the text bold and appending an asterisk sign to it.
  prefs: []
  type: TYPE_NORMAL
- en: In the second step, we find the node with the shortest distance using our table
    as a guide. Node **E** with its value 2 has the shortest distance. This is what
    we can infer from the table about node **E**. To get to node **E**, we must visit
    node **A** and cover a distance of 2\. From node A, we cover a distance of 0 to
    get to the starting node, which is node **A** itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The adjacent nodes of node **E** are **A** and **F**. But node **A** has already
    been visited, so we only consider node **F**. To find the shortest route or distance
    to node **F**, we must find the distance from the starting node to node **E**
    and add it to the distance between node **E** and **F**. We can find the distance
    from the starting node to node **E** by looking at the shortest distance column
    of node **E**, which has the value 2\. The distance from node **E** to **F** can
    be obtained from the adjacency list we developed in Python earlier in this section.
    This distance is 3\. These two sum up to 5, which is less than infinity. Remember
    we are on examining the adjacent node **F**. Since there are more adjacent nodes
    of node **E**, we mark node **E** as visited. Our updated table will have the
    following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Shortest distance from source** | **Previous node** |'
  prefs: []
  type: TYPE_TB
- en: '| **A*** | 0 | None |'
  prefs: []
  type: TYPE_TB
- en: '| B | 5 | A |'
  prefs: []
  type: TYPE_TB
- en: '| C | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: '| D | 9 | A |'
  prefs: []
  type: TYPE_TB
- en: '| **E*** | 2 | A |'
  prefs: []
  type: TYPE_TB
- en: '| F | 5 | E |'
  prefs: []
  type: TYPE_TB
- en: 'At this point, we initiate another step. The smallest value in the shortest
    distance column is 5\. We choose **B** instead of **F** purely on an alphabetical
    basis. The adjacent nodes of **B** are **A** and **C**, but node **A** has already
    been visited. Using the rule we established earlier, the shortest distance from
    **A** to **C** is 7\. We arrive at this number because the distance from the starting
    node to node **B** is 5, while the distance from node **B** to **C** is 2\. Since
    the sum, 7, is less than infinity, we update the shortest distance to 7 and update
    the previous node column with node **B**. Now **B** is also marked as visited.
    The new state of the table is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Shortest distance from source** | **Previous node** |'
  prefs: []
  type: TYPE_TB
- en: '| **A*** | 0 | None |'
  prefs: []
  type: TYPE_TB
- en: '| **B*** | 5 | A |'
  prefs: []
  type: TYPE_TB
- en: '| C | 7 | B |'
  prefs: []
  type: TYPE_TB
- en: '| D | 9 | A |'
  prefs: []
  type: TYPE_TB
- en: '| **E*** | 2 | A |'
  prefs: []
  type: TYPE_TB
- en: '| F | 5 | E |'
  prefs: []
  type: TYPE_TB
- en: 'The node with the shortest distance yet unvisited is node **F**. The adjacent
    nodes of **F** are nodes **D** and **E**. But node **E** has already been visited.
    As such, we focus on finding the shortest distance from the starting node to node
    **D**. We calculate this distance by adding the distance from node **A** to **F**
    to the distance from node **F** to **D**. This sums up to 7, which is less than
    9\. Thus, we update the 9 with 7 and replace **A** with **F** in node **D**''s
    previous node column. Node **F** is now marked as visited. Here is the updated
    table up to this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Shortest distance from source** | **Previous node** |'
  prefs: []
  type: TYPE_TB
- en: '| **A*** | 0 | None |'
  prefs: []
  type: TYPE_TB
- en: '| **B*** | 5 | A |'
  prefs: []
  type: TYPE_TB
- en: '| C | 7 | B |'
  prefs: []
  type: TYPE_TB
- en: '| D | 7 | F |'
  prefs: []
  type: TYPE_TB
- en: '| **E*** | 2 | A |'
  prefs: []
  type: TYPE_TB
- en: '| **F*** | 5 | E |'
  prefs: []
  type: TYPE_TB
- en: Now, the two unvisited nodes are **C** and **D**. In alphabetical order, we
    choose to examine **C** because both nodes have the same shortest distance from
    the starting node **A**.
  prefs: []
  type: TYPE_NORMAL
- en: However, all the adjacent nodes of **C** have been visited. Thus, we have nothing
    to do but mark node C as visited. The table remains unchanged at this point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we take node **D** and find out that all its adjacent nodes have been
    visited too. We only mark it as visited. The table remains unchanged:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Shortest distance from source** | **Previous node** |'
  prefs: []
  type: TYPE_TB
- en: '| **A*** | 0 | None |'
  prefs: []
  type: TYPE_TB
- en: '| **B*** | 5 | A |'
  prefs: []
  type: TYPE_TB
- en: '| **C*** | 7 | B |'
  prefs: []
  type: TYPE_TB
- en: '| **D*** | 7 | F |'
  prefs: []
  type: TYPE_TB
- en: '| **E*** | 2 | A |'
  prefs: []
  type: TYPE_TB
- en: '| **F*** | 5 | E |'
  prefs: []
  type: TYPE_TB
- en: Let's verify this table with our graph. From the graph, we know that the shortest
    distance from **A** to **F** is 5\. We will need to go through **E** to get to
    node **F**. According to the table, the shortest distance from the source column
    for node **F** is the value 5\. This is true. It is also tells us that to get
    to node **F**, we need to visit node **E**, and from **E**, node **A**, which
    is our starting node. This is actually the shortest path.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin the program for finding the shortest distance by representing the
    table that enables us to track the changes in our graph. For the given diagram
    we used, here is a dictionary representation of the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The initial state of the table uses `float("inf")` to represent infinity. Each
    key in the dictionary maps to a list. At the first index of the list is stored
    the shortest distance from the source `A`. At the second index is the stored the
    previous node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: To avoid the use of magic numbers, we use the preceding constants. The shortest
    path column's index is referenced by `DISTANCE`. The previous node column's index
    is referenced by `PREVIOUS_NODE`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now all is set for the main function. It will take the graph, represented by
    the adjacency list, the table, and the starting node as parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We keep the list of visited nodes in the list, `visited_nodes`. The `current_node`
    and `starting_node` variables will both point to the node in the graph we choose
    to make our starting node. The `origin` value is the reference point for all other
    nodes with respect to finding the shortest path.
  prefs: []
  type: TYPE_NORMAL
- en: 'The heavy lifting of the whole process is accomplished by the use of a `while`
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let's break down what the `while` loop is doing. In the body of the `while`
    loop, we obtain the current node in the graph we want to investigate with the
    line `adjacent_nodes = graph[current_node]`. `current_node` should have been set
    prior. The `if` statement is used to find out whether all the adjacent nodes of
    `current_node` have been visited. When the `while` loop is executed the fir*s*t
    time, `current_node` will contain A and `adjacent_nodes` will contain nodes B,
    D, and E. `visited_nodes` will be empty too. If all nodes have been visited, we
    only move on to the statements further down the program. Else, we begin a whole
    other step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The statement `set(adjacent_nodes).difference(set(visited_nodes))` returns
    the nodes that have not been visited. The loop iterates over this list of unvisited
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The helper method `get_shortest_distance(table, vertex)` will return the value
    stored in the shortest distance column of our table using one of the unvisited
    nodes referenced by `vertex`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'When we are examining the adjacent nodes of the starting node, `distance_from_starting_node
    == INFINITY and current_node == starting_node` will evaluate to True, in which
    case we only have to get the distance between the starting node and vertex by
    referencing the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `get_distance` method is another helper method we use to obtain the value
    (distance) of the edge between `vertex` and `current_node`.
  prefs: []
  type: TYPE_NORMAL
- en: If the condition fails, then we assign `total_distance` the sum of the distance
    from the starting node to `current_node` and the distance between `current_node`
    and `vertex`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our total distance, we need to check whether this `total_distance`
    is less than the existing data in the shortest distance column in our table. If
    it is less, then we use the two helper methods to update that row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we add the `current_node` to the list of visited nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: If all nodes have been visited, then we must exit the `while` loop. To check
    whether all the nodes have been visited, we compare the length of the `visited_nodes`
    list to the number of keys in our table. If they have become equal, we simply
    exit the `while` loop.
  prefs: []
  type: TYPE_NORMAL
- en: The helper method, `get_next_node`, is used to fetch the next node to visit.
    It is this method that helps us find the minimum value in the shortest distance
    column from the starting nodes using our table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole method ends by returning the updated table. To print the table, we
    use the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Output for the preceding statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'For the sake of completeness, let''s find out what the helper methods are doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get_shortest_distance` function returns the value stored in the zero^(th) index
    of our table. At that index, we always store the shortest distance from the starting
    node up to `vertex`. The `set_shortest_distance` function only sets this value
    by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'When we update the shortest distance of a node, we update its previous node
    using the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Remember that the constant, `PREVIOUS_NODE`, equals 1\. In the table, we store
    the value of the `previous_node` at `table[vertex][PREVIOUS_NODE]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the distance between any two nodes, we use the `get_distance` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The last helper method is the `get_next_node` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The `get_next_node` function resembles a function to find the smallest item
    in a list.
  prefs: []
  type: TYPE_NORMAL
- en: The function starts off by finding the unvisited nodes in our table by using
    `visited_nodes` to obtain the difference between the two sets of lists. The very
    first item in the list of `unvisited_nodes` is assumed to be the smallest in the
    shortest distance column of `table`. If a lesser value is found while the `for`
    loop runs, the `min_vertex` will be updated. The function then returns `min_vertex`
    as the unvisited vertex or node with the smallest shortest distance from the source.
  prefs: []
  type: TYPE_NORMAL
- en: The worst-case running time of Dijkstra's algorithm is **O**(*|E| + |V| log
    |V|*), where *|V|* is the number of vertices and *|E|* is the number of edges.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problems that computer algorithms try to solve fall within a range of difficulty
    by which their solutions are arrived at. In this section, we will discuss the
    complexity classes N, NP, NP-complete, and NP-hard problems.
  prefs: []
  type: TYPE_NORMAL
- en: P versus NP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The advent of computers has sped up the rate at which certain tasks are performed.
    In general, computers are good at perfecting the art of calculation and all problems
    that can be reduced to a set of mathematical computations. However, this assertion
    is not entirely true. There are some nature or classes of problems that just take
    an enormous amount of time for the computer to make a sound guess, let alone find
    the right solution.
  prefs: []
  type: TYPE_NORMAL
- en: In computer science, the class of problems that computers can solve within polynomial
    time using a step-wise process of logical steps is called P-type problems, where
    P stands for polynomial. These are relatively easy to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Then there is another class of problems that is considered very hard to solve.
    The word "hard problem" is used to refer to the way in which problems increase
    in difficulty when trying to find a solution. However, the good thing is that
    despite the fact that these problems have a high growth rate of difficulty, it
    is possible to determine whether a proposed solution solves the problem in polynomial
    time. These are the NP-Type problems. NP here stands for nondeterministic polynomial
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Now the million dollar question is, does N = NP?
  prefs: []
  type: TYPE_NORMAL
- en: The proof for *N = NP* is one of the Millennium Prize Problems from the Clay
    Mathematics Institute that attract a $1,000,000 prize for a correct solution.
    These problems number 7 in number.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Travelling Salesman problem is an example of an NP-Type problem. The problem
    statement says: given that there are *n* number of cities in some country, find
    the shortest route between all the cities, thus making the trip a cost-effective
    one. When the number of cities is small, this problem can be solved in a reasonable
    amount of time. However, when the number of cities is above any two-digit number,
    the time taken by the computer is remarkably long.'
  prefs: []
  type: TYPE_NORMAL
- en: A lot of computer systems and cybersecurity is based on the RSA encryption algorithm.
    The strength of the algorithm and its security is due to the fact that it is based
    on the integer factoring problem, which is an NP-Type problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding the prime factors of a prime number composed of many digits is very
    difficult. When two large prime numbers are multiplied, a large non-prime number
    is obtained with only two large prime factors. Factorization of this number is
    where many cryptographic algorithms borrow their strength:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/p_vs_np.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All P-type problems are subsets of NP problems. This means that any problem
    that can be solved in polynomial time can also be verified in polynomial time.
  prefs: []
  type: TYPE_NORMAL
- en: But the question, is P = NP? investigates whether problems that can be verified
    in polynomial time can also be solved in polynomial time. In particular, if they
    are equal, it would mean that problems that are solved by trying a number of possible
    solutions can be solved without the need to actually try all the possible solutions,
    invariably creating some sort of shortcut proof.
  prefs: []
  type: TYPE_NORMAL
- en: The proof, when finally discovered, will definitely have serious consequences
    in the fields of cryptography, game theory, mathematics, and many other fields.
  prefs: []
  type: TYPE_NORMAL
- en: NP-Hard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A problem is NP-Hard if all other problems in NP can be polynomial time reducible
    or mapped to it.
  prefs: []
  type: TYPE_NORMAL
- en: NP-Complete
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A problem is considered an NP-complete problem if it is first of all an NP hard
    and is also found in the `NP` class.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last chapter, we looked at the theories that support the computer science
    field. Without the use of too much mathematical rigor, we saw some of the main
    categories into which algorithms are classified. Other design techniques in the
    field, such as the divide and conquer, dynamic programming, and greedy algorithms,
    were also discussed, along with sample implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, one of the outstanding problems yet to be solved in the field of mathematics
    was tackled. We saw how the proof for P = NP? will definitely be a game-changer
    in a number of fields, if such a proof is ever discovered.
  prefs: []
  type: TYPE_NORMAL
