- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency and Asynchronous Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we covered architectural design patterns: patterns
    that help with solving some unique challenges that come with complex projects.
    Next, we need to discuss concurrency and asynchronous patterns, another important
    category in our solutions catalog.'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency allows your program to manage multiple operations simultaneously,
    leveraging the full power of modern processors. It’s akin to a chef preparing
    multiple dishes in parallel, each step orchestrated so that all dishes are ready
    at the same time. Asynchronous programming, on the other hand, lets your application
    move on to other tasks while waiting for operations to complete, such as sending
    a food order to the kitchen and serving other customers until the order is ready.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The Thread Pool pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Worker Model pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Future and Promise pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Observer pattern in reactive programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other concurrency and asynchronous patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'See the requirements presented in [*Chapter 1*](B21896_01.xhtml#_idTextAnchor017).
    The additional technical requirements for the code discussed in this chapter are
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Faker, using `pip` `install faker`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReactiveX, using `pip` `install reactivex`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Thread Pool pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, it’s important to understand what a thread is. In computing, a thread
    is the smallest unit of processing that can be scheduled by an operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Threads are like tracks of execution that can run on a computer at the same
    time, which enables many activities to be done simultaneously and thus improve
    performance. They are particularly important in applications that need multitasking,
    such as serving multiple web requests or carrying out multiple computations.
  prefs: []
  type: TYPE_NORMAL
- en: Now, onto the Thread Pool pattern itself. Imagine you have many tasks to complete
    but starting each task (which means in this case, creating a thread) can be expensive
    in terms of resources and time. It’s like hiring a new employee every time you
    have a job to do and then letting them go when the job is done. This process can
    be inefficient and costly. By maintaining a collection, or a pool, of worker threads
    that can be created for once and then reused upon several jobs, the Thread Pool
    pattern helps reduce this inefficiency. When one thread finishes a task, it does
    not terminate but goes back to the pool, awaiting another task that it can be
    used again for.
  prefs: []
  type: TYPE_NORMAL
- en: What are worker threads?
  prefs: []
  type: TYPE_NORMAL
- en: A worker thread is a thread of execution of a particular task or set of tasks.
    Worker threads are used to offload processing tasks from the main thread, helping
    to keep applications responsive by performing time-consuming or resource-intensive
    tasks asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to faster application performance, there are two benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reduced overhead**: By reusing threads, the application avoids the overhead
    of creating and destroying threads for each task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better resource management**: The thread pool limits the number of threads,
    preventing resource exhaustion that could occur if too many threads were created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In real life, imagine a small restaurant with a limited number of chefs (threads)
    who cook meals (tasks) for customers. The restaurant can only accommodate a certain
    number of chefs working at once due to kitchen space (system resources). When
    a new order comes in, if all chefs are busy, the order waits in a queue until
    there is an available chef. This way, the restaurant efficiently manages the flow
    of orders with its available chefs, ensuring all are utilized effectively without
    overwhelming the kitchen or needing to hire more staff for each new order.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also many examples in software:'
  prefs: []
  type: TYPE_NORMAL
- en: Web servers often use thread pools to handle incoming client requests. This
    allows them to serve multiple clients simultaneously without the overhead of creating
    a new thread for each request.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databases use thread pools to manage connections, ensuring that a pool of connections
    is always available for incoming queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task schedulers use thread pools to execute scheduled tasks such as *cron* jobs,
    backups, or updates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use cases for the Thread Pool pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three use cases where the Thread Pool pattern helps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch processing**: When you have many tasks that can be performed in parallel,
    a thread pool can distribute them among its worker threads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load balancing**: Thread pools can be used to distribute workload evenly
    among worker threads, ensuring that no single thread takes on too much work'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource optimization**: By reusing threads, the thread pool minimizes system
    resource usage, such as memory and CPU time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Thread Pool pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s stop to break down how a thread pool, for a given application,
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: When the application starts, the thread pool creates a certain number of worker
    threads. This is the initialization. This number of threads can be fixed or dynamically
    adjusted based on the application’s needs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we have the task submission step. When there’s a task to be done, it’s
    submitted to the pool rather than directly creating a new thread. The task can
    be anything that needs to be executed, such as processing user input, handling
    network requests, or performing calculations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following step is the task execution. The pool assigns the task to one of
    the available worker threads. If all threads are busy, the task might wait in
    a queue until a thread becomes available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once a thread completes its task, it doesn’t die. Instead, it returns to the
    pool, ready to be assigned a new task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For our example, let’s see some code where we create a thread pool with five
    worker threads to handle a set of tasks. We are going to use the `ThreadPoolExecutor`
    class from the `concurrent.futures` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing what we need for the example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create a function to simulate the tasks, by simply using `time.sleep(1)`
    in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use an instance of the `ThreadPoolExecutor` class, created with a
    maximum number of worker threads of 5, and we submit 10 tasks to the thread pool.
    So, the worker threads pick up these tasks and execute them. Once a worker thread
    completes a task, it picks up another from the queue. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When running the example code, using the `ch07/thread_pool.py` Python command,
    you should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We see that the tasks were completed in an order different from the order of
    submission. This shows that they were executed concurrently using the threads
    available in the thread pool.
  prefs: []
  type: TYPE_NORMAL
- en: The Worker Model pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea behind the Worker Model pattern is to divide a large task or many tasks
    into smaller, manageable units of work, called workers, that can be processed
    in parallel. This approach to concurrency and parallel processing not only accelerates
    processing time but also enhances the application’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: The workers could be threads within a single application (as we have just seen
    in the Thread Pool pattern), separate processes on the same machine, or even different
    machines in a distributed system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits of the Worker Model pattern are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: Easily scales with the addition of more workers, which can
    be particularly beneficial in distributed systems where tasks can be processed
    on multiple machines'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**: By distributing tasks across multiple workers, the system can
    make better use of available computing resources, processing tasks in parallel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: The Worker Model pattern can accommodate a range of processing
    strategies, from simple thread-based workers to complex distributed systems spanning
    multiple servers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider a delivery service where packages (tasks) are delivered by a team of
    couriers (workers). Each courier picks up a package from the distribution center
    (task queue) and delivers it. The number of couriers can vary depending on demand;
    more couriers can be added during busy periods and reduced when it’s quieter.
  prefs: []
  type: TYPE_NORMAL
- en: In big data processing, the Worker Model pattern is often employed where each
    worker is responsible for mapping or reducing a part of the data.
  prefs: []
  type: TYPE_NORMAL
- en: In systems such as RabbitMQ or Kafka, the Worker Model pattern is used to process
    messages from a queue concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: We can also cite image processing services. Services that need to process multiple
    images simultaneously often use the Worker Model pattern to distribute the load
    among multiple workers.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases for the Worker Model pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One use case for the Worker Model pattern is *data transformation*. When you
    have a large dataset that needs to be transformed, you can distribute the work
    among multiple workers.
  prefs: []
  type: TYPE_NORMAL
- en: Another one is *task parallelism*. In applications where different tasks are
    independent of each other, the Worker Model pattern can be very effective.
  prefs: []
  type: TYPE_NORMAL
- en: A third use case is *distributed computing*, where the Worker Model pattern
    can be extended to multiple machines, making it suitable for distributed computing
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Worker Model pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before discussing an implementation example, let’s understand how the Worker
    Model pattern works. Three components are involved in the Worker Model pattern:
    workers, a task queue, and, optionally, a dispatcher:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The workers**: The primary actors in this model. Each worker can perform
    a piece of the task independently of the others. Depending on the implementation,
    a worker might process one task at a time or handle multiple tasks concurrently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The task queue**: A central component where tasks are stored awaiting processing.
    Workers typically pull tasks from this queue, ensuring that tasks are distributed
    efficiently among them. The queue acts as a buffer, decoupling task submission
    from task processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The dispatcher**: In some implementations, a dispatcher component assigns
    tasks to workers based on availability, load, or priority. This can help optimize
    task distribution and resource utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now see an example where we execute a function in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing what we need for the example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create a `worker()` function that we are going to run tasks with.
    It takes as a parameter the `task_queue` object that contains the tasks to execute.
    The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `main()` function, we start by creating a queue of tasks, an instance
    of `multiprocessing.Queue`. Then, we create 10 tasks and add them to the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Five worker processes are then created, using the `multiprocessing.Process`
    class, and started. Each worker picks up a task from the queue, to execute it,
    and then picks up another until the queue is empty. Then, we start each worker
    process (using `p.start()`) in a loop, which means that the associated task will
    get executed concurrently. After that, we create another loop where we use the
    process’ `.join()` method so that the program waits for those processes to complete
    their work. That part of the code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When running the example code, using the `ch07/worker_model.py` Python command,
    you should get the following output, where you can see that the 5 workers process
    tasks from the task queue in a concurrent way until all 10 tasks are completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This demonstrates our implementation of the Worker Model pattern. This pattern
    is particularly useful for scenarios where tasks are independent and can be processed
    in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The Future and Promise pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the asynchronous programming paradigm, a Future represents a value that is
    not yet known but will be provided eventually. When a function initiates an asynchronous
    operation, instead of blocking until the operation completes and a result is available,
    it immediately returns a Future. This `Future` object acts as a placeholder for
    the actual result available later.
  prefs: []
  type: TYPE_NORMAL
- en: Futures are commonly used for I/O operations, network requests, and other time-consuming
    tasks that run asynchronously. They allow the program to continue executing other
    tasks rather than waiting for the operation to be completed. That property is
    referred to as *non-blocking*.
  prefs: []
  type: TYPE_NORMAL
- en: Once the Future is fulfilled, the result can be accessed through the Future,
    often via callbacks, polling, or blocking until the result is available.
  prefs: []
  type: TYPE_NORMAL
- en: A Promise is the writable, controlling counterpart to a Future. It represents
    the producer side of the asynchronous operation, which will eventually provide
    a result to its associated Future. When the operation completes, the Promise is
    fulfilled with a value or rejected with an error, which then resolves the Future.
  prefs: []
  type: TYPE_NORMAL
- en: Promises can be chained, allowing a sequence of asynchronous operations to be
    performed clearly and concisely.
  prefs: []
  type: TYPE_NORMAL
- en: 'By allowing a program to continue execution without waiting for asynchronous
    operations, applications become more responsive. Another benefit is *composability*:
    multiple asynchronous operations can be combined, sequenced, or executed in parallel
    in a clean and manageable way.'
  prefs: []
  type: TYPE_NORMAL
- en: Real-world examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ordering a custom dining table from a carpenter provides a tangible example
    of the Future and Promise pattern. When you place the order, you receive an estimated
    completion date and design sketch (Future), representing the carpenter’s promise
    to deliver the table. As the carpenter works, this promise moves toward fulfillment.
    The delivery of the completed table resolves the Future, marking the fulfillment
    of the carpenter’s promise to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also find several examples in the digital realm, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Online shopping order tracking**: When you place an order online, the website
    immediately provides you with an order confirmation and a tracking number (Future).
    As your order is processed, shipped, and delivered, status updates (Promise fulfillment)
    are reflected in real time on the tracking page, eventually resolving to a final
    delivery status.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Food delivery apps**: Upon ordering your meal through a food delivery app,
    you’re given an estimated delivery time (Future). The app continuously updates
    the order status—from preparation through pickup and delivery (Promise being fulfilled)—until
    the food arrives at your door, at which point the Future is resolved with the
    completion of your order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer support tickets**: When you submit a support ticket on a website,
    you immediately receive a ticket number and a message stating that someone will
    get back to you (Future). Behind the scenes, the support team addresses tickets
    based on priority or in the order they were received. Once your ticket is addressed,
    you receive a response, fulfilling the Promise made when you first submitted the
    ticket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use cases for the Future and Promise pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are at least four use cases where the Future and Promise pattern is recommended:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data pipelines**: In data processing pipelines, data is often transformed
    through multiple stages before reaching its final form. By representing each stage
    with a Future, you can effectively manage the asynchronous flow of data. For example,
    the output of one stage can serve as the input for the next, but because each
    stage returns a Future, subsequent stages don’t have to block while waiting for
    the previous ones to complete.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Task scheduling**: Task scheduling systems, such as those in an operating
    system or a high-level application, can use Futures to represent tasks that are
    scheduled to run at a future time. When a task is scheduled, a Future is returned
    to represent the eventual completion of that task. This allows the system or the
    application to keep track of the task’s state without blocking execution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Complex database queries or transactions**: Executing database queries asynchronously
    is crucial for maintaining application responsiveness, particularly in web applications
    where user experience is paramount. By using Futures to represent the outcome
    of database operations, applications can initiate a query and immediately return
    control to the user interface or the calling function. The Future will eventually
    resolve with the query result, allowing the application to update the UI or process
    the data without having frozen or become unresponsive while waiting for the database
    response.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**File I/O operations**: File I/O operations can significantly impact application
    performance, particularly if executed synchronously on the main thread. By applying
    the Future and Promise pattern, file I/O operations are offloaded to a background
    process, with a Future returned to represent the completion of the operation.
    This approach allows the application to continue running other tasks or responding
    to user interactions while the file is being read from or written to. Once the
    I/O operation completes, the Future resolves, and the application can process
    or display the file data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each of these use cases, the Future and Promise pattern facilitates asynchronous
    operation, allowing applications to remain responsive and efficient by not blocking
    the main thread with long-running tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Future and Promise pattern – using concurrent.futures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand how to implement the Future and Promise pattern, you must first
    understand the three steps of its mechanism. Let’s break those down next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initiation**: The initiation step involves starting an asynchronous operation
    using a function where, instead of waiting for the operation to complete, the
    function immediately returns a “Future” object. This object acts as a placeholder
    for the result that will be available later. Internally, the asynchronous function
    creates a “Promise” object. This object is responsible for handling the outcome
    of the asynchronous operation. The Promise is linked to the Future, meaning the
    state of the Promise (whether it’s fulfilled or rejected) will directly affect
    the Future.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Execution**: During the execution step, the operation proceeds independently
    of the main program flow. This allows the program to remain responsive and continue
    with other tasks. Once the asynchronous task completes, its result needs to be
    communicated back to the part of the program that initiated the operation. The
    outcome of the operation (be it a successful result or an error) is passed to
    the previously created Promise.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Resolution**: If the operation is successful, the Promise is “fulfilled”
    with the result. If the operation fails, the Promise is “rejected” with an error.
    The fulfillment or rejection of the Promise resolves the Future. Using the result
    is often done through a callback or continuation function, which is a piece of
    code that specifies what to do with the result. The Future provides mechanisms
    (for example, methods or operators) to specify these callbacks, which will execute
    once the Future is resolved.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In our example, we use an instance of the `ThreadPoolExecutor` class to execute
    tasks asynchronously. The submit method returns a `Future` object that will eventually
    contain the result of the computation. We start by importing what we need, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define a function for the task to be executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We submit tasks and get `Future` objects, then we collect the completed Futures.
    The `as_completed` function allows us to iterate over completed `Future` objects
    and retrieve their results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'When running the example, using the `ch07/future_and_promise/future.py` Python
    command, you should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This demonstrates our implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Future and Promise pattern – using asyncio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python’s `asyncio` library provides another way to execute tasks using asynchronous
    programming. It is particularly useful for I/O-bound tasks. Let’s see a second
    example using this technique.
  prefs: []
  type: TYPE_NORMAL
- en: What is asyncio?
  prefs: []
  type: TYPE_NORMAL
- en: The `asyncio` library provides support for asynchronous I/O, event loops, coroutines,
    and other concurrency-related tasks. So, using `asyncio`, developers can write
    code that efficiently handles I/O-bound operations.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines and async/await
  prefs: []
  type: TYPE_NORMAL
- en: A coroutine is a special kind of function that can pause and resume its execution
    at certain points, allowing other coroutines to run in the meantime. Coroutines
    are declared with the `async` keyword. Also, a coroutine can be awaited from other
    coroutines, using the `await` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the `asyncio` module, which contains everything we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create a function for the task of computing and returning the square
    of a number. We also want an I/O-bound operation, so we use `asyncio.sleep()`.
    Notice that in the `asyncio` style of programming, such a function is defined
    using the combined keywords `async def` – it is a coroutine. The `asyncio.sleep()`
    function itself is a coroutine, so we make sure to use the `await` keyword when
    calling it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we move to creating our `main()` function. We use the `asyncio.ensure_future()`
    function to create the `Future` objects we want, passing it `square(x)`, with
    `x` being the number to square. We create three `Future` objects, `future1`, `future2`,
    and `future3`. Then, we use the `asyncio.gather()` coroutine to wait for our Futures
    to complete and gather the results. The code for the `main()` function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of our code file, we have the usual `if __name__ == "__main__":`
    block. What is new here, since we are writing `asyncio`-based code, is that we
    need to run `asyncio`’s event loop, by calling `asyncio.run(main())`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To test the example, run the `ch07/future_and_promise/async.py` Python command.
    You should get an output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The order of the results may vary, depending on who is running the program and
    when. In fact, it is not predictable. You may have noticed similar behavior in
    our previous examples. This is generally the case with concurrency or asynchronous
    code.
  prefs: []
  type: TYPE_NORMAL
- en: This simple example shows that `asyncio` is a suitable choice for the Future
    and Promise pattern when we need to efficiently handle I/O-bound tasks (in scenarios
    such as web scraping or API calls).
  prefs: []
  type: TYPE_NORMAL
- en: The Observer pattern in reactive programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Observer pattern (covered in [*Chapter 5*](B21896_05.xhtml#_idTextAnchor121),
    *Behavioral Design Patterns*) is useful for notifying an object or a group of
    objects when the state of a given object changes. This type of traditional Observer
    allows us to react to some object change events. It provides a nice solution for
    many cases, but in a situation where we must deal with many events, some depending
    on each other, the traditional way could lead to complicated, difficult-to-maintain
    code. That is where another paradigm called reactive programming gives us an interesting
    option. In simple terms, the concept of reactive programming is to react to many
    events (streams of events) while keeping our code clean.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s focus on ReactiveX ([http://reactivex.io](http://reactivex.io)), which
    is a part of reactive programming. At the heart of ReactiveX is a concept known
    as an Observable. According to its official website, ReactiveX is about providing
    an API for asynchronous programming with what are called observable streams. This
    concept is added to the idea of the Observer, which we already discussed.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine an Observable like a river that flows data or events down to an Observer.
    This Observable sends out items one after another. These items travel through
    a path made up of different steps or operations until they reach an Observer,
    who takes them in or consumes them.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An airport’s flight information display system is analogous to an Observable
    in reactive programming. Such a system continuously streams updates about flight
    statuses, including arrivals, departures, delays, and cancellations. This analogy
    illustrates how observers (travelers, airline staff, and airport services subscribed
    to receive updates) subscribe to an Observable (the flight display system) and
    react to a continuous stream of updates, allowing for dynamic responses to real-time
    information.
  prefs: []
  type: TYPE_NORMAL
- en: A spreadsheet application can also be seen as an example of reactive programming,
    based on its internal behavior. In virtually all spreadsheet applications, interactively
    changing any one cell in the sheet will result in immediately reevaluating all
    formulas that directly or indirectly depend on that cell and updating the display
    to reflect these reevaluations.
  prefs: []
  type: TYPE_NORMAL
- en: The ReactiveX idea is implemented in a variety of languages, including Java
    (RxJava), Python (RxPY), and JavaScript (RxJS). The Angular framework uses RxJS
    to implement the Observable pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases for the Observer pattern in reactive programming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One use case is the idea of a collection pipeline, discussed by Martin Fowler
    on his blog ([https://martinfowler.com/articles/collection-pipeline](https://martinfowler.com/articles/collection-pipeline)).
  prefs: []
  type: TYPE_NORMAL
- en: Collection pipeline, described by Martin Fowler
  prefs: []
  type: TYPE_NORMAL
- en: Collection pipelines are a programming pattern where you organize some computation
    as a sequence of operations that compose by taking a collection as the output
    of one operation and feeding it into the next.
  prefs: []
  type: TYPE_NORMAL
- en: We can also use an Observable to do operations such as “map and reduce” or “groupby”
    on sequences of objects when processing data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Observables can be created for diverse functions such as button events,
    requests, and Twitter feeds.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Observer pattern in reactive programming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this example, we decided to build a stream of a list of (fake) people’s
    names (in the `ch07/observer_rx/people.txt`) text file, and an observable based
    on it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A first example text file containing fake names of people is provided (`ch07/observer_rx/people.txt`)
    as part of the book’s example files. But a new one can be generated whenever needed
    using a helper script (`ch07/observer_rx/peoplelist.py`), which will be presented
    in a minute.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of such a list of names will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Back to our implementation. We start by importing what we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a function, `firstnames_from_db()`, which returns an Observable from
    the text file (reading the content of the file) containing the names, with transformations
    (as we have already seen) using `flat_map()`, `filter()`, and `map()` methods,
    and a new operation, `group_by()`, to emit items from another sequence—the first
    name found in the file, with its number of occurrence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in the `main()` function, we define an Observable that emits data every
    5 seconds, merging its emission with what is returned from `firstnames_from_db(db_file)`,
    after setting `db_file` to the people names text file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a recap of the example (complete code in the `ch07/observer_rx/rx_peoplelist.py`
    file):'
  prefs: []
  type: TYPE_NORMAL
- en: We import the modules and classes we need.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We define a `firstnames_from_db()` function, which returns an Observable from
    the text file that is the source of the data. We collect and push the stored people’s
    first names from that file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, in the `main()` function, we define an Observable that emits data every
    5 seconds, merging its emission with what is returned from calling the `firstnames_from_db()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To test the example, run the `ch07/observer_rx/rx_peoplelist.py` Python command.
    You should get an output like the following (only an extract is shown here):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Once you press a key and press *Enter* on the keyboard, the emission is interrupted,
    and the program stops.
  prefs: []
  type: TYPE_NORMAL
- en: Handling new streams of data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our test worked, but in a sense, it was static; the stream of data was limited
    to what is currently in the text file. What we need now is to generate several
    streams of data. The technique we can use to generate the type of fake data in
    the text file is based on a third-party module called Faker ([https://pypi.org/project/Faker](https://pypi.org/project/Faker)).
    The code that produces the data is provided to you, for free (in the `ch07/observer_rx/peoplelist.py`
    file), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s see what happens when we execute both programs (`ch07/observer_rx/peoplelist.py`
    and `ch07/observer_rx/rx_peoplelis.py`):'
  prefs: []
  type: TYPE_NORMAL
- en: 'From one command-line window or terminal, you can generate people’s names,
    passing the right file path to the script; you would execute the following command:
    `python` `ch07/observer_rx/peoplelist.py ch07/observer_rx/people.txt`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a second shell window, you can run the program that implements the Observable
    via the `python` `ch07/observer_rx/rx_peoplelist.py` command.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, what is the output from both commands?
  prefs: []
  type: TYPE_NORMAL
- en: A new version of the `people.txt` file is created (with the random names in
    it, separated by a comma), to replace the existing file. And, each time you rerun
    that command (`python ch07/observer_rx/peoplelist.py`), a new set of names is
    added to the file.
  prefs: []
  type: TYPE_NORMAL
- en: The second command gives an output like the one you got with the first execution;
    the difference is that now it is not the same set of data that is emitted repeatedly.
    Now, new data can be generated in the source and emitted.
  prefs: []
  type: TYPE_NORMAL
- en: Other concurrency and asynchronous patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are some other concurrency and asynchronous patterns developers may use.
    We can cite the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Actor model**: A conceptual model to deal with concurrent computation.
    It defines some rules for how actor instances should behave: an actor can make
    local decisions, create more actors, send more messages, and determine how to
    respond to the next message received.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`asyncio` library).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Message passing**: Used in parallel computing, **object-oriented programming**
    (**OOP**), and **inter-process communication** (**IPC**), where software entities
    communicate and coordinate their actions by passing messages to each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backpressure**: A mechanism to manage the flow of data through software systems
    and prevent overwhelming components. It allows systems to gracefully handle overload
    by signaling the producer to slow down until the consumer can catch up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these patterns has its use cases and trade-offs. It is interesting to
    know they exist, but we cannot discuss all the available patterns and techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed concurrency and asynchronous patterns, patterns
    useful for writing efficient, responsive software that can handle multiple tasks
    at once.
  prefs: []
  type: TYPE_NORMAL
- en: The Thread Pool pattern is a powerful tool in concurrent programming, offering
    a way to manage resources efficiently and improve application performance. It
    helps us improve application performance but also reduces overhead and better
    manages resources because the thread pool limits the number of threads.
  prefs: []
  type: TYPE_NORMAL
- en: While the Thread Pool pattern focuses on reusing a fixed number of threads to
    execute tasks, the Worker Model pattern is more about the dynamic distribution
    of tasks across potentially scalable and flexible worker entities. This pattern
    is particularly useful for scenarios where tasks are independent and can be processed
    in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The Future and Promise pattern facilitates asynchronous operation, allowing
    applications to remain responsive and efficient by not blocking the main thread
    with long-running tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed the Observer pattern in reactive programming. The core idea
    of this pattern is to react to a stream of data and events, as with the streams
    of water we see in nature. We have lots of examples of this idea in the computing
    world. We have discussed an example of ReactiveX, which serves as an introduction
    for the reader to approach this programming paradigm and continue their own research
    via the ReactiveX official documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we touched upon the fact that there are other concurrency and asynchronous
    patterns. Each of these patterns has its use cases and trade-offs, but we cannot
    cover them all in a single book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss performance design patterns.
  prefs: []
  type: TYPE_NORMAL
