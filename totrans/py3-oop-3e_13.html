<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Concurrency</h1>
                </header>
            
            <article>
                
<p>Concurrency is the art of making a computer do (or appear to do) multiple things at once. Historically, this meant inviting the processor to switch between different tasks many times per second. In modern systems, it can also literally mean doing two or more things simultaneously on separate processor cores.</p>
<p>Concurrency is not inherently an object-oriented topic, but Python's concurrent systems provide object-oriented interfaces, as we've covered throughout the book. This chapter will introduce you to the following topics:</p>
<ul>
<li>Threads</li>
<li>Multiprocessing</li>
<li>Futures</li>
<li>AsyncIO</li>
</ul>
<p>Concurrency is complicated. The basic concepts are fairly simple, but the bugs that can occur are notoriously difficult to track down. However, for many projects, concurrency is the only way to get the performance we need. Imagine if a web server couldn't respond to a user's request until another user had completed! We won't be going into all the details of just how hard it is (another full book would be required), but we'll see how to implement basic concurrency in Python, and some common pitfalls to avoid.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Threads</h1>
                </header>
            
            <article>
                
<p>Most often, concurrency is created so that work can continue happening while the program is waiting for I/O to happen. For example, a server can start processing a new network request while it waits for data from a previous request to arrive. Or an interactive program might render an animation or perform a calculation while waiting for the user to press a key. Bear in mind that while a person can type more than 500 characters per minute, a computer can perform billions of instructions per second. Thus, a ton of processing can happen between individual key presses, even when typing quickly.</p>
<p>It's theoretically possible to manage all this switching between activities within your program, but it would be virtually impossible to get right. Instead, we can rely on Python and the operating system to take care of the tricky switching part, while we create objects that appear to be running independently, but simultaneously. These objects are called <strong>threads</strong>. Let's take a look at a basic example:</p>
<pre>from threading import Thread<br/><br/><br/>class InputReader(Thread):<br/>    def run(self):<br/>        self.line_of_text = input()<br/><br/><br/>print("Enter some text and press enter: ")<br/>thread = InputReader()<br/>thread.start()<br/><br/>count = result = 1<br/>while thread.is_alive():<br/>    result = count * count<br/>    count += 1<br/><br/>print("calculated squares up to {0} * {0} = {1}".format(count, result))<br/>print("while you typed '{}'".format(thread.line_of_text))</pre>
<p>This example runs two threads. Can you see them? Every program has (at least) one thread, called the main thread. The code that executes from startup is happening in this thread. The more visible second thread exists as the <kbd>InputReader</kbd> class.</p>
<p>To construct a thread, we must extend the <kbd>Thread</kbd> class and implement the <kbd>run</kbd> method. Any code executed by the <kbd>run</kbd> method happens in a separate thread.</p>
<p>The new thread doesn't start running until we call the <kbd>start()</kbd> method on the object. In this case, the thread immediately pauses to wait for input from the keyboard. In the meantime, the original thread continues executing from the point <kbd>start</kbd> was called. It starts calculating squares inside a <kbd>while</kbd> loop. The condition in the <kbd>while</kbd> loop checks whether the <kbd>InputReader</kbd> thread has exited its <kbd>run</kbd> method yet; once it does, it outputs some summary information to the screen.</p>
<p>If we run the example and type the string <kbd>hello world</kbd>, the output looks as follows:</p>
<pre><strong>Enter some text and press enter:</strong><br/>hello world<br/><strong>calculated squares up to 2448265 * 2448265 = 5993996613696</strong></pre>
<p>You will, of course, calculate more or less squares while typing the string as the numbers are related to both our relative typing speeds, and to the processor speeds of the computers we are running. When I updated this example between the first and third edition, my newer system was able to calculate more than twice as many squares.</p>
<p>A thread only starts running in concurrent mode when we call the <kbd>start</kbd> method. If we want to take out the concurrent call to see how it compares, we can call <kbd>thread.run()</kbd> in the place that we originally called <kbd>thread.start()</kbd>. As shown here, the output is telling:</p>
<pre>    <strong>Enter some text and press enter:</strong>
    <strong>hello world</strong>
    <strong>calculated squares up to 1 * 1 = 1</strong>
    <strong>while you typed 'hello world'</strong>  </pre>
<p>In this case, there is no second thread and the <kbd>while</kbd> loop never executes. We wasted a lot of CPU power sitting idle while we were typing.</p>
<p>There are a lot of different patterns for using threads effectively. We won't be covering all of them, but we will look at a common one so we can learn about the <kbd>join</kbd> method. Let's check the current temperature in the capital city of each province and territory in Canada:</p>
<pre>from threading import Thread<br/>import time<br/>from urllib.request import urlopen<br/>from xml.etree import ElementTree<br/><br/><br/>CITIES = {<br/>    "Charlottetown": ("PE", "s0000583"),<br/>    "Edmonton": ("AB", "s0000045"),<br/>    "Fredericton": ("NB", "s0000250"),<br/>    "Halifax": ("NS", "s0000318"),<br/>    "Iqaluit": ("NU", "s0000394"),<br/>    "Québec City": ("QC", "s0000620"),<br/>    "Regina": ("SK", "s0000788"),<br/>    "St. John's": ("NL", "s0000280"),<br/>    "Toronto": ("ON", "s0000458"),<br/>    "Victoria": ("BC", "s0000775"),<br/>    "Whitehorse": ("YT", "s0000825"),<br/>    "Winnipeg": ("MB", "s0000193"),<br/>    "Yellowknife": ("NT", "s0000366"),<br/>}<br/><br/><br/>class TempGetter(Thread):<br/>    def __init__(self, city):<br/>        super().__init__()<br/>        self.city = city<br/>        self.province, self.code = CITIES[self.city]<br/><br/>    def run(self):<br/>        url = (<br/>            "http://dd.weatheroffice.ec.gc.ca/citypage_weather/xml/"<br/>            f"{self.province}/{self.code}_e.xml"<br/>        )<br/>        with urlopen(url) as stream:<br/>            xml = ElementTree.parse(stream)<br/>            self.temperature = xml.find(<br/>                "currentConditions/temperature"<br/>            ).text<br/><br/><br/>threads = [TempGetter(c) for c in CITIES]<br/>start = time.time()<br/>for thread in threads:<br/>    thread.start()<br/><br/>for thread in threads:<br/>    thread.join()<br/><br/>for thread in threads:<br/>    print(f"it is {thread.temperature}°C in {thread.city}")<br/>print(<br/>    "Got {} temps in {} seconds".format(<br/>        len(threads), time.time() - start<br/>    )<br/>)</pre>
<p>This code constructs 10 threads before starting them. Notice how we can override the constructor to pass them into the <kbd>Thread</kbd> object, remembering to call <kbd>super</kbd> to ensure the <kbd>Thread</kbd> is properly initialized.</p>
<p>Data we construct in one thread is accessible from other running threads. The references to global variables inside the <kbd>run</kbd> method illustrate this. Less obviously, the data passed into the constructor is being assigned to <kbd>self</kbd> <em>in the main thread</em>, but is accessed inside the second thread. This can trip people up; just because a method is on a <kbd>Thread</kbd> instance does not mean it is magically executed inside that thread.</p>
<p>After the 10 threads have been started, we loop over them again, calling the <kbd>join()</kbd> method on each. This method says <em>wait for the thread to complete before doing anything</em>. We call this ten times in sequence; this <kbd>for</kbd> loop won't exit until all ten threads have completed.</p>
<p>At this point, we can print the temperature that was stored on each thread object. Notice, once again, that we can access data that was constructed within the thread from the main thread. In threads, all state is shared by default.</p>
<p>Executing the preceding code on my 100 megabit connection takes about three tenths of a second, and we get the following output:</p>
<pre><strong>it is 18.5°C in Charlottetown</strong><br/><strong>it is 1.6°C in Edmonton</strong><br/><strong>it is 16.6°C in Fredericton</strong><br/><strong>it is 18.0°C in Halifax</strong><br/><strong>it is -2.4°C in Iqaluit</strong><br/><strong>it is 18.4°C in Québec City</strong><br/><strong>it is 7.4°C in Regina</strong><br/><strong>it is 11.8°C in St. John's</strong><br/><strong>it is 20.4°C in Toronto</strong><br/><strong>it is 9.2°C in Victoria</strong><br/><strong>it is -5.1°C in Whitehorse</strong><br/><strong>it is 5.1°C in Winnipeg</strong><br/><strong>it is 1.6°C in Yellowknife</strong><br/><strong>Got 13 temps in 0.29401135444641113 seconds</strong></pre>
<p>I'm writing in September, but it's already below freezing up north! If I run this code in a single thread (by changing the <kbd>start()</kbd> call to <kbd>run()</kbd> and commenting out the <kbd>join()</kbd> loop), it takes closer to four seconds because each 0.3-second request has to complete before the next one begins. This order of magnitude speedup shows just how useful concurrent programming can be.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The many problems with threads</h1>
                </header>
            
            <article>
                
<p>Threads can be useful, especially in other programming languages, but modern Python programmers tend to avoid them for several reasons. As we'll see, there are other ways to code concurrent programming that are receiving more attention from the Python community. Let's discuss some of these pitfalls before moving on to them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shared memory</h1>
                </header>
            
            <article>
                
<p>The main problem with threads is also their primary advantage. Threads have access to all the program's memory and thus all the variables. This can too easily cause inconsistencies in the program state.</p>
<p>Have you ever encountered a room where a single light has two switches and two different people turn them on at the same time? Each person (thread) expects their action to turn the lamp (a variable) on, but the resulting value (the lamp) is off, which is inconsistent with those expectations. Now imagine if those two threads were transferring funds between bank accounts or managing the cruise control for a vehicle.</p>
<p>The solution to this problem in threaded programming is to <em>synchronize</em> access to any code that reads or (especially) writes a shared variable. There are a few different ways to do this, but we won't go into them here so we can focus on more Pythonic constructs.</p>
<p>The synchronization solution works, but it is way too easy to forget to apply it. Worse, bugs due to inappropriate use of synchronization are really hard to track down because the order in which threads perform operations is inconsistent. We can't easily reproduce the error. Usually, it is safest to force communication between threads to happen using a lightweight data structure that already uses locks appropriately. Python offers the <kbd>queue.Queue</kbd> class to do this; its functionality is basically the same as <kbd>multiprocessing.Queue</kbd>, which we will discuss in the next section.</p>
<p>In some cases, these disadvantages might be outweighed by the one advantage of allowing shared memory: it's fast. If multiple threads need access to a huge data structure, shared memory can provide that access quickly. However, this advantage is usually nullified by the fact that, in Python, it is impossible for two threads running on different CPU cores to be performing calculations at exactly the same time. This brings us to our second problem with threads.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The global interpreter lock</h1>
                </header>
            
            <article>
                
<p>In order to efficiently manage memory, garbage collection, and calls to machine code in native libraries, Python has a utility called the <strong>global interpreter lock</strong>, or <strong>GIL</strong>. It's impossible to turn off, and it means that threads are useless in Python for one thing that they excel at in other languages: parallel processing. The GIL's primary effect, for our purposes, is to prevent any two threads from doing work at the exact same time, even if they have work to do. In this case, <em>doing work</em> means using the CPU, so it's perfectly okay for multiple threads to access the disk or network; the GIL is released as soon as the thread starts to wait for something. This is why the weather example worked.</p>
<p>The GIL is highly disparaged, mostly by people who don't understand what it is or all the benefits it brings to Python. It would definitely be nice if our language didn't have this restriction, but the Python development team have determined that it brings more value than it costs. It makes the reference implementation easier to maintain and develop, and during the single-core processor days when Python was originally developed, it actually made the interpreter faster. The net result of the GIL, however, is that it limits the benefits that threads bring us, without alleviating the costs.</p>
<div class="packt_infobox">While the GIL is a problem in the reference implementation of Python that most people use, it has been solved in some of the non-standard implementations, such as IronPython and Jython. Unfortunately, at the time of publication, none of these support Python 3.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thread overhead</h1>
                </header>
            
            <article>
                
<p>One final limitation of threads, as compared to the asynchronous system we will be discussing later, is the cost of maintaining each thread. Each thread takes up a certain amount of memory (both in the Python process and the operating system kernel) to record the state of that thread. Switching between the threads also uses a (small) amount of CPU time. This work happens seamlessly without any extra coding (we just have to call <kbd>start()</kbd> and the rest is taken care of), but the work still has to happen somewhere.</p>
<p>This can be alleviated somewhat by structuring our workload so that threads can be reused to perform multiple jobs. Python provides a <kbd>ThreadPool</kbd> feature to handle this. It is shipped as part of the multiprocessing library and behaves identically to <kbd>ProcessPool</kbd>, which we will discuss shortly, so let's defer that discussion until the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multiprocessing</h1>
                </header>
            
            <article>
                
<p>The multiprocessing API was originally designed to mimic the thread API. However, it has evolved, and in recent versions of Python 3, it supports more features more robustly. The multiprocessing library is designed for when CPU-intensive jobs need to happen in parallel and multiple cores are available (almost all computers, even a little smartwatch, have multiple cores). Multiprocessing is not useful when the processes spend a majority of their time waiting on I/O (for example, network, disk, database, or keyboard), but it is the way to go for parallel computation.</p>
<p>The multiprocessing module spins up new operating system processes to do the work. This means there is an entirely separate copy of the Python interpreter running for each process. Let's try to parallelize a compute-heavy operation using similar constructs to those provided by the <kbd>threading</kbd> API<span>, as follows</span>:</p>
<pre>from multiprocessing import Process, cpu_count<br/>import time<br/>import os<br/><br/><br/>class MuchCPU(Process):<br/>    def run(self):<br/>        print(os.getpid())<br/>        for i in range(200000000):<br/>            pass<br/><br/><br/>if __name__ == "__main__":<br/>    procs = [MuchCPU() for f in range(cpu_count())]<br/>    t = time.time()<br/>    for p in procs:<br/>        p.start()<br/>    for p in procs:<br/>        p.join()<br/>    print("work took {} seconds".format(time.time() - t))</pre>
<p>This example just ties up the CPU for 200 million iterations. You may not consider this to be useful work, but it can warm up your laptop on a chilly day!</p>
<p>The API should be familiar; we implement a subclass of <kbd>Process</kbd> (instead of <kbd>Thread</kbd>) and implement a <kbd>run</kbd> method. This method prints out the process ID (a unique number the operating system assigns to each process on the machine) before doing some intense (if misguided) work.</p>
<p>Pay special attention to the <kbd>if __name__ == '__main__':</kbd> guard around the module level code that prevents it running if the module is being imported, rather than run as a program. This is good practice in general, but when using multiprocessing on some operating systems, it is essential. Behind the scenes, multiprocessing may have to reimport the module inside the new process in order to execute the <kbd>run()</kbd> method. If we allowed the entire module to execute at that point, it would start creating new processes recursively until the operating system ran out of resources, crashing your computer.</p>
<p>We construct one process for each processor core on our machine, then start and join each of those processes. On my 2017-era 8-core ThinkCenter, the output looks<span> as follows</span>:</p>
<pre><strong>25812</strong><br/><strong>25813</strong><br/><strong>25814</strong><br/><strong>25815</strong><br/><strong>25816</strong><br/><strong>25817</strong><br/><strong>25818</strong><br/><strong>25819</strong><br/><strong>work took 6.97506308555603 seconds</strong></pre>
<p>The first four lines are the process ID that was printed inside each <kbd>MuchCPU</kbd> instance. The last line shows that the 200 million iterations can run in about 13 seconds on my machine. During that 13 seconds, my process monitor indicated that all four of my cores were running at 100 percent.</p>
<p>If we subclass <kbd>threading.Thread</kbd> instead of <kbd>multiprocessing.Process</kbd> in <kbd>MuchCPU</kbd>, the output looks<span>, as follows</span>:</p>
<pre><strong>26083</strong><br/><strong>26083</strong><br/><strong>26083</strong><br/><strong>26083</strong><br/><strong>26083</strong><br/><strong>26083</strong><br/><strong>26083</strong><br/><strong>26083</strong><br/><strong>work took 26.710845470428467 seconds</strong></pre>
<p>This time, the four threads are running inside the same process and take over three times as long to run. This is the cost of the GIL; in other languages, the threaded version would run at least as fast as the multiprocessing version.</p>
<p>We might expect it to be at least four times as long, but remember that many other programs are running on my laptop. In the multiprocessing version, these programs also need a share of the four CPUs. In the threading version, those programs can use the other seven CPUs instead.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multiprocessing pools</h1>
                </header>
            
            <article>
                
<p>In general, there is no reason to have more processes than there are processors on the computer. There are a few reasons for this:</p>
<ul>
<li>Only <kbd>cpu_count()</kbd> processes can run simultaneously</li>
<li>Each process consumes resources with a full copy of the Python interpreter</li>
<li>Communication between processes is expensive</li>
<li>Creating processes takes a non-zero amount of time</li>
</ul>
<p>Given these constraints, it makes sense to create at most <kbd>cpu_count()</kbd> processes when the program starts and then have them execute tasks as needed. This has much less overhead than starting a new process for each task.</p>
<p>It is not difficult to implement a basic series of communicating processes that does this, but it can be tricky to debug, test, and get right. Of course, other Python developers have already done it for us in the form of multiprocessing pools.</p>
<p>Pools abstract away the overhead of figuring out what code is executing in the main process and which code is running in the subprocess. The pool abstraction restricts the number of places in which code in different processes interacts, making it much easier to keep track of.</p>
<p>Unlike threads, multiprocessing cannot directly access variables set up by other threads. Multiprocessing provides a few different ways to implement interprocess communication. Pools seamlessly hide the process of passing data between processes. Using a pool looks much like a function call: you pass data into a function, it is executed in another process or processes, and when the work is done, a value is returned. It is important to understand that under the hood, a lot of work is being done to support this: objects in one process are being pickled and passed into an operating system process pipe. Then, another process retrieves data from the pipe and unpickles it. The requested work is done in the subprocess and a result is produced. The result is pickled and passed back through the pipe. Eventually, the original process unpickles and returns it.</p>
<p>All this pickling and passing data into pipes takes time and memory. Therefore, it is ideal to keep the amount and size of data passed into and returned from the pool to a minimum, and it is only advantageous to use the pool if a lot of processing has to be done on the data in question.</p>
<div class="packt_tip">Pickling is an expensive operation for even medium-sized Python operations. It is frequently more expensive to pickle a large object for use in a separate process than it would be to do the work in the original process using threads. Make sure you profile your program to ensure the overhead of multiprocessing is actually worth the overhead of implementing and maintaining it.</div>
<p>Armed with this knowledge, the code to make all this machinery work is surprisingly simple. Let's look at the problem of calculating all the prime factors of a list of random numbers. This is a common and expensive part of a variety of cryptography algorithms (not to mention attacks on those algorithms!). It requires years of processing power to crack the extremely large numbers used to secure your bank accounts. The following implementation, while readable, is not at all efficient, but that's okay because we want to see it using lots of CPU time:</p>
<pre>import random<br/><strong>from multiprocessing.pool import Pool</strong><br/><br/><br/>def prime_factor(value):<br/>    factors = []<br/>    for divisor in range(2, value - 1):<br/>        quotient, remainder = divmod(value, divisor)<br/>        if not remainder:<br/>            factors.extend(prime_factor(divisor))<br/>            factors.extend(prime_factor(quotient))<br/>            break<br/>    else:<br/>        factors = [value]<br/>    return factors<br/><br/><br/>if __name__ == "__main__":<br/><strong>    pool = Pool()</strong><br/><br/>    to_factor = [random.randint(100000, 50000000) for i in range(20)]<br/><strong>    results = pool.map(prime_factor, to_factor)</strong><br/>    for value, factors in zip(to_factor, results):<br/>        print("The factors of {} are {}".format(value, factors))</pre>
<p>Let's focus on the parallel processing aspects, as the brute force recursive algorithm for calculating factors is pretty clear. We first construct a multiprocessing pool instance. By default, this pool creates a separate process for each of the CPU cores in the machine it is running on.</p>
<p>The <kbd>map</kbd> method accepts a function and an iterable. The pool pickles each of the values in the iterable and passes it into an available process, which executes the function on it. When that process is finished doing its work, it pickles the resulting list of factors and passes it back to the pool. Then, if the pool has more work available, it takes on the next job.</p>
<p>Once all the pools are finished processing work (which could take some time), the results list is passed back to the original process, which has been waiting patiently for all this work to complete.</p>
<p>It is often more useful to use the similar <kbd>map_async</kbd> method, which returns immediately even though the processes are still working. In that case, the results variable would not be a list of values, but a promise to return a list of values later by calling <kbd>results.get()</kbd>. This promise object also has methods such as <kbd>ready()</kbd> and <kbd>wait()</kbd>, which allow us to check whether all the results are in yet. I'll leave you to the Python documentation to discover more about their usage.</p>
<p>Alternatively, if we don't know all the values we want to get results for in advance, we can use the <kbd>apply_async</kbd> method to queue up a single job. If the pool has a process that isn't already working, it will start immediately; otherwise, it will hold onto the task until there is a free process available.</p>
<p>Pools can also be <kbd>closed</kbd>, which refuses to take any further tasks, but processes everything currently in the queue, or <kbd>terminated</kbd>, which goes one step further and refuses to start any jobs still in the queue, although any jobs currently running are still permitted to complete.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Queues</h1>
                </header>
            
            <article>
                
<p>If we need more control over communication between processes, we can use a <kbd>Queue</kbd>. <kbd>Queue</kbd> data structures are useful for sending messages from one process into one or more other processes. Any picklable object can be sent into a <kbd>Queue</kbd>, but remember that pickling can be a costly operation, so keep such objects small. To illustrate queues, let's build a little search engine for text content that stores all relevant entries in memory.</p>
<p>This is not the most sensible way to build a text-based search engine, but I have used this pattern to query numerical data that needed to use CPU-intensive processes to construct a chart that was then rendered to the user.</p>
<p>This particular search engine scans all files in the current directory in parallel. A process is constructed for each core on the CPU. Each of these is instructed to load some of the files into memory. Let's look at the function that does the loading and searching:</p>
<pre>def search(paths, query_q, results_q): 
    lines = [] 
    for path in paths: 
        lines.extend(l.strip() for l in path.open()) 
 
    query = query_q.get() 
    while query: 
        results_q.put([l for l in lines if query in l]) 
        query = query_q.get() </pre>
<p>Remember, this function is run in a different process (in fact, it is run in <kbd>cpucount()</kbd> different processes) from the main thread. It passes a list of <kbd>path.path</kbd> objects, and two <kbd>multiprocessing.Queue</kbd> objects; one for incoming queries and one to send outgoing results. These queues automatically pickle the data in the queue and pass it into the subprocess over a pipe. These two queues are set up in the main process and passed through the pipes into the search function inside the child processes.</p>
<p>The search code is pretty dumb, both in terms of efficiency and of capabilities; it loops over every line stored in memory and puts the matching ones in a list. The list is placed in a queue and passed back to the main process.</p>
<p>Let's look at the <kbd>main</kbd> process, which sets up these queues:</p>
<pre>if __name__ == '__main__': 
    from multiprocessing import Process, Queue, cpu_count 
    from path import path 
    cpus = cpu_count() 
    pathnames = [f for f in path('.').listdir() if f.isfile()] 
    paths = [pathnames[i::cpus] for i in range(cpus)] 
    query_queues = [Queue() for p in range(cpus)] 
    results_queue = Queue() 
     
    search_procs = [ 
        Process(target=search, args=(p, q, results_queue)) 
        for p, q in zip(paths, query_queues) 
    ] 
    for proc in search_procs: proc.start() </pre>
<p>For an easier description, let's assume <kbd>cpu_count</kbd> is four. Notice how the <kbd>import</kbd> statements are placed inside the <kbd>if</kbd> guard? This is a small optimization that prevents them from being imported in each subprocess (where they aren't needed) on some operating systems. We list all the paths in the current directory and then split the list into four approximately equal parts. We also construct a list of four <kbd>Queue</kbd> objects to send data into each subprocess. Finally, we construct a <strong>single</strong> results queue. This is passed into all four of the subprocesses. Each of them can put data into the queue and it will be aggregated in the main process.</p>
<p>Now let's look at the code that makes a search actually happen:</p>
<pre>    for q in query_queues:<br/>        q.put("def")<br/>        q.put(None) # Signal process termination<br/><br/>    for i in range(cpus):<br/>        for match in results_queue.get():<br/>            print(match)<br/>    for proc in search_procs:<br/>        proc.join()</pre>
<p>This code performs a single search for <kbd>"def"</kbd> (because it's a common phrase in a directory full of Python files!).</p>
<p>This use of queues is actually a local version of what could become a distributed system. Imagine if the searches were being sent out to multiple computers and then recombined. Now imagine you had access to the millions of computers in Google's data centers and you might understand why they can return search results so quickly!</p>
<p>We won't discuss it here, but the multiprocessing module includes a manager class that can take a lot of the boilerplate out of the preceding code. There is even a version of <kbd>multiprocessing.Manager</kbd> that can manage subprocesses on remote systems to construct a rudimentary distributed application. Check the Python multiprocessing documentation if you are interested in pursuing this further.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The problems with multiprocessing</h1>
                </header>
            
            <article>
                
<p>As threads do, multiprocessing also has problems, some of which we have already discussed. There is no best way to do concurrency; this is especially true in Python. We always need to examine the parallel problem to figure out which of the many available solutions is the best one for that problem. Sometimes, there is no best solution.</p>
<p>In the case of multiprocessing, the primary drawback is that sharing data between processes is costly. As we have discussed, all communication between processes, whether by queues, pipes, or a more implicit mechanism, requires pickling the objects. Excessive pickling quickly dominates processing time. Multiprocessing works best when relatively small objects are passed between processes and a tremendous amount of work needs to be done on each one. On the other hand, if no communication between processes is required, there may not be any point in using the module at all; we can spin up four separate Python processes (by running each in a separate terminal, for example) and use them independently.</p>
<p>The other major problem with multiprocessing is that, like threads, it can be hard to tell which process a variable or method is being accessed in. In multiprocessing, if you access a variable from another process it will usually overwrite the variable in the currently running process while the other process keeps the old value. This is really confusing to maintain, so don't do it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Futures</h1>
                </header>
            
            <article>
                
<p>Let's start looking at a more asynchronous way of implementing concurrency. Futures wrap either multiprocessing or threading depending on what kind of concurrency we need (tending toward I/O versus tending toward CPU). They don't completely solve the problem of accidentally altering shared state, but they allow us to structure our code such that it is easier to track down when we do so.</p>
<p>Futures provide distinct boundaries between the different threads or processes. Similar to the multiprocessing pool, they are useful for <em>call and answer</em> type interactions, in which processing can happen in another thread and then at some point in the future (they are aptly named, after all), you can ask it for the result. It's really just a wrapper around multiprocessing pools and thread pools, but it provides a cleaner API and encourages nicer code.</p>
<p>A future is an object that wraps a function call. That function call is run in the <em>background</em>, in a thread or process. The <kbd>future</kbd> object has methods the main thread can use to check whether the future has completed and to get the results after it has completed.</p>
<p>Let's see another file search example. In the last section, we implemented a version of the <kbd>unix grep</kbd> command. This time, we'rr create a simple version of the <kbd>find</kbd> command. The example will search the entire filesystem for paths that contain a given string of characters<span>, as follows</span>:</p>
<pre>from concurrent.futures import ThreadPoolExecutor 
from pathlib import Path 
from os.path import sep as pathsep 
from collections import deque 
 <br/>def find_files(path, query_string): 
    subdirs = [] 
    for p in path.iterdir(): 
        full_path = str(p.absolute()) 
        if p.is_dir() and not p.is_symlink(): 
            subdirs.append(p) 
        if query_string in full_path: 
                print(full_path) 
 
    return subdirs 
<br/> 
query = '.py' 
futures = deque() 
basedir = Path(pathsep).absolute() 
 
with ThreadPoolExecutor(max_workers=10) as executor: 
    futures.append( 
        executor.submit(find_files, basedir, query)) 
    while futures: 
        future = futures.popleft() 
        if future.exception(): 
            continue 
        elif future.done(): 
            subdirs = future.result() 
            for subdir in subdirs: 
                futures.append(executor.submit( 
                    find_files, subdir, query)) 
        else: 
            futures.append(future) </pre>
<p>This code consists of a function named <kbd>find_files</kbd>, which is run in a separate thread (or process, if we used <kbd>ProcessPoolExecutor</kbd> instead). There isn't anything particularly special about this function, but note how it does not access any global variables. All interaction with the external environment is passed into the function or returned from it. This is not a technical requirement, but it is the best way to keep your brain inside your skull when programming with futures.</p>
<div class="packt_infobox">Accessing outside variables without proper synchronization results in something called a <strong>race</strong> <strong>condition</strong>. For example, imagine two concurrent writes trying to increment an integer counter. They start at the same time and both read the value as 5. Then, they both increment the value and write back the result as 6. But if two processes are trying to increment a variable, the expected result would be that it gets incremented by two, so the result should be 7. Modern wisdom is that the easiest way to avoid doing this is to keep as much state as possible private and share them through known-safe constructs, such as queues or futures.</div>
<p>We set up a couple of variables before we get started; we'll be searching for all files that contain the characters <kbd>'.py'</kbd> for this example. We have a queue of futures, which we'll discuss shortly. The <kbd>basedir</kbd> variable points to the root of the filesystem: <kbd>'/'</kbd> on Unix machines and probably <kbd>C:\</kbd> on Windows.</p>
<p>First, let's take a short course on search theory. This algorithm implements breadth-first search in parallel. Rather than recursively searching every directory using a depth-first search, it adds all the subdirectories in the current folder to the queue, then all the subdirectories of each of those folders, and so on.</p>
<p>The meat of the program is known as an event loop. We can construct a <kbd>ThreadPoolExecutor</kbd> as a context manager so that it is automatically cleaned up and closes its threads when it is done. It requires a <kbd>max_workers</kbd> argument to indicate the number of threads running at a time. If more than this many jobs are submitted, it queues up the rest until a worker thread becomes available. When using <kbd>ProcessPoolExecutor</kbd>, this is normally constrained to the number of CPUs on the machine, but with threads, it can be much higher, depending how many are waiting on I/O at a time. Each thread takes up a certain amount of memory, so it shouldn't be too high. It doesn't take all that many threads before the speed of the disk, rather than the number of parallel requests, is the bottleneck.</p>
<p>Once the executor has been constructed, we submit a job to it using the root directory. The <kbd>submit()</kbd> method immediately returns a <kbd>Future</kbd> object, which promises to give us a result eventually. The future is placed in the queue. The loop then repeatedly removes the first future from the queue and inspects it. If it is still running, it gets added back to the end of the queue. Otherwise, we check whether the function raised an exception with a call to <kbd>future.exception()</kbd>. If it did, we just ignore it (it's usually a permission error, although a real app would need to be more careful about what the exception was). If we didn't check this exception here, it would be raised when we called <kbd>result()</kbd> and could be handled through the normal <kbd>try...except</kbd> mechanism.</p>
<p>Assuming no exception occurred, we can call <kbd>result()</kbd> to get the return value. Since the function returns a list of subdirectories that are not symbolic links (my lazy way of preventing an infinite loop), <kbd>result()</kbd> returns the same thing. These new subdirectories are submitted to the executor and the resulting futures are tossed onto the queue to have their contents searched in a later iteration.</p>
<p>And that's all that is required to develop a future-based I/O-bound application. Under the hood, it's using the same thread or process APIs we've already discussed, but it provides a more understandable interface and makes it easier to see the boundaries between concurrently running functions (just don't try to access global variables from inside the future!).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AsyncIO</h1>
                </header>
            
            <article>
                
<p>AsyncIO is the current state of the art in Python concurrent programming. It combines the concept of futures and an event loop with the coroutines we discussed in <a href="0abbcae0-eb3f-4237-adda-32765e1cce32.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>The Iterator Pattern</em>. The result is about as elegant and easy to understand as it is possible to get when writing concurrent code, though that isn't saying a lot!</p>
<p>AsyncIO can be used for a few different concurrent tasks, but it was specifically designed for network I/O. Most networking applications, especially on the server side, spend a lot of time waiting for data to come in from the network. This can be solved by handling each client in a separate thread, but threads use up memory and other resources. AsyncIO uses coroutines as a sort of lightweight thread.</p>
<p>The library provides its own event loop, obviating the need for the several lines long the <kbd>while</kbd> loop in the previous example. However, event loops come with a cost. When we run code in an <kbd>async</kbd> task on the event loop, that code <strong>must</strong> return immediately, blocking neither on I/O nor on long-running calculations. This is a minor thing when writing our own code, but it means that any standard library or third-party functions that block on I/O have to have non-blocking versions created.</p>
<p>AsyncIO solves this by creating a set of coroutines that use <kbd>async</kbd> and <kbd>await</kbd> syntax to return control to the event loop immediately when code will block. These keywords replace the <kbd>yield</kbd>, <kbd>yield from</kbd>, and <kbd>send</kbd> syntax we used in the raw coroutines we saw earlier, as well as the need to manually advance to the first <em>send</em> location. The result is concurrent code that we can reason about as if it were sequential. The event loop takes care of checking whether the blocking call has completed and performing any subsequent tasks, much as we did manually in the previous section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AsyncIO in action</h1>
                </header>
            
            <article>
                
<p>A canonical example of a blocking function is the <kbd>time.sleep</kbd> call. Let's use the asynchronous version of this call to illustrate the basics of an AsyncIO event loop<span>, as follows</span>:</p>
<pre>import asyncio<br/>import random<br/><br/><br/>async def random_sleep(counter):<br/>    delay = random.random() * 5<br/>    print("{} sleeps for {:.2f} seconds".format(counter, delay))<br/>    await asyncio.sleep(delay)<br/>    print("{} awakens".format(counter))<br/><br/><br/>async def five_sleepers():<br/>    print("Creating five tasks")<br/>    tasks = [asyncio.create_task(random_sleep(i)) for i in range(5)]<br/>    print("Sleeping after starting five tasks")<br/>    await asyncio.sleep(2)<br/>    print("Waking and waiting for five tasks")<br/>    await asyncio.gather(*tasks)<br/><br/><br/>asyncio.get_event_loop().run_until_complete(five_sleepers())<br/>print("Done five tasks")</pre>
<p>This is a fairly basic example, but it covers several features of AsyncIO programming. It is easiest to understand in the order that it executes, which is more or less bottom to top.</p>
<p>Here's how one execution of the script looks:</p>
<pre><strong>Creating five tasks<br/>Sleeping after starting five tasks<br/>0 sleeps for 3.42 seconds<br/>1 sleeps for 4.16 seconds<br/>2 sleeps for 0.75 seconds<br/>3 sleeps for 3.55 seconds<br/>4 sleeps for 0.39 seconds<br/>4 awakens<br/>2 awakens<br/>Waking and waiting for five tasks<br/>0 awakens<br/>3 awakens<br/>1 awakens<br/>Done five tasks</strong></pre>
<p>The second to last line gets the event loop and instructs it to run a task until it is finished. The task in question is named <kbd>five_sleepers</kbd>. Once that task has done its work, the loop will exit and our code will terminate. As asynchronous programmers, we don't need to know too much about what happens inside that <kbd>run_until_complete</kbd> call, but be aware that a lot is going on. It's a souped-up coroutine version of the futures loop we wrote in the previous chapter, which knows how to deal with iteration, exceptions, function returns, parallel calls, and more.</p>
<p>A task, in this context, is an object that <kbd>asyncio</kbd> knows how to schedule on the event loop. This includes the following:</p>
<ul>
<li>Coroutines defined with the <kbd>async</kbd> and <kbd>await</kbd> syntax.</li>
<li>Coroutines decorated with <kbd>@asyncio.coroutine</kbd> and using the <kbd>yield from</kbd> syntax (this is an older model, deprecated in favor of <kbd>async</kbd> and <kbd>await</kbd>).</li>
<li><kbd>asyncio.Future</kbd> objects. These are almost identical to the <kbd>concurrent.futures</kbd> you saw in the previous section, but for use with <kbd>asyncio</kbd>.</li>
<li>Any awaitable object, that is, one with an <kbd>__await__</kbd> function.</li>
</ul>
<p>In this example, all the tasks are coroutines; we'll see some of the others in later examples.</p>
<p>Look a little more closely at that <kbd>five_sleepers</kbd> future. The coroutine first constructs five instances of the <kbd>random_sleep</kbd> coroutine. These are each wrapped in a <kbd>asyncio.create_task</kbd> call, which adds the future to the loop's task queue so they can execute and start immediately when control is returned to the loop.</p>
<p>That control is returned whenever we call <kbd>await</kbd>. In this case, we call <kbd>await asyncio.sleep</kbd> to pause the execution of the coroutine for two seconds. During the break, the event loop executes the tasks that it has queued up: namely, the five <kbd>random_sleep</kbd> tasks.</p>
<p>When the sleep call in the <kbd>five_sleepers</kbd> task wakes up, it calls <kbd>asyncio.gather</kbd>. This function accepts tasks as varargs, and awaits each of them (among other things, to keep the loop running safely) before returning.</p>
<p>Each of the <kbd>random_sleep</kbd> coroutines prints a starting message, then sends control back to the event loop for a specific amount of time using its own <kbd>await</kbd> calls. When the sleep has completed, the event loop passes control back to the relevant <kbd>random_sleep</kbd> task, which prints its awakening message before returning.</p>
<p>Note that any tasks that take less than two seconds to complete will output their own awakening messages before the original <kbd>five_sleepers</kbd> coroutine awakes to run until the <kbd>gather</kbd> task is called. Since the event queue is now empty (all six coroutines have run to completion and are not awaiting any tasks), the <kbd>run_until_complete</kbd> call is able to terminate and the program ends.</p>
<p>The <kbd>async</kbd> keyword acts as documentation notifying the python interpreter (and coder) that the coroutine contains the <kbd>await</kbd> calls. It also does some work to prepare the coroutine to run on the event loop. It behaves much like a decorator; in fact, back in Python 3.4, this was implemented as an <kbd>@asyncio.coroutine</kbd> decorator.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading an AsyncIO Future</h1>
                </header>
            
            <article>
                
<p>An AsyncIO coroutine executes each line in order until it encounters an <kbd>await</kbd> statement, at which point, it returns control to the event loop. The event loop then executes any other tasks that are ready to run, including the one that the original coroutine was waiting on. Whenever that child task completes, the event loop sends the result back into the coroutine so that it can pick up execution until it encounters another <kbd>await</kbd> statement or returns.</p>
<p>This allows us to write code that executes synchronously until we explicitly need to wait for something. As a result, there is no nondeterministic behavior of threads, so we don't need to worry nearly so much about shared state.</p>
<div class="packt_tip">It's still a good idea to avoid accessing shared state from inside a coroutine. It makes your code much easier to reason about. More importantly, even though an ideal world might have all asynchronous execution happening inside coroutines, the reality is that some futures are executed behind the scenes inside threads or processes. Stick to a <em>share nothing</em> philosophy to avoid a ton of difficult bugs.</div>
<p>In addition, AsyncIO allows us to collect logical sections of code together inside a single coroutine, even if we are waiting for other work elsewhere. As a specific instance, even though the <kbd>await asyncio.sleep</kbd> call in the <kbd>random_sleep</kbd> coroutine is allowing a ton of stuff to happen inside the event loop, the coroutine itself looks like it's doing everything in order. This ability to read related pieces of asynchronous code without worrying about the machinery that waits for tasks to complete is the primary benefit of the AsyncIO module.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AsyncIO for networking</h1>
                </header>
            
            <article>
                
<p>AsyncIO was specifically designed for use with network sockets, so let's implement a DNS server. More accurately, let's implement one extremely basic feature of a DNS server.</p>
<p>The DNS's basic purpose is to translate domain names, such as <a href="https://www.python.org/">https://www.python.org/</a>, into IP addresses, such as IPv4 addresses (for example <kbd>23.253.135.79</kbd>) or IPv6 addresses (such as <kbd>2001:4802:7901:0:e60a:1375:0:6</kbd>). It has to be able to perform many types of queries and know how to contact other DNS servers if it doesn't have the answer required. We won't be implementing any of this, but the following example is able to respond directly to a standard DNS query to look up IPs for a few sites:</p>
<pre>import asyncio<br/>from contextlib import suppress<br/><br/>ip_map = {<br/>    b"facebook.com.": "173.252.120.6",<br/>    b"yougov.com.": "213.52.133.246",<br/>    b"wipo.int.": "193.5.93.80",<br/>    b"dataquest.io.": "104.20.20.199",<br/>}<br/><br/><br/>def lookup_dns(data):<br/>    domain = b""<br/>    pointer, part_length = 13, data[12]<br/>    while part_length:<br/>        domain += data[pointer : pointer + part_length] + b"."<br/>        pointer += part_length + 1<br/>        part_length = data[pointer - 1]<br/><br/>    ip = ip_map.get(domain, "127.0.0.1")<br/><br/>    return domain, ip<br/><br/><br/>def create_response(data, ip):<br/>    ba = bytearray<br/>    packet = ba(data[:2]) + ba([129, 128]) + data[4:6] * 2<br/>    packet += ba(4) + data[12:]<br/>    packet += ba([192, 12, 0, 1, 0, 1, 0, 0, 0, 60, 0, 4])<br/>    for x in ip.split("."):<br/>        packet.append(int(x))<br/>    return packet<br/><br/><br/>class DNSProtocol(asyncio.DatagramProtocol):<br/>    def connection_made(self, transport):<br/>        self.transport = transport<br/><br/>    def datagram_received(self, data, addr):<br/>        print("Received request from {}".format(addr[0]))<br/>        domain, ip = lookup_dns(data)<br/>        print(<br/>            "Sending IP {} for {} to {}".format(<br/>                domain.decode(), ip, addr[0]<br/>            )<br/>        )<br/>        self.transport.sendto(create_response(data, ip), addr)<br/><br/><br/>loop = asyncio.get_event_loop()<br/>transport, protocol = loop.run_until_complete(<br/>    loop.create_datagram_endpoint(<br/>        DNSProtocol, local_addr=("127.0.0.1", 4343)<br/>    )<br/>)<br/>print("DNS Server running")<br/><br/>with suppress(KeyboardInterrupt):<br/>    loop.run_forever()<br/>transport.close()<br/>loop.close()</pre>
<p>This example sets up a dictionary that dumbly maps a few domains to IPv4 addresses. It is followed by two functions that extract information from a binary DNS query packet and construct the response. We won't be discussing these; if you want to know more about DNS read RFC (<em>request for comment</em>, the format for defining most IPs) <kbd>1034</kbd> and <kbd>1035</kbd>.</p>
<p>You can test this service by running the following command in another terminal:</p>
<pre>    <strong>nslookup -port=4343 facebook.com localhost</strong>  </pre>
<p>Let's get on with the entree. AsyncIO networking revolves around the intimately linked concepts of transports and protocols. A protocol is a class that has specific methods that are called when relevant events happen. Since DNS runs on top of <strong>UDP</strong> (<strong>User Datagram Protocol</strong>), we build our protocol class as a subclass of <kbd>DatagramProtocol</kbd>. There are a variety of events this class can respond to. We are specifically interested in the initial connection occurring (solely so that we can store the transport for future use) and the <kbd>datagram_received</kbd> event. For DNS, each received datagram must be parsed and responded to, at which point, the interaction is over.</p>
<p>So, when a datagram is received, we process the packet, look up the IP, and construct a response using the functions we aren't talking about (they're black sheep in the family). Then, we instruct the underlying transport to send the resulting packet back to the requesting client using its <kbd>sendto</kbd> method.</p>
<p>The transport essentially represents a communication stream. In this case, it abstracts away all the fuss of sending and receiving data on a UDP socket on an event loop. There are similar transports for interacting with TCP sockets and subprocesses, for example.</p>
<p>The UDP transport is constructed by calling the loop's <kbd>create_datagram_endpoint</kbd> coroutine. This constructs the appropriate UDP socket and starts listening on it. We pass it the address that the socket needs to listen on and, importantly, the protocol class we created so that the transport knows what to call when it receives data.</p>
<p>Since the process of initializing a socket takes a non-trivial amount of time and would block the event loop, the <kbd>create_datagram_endpoint</kbd> function is a coroutine. In our example, we don't need to do anything while we wait for this initialization, so we wrap the call in <kbd>loop.run_until_complete</kbd>. The event loop takes care of managing the future, and when it's complete, it returns a tuple of two values: the newly initialized transport and the protocol object that was constructed from the class we passed in.</p>
<p>Behind the scenes, the transport has set up a task on the event loop that is listening for incoming UDP connections. All we have to do, then, is start the event loop running with the call to <kbd>loop.run_forever()</kbd> so that the task can process these packets. When the packets arrive, they are processed on the protocol and everything just works.</p>
<p>The only other major thing to pay attention to is that transports (and, indeed, event loops) are supposed to be closed when we are finished with them. In this case, the code runs just fine without the two calls to <kbd>close()</kbd>, but if we were constructing transports on the fly (or just doing proper error handling!), we'd need to be quite a bit more conscious of it.</p>
<p>You may have been dismayed to see how much boilerplate is required in setting up a protocol class and the underlying transport. AsyncIO provides an abstraction on top of these two key concepts, called streams. We'll see an example of streams in the TCP server in the next example.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using executors to wrap blocking code</h1>
                </header>
            
            <article>
                
<p>AsyncIO provides its own version of the futures library to allow us to run code in a separate thread or process when there isn't an appropriate non-blocking call to be made. This allows us to combine threads and processes with the asynchronous model. One of the more useful applications of this feature is to get the best of both worlds when an application has bursts of I/O-bound and CPU-bound activity. The I/O-bound portions can happen in the event loop, while the CPU-intensive work can be spun off to a different process. To illustrate this, let's implement <em>sorting as a service</em> using AsyncIO:</p>
<pre>import asyncio<br/>import json<br/>from concurrent.futures import ProcessPoolExecutor<br/><br/><br/>def sort_in_process(data):<br/>    nums = json.loads(data.decode())<br/>    curr = 1<br/>    while curr &lt; len(nums):<br/>        if nums[curr] &gt;= nums[curr - 1]:<br/>            curr += 1<br/>        else:<br/>            nums[curr], nums[curr - 1] = nums[curr - 1], nums[curr]<br/>            if curr &gt; 1:<br/>                curr -= 1<br/><br/>    return json.dumps(nums).encode()<br/><br/><br/>async def sort_request(reader, writer):<br/>    print("Received connection")<br/>    length = await reader.read(8)<br/>    data = await reader.readexactly(int.from_bytes(length, "big"))<br/>    result = await asyncio.get_event_loop().run_in_executor(<br/>        None, sort_in_process, data<br/>    )<br/>    print("Sorted list")<br/>    writer.write(result)<br/>    writer.close()<br/>    print("Connection closed")<br/><br/><br/>loop = asyncio.get_event_loop()<br/>loop.set_default_executor(ProcessPoolExecutor())<br/>server = loop.run_until_complete(<br/>    asyncio.start_server(sort_request, "127.0.0.1", 2015)<br/>)<br/>print("Sort Service running")<br/><br/>loop.run_forever()<br/>server.close()<br/>loop.run_until_complete(server.wait_closed())<br/>loop.close()</pre>
<p>This is an example of good code implementing some really stupid ideas. The whole idea of sorting as a service is pretty ridiculous. Using our own sorting algorithm instead of calling Python's <kbd>sorted</kbd> is even worse. The algorithm we used is called gnome sort, or in some cases, <em>stupid sort</em>. It is a slow sort algorithm implemented in pure Python. We defined our own protocol instead of using one of the many perfectly suitable application protocols that exist in the wild. Even the idea of using multiprocessing for parallelism might be suspect here; we still end up passing all the data into and out of the subprocesses. Sometimes, it's important to take a step back from the program you are writing and ask yourself whether you are trying to meet the right goals.</p>
<p>But ignoring the workload, let's look at some of the smart features of this design. First, we are passing bytes into and out of the subprocess. This is a lot smarter than decoding the JSON in the main process. It means the (relatively expensive) decoding can happen on a different CPU. Also, pickled JSON strings are generally smaller than pickled lists, so less data is passed between processes.</p>
<p>Second, the two methods are very linear; it looks like code is being executed one line after another. Of course, in AsyncIO, this is an illusion, but we don't have to worry about shared memory or concurrency primitives.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Streams</h1>
                </header>
            
            <article>
                
<p>The sort service example should look familiar by now, as it has a similar boilerplate to other AsyncIO programs. However, there are a few differences. We called <kbd>start_server</kbd> instead of <kbd>create_server</kbd>. This method hooks into AsyncIO's streams instead of using the underlying transport/protocol code. It allows us to pass in a normal coroutine, which receives reader and writer parameters. These both represent streams of bytes that can be read from and written, like files or sockets. Second, because this is a TCP server instead of UDP, there is some socket cleanup required when the program finishes. This cleanup is a blocking call, so we have to run the <kbd>wait_closed</kbd> coroutine on the event loop.</p>
<p>Streams are fairly simple to understand. Reading is a potentially blocking call so we have to call it with <kbd>await</kbd>. Writing doesn't block; it just puts the data in a queue, which AsyncIO sends out in the background.</p>
<p>Our code inside the <kbd>sort_request</kbd> method makes two read requests. First, it reads 8 bytes from the wire and converts them to an integer using big endian notation. This integer represents the number of bytes of data the client intends to send. So, in the next call, to <kbd>readexactly</kbd>, it reads that many bytes. The difference between <kbd>read</kbd> and <kbd>readexactly</kbd> is that the former will read up to the requested number of bytes, while the latter will buffer reads until it receives all of them, or until the connection closes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Executors</h1>
                </header>
            
            <article>
                
<p>Now let's look at the executor code. We import the exact same <kbd>ProcessPoolExecutor</kbd> that we used in the previous section. Notice that we don't need a special AsyncIO version of it. The event loop has a handy <kbd>run_in_executor</kbd> coroutine that we can use to run futures on. By default, the loop runs code in <kbd>ThreadPoolExecutor</kbd>, but we can pass in a different executor if we wish. Or, as we did in this example, we can set a different default when we set up the event loop by calling <kbd>loop.set_default_executor()</kbd>.</p>
<p>As you probably recall, there is not a lot of boilerplate for using futures with an executor. However, when we use them with AsyncIO, there is none at all! The coroutine automatically wraps the function call in a future and submits it to the executor. Our code blocks until the future completes, while the event loop continues processing other connections, tasks, or futures. When the future is done, the coroutine wakes up and continues on to write the data back to the client.</p>
<p>You may be wondering if, instead of running multiple processes inside an event loop, it might be better to run multiple event loops in different processes. The answer is: <em>maybe</em>. However, depending on the exact problem space, we are probably better off running independent copies of a program with a single event loop than trying to coordinate everything with a master multiprocessing process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AsyncIO clients</h1>
                </header>
            
            <article>
                
<p>Because it is capable of handling many thousands of simultaneous connections, AsyncIO is very common for implementing servers. However, it is a generic networking library, and provides full support for client processes as well. This is pretty important, since many microservices run servers that act as clients to other servers.</p>
<p>Clients can be much simpler than servers, as they don't have to be set up to wait for incoming connections. Like most networking libraries, you just open a connection, submit your requests, and process any responses. The main difference is that you need to use <kbd>await</kbd> any time you make a potentially blocking call. Here's an example client for the sort service we implemented in the last section:</p>
<pre>import asyncio<br/>import random<br/>import json<br/><br/><br/>async def remote_sort():<br/><strong>    reader, writer = await asyncio.open_connection("127.0.0.1", 2015)</strong><br/>    print("Generating random list...")<br/>    numbers = [random.randrange(10000) for r in range(10000)]<br/>    data = json.dumps(numbers).encode()<br/>    print("List Generated, Sending data")<br/>    writer.write(len(data).to_bytes(8, "big"))<br/>    writer.write(data)<br/><br/>    print("Waiting for data...")<br/><strong>    data = await reader.readexactly(len(data))</strong><br/>    print("Received data")<br/>    sorted_values = json.loads(data.decode())<br/>    print(sorted_values)<br/>    print("\n")<br/>    writer.close()<br/><br/><br/><strong>loop = asyncio.get_event_loop()</strong><br/><strong>loop.run_until_complete(remote_sort())</strong><br/><strong>loop.close()</strong></pre>
<p>We've hit most of the high points of AsyncIO in this section, and the chapter has covered many other concurrency primitives. Concurrency is a hard problem to solve, and no one solution fits all use cases. The most important part of designing a concurrent system is deciding which of the available tools is the correct one to use for the problem. We have seen the advantages and disadvantages of several concurrent systems, and now have some insight into which are the better choices for different types of requirements.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Case study</h1>
                </header>
            
            <article>
                
<p>To wrap up this chapter, and the book, let's build a basic image compression tool. It will take black and white images (with 1 bit per pixel, either on or off) and attempt to compress it using a very basic form of compression known as run-length encoding. You may find black and white images a bit far-fetched. If so, you haven't enjoyed enough hours at <a href="http://xkcd.com"><span class="URLPACKT">http://xkcd.com</span></a>!</p>
<p>I've included some sample black and white BMP images (which are easy to read data into and present plenty of opportunity to improve on file size) with the example code for this chapter.</p>
<p>Run-length encoding takes a sequence of bits and replaces any strings of repeated bits with the number of bits that are repeated. For example, the string 000011000 might be replaced with 04 12 03 to indicate that four zeros are followed by two ones and then three more zeroes. To make things a little more interesting, we will break each row into 127-bit chunks.</p>
<p>I didn't pick 127 bits arbitrarily. 127 different values can be encoded into 7 bits, which means that if a row contains all ones or all zeros, we can store it in a single byte, with the first bit indicating whether it is a row of 0s or a row of 1s, and the remaining seven bits indicating how many of that bit exists.</p>
<p>Breaking up the image into blocks has another advantage: we can process individual blocks in parallel without them depending on each other. However, there's a major disadvantage as well: if a run has just a few ones or zeros in it, then it will take up <kbd>more</kbd> space in the compressed file. When we break up long runs into blocks, we may end up creating more of these small runs and bloat the size of the file.</p>
<p>We have the freedom to design the layout of the bytes within the compressed file as we see fit. For this simple example, our compressed file will store two byte little-endian integers at the beginning of the file representing the width and height of the completed file. Then, it will write bytes representing the 127 bit chunks of each row.</p>
<p>Now, before we start designing a concurrent system to build such compressed images, we should ask a fundamental question: is this application I/O-bound or CPU-bound?</p>
<p>My answer, honestly, is <em>I don't know</em>. I'm not sure whether the app will spend more time loading data from disk and writing it back or doing the compression in memory. I suspect that it is a CPU-bound app in principle, but once we start passing image strings into subprocesses, we may lose any benefit of parallelism.</p>
<p>We'll build this application using bottom-up design. That way, we'll have some building blocks that we can combine into different concurrency patterns to see how they compare. Let's start with the code that compresses a 127-bit chunk using run-length encoding:</p>
<pre>from bitarray import bitarray 
def compress_chunk(chunk): 
    compressed = bytearray() 
    count = 1 
    last = chunk[0] 
    for bit in chunk[1:]: 
        if bit != last: 
            compressed.append(count | (128 * last)) 
            count = 0 
            last = bit 
        count += 1 
    compressed.append(count | (128 * last)) 
    return compressed</pre>
<p>This code uses the <kbd>bitarray</kbd> class to manipulate individual zeros and ones. It is distributed as a third-party module, which you can install with the <kbd>pip install bitarray</kbd><span> command.</span> The chunk that is passed into <kbd>compress_chunks</kbd> is an instance of this class (although the example would work just as well with a list of Booleans). The primary benefit of the <kbd>bitarray</kbd> in this case is that, when pickling them between processes, they take up an eighth of the space of a list of Booleans or a bytestring of 1s and 0s. Therefore, they pickle faster. They are also a little easier to work with than doing a ton of bitwise operations.</p>
<p>The method compresses the data using run-length encoding and returns <kbd>bytearray</kbd> containing the packed data. Where a <kbd>bitarray</kbd> is like a list of ones and zeros, <kbd>bytearray</kbd> is like a list of byte objects (each byte, of course, containing eight ones or zeros).</p>
<p>The algorithm that performs the compression is pretty simple (although I'd like to point out that it took me two days to implement and debug it–simple to understand does not necessarily imply easy to write!). It first sets the <kbd>last</kbd> variable to the type of bit in the current run (either <kbd>True</kbd> or <kbd>False</kbd>). It then loops over the bits, counting each one, until it finds one that is different. When it does, it constructs a new byte by making the leftmost bit of the byte (the 128 position) either a zero or a one, depending on what the <kbd>last</kbd> variable contained. Then, it resets the counter and repeats the operation. Once the loop is done, it creates one last byte for the last run and returns the result.</p>
<p>While we're creating building blocks, let's make a function that compresses a row of image data<span>, as follows</span>:</p>
<pre>def compress_row(row): 
    compressed = bytearray() 
    chunks = split_bits(row, 127) 
    for chunk in chunks: 
        compressed.extend(compress_chunk(chunk)) 
    return compressed</pre>
<p>This function accepts a <kbd>bitarray</kbd> named <kbd>row</kbd>. It splits it into chunks that are each 127 bits wide using a function that we'll define very shortly. Then, it compresses each of those chunks using the previously defined <kbd>compress_chunk</kbd>, concatenating the results into <kbd>bytearray</kbd>, which it returns.</p>
<p>We define <kbd>split_bits</kbd> as a generator<span>, as follows</span>:</p>
<pre>def split_bits(bits, width): 
    for i in range(0, len(bits), width): 
        yield bits[i:i+width]</pre>
<p>Now, since we aren't certain yet whether this will run more effectively in threads or processes, let's wrap these functions in a method that runs everything in a provided executor:</p>
<pre>def compress_in_executor(executor, bits, width): 
    row_compressors = [] 
    for row in split_bits(bits, width): 
        compressor = executor.submit(compress_row, row) 
        row_compressors.append(compressor) 
 
    compressed = bytearray() 
    for compressor in row_compressors: 
        compressed.extend(compressor.result()) 
    return compressed</pre>
<p>This example barely needs explaining; it splits the incoming bits into rows based on the width of the image using the same <kbd>split_bits</kbd> function we have already defined (hooray for bottom-up design!).</p>
<p>Note that this code will compress any sequence of bits, although it would bloat, rather than compress binary data that has frequent changes in bit values. Black and white images are definitely good candidates for the compression algorithm in question. Let's now create a function that loads an image file using the third-party pillow module, converts it to bits, and compresses it. We can easily switch between executors using the venerable comment statement, as follows:</p>
<pre>from PIL import Image 
def compress_image(in_filename, out_filename, executor=None): 
    executor = executor if executor else ProcessPoolExecutor() 
    with Image.open(in_filename) as image: 
        bits = bitarray(image.convert('1').getdata()) 
        width, height = image.size 
 
    compressed = compress_in_executor(executor, bits, width) 
 
    with open(out_filename, 'wb') as file: 
        file.write(width.to_bytes(2, 'little')) 
        file.write(height.to_bytes(2, 'little')) 
        file.write(compressed) 
 
def single_image_main(): 
    in_filename, out_filename = sys.argv[1:3] 
    #executor = ThreadPoolExecutor(4) 
    executor = ProcessPoolExecutor() 
    compress_image(in_filename, out_filename, executor) </pre>
<p>The <kbd>image.convert()</kbd> call changes the image to black and white (one bit) mode, while <kbd>getdata()</kbd> returns an iterator over those values. We pack the results into a <kbd>bitarray</kbd> so they transfer across the wire more quickly. When we output the compressed file, we first write the width and height of the image followed by the compressed data, which arrives as <kbd>bytearray</kbd>, which can be written directly to the binary file.</p>
<p>Having written all this code, we are finally able to test whether thread pools or process pools give us better performance. I created a large (7,200 x 5,600 pixels) black and white image and ran it through both pools. <kbd>ProcessPool</kbd> takes about 7.5 seconds to process the image on my system, while <kbd>ThreadPool</kbd> consistently takes about 9. Thus, as we suspected, the cost of pickling bits and bytes back and forth between processes is eating almost all of the efficiency gains from running on multiple processors (though, looking at my CPU monitor, it does fully utilize all four cores on my machine).</p>
<p>So, it looks like compressing a single image is most effectively done in a separate process, but only barely, because we are passing so much data back and forth between the parent and subprocesses. Multiprocessing is more effective when the amount of data passed between processes is quite low.</p>
<p>So, let's extend the app to compress all the bitmaps in a directory in parallel. The only thing we'll have to pass into the subprocesses are filenames, so we should get a speed gain compared to using threads. Also, to be kind of crazy, we'll use the existing code to compress individual images. This means we'll be running <kbd>ProcessPoolExecutor</kbd> inside each subprocess to create even more <span>subprocesses, as follows</span> (I don't recommend doing this in real life!):</p>
<pre>from pathlib import Path 
def compress_dir(in_dir, out_dir): 
    if not out_dir.exists(): 
        out_dir.mkdir() 
 
    executor = ProcessPoolExecutor() 
    for file in ( 
            f for f in in_dir.iterdir() if f.suffix == '.bmp'): 
        out_file = (out_dir / file.name).with_suffix('.rle') 
        executor.submit( 
            compress_image, str(file), str(out_file)) 
 
def dir_images_main(): 
    in_dir, out_dir = (Path(p) for p in sys.argv[1:3]) 
    compress_dir(in_dir, out_dir)</pre>
<p class="mce-root"/>
<p>This code uses the <kbd>compress_image</kbd> function we defined previously, but runs it in a separate process for each image. It doesn't pass an executor into the function, so <kbd>compress_image</kbd> creates <kbd>ProcessPoolExecutor</kbd> once the new process has started running.</p>
<p>Now that we are running executors inside executors, there are four combinations of threads and process pools that we can be using to compress images. They each have quite different timing profiles, as follows:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td/>
<td>
<p><strong>Process pool per image</strong></p>
</td>
<td>
<p><strong>Thread pool per image</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>Process pool per row</strong></p>
</td>
<td>
<p>42 seconds</p>
</td>
<td>
<p>53 seconds</p>
</td>
</tr>
<tr>
<td>
<p><strong>Thread pool per row</strong></p>
</td>
<td>
<p>34 seconds</p>
</td>
<td>
<p>64 seconds</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>As we might expect, using threads for each image and again using threads for each row is the slowest configuration, since the GIL prevents us from doing any work in parallel. Given that we were slightly faster when using separate processes for each row when we were using a single image, you may be surprised to see that it is faster to use a <kbd>ThreadPool</kbd> feature for rows if we are processing each image in a separate process. Take some time to understand why this might be.</p>
<p>My machine contains only four processor cores. Each row in each image is being processed in a separate pool, which means that all those rows are competing for processing power. When there is only one image, we get a (very modest) speedup by running each row in parallel. However, when we increase the number of images being processed at once, the cost of passing all that row data into and out of a subprocess is actively stealing processing time from each of the other images. So, if we can process each image on a separate processor, where the only thing that has to get pickled into the subprocess pipe is a couple of filenames, we get a solid speedup.</p>
<p>Thus, we see that different workloads require different concurrency paradigms. Even if we are just using futures, we have to make informed decisions about what kind of executor to use.</p>
<p>Also note that for typically-sized images, the program runs quickly enough that it really doesn't matter which concurrency structures we use. In fact, even if we didn't use any concurrency at all, we'd probably end up with about the same user experience.</p>
<p>This problem could also have been solved using the threading and/or multiprocessing modules directly, though there would have been quite a bit more boilerplate code to write. You may be wondering whether or not AsyncIO would be useful here. The answer is: <em>probably not</em>. Most operating systems don't have a good way to perform non-blocking reads from the filesystem, so the library ends up wrapping all the calls in futures anyway.</p>
<p>For completeness, here's the code that I used to decompress the <strong>run-length encoding</strong> (<strong>RLE</strong>) images to confirm that the algorithm was working correctly (indeed, it wasn't until I fixed bugs in both compression and decompression, and I'm still not sure if it is perfect–I should have used test-driven development!):</p>
<pre>from PIL import Image 
import sys 
 
def decompress(width, height, bytes): 
    image = Image.new('1', (width, height)) 
 
    col = 0 
    row = 0 
    for byte in bytes: 
        color = (byte &amp; 128) &gt;&gt; 7 
        count = byte &amp; ~128 
        for i in range(count): 
            image.putpixel((row, col), color) 
            row += 1 
        if not row % width: 
            col += 1 
            row = 0 
    return image 
 
 
with open(sys.argv[1], 'rb') as file: 
    width = int.from_bytes(file.read(2), 'little') 
    height = int.from_bytes(file.read(2), 'little') 
 
    image = decompress(width, height, file.read()) 
    image.save(sys.argv[2], 'bmp') </pre>
<p>This code is fairly straightforward. Each run is encoded in a single byte. It uses some bitwise math to extract the color of the pixel and the length of the run. Then, it sets each pixel from that run in the image, incrementing the row and column of the next pixel to analyze at appropriate intervals.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>We've covered several different concurrency paradigms in this chapter and still don't have a clear idea of when each one is useful. As we saw in the case study, it is often a good idea to prototype a few different strategies before committing to one.</p>
<p>Concurrency in Python 3 is a huge topic and an entire book of this size could not cover everything there is to know about it. As your first exercise, I encourage you to search the web to discover what are considered to be the latest Python concurrency best practices.</p>
<p>If you have used threads in a recent application, take a look at the code and see how you can make it more readable and less bug-prone by using futures. Compare thread and multiprocessing futures to see whether you can gain anything by using multiple CPUs.</p>
<p>Try implementing an AsyncIO service for some basic HTTP requests. If you can get it to the point that a web browser can render a simple GET request, you'll have a good understanding of AsyncIO network transports and protocols.</p>
<p>Make sure you understand the race conditions that happen in threads when you access shared data. Try to come up with a program that uses multiple threads to set shared values in such a way that the data deliberately becomes corrupt or invalid.</p>
<p>Remember the link collector we covered for the case study in <a href="6a121a79-7716-4a8f-94ab-f96781e82d25.xhtml"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em>Python Data Structures</em>? Can you make it run faster by making requests in parallel? Is it better to use raw threads, futures, or AsyncIO for this?</p>
<p>Try writing the run-length encoding example using threads or multiprocessing directly. Do you get any speed gains? Is the code easier or harder to reason about? Is there any way to speed up the decompression script by using concurrency or parallelism?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter ends our exploration of object-oriented programming with a topic that isn't very object-oriented. Concurrency is a difficult problem, and we've only scratched the surface. While the underlying OS abstractions of processes and threads do not provide an API that is remotely object-oriented, Python offers some really good object-oriented abstractions around them. The threading and multiprocessing packages both provide an object-oriented interface to the underlying mechanics. Futures are able to encapsulate a lot of the messy details into a single object. AsyncIO uses coroutine objects to make our code read as though it runs synchronously, while hiding ugly and complicated implementation details behind a very simple loop abstraction.</p>
<p>Thank you for reading <em>Python 3 Object-Oriented Programming</em>, <em>Third Edition</em>. I hope you've enjoyed the ride and are eager to start implementing object-oriented software in all your future projects!</p>


            </article>

            
        </section>
    </body></html>