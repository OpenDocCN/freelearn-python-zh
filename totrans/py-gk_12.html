<html><head></head><body>
		<div id="_idContainer075">
			<h1 id="_idParaDest-209"><em class="italic"><a id="_idTextAnchor247"/>Chapter 9</em>: Python Programming for the Cloud</h1>
			<p>Cloud computing is a broad term that is used for a wide variety of use cases. These use cases include an offering of physical or virtual compute platforms, software development platforms, big data processing platforms, storage, network functions, software services, and many more. In this chapter, we will explore Python for cloud computing from two correlated aspects. First, we will investigate the options of using Python for building applications for cloud runtimes. Then, we will extend our discussion of data-intensive processing, which we started in <a href="B17189_08_Final_PG_ePub.xhtml#_idTextAnchor227"><em class="italic">Chapter 8</em></a>, <em class="italic">Scaling Out Python using Clusters</em>, from clusters to cloud environments. The focus of the discussion in this chapter will largely center on the three public cloud platforms; that is, <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>), <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), and <strong class="bold">Microsoft Azure</strong>. </p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>Learning about the cloud options for Python applications</li>
				<li>Building Python web services for cloud deployment </li>
				<li>Using Google Cloud Platform for data processing </li>
			</ul>
			<p>By the end of this chapter, you will know how to develop and deploy applications to a cloud platform and how to use Apache Beam in general and for Google Cloud Platform.</p>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor248"/>Technical requirements</h1>
			<p>The following are the technical requirements for this chapter:</p>
			<ul>
				<li>You need to have Python 3.7 or later installed on your computer.</li>
				<li>You will need a service account for Google Cloud Platform. A free account will work fine. </li>
				<li>You will need the Google Cloud SDK installed on your computer.</li>
				<li>You will need Apache Beam installed on your computer.</li>
			</ul>
			<p>The sample code for this chapter can be found at <a href="https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter09">https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter09</a>.</p>
			<p>We will start our discussion by looking at the cloud options that are available for developing applications for cloud deployments. </p>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor249"/>Learning about the cloud options for Python applications</h1>
			<p>Cloud computing is the ultimate frontier for programmers these days. In this section, we will investigate <a id="_idIndexMarker963"/>how we can develop Python <a id="_idIndexMarker964"/>applications using a cloud development <a id="_idIndexMarker965"/>environment or using a specific <strong class="bold">Software Development Kit</strong> (<strong class="bold">SDK</strong>) for cloud deployment, and then how we can execute the Python code in a cloud environment. We will also investigate options regarding data-intensive processing, such as Apache Spark on the cloud. We will start with the cloud-based development environments.</p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor250"/>Introducing Python development environments for the cloud</h2>
			<p>When it <a id="_idIndexMarker966"/>comes to setting up a Python <a id="_idIndexMarker967"/>development environment for one of the three main public clouds, two types of models are available: </p>
			<ul>
				<li>Cloud-native <strong class="bold">Integrated Development Environment</strong> (<strong class="bold">IDE</strong>)</li>
				<li>Locally <a id="_idIndexMarker968"/>installed IDE with an integration option for the cloud</li>
			</ul>
			<p>We will discuss these two modes next.</p>
			<h3>Cloud-native IDE</h3>
			<p>There are <a id="_idIndexMarker969"/>several cloud-native development environments available <a id="_idIndexMarker970"/>in general that are not specifically attached to the three public cloud providers. These include <strong class="bold">PythonAnyWhere</strong>, <strong class="bold">Repl.it</strong>, <strong class="bold">Trinket</strong>, and <strong class="bold">Codeanywhere</strong>. Most of these cloud environments <a id="_idIndexMarker971"/>offer a free license in addition to a paid one. These <a id="_idIndexMarker972"/>public cloud <a id="_idIndexMarker973"/>platforms offer a mixture of tools for development <a id="_idIndexMarker974"/>environments, as explained here:</p>
			<ul>
				<li><strong class="bold">AWS</strong>: This offers a sophisticated cloud IDE in the form of <strong class="bold">AWS Cloud9</strong>, which can be accessed <a id="_idIndexMarker975"/>through a web browser. This cloud IDE has a rich <a id="_idIndexMarker976"/>set of features for developers and the option of supporting several programming languages, including Python. It is important to understand that AWS Cloud9 is offered as an application hosted on an Amazon EC2 instance (a virtual machine). There is no direct fee for using AWS Cloud9, but there will be a fee for using the underlying Amazon EC2 instance and storage space, which is very nominal <a id="_idIndexMarker977"/>for limited use. The AWS platform <a id="_idIndexMarker978"/>also offers tools for building and testing the code for <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) and <strong class="bold">continuous delivery</strong> (<strong class="bold">CD</strong>) goals.<p><strong class="bold">AWS CodeBuild</strong> is another <a id="_idIndexMarker979"/>service that's available that compiles our source code, runs tests, and builds software packages for deployment. It is a build server similar to Bamboo. <strong class="bold">AWS CodeStar</strong> is commonly <a id="_idIndexMarker980"/>used with AWS Cloud9 and offers a projects-based platform to help develop, build, and deploy software. AWS CodeStar offers predefined project templates to define an entire continuous delivery toolchain until the code is released.</p></li>
				<li><strong class="bold">Microsoft Azure</strong>: This comes with the <strong class="bold">Visual Studio</strong> IDE, which is available online (cloud-based) if you <a id="_idIndexMarker981"/>are part of the Azure DevOps platform. Online <a id="_idIndexMarker982"/>access to the Visual Studio IDE is based on a paid subscription. Visual Studio IDE is well-known for its rich features and capabilities for offering an environment <a id="_idIndexMarker983"/>for team-level collaboration. Microsoft Azure offers <strong class="bold">Azure Pipelines</strong> for building, testing, and deploying your code to any platform such as Azure, AWS, and GCP. Azure Pipelines support many languages, such as Node.js, Python, Java, PHP, Ruby, C/C++, and .NET, and even mobile development toolkits. </li>
				<li><strong class="bold">Google</strong>: Google offers <strong class="bold">Cloud Code</strong> to write, test, and deploy the code that can be written <a id="_idIndexMarker984"/>either through your browser (such as via ASW Cloud9) or using <a id="_idIndexMarker985"/>a local IDE of your choice. Cloud Code comes with plugins for the most popular IDEs, such as IntelliJ IDE, Visual Studio Code, and JetBrains PyCharm. Google Cloud Code is available free of charge and is targeted at container runtime environments. Like AWS CodeBuild and Azure Pipelines, Google offers <a id="_idIndexMarker986"/>an equivalent service that is also called <strong class="bold">Cloud Build</strong> for the continuous building, testing, and deploying of software in multiple environments, such as <a id="_idIndexMarker987"/>virtual machines and containers. Google also offers Google <strong class="bold">Colaboratory</strong> or <strong class="bold">Google Colab</strong> that offers Jupyter Notebooks remotely. The Google Colab option is popular among data scientists<p>Google Cloud <a id="_idIndexMarker988"/>also offers <strong class="bold">Tekton</strong> and the <strong class="bold">Jenkins</strong> service for building CI/CD development and delivery models.</p></li>
			</ul>
			<p>In addition to all these dedicated tools and services, these cloud platforms offer online as well as locally installed shell environments. These shell environments are also a quick way to manage code in a limited capacity. </p>
			<p>Next, we will discuss the local IDE options for using Python for the cloud. </p>
			<h3>Local IDE for cloud development</h3>
			<p>The cloud-native development environment is a great tool for having native integration options in <a id="_idIndexMarker989"/>the rest of your cloud ecosystem. This makes instantiating on-demand resources and then deploying them convenient, and doesn't require any authentication tokens. But this comes with some caveats. First, although the tools are mostly free, the underlying resources that they are using are not. The second caveat is that the offline availability of these cloud-native tools is not seamless. Developers like to write code without any online ties so that they can do this anywhere, such as on a train or in a park. </p>
			<p>Due to these caveats, developers like to use local editors or IDEs for developing and testing the software before using additional tools to deploy on one of the cloud platforms. Microsoft Azure IDEs such as Visual Studio and Visual Studio Code are available for local machines. AWS and Google platform offer their own SDKs (shell-like environments) and plugins to be integrated with your IDE of choice. We will explore these models of development later in this chapter. </p>
			<p>Next, we will discuss the runtime environments that are available on the public clouds.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor251"/>Introducing cloud runtime options for Python</h2>
			<p>The simplest way to get a Python runtime environment is to get a Linux virtual machine or a container with Python installed. Once we have a virtual machine or a container, we can also <a id="_idIndexMarker990"/>install the Python version of our choice. For data-intensive workloads, the Apache Spark cluster can be set up on the compute nodes of the cloud. But this requires us to own all platform-related tasks and maintenance in case anything goes wrong. Almost all public cloud platforms offer more elegant solutions to simplify developers' and IT administrators' lives. These cloud providers offer one or more pre-built runtime environments based on the application types. We will discuss a few of the runtime environments that are available from the three public cloud providers – Amazon AWS, GCP, and Microsoft Azure.</p>
			<p class="callout-heading">What is a runtime environment?</p>
			<p class="callout">A runtime environment is an execution platform that runs Python code.</p>
			<h3>Runtime options offered by Amazon AWS</h3>
			<p>Amazon <a id="_idIndexMarker991"/>AWS offers the following runtime options:</p>
			<ul>
				<li><strong class="bold">AWS Beanstalk</strong>: This <strong class="bold">Platform-as-a-Service</strong> (<strong class="bold">PAAS</strong>) offering can be used to deploy <a id="_idIndexMarker992"/>web applications <a id="_idIndexMarker993"/>that have been developed using Java, .NET, PHP, Node.js, Python, and many more. This service also offers the option of using <a id="_idIndexMarker994"/>Apache, Nginx, Passenger, or IIS as a web server. This service provides flexibility in managing the underlining infrastructure, which is sometimes required for deploying complex applications.</li>
				<li><strong class="bold">AWS App Runner</strong>: This service can be used to run containerized web applications <a id="_idIndexMarker995"/>and microservices <a id="_idIndexMarker996"/>with an API. This service is fully managed, which means you have no administrative responsibilities, as well as no access to the underlying infrastructure. </li>
				<li><strong class="bold">AWS Lambda</strong>: This is a serverless compute runtime that allows you to run your code <a id="_idIndexMarker997"/>without the worry of managing any underlying servers. This server supports multiple languages, including Python. Although Lambda code can be executed directly from an application, this is well-suited for cases when we must run a certain piece of code in case an event is triggered by the other AWS services. </li>
				<li><strong class="bold">AWS Batch</strong>: This option is used to run computing jobs in large volumes in the form <a id="_idIndexMarker998"/>of batches. This is a cloud option from Amazon that's an alternative to the Apache Spark and Hadoop MapReduce cluster options.</li>
				<li><strong class="bold">AWS Kinesis</strong>: This <a id="_idIndexMarker999"/>service is also for data processing, but for real-time streaming data.</li>
			</ul>
			<h3>Runtime options offered by GCP</h3>
			<p>The following <a id="_idIndexMarker1000"/>are the runtime options that are available from GCP:</p>
			<ul>
				<li><strong class="bold">App Engine</strong>: This is a PaaS option from GCP that can be used to develop and host <a id="_idIndexMarker1001"/>web applications at scale. The applications are deployed as containers on App Engine, but your source code is packed into a container by the deployment tool. This complexity is hidden from developers. </li>
				<li><strong class="bold">CloudRun</strong>: This <a id="_idIndexMarker1002"/>option is used to host any code that has been built as a container. The container applications must have HTTP endpoints to be deployed on CloudRun. In comparison to App Engine, packaging applications to a container is the developer's responsibility. </li>
				<li><strong class="bold">Cloud Function</strong>: This is an event-driven, serverless, and single-purpose solution <a id="_idIndexMarker1003"/>for hosting lightweight Python code. The hosted code is typically triggered by listening to events on other GCP services or via direct HTTP requests. This is comparable to the AWS Lambda service.</li>
				<li><strong class="bold">Dataflow</strong>: This is <a id="_idIndexMarker1004"/>another serverless option but mainly for data processing with minimal latency. This simplifies a data scientist's life by taking away the complexity of the underlying processing platform and offering a data pipeline model based on Apache Beam.</li>
				<li><strong class="bold">Dataproc</strong>: This service <a id="_idIndexMarker1005"/>offers a computer platform based on Apache Spark, Apache Flink, Presto, and many more tools. This platform is suitable for those who have data processing jobs with dependencies on a Spark or Hadoop ecosystem. This service requires that we manually provision clusters. </li>
			</ul>
			<h3>Runtime options offered by Microsoft Azure</h3>
			<p>Microsoft <a id="_idIndexMarker1006"/>Azure offers the following runtime environments:</p>
			<ul>
				<li><strong class="bold">App Service</strong>: This <a id="_idIndexMarker1007"/>service is used to build and deploy web apps at scale. This web application can be deployed as a container or run on Windows or Linux.</li>
				<li><strong class="bold">Azure Functions</strong>: This is a serverless event-driven runtime environment that's used <a id="_idIndexMarker1008"/>to execute code based on a certain event or direct request. This is comparable to AWS Lambda and GCP CloudRun.  </li>
				<li><strong class="bold">Batch</strong>: As <a id="_idIndexMarker1009"/>its name suggests, this service is used to run cloud-scale jobs that require hundreds or thousands of virtual machines.</li>
				<li><strong class="bold">Azure Databricks</strong>: Microsoft has partnered with Databricks to offer this Apache <a id="_idIndexMarker1010"/>Spark-based platform for large-scale data processing. </li>
				<li><strong class="bold">Azure Data Factory</strong>: This is a serverless option from Azure that you can use to <a id="_idIndexMarker1011"/>process streaming data and transform the data into meaningful outcomes.  </li>
			</ul>
			<p>As we have seen, the three main cloud providers offer a variety of execution environments based on the applications and workloads that are available. The following use cases can be deployed on cloud platforms:</p>
			<ul>
				<li>Developing web services and web applications</li>
				<li>Data processing using a cloud runtime</li>
				<li>Microservice-based applications (containers) using Python </li>
				<li>Serverless functions or applications for the cloud</li>
			</ul>
			<p>We will <a id="_idIndexMarker1012"/>address the first two use cases in the upcoming sections of this chapter. The remaining use cases will be discussed in the upcoming chapters as they require more extensive discussion. In the next section, we will start building a web service using Python and explore how to deploy it on the GCP App Engine runtime environment.</p>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor252"/>Building Python web services for cloud deployment</h1>
			<p>Building an <a id="_idIndexMarker1013"/>application for cloud <a id="_idIndexMarker1014"/>deployment is slightly different than doing so for a local deployment. There are three key requirements we must consider while developing and deploying an application to any cloud. These requirements are as follows:</p>
			<ul>
				<li><strong class="bold">Web interface</strong>: For <a id="_idIndexMarker1015"/>most cloud deployments, applications that have a <strong class="bold">graphical user interface</strong> (<strong class="bold">GUI</strong>) or <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) are the main candidates. Command-line interface-based applications will not get their usability from a cloud environment <a id="_idIndexMarker1016"/>unless they are deployed in a dedicated virtual machine instance, and we can execute them on a VM instance using SSH or Telnet. This is why we selected a web interface-based application for our discussion.</li>
				<li><strong class="bold">Environment setup</strong>: All public cloud platforms support multiple languages, as well <a id="_idIndexMarker1017"/>as different versions of a single language. For example, GCP App Engine supports Python versions 3.7, 3.8, and 3.9 as of June 2021. Sometimes, cloud services allow <a id="_idIndexMarker1018"/>you to bring your own version for deployment as well. For web applications, it is also important to set an entry point for accessing the code and project-level settings. These are typically defined in a single file (a YAML file, in the case of the GCP App Engine application). </li>
				<li><strong class="bold">Dependency management</strong>: The main challenge regarding the portability of any <a id="_idIndexMarker1019"/>application is its dependency on third-party libraries. For GCP App Engine applications, we document all dependencies in a text file (<strong class="source-inline">requirements.txt</strong>) manually or using the <strong class="source-inline">PIP freeze</strong> command. There are other elegant ways available to solve this problem as well. One such way is to package all third-party libraries with applications into a single file for cloud deployment, such as the Java web archive file (<strong class="source-inline">.war</strong> file). Another approach is to bundle all the dependencies containing application code and the target execution platform into a container and deploy the container directly on a container hosting platform. We will explore container-based deployment options in <a href="B17189_11_Final_PG_ePub.xhtml#_idTextAnchor289"><em class="italic">Chapter 11</em></a>, <em class="italic">Using Python for Microservices Development</em>.</li>
			</ul>
			<p>There are <a id="_idIndexMarker1020"/>at least three options <a id="_idIndexMarker1021"/>for deploying a Python web service application on GCP App Engine, which are as follows: </p>
			<ul>
				<li>Using the Google Cloud SDK via the CLI interface</li>
				<li>Using the GCP web console (portal) along with Cloud Shell (CLI interface)</li>
				<li>Using a third-party IDE such as PyCharm </li>
			</ul>
			<p>We will discuss the first option in detail and provide a summary of our experience with the other two options. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">To deploy a Python application in AWS and Azure, the procedural steps are the same in principle, but the details are different, depending on the SDK and API support available from each cloud provider.</p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor253"/>Using Google Cloud SDK</h2>
			<p>In this section, we will discuss how to use Google Cloud SDK (mainly the CLI interface) to create <a id="_idIndexMarker1022"/>and deploy a sample application. This sample application will be deployed on the <strong class="bold">Google App Engine</strong> (<strong class="bold">GAE</strong>) platform. GAE is a PaaS platform <a id="_idIndexMarker1023"/>and is best suited for deploying web applications using a wide variety of programming languages, including Python. </p>
			<p>To use the <a id="_idIndexMarker1024"/>Google Cloud SDK for Python application deployment, we must have the following prerequisites on our local machine:</p>
			<ul>
				<li>Install and initialize the Cloud SDK. Once installed, you can access it via the CLI interface and check its version with the following command. Note that almost all Cloud SDK commands start with <strong class="source-inline">gcloud</strong>:<p class="source-code"><strong class="source-inline">gcloud –version</strong></p></li>
				<li>Install the Cloud SDK components to add the App Engine extension for Python 3. This can be done by using the following command:<p class="source-code">gcloud components install app-engine-python</p></li>
				<li>The GCP CloudBuild API must be enabled for the GCP cloud project.</li>
				<li>Cloud billing must be enabled for the GCP cloud project, even if you are using a trial account, by associating your GCP billing account with the project.</li>
				<li>The GCP user privileges to set up a new App Engine application and to enable API services should be done at the <em class="italic">Owner</em> level.</li>
			</ul>
			<p>Next, we will describe how to set up a GCP cloud project, create a sample web service application, and deploy it to the GAE. </p>
			<h3>Setting up a GCP cloud project</h3>
			<p>The concept of a GCP cloud project is the same as we see in most development IDEs. A GCP cloud <a id="_idIndexMarker1025"/>project consists of a set of project-level settings that manage how our code interacts with GCP services and tracks the resources in use by the project. A GCP project must be associated with a billing account. This is a prerequisite, in terms of billing, for tracking how many GCP services and resources are consumed on a per-project basis. </p>
			<p>Next, we will explain how to set up a project using Cloud SDK:</p>
			<ol>
				<li>Log in to the Cloud SDK using the following command. This will take you to the web browser so that you can sign in, in case you have not already done so:<p class="source-code">gcloud init</p></li>
				<li>Create a new project called <strong class="source-inline">time-wsproj</strong>. The project's name should be short and use only letters and numbers. The use of <strong class="source-inline">-</strong> is allowed for better readability: <p class="source-code">gcloud projects create time-wsproj</p></li>
				<li>Switch your default scope of the Cloud SDK to the newly created project, if you haven't done so already, by using the following command:<p class="source-code">gcloud config set project time-wsproj</p><p>This will enable Cloud SDK to use this project as a default project for any command we push through the Cloud SDK CLI. </p></li>
				<li>Create an App Engine instance under a default project or for any project by using the <strong class="source-inline">project</strong> attribute with one of the following commands:<p class="source-code">gcloud app create                   #for default project</p><p class="source-code">gcloud app create --project=time-wsproj  #for specific project</p><p>Note that this command will reserve cloud resources (mainly compute and storage) and will prompt you to select a region and zone to host the resources. You can select the region and zone that's closest to you and also more appropriate from your audience's point of view.</p></li>
				<li>Enable the Cloud Build API service for the current project. As we've discussed already, the Google Cloud Build service is used to build the application before it's deployed to a Google runtime such as App Engine. The Cloud Build API service is easier to enable through the GCP web console as it only takes a few clicks. To enable it using Cloud SDK, first, we need to know the exact name of the service. We can get the list of available GCP services by using the <strong class="source-inline">gcloud services list</strong> command. <p>This command will give you a long list of GCP services so that you can look for a service related to Cloud Build. You can also use <strong class="source-inline">format</strong>, attributed with any command, to beautify the Cloud SDK's output. To make this even more convenient, you can use the Linux <strong class="source-inline">grep</strong> utility (if you are using Linux or macOS) with this <a id="_idIndexMarker1026"/>command to filter the results and then enable the service using the <strong class="source-inline">enable</strong> command:</p><p class="source-code">gcloud services list --available | grep cloudbuild</p><p class="source-code">#output will be like: NAME: cloudbuild.googleapis.com</p><p class="source-code">#Cloud SDK command to enable this service</p><p class="source-code">gcloud services enable cloudbuild.googleapis.com</p></li>
				<li>To enable the Cloud Billing API service for our project, first, we need to associate a billing account with our project. Support for billing accounts in Cloud SDK hasn't been <a id="_idIndexMarker1027"/>achieved with <strong class="bold">General Availability</strong> (<strong class="bold">GA</strong>) yet, as per Cloud SDK release 343.0.0. Attaching a billing account to a project can be done through the GCP web console. But there is also a beta version of the Cloud SDK commands available so that you can achieve the same. As a first step, we need to know the billing account ID that will be used. The billing accounts associated with the logged-in user can be retrieved by using the <strong class="source-inline">beta</strong> command presented here:<p class="source-code">gcloud <strong class="bold">beta</strong> billing accounts list</p><p class="source-code">#output will include following </p><p class="source-code">#billingAccounts/<strong class="bold">0140E8-51G144-2AB62E</strong></p><p class="source-code">#enable billing on the current project using </p><p class="source-code">gcloud beta billing projects <strong class="bold">link</strong> <strong class="bold">time-wsproj</strong> --billing-account <strong class="bold">0140E8-51G144-2AB62E</strong></p><p>Note that if you are using the <strong class="source-inline">beta</strong> commands for the first time, you will be prompted to install the beta component. You should go ahead and install it. If you are already using a Cloud SDK version with a billing component included for GA, you can skip using the beta keyword or use the appropriate commands, as per the Cloud SDK release documentation.</p></li>
				<li>Enable the <a id="_idIndexMarker1028"/>Cloud Billing API service for the current project by following the same steps we followed for enabling the Cloud Build API. First, we must find the name of the API service and then enable it using the following set of Cloud SDK commands:<p class="source-code">gcloud services list --available | grep cloudbilling</p><p class="source-code">#output will be: NAME: cloudbilling.googleapis.com</p><p class="source-code">#command to enable this service</p><p class="source-code">gcloud services enable cloudbilling.googleapis.com</p></li>
			</ol>
			<p>The steps you must follow to set up a cloud project are straightforward for an experienced cloud user and will not take more than a few minutes. Once the project has been set up, we can get the project configuration details by running the following command:</p>
			<p class="source-code">gcloud projects describe time-wsproj</p>
			<p>The output of this command will provide the project life cycle's status, the project's name, the project's ID, and the project's number. The following is some example output:</p>
			<p class="source-code">createTime: '2021-06-05T12:03:31.039Z'</p>
			<p class="source-code">lifecycleState: ACTIVE</p>
			<p class="source-code">name: time-wsproj</p>
			<p class="source-code">projectId: time-wsproj</p>
			<p class="source-code">projectNumber: '539807460484'</p>
			<p>Now that the project has been set up, we can start developing our Python web application. We will do this in the next section. </p>
			<h3>Building a Python application</h3>
			<p>For cloud deployments, we can build a Python application using an IDE or system editor and then <a id="_idIndexMarker1029"/>emulate the App Engine runtime locally using the Cloud SDK and the <em class="italic">app-engine-python component</em>, which we have installed as a prerequisite. As an example, we will build a web service-based application <a id="_idIndexMarker1030"/>that will provide us with the date and time through a REST API. The application can be triggered via an API client or using a web browser. We did not enable any authentication to keep the deployment simple.</p>
			<p>To build the Python application, we will set up a Python virtual environment using the Python <strong class="source-inline">venv</strong> package. A virtual environment, created using the <strong class="source-inline">venv</strong> package, will be used to wrap the Python interpreter, core, and third-party libraries and scripts to keep them separate from the system Python environment and other Python virtual environments. Creating and managing a virtual environment in Python using the <strong class="source-inline">venv</strong> package has been supported in Python since v3.3. There are other tools available for creating virtual environments, such as <strong class="source-inline">virtualenv</strong> and <strong class="source-inline">pipenv</strong>. PyPA recommends using <strong class="source-inline">venv</strong> for creating a virtual environment, so we selected it for most of the examples presented in this book. </p>
			<p>As a first step, we will create a web application project directory named <strong class="source-inline">time-wsproj</strong> that contains the following files:</p>
			<ul>
				<li><strong class="source-inline">app.yaml</strong></li>
				<li><strong class="source-inline">main.py</strong></li>
				<li><strong class="source-inline">requirements.txt</strong></li>
			</ul>
			<p>We used the same name for the directory that we used to create the cloud project just for convenience, but this is not a requirement. Let's look at these files in more detail.</p>
			<h4>YAML file</h4>
			<p>This file <a id="_idIndexMarker1031"/>contains the deployment and runtime settings for <a id="_idIndexMarker1032"/>an App Engine application, such as runtime version number. For Python 3, the <strong class="source-inline">app.yaml</strong> file must have at least a runtime parameter (<strong class="source-inline">runtime: python38</strong>). Each service in the web application can have its own YAML file. For the sake of simplicity, we will use only one YAML file. In our case, this YAML file will only <a id="_idIndexMarker1033"/>contain the runtime attribute. We added a few more attributes <a id="_idIndexMarker1034"/>to the sample YAML file for illustration purposes:</p>
			<p class="source-code">runtime: python38</p>
			<h4>main.py Python file</h4>
			<p>We selected the <strong class="source-inline">Flask</strong> library to build our sample application. <strong class="source-inline">Flask</strong> is a well-known library <a id="_idIndexMarker1035"/>for web development, mainly because of the powerful features <a id="_idIndexMarker1036"/>it offers, along with its ease of use. We will cover Flask in the next chapter in detail. </p>
			<p>This <strong class="source-inline">main.py</strong> Python module is the entry point of our application. The complete code of the application is presented here:</p>
			<p class="source-code">from flask import Flask</p>
			<p class="source-code">from datetime import date, datetime</p>
			<p class="source-code"># If 'entrypoint' is not defined in app.yaml, App Engine will look #for an app variable. This is the case in our YAML file</p>
			<p class="source-code"><strong class="bold">app = Flask(__name__)</strong></p>
			<p class="source-code"><strong class="bold">@app.route('/')</strong></p>
			<p class="source-code">def welcome():</p>
			<p class="source-code">    return 'Welcome Python Geek! Use appropriate URI for date       and time'</p>
			<p class="source-code"><strong class="bold">@app.route('/date')</strong></p>
			<p class="source-code">def today():</p>
			<p class="source-code">    today = date.today()</p>
			<p class="source-code">    return "{date:" + today.strftime("%B %d, %Y") + '}'</p>
			<p class="source-code"><strong class="bold">@app.route('/time')</strong></p>
			<p class="source-code">def time():</p>
			<p class="source-code">    now = datetime.now()</p>
			<p class="source-code">    return "{time:" + now.strftime("%H:%M:%S") + '}'</p>
			<p class="source-code">if __name__ == '__main__':</p>
			<p class="source-code">    # For local testing</p>
			<p class="source-code">    app.run(host='127.0.0.1', port=8080, debug=True)</p>
			<p>This <a id="_idIndexMarker1037"/>module provides the following key features:</p>
			<ul>
				<li>There is <a id="_idIndexMarker1038"/>a default entrypoint called <strong class="source-inline">app</strong> that's defined in this module. The <strong class="source-inline">app</strong> variable is used to redirect the requests that are sent to this module. </li>
				<li>Using Flask's annotation, we have defined handlers for three URLs: <p>a) The root <strong class="source-inline">/</strong> URL will trigger a function named <strong class="source-inline">welcome</strong>. The <strong class="source-inline">welcome</strong> function returns a greeting message as a string.</p><p>b) The <strong class="source-inline">/date</strong> URL will trigger the <strong class="source-inline">today</strong> function, which will return today's date in JSON format.</p><p>c) The <strong class="source-inline">/time</strong> URL will execute the <strong class="source-inline">time</strong> function, which will return the current time in JSON format.</p></li>
				<li>At the end of the module, we added a <strong class="source-inline">__main__</strong> function to initiate a local web server that comes with Flask for testing purposes.</li>
			</ul>
			<h4>Requirements file</h4>
			<p>This file <a id="_idIndexMarker1039"/>contains a list of project dependencies for third-party libraries. The contents of this file will be used by App Engine to make the required libraries available to our application. In our case, we will need the Flask library to build our sample web application. The contents of this file for our project are as follows:</p>
			<p class="source-code">Flask==2.0.1</p>
			<p>Once we have created the project directory and made these files, we must create a virtual <a id="_idIndexMarker1040"/>environment inside or outside the project directory and activate it using the source command:</p>
			<p class="source-code">python -m venv myenv</p>
			<p class="source-code">source myenv/bin/activate</p>
			<p>After activating the virtual environment, we must install the necessary dependencies, as per the <strong class="source-inline">requirements.txt</strong> file. We will use the <strong class="source-inline">pip</strong> utility from the same directory where the <strong class="source-inline">requirements.txt</strong> file resides:</p>
			<p class="source-code">pip install -r requirements.txt</p>
			<p>Once the Flask library and its dependencies have been installed, the directory structure will look like this in our PyCharm IDE:</p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B17189_09_01.jpg" alt="Figure 9.1 – Directory structure for a sample web application &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Directory structure for a sample web application </p>
			<p>Once the <a id="_idIndexMarker1041"/>project file and dependencies have been set up, we will start the web server locally using the following command:</p>
			<p class="source-code">python main.py</p>
			<p>The server will start with the following debug messages, which makes it clear that this server option is only for testing purposes and not for production environments:</p>
			<p class="source-code">* Serving Flask app 'main' (lazy loading)</p>
			<p class="source-code">* Environment: production</p>
			<p class="source-code">   WARNING: This is a development server. Do not use it in a      production deployment.</p>
			<p class="source-code">   Use a production WSGI server instead.</p>
			<p class="source-code">* Debug mode: on</p>
			<p class="source-code">* Running on http://127.0.0.1:8080/ (Press CTRL+C to quit)</p>
			<p class="source-code">* Restarting with stat</p>
			<p class="source-code">* Debugger is active!</p>
			<p class="source-code">* Debugger PIN: 668-656-035</p>
			<p>Our web service application can be accessed using the following URIs:</p>
			<ul>
				<li><strong class="source-inline">http://localhost:8080/</strong></li>
				<li><strong class="source-inline">http://localhost:8080/date</strong></li>
				<li><strong class="source-inline">http://localhost:8080/time</strong></li>
			</ul>
			<p>The response <a id="_idIndexMarker1042"/>from the web servers for these URIs is shown here:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B17189_09_02.jpg" alt="Figure 9.2 – Response in the web browser from our sample web service application&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Response in the web browser from our sample web service application</p>
			<p>The web server will be stopped before we move on to the next phase – that is, deploying this application to Google App Engine. </p>
			<h3>Deploying to Google App Engine</h3>
			<p>To deploy our web service application to GAE, we must use the following command from the project directory: </p>
			<p class="source-code">gcloud app <strong class="bold">deploy</strong></p>
			<p>Cloud SDK <a id="_idIndexMarker1043"/>will read the <strong class="source-inline">app.yaml</strong> file, which provides input for creating an App Engine instance for this application. During the deployment, a container image is created using the Cloud Build service; then, this container image is uploaded to GCP storage for deployment. Once it has been successfully deployed, we can access the web service using the following command:</p>
			<p class="source-code">gcloud app <strong class="bold">browse</strong></p>
			<p>This command will open the application using the default browser on your machine. The URL of the hosted application will vary, depending on the region and zone that was selected during app creation. </p>
			<p>It is important to understand that every time we execute the <strong class="source-inline">deploy</strong> command, it will create a new version of our application in App Engine, which means more resources will be consumed. We can check the versions that have been installed for a web application using the following command:</p>
			<p class="source-code">gcloud app versions <strong class="bold">list</strong></p>
			<p>The older versions of the application still can be in a serving state with slightly different URLs assigned to them. The older versions can be stopped, started, or deleted using the <strong class="source-inline">gcloud app versions</strong> Cloud SDK command and the version ID. An application can be stopped or started using the <strong class="source-inline">stop</strong> or <strong class="source-inline">start</strong> commands, as shown here:</p>
			<p class="source-code">gcloud app versions <strong class="bold">stop</strong> &lt;version id&gt;</p>
			<p class="source-code">gcloud app versions <strong class="bold">start</strong> &lt;version id&gt;</p>
			<p class="source-code">gcloud app versions <strong class="bold">delete</strong> &lt;version id&gt;</p>
			<p>The version ID is available when we run the <strong class="source-inline">gcloud app versions list</strong> command. This concludes our discussion on building and deploying a Python web application to Google Cloud. Next, we will summarize how we can leverage the GCP console to deploy the same application.</p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor254"/>Using the GCP web console</h2>
			<p>The GCP console provides an easy-to-use web portal for accessing and managing GCP projects, as well <a id="_idIndexMarker1044"/>as an online version of Google <strong class="bold">Cloud Shell</strong>. The console also offers customizable dashboards, visibility to <a id="_idIndexMarker1045"/>cloud resources used by projects, billing details, activity logging, and many more features. When it comes to developing and deploying a web <a id="_idIndexMarker1046"/>application using the GCP console, we have some features we can use thanks to the web UI, but most of the steps will require the use of Cloud Shell. This is a Cloud SDK that's available online through any browser.</p>
			<p>Cloud Shell is more than Cloud SDK in several ways:</p>
			<ul>
				<li>It offers access to the <strong class="source-inline">gcloud</strong> CLI, as well as the <strong class="source-inline">kubectl</strong> CLI. <strong class="source-inline">kubectl</strong> is used for managing resources on the GCP Kubernetes engine. </li>
				<li>With <a id="_idIndexMarker1047"/>Cloud Shell, we can develop, debug, build, and deploy our applications using <strong class="bold">Cloud Shell Editor</strong>. </li>
				<li>Cloud Shell also offers an online development server for testing an application before deploying it to App Engine. </li>
				<li>Cloud Shell comes with tools to upload and download files between the Cloud Shell platform and your machine.</li>
				<li>Cloud Shell comes with the ability to preview the web application on port 8080 or a port of your choice.</li>
			</ul>
			<p>The Cloud Shell commands that are required to set up a new project, build the application, and deploy to App Engine are the same ones we discussed for Cloud SDK. That is why we will leave this for you to explore by following the same steps that we described in the previous section. Note that the project can be set up using the GCP console. The Cloud Shell interface can be enabled using the Cloud Shell icon on the top menu bar, on the right-hand side. Once Cloud Shell has been enabled, a command-line interface will appear at the bottom of the console's web page. This is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B17189_09_03.jpg" alt="Figure 9.3 – GCP console with Cloud Shell&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – GCP console with Cloud Shell</p>
			<p>As we <a id="_idIndexMarker1048"/>mentioned earlier, Cloud Shell comes with an editor tool <a id="_idIndexMarker1049"/>that can be started by using the <strong class="bold">Open editor</strong> button. The following screenshot shows the Python file opened inside <strong class="bold">Cloud Shell Editor</strong>:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B17189_09_04.jpg" alt="Figure 9.4 – GCP console with Cloud Shell Editor "/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – GCP console with Cloud Shell Editor </p>
			<p>Another option when it comes to building and deploying web applications is using third-party IDEs <a id="_idIndexMarker1050"/>with Google App Engine plugins. Based <a id="_idIndexMarker1051"/>on our experience, the plugins that are available for commonly used IDEs such as PyCharm and Eclipse are mostly built for Python 2 and legacy web application libraries. Directly integrating IDEs with GCP requires more work and evolution. At the time of writing, the best option is to use Cloud SDK or Cloud Shell directly in conjunction with the editor or IDE of your choice for application development.</p>
			<p>In this section, we covered developing web applications using Python and deploying them to the GCP App Engine platform. Amazon offers the AWS Beanstalk service for web application deployment. The steps for deploying a web application in AWS Beanstalk are nearly the same as for GCP App Engine, except that AWS Beanstalk does not need projects to be set up as a prerequisite. Therefore, we can deploy applications faster in AWS Beanstalk.</p>
			<p>To deploy our web service application in AWS Beanstalk, we must provide the following information, either using the AWS console or using the AWS CLI: </p>
			<ul>
				<li>Application name</li>
				<li>Platform (Python version 3.7 or 3.8, in our case)</li>
				<li>Source code version </li>
				<li>Source code, along with a <strong class="source-inline">requirements.txt</strong> file</li>
			</ul>
			<p>We recommend <a id="_idIndexMarker1052"/>using the AWS CLI for web <a id="_idIndexMarker1053"/>applications that have a dependency on third-party libraries. We can upload our source code as a ZIP file or as a web archive (<strong class="source-inline">WAR</strong> file) from our local <a id="_idIndexMarker1054"/>machine or copy them from an <strong class="bold">Amazon S3</strong> location. </p>
			<p>The exact steps of deploying a web application on AWS Beanstalk are available at <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-flask.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-flask.html</a>. Azure offers App Service for building and deploying web applications. You can find the steps for creating and deploying a web application on Azure at <a href="https://docs.microsoft.com/en-us/azure/app-service/quickstart-python">https://docs.microsoft.com/en-us/azure/app-service/quickstart-python</a>.</p>
			<p>Next, we will explore building driver programs for data processing using cloud platforms. </p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor255"/>Using Google Cloud Platform for data processing</h1>
			<p>Google Cloud Platform offers Cloud Dataflow as a data processing service to serve both batch and <a id="_idIndexMarker1055"/>real-time data streaming <a id="_idIndexMarker1056"/>applications. This service is meant for data scientists and analytics application developers so that they can set up a processing <strong class="bold">pipeline</strong> for data analysis and data processing. Cloud Dataflow uses Apache Beam <a id="_idIndexMarker1057"/>under the hood. <strong class="bold">Apache Beam</strong> originated from Google, but it is now an open source project under Apache. This project offers a programming model for building data processing using pipelines. Such pipelines can be created using Apache Beam and then executed using the Cloud Dataflow service. </p>
			<p>The Google Cloud Dataflow service is similar to <strong class="source-inline">Amazon Kinesis</strong>, Apache Storm, <strong class="source-inline">Apache Spark</strong>, and Facebook Flux. Before we discuss how to use Google Dataflow with Python, we will introduce Apache Beam and its pipeline concepts. </p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor256"/>Learning the fundamentals of Apache Beam</h2>
			<p>In the current era, data is like a cash cow for many organizations. A lot of data is generated by applications, by devices, and by human interaction with systems. Before consuming the data, it is important to process it. The steps that are defined for data processing are <a id="_idIndexMarker1058"/>typically called pipelines in Apache Beam nomenclature. In other words, a data pipeline is a series of actions that are performed on raw data that originates from different sources, and then moves that data to a destination for consumption by analytic or business applications. </p>
			<p>Apache Beam is used to break the problem into small bundles of data that can be processed <a id="_idIndexMarker1059"/>in parallel. One of the main use cases of Apache Beam is <strong class="bold">Extract, Transform, and Load</strong> (<strong class="bold">ETL</strong>) applications. These three ETL steps are core for a pipeline whenever we have to move the data from a raw form to a refined form for data consumption. </p>
			<p>The core concepts and components of Apache Beam are as follows:</p>
			<ul>
				<li><strong class="bold">Pipeline</strong>: A pipeline is <a id="_idIndexMarker1060"/>a scheme for transforming the data from one form into the other as part of data processing.</li>
				<li><strong class="bold">PCollection</strong>: A Pcollection, or Parallel Collection, is analogous to RDD in Apache Spark. It is a distributed dataset that contains an immutable and unordered <a id="_idIndexMarker1061"/>bag of elements. The size of the dataset can be fixed or bounded, similar to batch processing, where we know how many jobs to process in one batch. The size can also be flexible or unbounded based on the continuously updating and streaming data source.</li>
				<li><strong class="bold">PTransforms</strong>: These <a id="_idIndexMarker1062"/>are the operations that are defined in a pipeline to transform the data. These operations are operated on PCollection objects. </li>
				<li><strong class="bold">SDK</strong>: A language-specific <a id="_idIndexMarker1063"/>software development kit that's available for Java, Python, and Go to build pipelines and submit them to a runner for execution.</li>
				<li><strong class="bold">Runner</strong>: This is an execution platform for Apache Beam pipelines. Runner software has <a id="_idIndexMarker1064"/>to implement a single method called <strong class="source-inline">run (Pipeline)</strong> that is asynchronous by default. A few runners that are available are Apache Flink, Apache Spark, and Google Cloud Dataflow.</li>
				<li><strong class="bold">User-Defined Functions (ParDo/DoFn)</strong>: Apache Beam offers several types of <strong class="bold">user-defined functions</strong> (<strong class="bold">UDFs</strong>). The <a id="_idIndexMarker1065"/>most commonly <a id="_idIndexMarker1066"/>used function is <strong class="source-inline">DoFn</strong>, which operates on a per-element basis. The provided <strong class="source-inline">DoFn</strong> implementation is wrapped inside an <strong class="source-inline">ParDo</strong> object that is designed for parallel execution. </li>
			</ul>
			<p>A simple <a id="_idIndexMarker1067"/>pipeline looks as follows:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B17189_09_05.jpg" alt="Figure 9.5 – Flow of a pipeline with three PTransform operations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – Flow of a pipeline with three PTransform operations</p>
			<p>To design a pipeline, we must typically consider three elements:</p>
			<ol>
				<li value="1">First, we need to understand the source of the data. Is it stored in a file or in a database, or is it coming as a stream? Based on this, we will determine what type of Read Transform operation we have to implement. As part of the read operation or a separate operation, we also need to understand the data format or structure. </li>
				<li>The next step is to define and design what to do with this data. This is our main transform operation(s). We can have multiple transform operations one after the other in a serial way or in parallel on the same data. The Apache Beam SDK provides several pre-built transforms that can be used. It also allows us to write our own transforms using ParDo/DoFn functions.</li>
				<li>Last, we need to know what the output of our pipeline will be and where to store the output results. This is shown as a Write Transform in the preceding diagram. </li>
			</ol>
			<p>In this section, we discussed a simple pipeline structure to explain different concepts related to Apache Beam and pipelines. In practice, the pipeline can be relatively complex. A pipeline can have multiple input data sources and multiple output sinks. The PTransforms <a id="_idIndexMarker1068"/>operations may result in multiple PCollection objects, which requires PTransform operations to run in parallel.</p>
			<p>In the next few sections, we will learn how to create a new pipeline and execute a pipeline using an Apache Beam runner or Cloud Dataflow runner.</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor257"/>Introducing Apache Beam pipelines</h2>
			<p>In this section, we will discuss how to create Apache Beam pipelines. As we've discussed already, a pipeline is a set of actions or operations that are orchestrated to achieve certain data <a id="_idIndexMarker1069"/>processing goals. The pipeline requires an input data source that can contain in-memory data, local or remote files, or streaming data. The pseudocode for a typical pipeline will look as follows:</p>
			<p class="source-code">[Final PColletcion] = ([Initial Input PCollection] <strong class="bold">|</strong>     [First PTransform] <strong class="bold">|</strong> [Second PTransform] <strong class="bold">|</strong> [Third PTransform])</p>
			<p>The initial PCollection is used as input to the First PTransform operation. The output PCollection of First PTransform will be used as input to Second PTransform, and so on. The final output of the PCollection of the last PTransform will be captured as a Final PCollection object and be used to export the results to the target destination.</p>
			<p>To illustrate this concept, we will build a few example pipelines of different complexity levels. These examples are designed to show the roles of different Apache components and libraries that are used in building and executing a pipeline. In the end, we will build a pipeline for a famous <em class="italic">word count</em> application that is also referenced in the Apache Beam and GCP Dataflow documentation. It is important to highlight that we must install the <strong class="source-inline">apache-beam</strong> Python library using the <strong class="source-inline">pip</strong> utility for all the code examples in this section.</p>
			<h3>Example 1 – creating a pipeline with in-memory string data</h3>
			<p>In this example, we will create an input PCollection from an in-memory collection of strings, apply <a id="_idIndexMarker1070"/>a couple of transform operations, and then print the results to the output console. The following is the complete example code:</p>
			<p class="source-code">#<strong class="bold">pipeline1.py</strong>: Separate strings from a PCollection</p>
			<p class="source-code">import apache_beam as beam</p>
			<p class="source-code">with <strong class="bold">beam.Pipeline</strong>() as pipeline:</p>
			<p class="source-code">  subjects = (</p>
			<p class="source-code">      pipeline</p>
			<p class="source-code">      | 'Subjects' &gt;&gt; <strong class="bold">beam.Create</strong>([</p>
			<p class="source-code">          'English Maths Science French Arts', ])</p>
			<p class="source-code">      | 'Split subjects' &gt;&gt; <strong class="bold">beam.FlatMap</strong>(str.split)</p>
			<p class="source-code">      | <strong class="bold">beam.Map</strong>(print))</p>
			<p>For this example, it is important to highlight a few points:</p>
			<ul>
				<li>We used <strong class="source-inline">|</strong> to write different PTransform operations in a pipeline. This is an overloaded operator that is more like applying a PTransform to a PCollection to produce another PCollection. </li>
				<li>We used the <strong class="source-inline">&gt;&gt;</strong> operator to name each PTransform operation for logging and tracking purposes. The string between <strong class="source-inline">|</strong> and <strong class="source-inline">&gt;&gt;</strong> is used for displaying and logging purposes.</li>
				<li>We used three transform operations; all are part of the Apache Beam library:<p>a) The first transform operation is used to create a PCollection object, which is a string containing five subject names.</p><p>b) The second transform operation is used to split the string data into a new PCollection using a built-in <strong class="source-inline">String</strong> object method (<strong class="source-inline">split</strong>).</p><p>c) The third transform operation is used to print each entry in the PCollection to the console output.</p></li>
			</ul>
			<p>The <a id="_idIndexMarker1071"/>console's output will show a list of subject names, with one name in one line. </p>
			<h3>Example 2 – creating and processing a pipeline with in-memory tuple data</h3>
			<p>In this code example, we will create a PCollection of tuples. Each tuple will have a subject <a id="_idIndexMarker1072"/>name and a grade <a id="_idIndexMarker1073"/>associated with it. The core PTransform operation of this pipeline is to separate the subject and its grade from the data. The sample code is as follows:</p>
			<p class="source-code">#<strong class="bold">pipeline2.py</strong>: Separate subjects with grade from a PCollection</p>
			<p class="source-code">import apache_beam as beam</p>
			<p class="source-code">def <strong class="bold">my_format</strong>(sub, marks):</p>
			<p class="source-code">    yield '{}\t{}'.format(sub,marks)</p>
			<p class="source-code">with beam.Pipeline() as pipeline:</p>
			<p class="source-code">  plants = (</p>
			<p class="source-code">      pipeline</p>
			<p class="source-code">      | 'Subjects' &gt;&gt; <strong class="bold">beam.Create</strong>([</p>
			<p class="source-code">      ('English','A'),</p>
			<p class="source-code">      ('Maths', 'B+'),</p>
			<p class="source-code">      ('Science', 'A-'),</p>
			<p class="source-code">      ('French', 'A'),</p>
			<p class="source-code">      ('Arts', 'A+'),</p>
			<p class="source-code">      ])</p>
			<p class="source-code">      | 'Format subjects with marks' &gt;&gt; <strong class="bold">beam.FlatMapTuple</strong>(<strong class="bold">my_        format</strong>)</p>
			<p class="source-code">      | <strong class="bold">beam.Map</strong>(print))</p>
			<p>In <a id="_idIndexMarker1074"/>comparison to the <a id="_idIndexMarker1075"/>first example, we used the <strong class="source-inline">FlatMapTuple</strong> transform operation with a custom function to format the tuple data. The console output will show each subject's name, along with its grade, in a separate line.</p>
			<h3>Example 3 – creating a pipeline with data from a text file</h3>
			<p>In the first two examples, we focused on building a simple pipeline to parse string data from <a id="_idIndexMarker1076"/>a large string and to split tuples from a PCollection of tuples. In practice, we are working on a large volume of data that is either loaded from a file or storage system or coming from a streaming source. In this example, we will read data from a local text file to build our initial PCollection object and also output the final results to an output file. The complete code example is as follows:</p>
			<p class="source-code">#<strong class="bold">pipeline3</strong>.py: Read data from a file and give results back to another file</p>
			<p class="source-code">import apache_beam as beam</p>
			<p class="source-code">from apache_beam.io import WriteToText, ReadFromText</p>
			<p class="source-code">with beam.Pipeline() as pipeline:</p>
			<p class="source-code">    lines = pipeline | <strong class="bold">ReadFromText</strong>('sample1.txt')</p>
			<p class="source-code">    subjects = (</p>
			<p class="source-code">      lines</p>
			<p class="source-code">      | 'Subjects' &gt;&gt; beam.<strong class="bold">FlatMap(str.split)</strong>)</p>
			<p class="source-code">    subjects | <strong class="bold">WriteToText</strong>(<strong class="bold">file_path_prefix</strong>='subjects', </p>
			<p class="source-code">                           <strong class="bold">file_name_suffix</strong>='.txt',</p>
			<p class="source-code">                           <strong class="bold">shard_name_template</strong>='')</p>
			<p>In this code example, we applied a PTransform operation to read the text data from a file before <a id="_idIndexMarker1077"/>applying any data processing-related PTransforms. Finally, we applied a PTransform operation to write the data to an output file. We used two new functions in this code example called <strong class="source-inline">ReadFromText</strong> and <strong class="source-inline">WriteToText</strong>, as explained here:</p>
			<ul>
				<li><strong class="source-inline">ReadFromText</strong>: This function is part of the Apache Beam I/O module and is used to read data from text files into a PCollection of strings. The file path or file pattern can be provided as an input argument to read from a local path. We can also use <strong class="source-inline">gs://</strong> to access any file in GCS storage locations. </li>
				<li><strong class="source-inline">WriteToText</strong>: This function is used to write PCollection data to a text file. This requires the <strong class="source-inline">file_path_prefix</strong> argument at a minimum. We can also provide the <strong class="source-inline">file_path_suffix</strong> argument to set the file extension. <strong class="source-inline">shard_name_template</strong> is set to empty to create the file with a name using the prefix and suffix arguments. Apache Beam supports a shard name template for defining the filename based on a template.</li>
			</ul>
			<p>When this pipeline is executed locally, it will create a file named <strong class="source-inline">subjects.txt</strong> with subject names captured in it, as per the PTransform operation. </p>
			<h3>Example 4 – creating a pipeline for an Apache Beam runner with arguments</h3>
			<p>So far, we have learned how to create a simple pipeline, how to build a PCollection object <a id="_idIndexMarker1078"/>from a text file, and how to write the results back to a file. In addition to these core steps, we need to perform a few more steps, to make sure our driver program is ready to submit the job to a GCP Dataflow runner or any other cloud runner. These additional steps are as follows:</p>
			<ul>
				<li>In the previous example, we provided the names of the input file and the output file pattern that are set in the driver program. In practice, we should expect these parameters to be provided through command-line arguments. We will use the <strong class="source-inline">argparse</strong> library to parse and manage command-line arguments.</li>
				<li>We will add extended arguments such as setting a runner. This argument will be used to set the target runner of the pipeline using DirectRunner or a GCP Dataflow runner. Note that DirectRunner is a pipeline runtime for your local machine. It makes sure that those pipelines follow the Apache Beam model as closely as possible. </li>
				<li>We will also implement and use the <strong class="source-inline">ParDo</strong> function, which will utilize our custom-built function for parsing strings from text data. We can achieve this using <strong class="source-inline">String</strong> functions, but it has been added here to illustrate how to use <strong class="source-inline">ParDo</strong> and <strong class="source-inline">DoFn</strong> with PTransform.</li>
			</ul>
			<p>Here are the steps:</p>
			<ol>
				<li value="1">First, we will build the argument parser and define the arguments we expect from the <a id="_idIndexMarker1079"/>command line. We will set the default values for those arguments and set additional helping text to be shown with the <strong class="source-inline">help</strong> switch on the command line. The <strong class="source-inline">dest</strong> attribute is important because it is used to identify any argument to be used in programming statements. We will also define the <strong class="source-inline">ParDo</strong> function, which will be used to execute the pipeline. Some sample code is presented here: <p class="source-code">#<strong class="bold">pipeline4.py(part 1)</strong>: Using argument for a pipeline</p><p class="source-code">import re, argparse, apache_beam as beam</p><p class="source-code">from apache_beam.io import WriteToText, ReadFromText</p><p class="source-code">from apache_beam.options.pipeline_options import   PipelineOptions</p><p class="source-code">class <strong class="bold">WordParsingDoFn</strong>(<strong class="bold">beam.DoFn</strong>):</p><p class="source-code">  def <strong class="bold">process</strong>(self, element):</p><p class="source-code">    return re.findall(r'[\w\']+', element, re.UNICODE)</p><p class="source-code">def run(argv=None):</p><p class="source-code">    parser = <strong class="bold">argparse.ArgumentParser</strong>()</p><p class="source-code">    parser.add_argument(</p><p class="source-code">        '<strong class="bold">--input</strong>',</p><p class="source-code">        dest='input',</p><p class="source-code">        default='sample1.txt',</p><p class="source-code">        help='Input file to process.')</p><p class="source-code">    parser.add_argument(</p><p class="source-code">        '<strong class="bold">--output</strong>',</p><p class="source-code">        dest='output',</p><p class="source-code">        default='subjects',</p><p class="source-code">        help='Output file to write results to.')</p><p class="source-code">    parser.add_argument(</p><p class="source-code">        '<strong class="bold">--extension</strong>',</p><p class="source-code">        dest='ext',</p><p class="source-code">        default='.txt',</p><p class="source-code">        help='Output file extension to use.')</p><p class="source-code">    known_args, pipeline_args = parser.parse_known_      args(argv)</p></li>
				<li>Now, we will <a id="_idIndexMarker1080"/>set <strong class="source-inline">DirectRunner</strong> as our pipeline runtime and name the job to be executed. The sample code for this step is as follows:<p class="source-code">#<strong class="bold">pipeline4.py(part 2): under the run method</strong></p><p class="source-code">    pipeline_args.extend([</p><p class="source-code">        '<strong class="bold">--runner=DirectRunner</strong>',</p><p class="source-code">        '<strong class="bold">--job_name=demo-local-job</strong>',</p><p class="source-code">    ])</p><p class="source-code">    pipeline_options = PipelineOptions(pipeline_args)</p></li>
				<li>Finally, we will create a pipeline using the <strong class="source-inline">pipeline_options</strong> object that we created <a id="_idIndexMarker1081"/>in the previous step. The pipeline will be reading data from an input text file, transforming data as per our <strong class="source-inline">ParDo</strong> function, and then saving the results as output:<p class="source-code">#<strong class="bold">pipeline4.py(part 3): under the run method</strong></p><p class="source-code">    </p><p class="source-code">    with beam.Pipeline(options=pipeline_options) as       pipeline:</p><p class="source-code">        lines = pipeline | <strong class="bold">ReadFromText</strong>(known_args.input)</p><p class="source-code">        subjects = (</p><p class="source-code">                lines</p><p class="source-code">                | 'Subjects' &gt;&gt; beam.                  ParDo(WordParsingDoFn()).</p><p class="source-code">                with_output_types(str))</p><p class="source-code">        subjects | <strong class="bold">WriteToText</strong>(known_args.output, known_            args.ext)</p><p>When we execute this program directly through an IDE using the default values for the argument or initiate it from a command-line interface using the following command, we will get the same output: </p><p class="source-code"><strong class="bold">python pipeline4.py --input sample1.txt --output myoutput --extension .txt</strong></p><p>The words from the input text file (<strong class="source-inline">sample1.txt</strong>) are parsed and are put as one word in one line in the output file. </p></li>
			</ol>
			<p>Apache Beam is a vast topic, so it is not possible to cover all its features without writing a few <a id="_idIndexMarker1082"/>chapters on it. However, we have covered the fundamentals by providing code examples that will enable us to start writing simple pipelines that we can deploy on GCP Cloud Dataflow. We will cover this in the next section. </p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor258"/>Building pipelines for Cloud Dataflow</h2>
			<p>The code examples we've discussed so far focused on building simple pipelines and executing <a id="_idIndexMarker1083"/>them using DirectRunner. In this section, we will build a driver program to deploy a word count data processing pipeline on Google Cloud Dataflow. This driver program is important for Cloud Dataflow deployments because we will set all cloud-related parameters inside the program. Due to this, there will be no need to use Cloud SDK or Cloud Shell to execute additional commands. </p>
			<p>The word count pipeline will be an extended version of our <strong class="source-inline">pipeline4.py</strong> example. The additional components and steps required to deploy the word count pipeline are summarized here:</p>
			<ol>
				<li value="1">First, we will create a new GCP cloud project using steps that are similar to the ones we followed for our web service application for App Engine deployment. We can use Cloud SDK, Cloud Shell, or the GCP console for this task.</li>
				<li>We will enable Dataflow Engine API for the new project.</li>
				<li>Next, we will create a storage bucket for storing the input and output files and to provide temporary and staging directories for Cloud Dataflow. We can achieve this by using the GCP console, Cloud Shell, or Cloud SDK. We can use the following command if we are using Cloud Shell or Cloud SDK to create a new bucket:<p class="source-code">gsutil mb gs://&lt;bucket name&gt;</p></li>
				<li>You may need to associate a service account with the newly created bucket if it is not under the same project as the dataflow pipeline job is and select the <em class="italic">storage object admin</em> role for access control.</li>
				<li>We must install Apache Beam with the necessary <strong class="source-inline">gcp</strong> libraries. This can be achieved by using the <strong class="source-inline">pip</strong> utility, as follows:<p class="source-code">pip install apache-beam[gcp]</p></li>
				<li>We must create a key for authentication for the GCP service account used for the GCP cloud project. This is not required if we will be running the driver program from a GCP platform such as Cloud Shell. The service account key must be downloaded on your local machine. To make the key available to the Apache Beam SDK, we need to set the path of the key file (a JSON file) to an environment variable called <strong class="source-inline">GOOGLE_APPLICATION_CREDENTIALS</strong>.</li>
			</ol>
			<p>Before discussing how to execute a pipeline on Cloud Dataflow, we will take a quick look at the <a id="_idIndexMarker1084"/>sample word count driver program for this exercise. In this driver program, we will define command-line arguments, very similar to the ones we did in the previous code example (<strong class="source-inline">pipeline4.py</strong>), except we will do the following:</p>
			<ul>
				<li>Instead of setting the <strong class="source-inline">GOOGLE_APPLICATION_CREDENTIALS</strong> environment variable through the operating system, we will set it using our driver program for ease of execution for testing purposes.</li>
				<li>We will upload the <strong class="source-inline">sample.txt</strong> file to Google storage, which is the <strong class="source-inline">gs//muasif/input</strong> directory in our case. We will use the path to this Google storage as the default value of the <strong class="source-inline">input</strong> argument.</li>
			</ul>
			<p>The complete sample code is as follows:</p>
			<p class="source-code"># <strong class="bold">wordcount.py(part 1)</strong>: count words in a text file</p>
			<p class="source-code">import argparse, os, re, apache_beam as beam</p>
			<p class="source-code">from apache_beam.io import ReadFromText, WriteToText</p>
			<p class="source-code">from apache_beam.options.pipeline_options import PipelineOptions</p>
			<p class="source-code">from apache_beam.options.pipeline_options import SetupOptions</p>
			<p class="source-code">def run(argv=None, save_main_session=True):</p>
			<p class="source-code">  os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "some folder/    key.json"</p>
			<p class="source-code">  parser = <strong class="bold">argparse.ArgumentParser</strong>()</p>
			<p class="source-code">  parser.<strong class="bold">add_argument</strong>(</p>
			<p class="source-code">      '--input',</p>
			<p class="source-code">      dest='input',</p>
			<p class="source-code">      default='<strong class="bold">gs://muasif/input/sample.txt'</strong>,</p>
			<p class="source-code">      help='Input file to process.')</p>
			<p class="source-code">  parser.<strong class="bold">add_argument</strong>(</p>
			<p class="source-code">      '--output',</p>
			<p class="source-code">      dest='output',</p>
			<p class="source-code">      default=<strong class="bold">'gs://muasif/input/result</strong>',</p>
			<p class="source-code">      help='Output file to write results to.')</p>
			<p class="source-code">  known_args, pipeline_args = parser.parse_known_args(argv)</p>
			<p>In the <a id="_idIndexMarker1085"/>next step, we will set up extended arguments for the pipeline options to execute our pipeline on the Cloud Dataflow runtime. These arguments are as follows:</p>
			<ul>
				<li>Runtime platform (runner) for pipeline execution (DataflowRunner, in this case)</li>
				<li>GCP Cloud Project ID</li>
				<li>GCP region</li>
				<li>Google storage bucket paths for storing input, output, and temporary files</li>
				<li>Job name for tracking purposes</li>
			</ul>
			<p>Based on these arguments, we will create a pipeline options object to be used for pipeline execution. The sample code for these tasks is as follows:</p>
			<p class="source-code"># <strong class="bold">wordcount.py (part 2)</strong>: under the run method</p>
			<p class="source-code">  pipeline_args.extend([</p>
			<p class="source-code">      '--runner=<strong class="bold">DataflowRunner</strong>',</p>
			<p class="source-code">      '--project=<strong class="bold">word-count-316612</strong>',</p>
			<p class="source-code">      '--region=<strong class="bold">us-central1</strong>',</p>
			<p class="source-code">      '--staging_location=<strong class="bold">gs://muasif/staging</strong>',</p>
			<p class="source-code">      '--temp_location=<strong class="bold">gs://muasif/temp</strong>',</p>
			<p class="source-code">      '--job_name=my-wordcount-job',</p>
			<p class="source-code">  ])</p>
			<p class="source-code">  pipeline_options = <strong class="bold">PipelineOption</strong>s(pipeline_args)</p>
			<p class="source-code">  pipeline_options.view_as(SetupOptions).\</p>
			<p class="source-code">      save_main_session = save_main_session</p>
			<p>Finally, we will implement a pipeline with the pipeline options that have already been defined <a id="_idIndexMarker1086"/>and add our PTransform operations. For this code example, we added an extra PTransform operation to build a pair of each word with <strong class="source-inline">1</strong>. In the follow-up PTransform operation, we grouped the pairs and applied the <strong class="source-inline">sum</strong> operation to count their frequency. This gives us the count of each word in the input text file:</p>
			<p class="source-code"># <strong class="bold">wordcount.py (part 3)</strong>: under the run method</p>
			<p class="source-code">  with beam.Pipeline(options=pipeline_options) as p:</p>
			<p class="source-code">    lines = p | <strong class="bold">ReadFromText</strong>(known_args.input)</p>
			<p class="source-code">    # Count the occurrences of each word.</p>
			<p class="source-code">    counts = (</p>
			<p class="source-code">        lines</p>
			<p class="source-code">        | 'Split words' &gt;&gt; (</p>
			<p class="source-code">            beam.<strong class="bold">FlatMap</strong>(</p>
			<p class="source-code">                lambda x: re.<strong class="bold">findall</strong>(r'[A-Za-z\']+', x)).</p>
			<p class="source-code">                with_output_types(str))</p>
			<p class="source-code">        | 'Pair with 1' &gt;&gt; beam.<strong class="bold">Map</strong>(lambda x: (x, 1))</p>
			<p class="source-code">        | 'Group &amp; Sum' &gt;&gt; beam.<strong class="bold">CombinePerKey</strong>(sum))</p>
			<p class="source-code">    def <strong class="bold">format_result</strong>(word_count):</p>
			<p class="source-code">      (word, count) = word_count</p>
			<p class="source-code">      return '%s: %s' % (word, count)</p>
			<p class="source-code">    output = counts | 'Format' &gt;&gt; beam.<strong class="bold">Map</strong>(<strong class="bold">format_result</strong>)</p>
			<p class="source-code">    output | <strong class="bold">WriteToText</strong>(known_args.output)</p>
			<p>We set <a id="_idIndexMarker1087"/>default values for each argument within the driver program. This means that we can execute the program directly with the <strong class="source-inline">python wordcount.py</strong> command or we can use the following command to pass the arguments through the CLI:</p>
			<p class="source-code">python wordcount.py \</p>
			<p class="source-code">    --project word-count-316612 \</p>
			<p class="source-code">    --region us-central1 \</p>
			<p class="source-code">    --input gs://muasif/input/sample.txt \</p>
			<p class="source-code">    --output gs://muasif/output/results \</p>
			<p class="source-code">    --runner DataflowRunner \</p>
			<p class="source-code">    --temp_location gs://muasif/temp \</p>
			<p class="source-code">    --staging_location gs://muasif/staging</p>
			<p>The output file will contain the results, along with the count of each word in the file. GCP Cloud Dataflow provides additional tools for monitoring the progress of submitted jobs and <a id="_idIndexMarker1088"/>for understanding the resource utilization to perform the job. The following screenshot of the GCP console shows a list of jobs that have been submitted to Cloud Dataflow. The summary view shows their statuses and a few key metrics: </p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B17189_09_06.jpg" alt="Figure 9.6 – Cloud Dataflow jobs summary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Cloud Dataflow jobs summary</p>
			<p>We can navigate to the detailed job view (by clicking any job name), as shown in the following screenshot. This view shows the job and environment details on the right-hand side and a progress summary of the different PTransforms we defined for our pipeline. When the job is running, the status of each PTransform operation is updated in real time, as shown here: </p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B17189_09_07.jpg" alt="Figure 9.7 – Cloud Dataflow job detail view with a flowchart and metrics&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Cloud Dataflow job detail view with a flowchart and metrics</p>
			<p>A very important point to notice is that the different PTransform operations are named according <a id="_idIndexMarker1089"/>to the strings we used with the <strong class="source-inline">&gt;&gt;</strong> operator. This is helpful for visualizing the operations conveniently. This concludes our discussion on building and deploying a pipeline for Google Dataflow. In comparison to Apache Spark, Apache Beam provides more flexibility for parallel and distributed data processing. With the availability of cloud data processing options, we can focus entirely on modeling the pipelines and leave the job of executing pipelines to cloud providers. </p>
			<p>As we mentioned earlier, Amazon offers a similar service (AWS Kinesis) for deploying and executing pipelines. AWS Kinesis is more focused on data streams for real-time data. Like AWS Beanstalk, AWS Kinesis does not require that we set up a project as a prerequisite. The user <a id="_idIndexMarker1090"/>guides for data processing using AWS Kinesis are available at <a href="https://docs.aws.amazon.com/kinesis/">https://docs.aws.amazon.com/kinesis/</a>.</p>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor259"/>Summary</h1>
			<p>In this chapter, we discussed the role of Python for developing applications for cloud deployment in general, as well as the use of Apache Beam with Python for deploying data processing pipelines on Google Cloud Dataflow. We started this chapter by comparing three main public cloud providers in terms of what they offer for developing, building, and deploying different types of applications. We also compared the options that are available from each cloud provider for runtime environments. We learned that each cloud provider offers a variety of runtime engines based on the application or program. For example, we have separate runtime engines for classic web applications, container-based applications, and serverless functions. To explore the effectiveness of Python for cloud-native web applications, we built a sample application and learned how to deploy such an application on Google App Engine by using Cloud SDK. In the last section, we extended our discussion of the data process, which we started in the previous chapter. We introduced a new modeling approach for data processing (pipelines) using Apache Beam. Once we learned how to build pipelines with a few code examples, we extended our discussion to how to build pipelines for Cloud Dataflow deployment. </p>
			<p>This chapter provided a comparative analysis of public cloud service offerings. This was followed by hands-on knowledge of building web applications and data processing applications for the cloud. The code examples included in this chapter will enable you to start creating cloud projects and writing code for Apache Beam. This knowledge is important for anyone who wants to solve their big data problems using cloud-based data processing services. </p>
			<p>In the next chapter, we will explore the power of Python for developing web applications using the Flask and Django frameworks. </p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor260"/>Questions</h1>
			<ol>
				<li value="1">How is AWS Beanstalk different from AWS App Runner?</li>
				<li>What is the GCP Cloud Function service?</li>
				<li>What services from GCP are available for data processing?</li>
				<li>What is an Apache Beam pipeline?</li>
				<li>What is the role of PCollection in a data processing pipeline?</li>
			</ol>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor261"/>Further reading</h1>
			<ul>
				<li><em class="italic">Flask Web Development</em>, by Miguel Grinberg.</li>
				<li><em class="italic">Advanced Guide to Python 3 Programming</em>, by John Hunt.</li>
				<li><em class="italic">Apache Beam: A Complete Guide</em>, by Gerardus Blokdyk.</li>
				<li><em class="italic">Google Cloud Platform for Developers</em>, by Ted Hunter, Steven Porter.</li>
				<li><em class="italic">Google Cloud Dataflow documentation</em> is available at<em class="italic"> </em><a href="https://cloud.google.com/dataflow/docs">https://cloud.google.com/dataflow/docs</a>.</li>
				<li><em class="italic">AWS Elastic Beanstalk documentation</em> is available at <a href="https://docs.aws.amazon.com/elastic-beanstalk">https://docs.aws.amazon.com/elastic-beanstalk</a>.</li>
				<li><em class="italic">Azure App Service documentation</em> is available at <a href="https://docs.microsoft.com/en-us/azure/app-service/">https://docs.microsoft.com/en-us/azure/app-service/</a>.</li>
				<li><em class="italic">AWS Kinesis documentation</em> is available at <a href="https://docs.aws.amazon.com/kinesis/">https://docs.aws.amazon.com/kinesis/</a>.</li>
			</ul>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor262"/>Answers</h1>
			<ol>
				<li value="1">AWS Beanstalk is a general-purpose PaaS offering for deploying web applications, whereas AWS App Runner is a fully managed service for deploying container-based web applications.</li>
				<li>GCP Cloud Function is a serverless, event-driven service for executing a program. The specified event can be triggered from another GCP service or through an HTTP request.</li>
				<li>Cloud Dataflow and Cloud Dataproc are two popular services for data processing offered by GCP.</li>
				<li>An Apache Beam pipeline is a set of actions that have been defined to load the data, transform the data from one form into another, and write the data to a destination.</li>
				<li>PCollection is like an RDD in Apache Spark that holds data elements. In pipeline data processing, a typical PTransform operation takes one or more PCollection objects as input and produces the results as one or more PCollection objects.</li>
			</ol>
		</div>
	</body></html>