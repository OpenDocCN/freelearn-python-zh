- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Algorithm Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of this chapter is to understand the principles of designing algorithms,
    and the importance of analyzing algorithms in solving real-world problems. Given
    input data, an algorithm is a step-by-step set of instructions that should be
    executed in sequence to solve a given problem.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will also learn how to compare different algorithms and
    determine the best algorithm for the given use-case. There can be many possible
    correct solutions for a given problem, for example, we can have several algorithms
    for the problem of sorting *n* numeric values. So, there is no one algorithm to
    solve any real-world problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance analysis of an algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asymptotic notation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amortized analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing complexity classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the running time complexity of an algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An algorithm is a sequence of steps that should be followed in order to complete
    a given task/problem.
  prefs: []
  type: TYPE_NORMAL
- en: It is a well-defined procedure that takes input data, processes it, and produces
    the desired output. A representation of this is shown in *Figure 2.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Introduction to algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summarized below are some important reasons for studying algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Essential for computer science and engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important in many other domains (such as computational biology, economics, ecology,
    communications, ecology, physics, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They play a role in technology innovation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They improve problem-solving and analytical thinking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two aspects that are of prime importance in solving a given problem.
    Firstly, we need an efficient mechanism to store, manage, and retrieve data, which
    is required to solve a problem (this comes under data structures); secondly, we
    require an efficient algorithm that is a finite set of instructions to solve that
    problem. Thus, the study of data structures and algorithms is key to solving any
    problem using computer programs. An efficient algorithm should have the following
    characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: It should be as specific as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should have each instruction properly defined
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There should not be any ambiguous instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the instructions of the algorithm should be executable in a finite amount
    of time and in a finite number of steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should have clear input and output to solve the problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each instruction of the algorithm should be integral in solving the given problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider an example of an algorithm (an analogy) to complete a task in our
    daily lives; let us take the example of preparing a cup of tea. The algorithm
    to prepare a cup of tea can include the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Pour water into the pan
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Put the pan on the stove and light the stove
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add crushed ginger to the warming water
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add tea leaves to the pan
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add milk
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When it starts boiling, add sugar to it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After 2-3 minutes, the tea can be served
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The above procedure is one of the possible ways to prepare tea. In the same
    way, the solution to a real-world problem can be converted into an algorithm,
    which can be developed into computer software using a programming language. Since
    it is possible to have several solutions for a given problem, it should be as
    efficient as possible when it is to be implemented using software. Given a problem,
    there may be more than one correct algorithm, defined as the one that produces
    exactly the desired output for all valid input values. The costs of executing
    different algorithms may be different; it may be measured in terms of the time
    required to run the algorithm on a computer system and the memory space required
    for it.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are primarily two things that one should keep in mind while designing
    an efficient algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm should be correct and should produce the results as expected for
    all valid input values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm should be optimal in the sense that it should be executed on the
    computer within the desired time limit, in line with an optimal memory space requirement
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performance analysis of the algorithm is very important for deciding the best
    solution for a given problem. If the performance of an algorithm is within the
    desired time and space requirements, it is optimal. One of the most popular and
    common methods of estimating the performance of an algorithm is through analyzing
    its complexity. Analysis of the algorithm helps us to determine which one is most
    efficient in terms of the time and space consumed.
  prefs: []
  type: TYPE_NORMAL
- en: Performance analysis of an algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The performance of an algorithm is generally measured by the size of its input
    data, *n*, and the time and the memory space used by the algorithm. The time required
    is measured by the key operations to be performed by the algorithm (such as comparison
    operations), where key operations are instructions that take a significant amount
    of time during execution. Whereas the space requirement of an algorithm is measured
    by the memory needed to store the variables, constants, and instructions during
    the execution of the program.
  prefs: []
  type: TYPE_NORMAL
- en: Time complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The time complexity of the algorithm is the amount of time that an algorithm
    will take to execute on a computer system to produce the output. The aim of analyzing
    the time complexity of the algorithm is to determine, for a given problem and
    more than one algorithm, which one of the algorithms is the most efficient with
    respect to the time required to execute. The running time required by an algorithm
    depends on the input size; as the input size, *n*, increases, the runtime also
    increases. Input size is measured as the number of items in the input, for example,
    the input size for a sorting algorithm will be the number of items in the input.
    So, a sorting algorithm will have an increased runtime to sort a list of input
    size 5,000 than that of a list of input size 50.
  prefs: []
  type: TYPE_NORMAL
- en: The runtime of an algorithm for a specific input depends on the key operations
    to be executed in the algorithm. For example, the key operation for a sorting
    algorithm is a comparison operation that will take up most of the runtime, compared
    to assignment or any other operation. Ideally, these key operations should not
    depend upon the hardware, the operating system, or the programming language being
    used to implement the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'A constant amount of time is required to execute each line of code; however,
    each line may take a different amount of time to execute. In order to understand
    the running time required for an algorithm, consider the below code as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Code** | **Time required (Cost)** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Here, in statement 1 of the above example, if the condition is true then `"data"`
    will be printed, and if the condition is not true then the `for` loop will execute
    `n` times. The time required by the algorithm depends on the time required for
    each statement, and how many times a statement is executed. The running time of
    the algorithm is the sum of time required by all the statements. For the above
    code, assume statement 1 takes `c1` amount of time, statement 2 takes `c2` amount
    of time, and so on. So, if the *i*^(th) statement takes a constant amount of time
    *c*[i]and if the *i*^(th) statement is executed `n` times, then it will take *c*[i]*n*
    time. The total running time `T(n)` of the algorithm for a given value of *n*
    (assuming the value of *n* is not zero or three) will be as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '*T*(*n*) = *c*[1] + *c*[3] + *c*[4] x *n* + *c*[5] x *n*'
  prefs: []
  type: TYPE_NORMAL
- en: If the value of `n` is equal to zero or three, then the time required by the
    algorithm will be as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '*T*(*n*) = *c*[1] + *c*[2]'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the running time required for an algorithm also depends upon what
    input is given in addition to the size of the input given. For the given example,
    the best case will be when the input is either zero or three, and in that case,
    the running time of the algorithm will be constant. In the worst case, the value
    of `n` is not equal to zero or three, then, the running time of the algorithm
    can be represented as *a* x *n* + *b*. Here, the values of `a` and `b` are constants
    that depend on the statement costs, and the constant times are not considered
    in the final time complexity. In the worst case, the runtime required by the algorithm
    is a linear function of *n*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider another example, linear search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output in this instance will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The **worst-case running time** of the algorithm is the upper-bound complexity;
    it is the maximum runtime required for an algorithm to execute for any given input.
    The worst-case time complexity is very useful in that it guarantees that for any
    input data, the runtime required will not take more time as compared to the worst-case
    running time. For example, in the linear search problem, the worst case occurs
    when the element to be searched is found in the last comparison or not found in
    the list. In this case, the running time required will linearly depend upon the
    length of the list, whereas, in the best case, the search element will be found
    in the first comparison.
  prefs: []
  type: TYPE_NORMAL
- en: The **average-case running time** is the average running time required for an
    algorithm to execute. In this analysis, we compute the average over the running
    time for all possible input values. Generally, probabilistic analysis is used
    to analyze the average-case running time of an algorithm, which is computed by
    averaging the cost over the distribution of all the possible inputs. For example,
    in the linear search, the number of comparisons at all positions would be 1 if
    the element to be searched was found at the 0^(th) index; and similarly, the number
    of comparisons would be 2, 3, and so forth, up to `n`, respectively, for elements
    found at the `1, 2, 3, … (`*n*`-1)` index positions. Thus, the average-case running
    time will be as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_001.png)'
  prefs: []
  type: TYPE_IMG
- en: For average-case, the running time required is also linearly dependent upon
    the value of *n*. However, in most real-world applications, worst-case analysis
    is mostly used, since it gives a guarantee that the running time will not take
    any longer than the worst-case running time of the algorithm for any input value.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best-case running time** is the minimum time needed for an algorithm to run;
    it is the lower bound on the running time required for an algorithm; in the example
    above, the input data is organized in such a way that it takes its minimum running
    time to execute the given algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Space complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The space complexity of the algorithm estimates the memory requirement to execute
    it on a computer to produce the output as a function of input data. The memory
    space requirement of an algorithm is one of the criteria used to decide how efficient
    it is. While executing the algorithm on the computer system, storage of the input
    is required, along with intermediate and temporary data in data structures, which
    are stored in the memory of the computer. In order to write a programming solution
    for any problem, some memory is required for storing variables, program instructions,
    and executing the program on the computer. The space complexity of an algorithm
    is the amount of memory required for executing and producing the result.
  prefs: []
  type: TYPE_NORMAL
- en: For computing the space complexity, consider the following example, in which,
    given a list of integer values, the function returns the square value of the corresponding
    integer number.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the above code, the algorithm will require allocating memory for the number
    of items in the input list. Say the number of elements in the input is `n`, then
    the space requirement increases with the input size, therefore, the space complexity
    of the algorithm becomes `O(n)`.
  prefs: []
  type: TYPE_NORMAL
- en: Given two algorithms to solve a given problem, with all other requirements being
    equal, then the algorithm that requires less memory can be considered more efficient.
    For example, suppose there are two search algorithms, one has `O(n)` and another
    algorithm has `O(nlogn)` space complexity. The first algorithm is the better algorithm
    as compared to the second with respect to the space requirements. Space complexity
    analysis is important to understand the efficiency of an algorithm, especially
    for applications where the memory space requirement is high.
  prefs: []
  type: TYPE_NORMAL
- en: When the input size becomes large enough, the order of growth also becomes important.
    In such situations, we study the asymptotic efficiency of algorithms. Generally,
    algorithms that are asymptotically efficient are considered to be better algorithms
    for large-size inputs. In the next section, we will study asymptotic notation.
  prefs: []
  type: TYPE_NORMAL
- en: Asymptotic notation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To analyze the time complexity of an algorithm, the rate of growth (order of
    growth) is very important when the input size is large. When the input size becomes
    large, we only consider the higher-order terms and ignore the insignificant terms.
    In asymptotic analysis, we analyze the efficiency of algorithms for large input
    sizes considering the higher order of growth and ignoring the multiplicative constants
    and lower-order terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'We compare two algorithms with respect to input size rather than the actual
    runtime and measure how the time taken increases with an increased input size.
    The algorithm which is more efficient asymptotically is generally considered a
    better algorithm as compared to the other algorithm. The following asymptotic
    notations are commonly used to calculate the running time complexity of an algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'θ notation: It denotes the worst-case running time complexity with a tight
    bound.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ο notation: It denotes the worst-case running time complexity with an upper
    bound, which ensures that the function never grows faster than the upper bound.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ω notation: It denotes the lower bound of an algorithm’s running time. It measures
    the best amount of time to execute the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theta notation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following function characterizes the worst-case running time for the first
    example discussed in the *Time complexity* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '*T*(*n*) = *c*[1] + *c*[3] x *n* + *c*[5] x *n*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, for a large input size, the worst-case running time will be `ϴ(n)` (pronounced
    as theta of `n`). We usually consider one algorithm to be more efficient than
    another if its worst-case running time has a lower order of growth. Due to constant
    factors and lower-order terms, an algorithm whose running time has a higher order
    of growth might take less time for small inputs than an algorithm whose running
    time has a lower order of growth. For example, once the input size `n` becomes
    large enough, the merge sort algorithm performs better as compared to insertion
    sort with worst-case running times of `ϴ(logn)` and `` ϴ(n²`)` `` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Theta notation (`ϴ`) denotes the worst-case running time for an algorithm with
    a tight bound. For a given function *F*(*n*), the asymptotic worst-case running
    time complexity can be defined as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'iff there exists constants *n*[0], *c*[1], and *c*[2] such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_003.png)'
  prefs: []
  type: TYPE_IMG
- en: The function *T*(*n*) belongs to a set of functions *ϴ*(*F*(*n*)) if there exists
    positive constants *c*[1] and c[2] such that the value of *T*(*n*) always lies
    in between *c*[1]*F*(*n*) and *c*[2]*F*(*n*) for all large values of *n*. If this
    condition is true, then we say *F*(*n*) is asymptotically tight bound for *T*(*n*).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2.2* shows the graphic example of the theta notation (*ϴ*). It can
    be observed from the figure that the value of *T*(*n*) always lies in between
    *c*[1]*F*(*n*) and *c*[2]*F*(*n*) for values of n greater than *n*[0].'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Graphical example of theta notation (ϴ)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider an example to understand what should be the worst case running
    time complexity with the formal definition of theta notation for a given function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_004.png)'
  prefs: []
  type: TYPE_IMG
- en: In order to determine the time complexity with the *ϴ* notation definition,
    we have to first identify the constants c[1], c[2], n[0] such that
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dividing by n² will produce:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_006.png)'
  prefs: []
  type: TYPE_IMG
- en: By choosing c[1] = 1, c[2] = 2, and n[0] = 1, the following condition can satisfy
    the definition of theta notation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Consider another example to find out the asymptotically tight bound (`ϴ`) for
    another function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to identify the constants c[1], c[2], and n[0], such that they satisfy
    the condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By choosing c[1] = 1/5, c[2] =1, and n[0] = 1, the following condition can
    satisfy the definition of theta notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_011.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17217_02_012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_013.png)'
  prefs: []
  type: TYPE_IMG
- en: It shows that the given function has the complexity of `ϴ`(n²) as per the definition
    of theta notation.
  prefs: []
  type: TYPE_NORMAL
- en: So, the theta notation provides a tight bound for the time complexity of an
    algorithm. In the next section, we will discuss Big O notation.
  prefs: []
  type: TYPE_NORMAL
- en: Big O notation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have seen that the theta notation is asymptotically bound from the upper
    and lower sides of the function whereas the Big O notation characterizes the worst-case
    running time complexity, which is only the asymptotic upper bound of the function.
    Big O notation is defined as follows. Given a function *F*(*n*), the *T*(*n*)
    is a Big O of function *F*(*n*), and we define this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*T*(*n*) = O(*F*(*n*))'
  prefs: []
  type: TYPE_NORMAL
- en: 'iff there exists constants n[0] and *c* such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_014.png)'
  prefs: []
  type: TYPE_IMG
- en: In Big O notation, a constant multiple of *F*(*n*) is an asymptotic upper bound
    on *T*(*n*), and the positive constants n[0] and *c* should be in such a way that
    all values of `n` greater than n[0] always lie on or below function *c***F*(*n*).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we only care what happens at higher values of *n*. The variable n[0]
    represents the threshold below which the rate of growth is not important. The
    plot shown in *Figure 2.3* shows a graphical representation of function *T*(*n*)
    with a varying value of `n`. We can see that *T*(*n*) = n² + 500 = O(n²), with
    *c* = 2 and n[0] being approximately `23`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: Graphical example of O notation'
  prefs: []
  type: TYPE_NORMAL
- en: In O notation, *O*(*F*(*n*)) is really a set of functions that includes all
    functions with the same or smaller rates of growth than *F*(*n*). For example,
    *O*(n²) also includes *O*(*n*), *O*(log *n*), and so on. However, Big O notation
    should characterize a function as closely as possible, for example, it is true
    that function *F*(*n*) = 2n³+2n²+5 is O(n⁴), however, it is more accurate that
    *F*(*n*) is *O*(n³).
  prefs: []
  type: TYPE_NORMAL
- en: In the following table, we list the most common growth rates in order from lowest
    to highest.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Time Complexity** | **Name** |'
  prefs: []
  type: TYPE_TB
- en: '| `O(1)` | Constant |'
  prefs: []
  type: TYPE_TB
- en: '| `O(logn)` | Logarithmic |'
  prefs: []
  type: TYPE_TB
- en: '| `O(n)` | Linear |'
  prefs: []
  type: TYPE_TB
- en: '| `O(nlogn)` | Linear-logarithmic |'
  prefs: []
  type: TYPE_TB
- en: '| `O(n2)` | Quadratic |'
  prefs: []
  type: TYPE_TB
- en: '| `O(n3)` | Cubic |'
  prefs: []
  type: TYPE_TB
- en: '| `O(2n)` | Exponential |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.1: Runtime complexity of different functions'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Big O notation, the running time of an algorithm can be computed by analyzing
    the structure of the algorithm. For example, a double nested loop in an algorithm
    will have an upper bound on the worst-case running time of O(n²), since the values
    of `i` and `j` will be at most *n*, and both the loops will run n² times as shown
    in the below example code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us consider a few examples in order to compute the upper bound of a function
    using the O-notation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the upper bound for the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T(n) = 2n + 7
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Solution**: Using O notation, the condition for the upper bound is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*T*(*n*) <= *c* * *F*(*n*)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This condition holds true for all values of n > 7 and *c*=3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*2n + 7 <= 3n* This is true for all values of n, with *c*=3, n[0]=7'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*T(n) = 2n+7 = O(n)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Find F(n) for functions *T(n) =2n+5 such that T(n) = O(F(n))*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Solution**: Using O notation, the condition for the upper bound is *T(n)
    <=c * F(n)*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since, 2n+5 ≤ 3n, for all n ≥ 5.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The condition is true for *c*=3, n[0]=5.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*2n + 5 ≤ O(n)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*F(n) = n*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Find F(n) for the function *T(n) = n*² *+n, such that T(n) = O(F(n))*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Solution**: Using O notation, since, *n*²*+ n ≤ 2n*²*, for all n ≥ 1 (with
    c = 2, n*[0]*=2)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: n²+ n ≤ O(n²)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: F(n) = n²
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Prove that f(n) =2n³ - 6n ≠ O(n²).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Solution**: Clearly, 2n³-6n ≥ n², for n ≥ 2\. So it cannot be true that 2n³
    - 6n ≠ O(n²).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Prove that: 20n²+2n+5 = O(n²).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Solution**: It is clear that:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 20n² + 2n + 5 <= 21n² for all n > 4 (let c = 21 and n[0] = 4)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: n² > 2n + 5 for all n > 4
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So, the complexity is O(n²).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So, Big-O notation provides an upper bound on a function, which ensures that
    the function never grows faster than the upper-bounded function. In the next section,
    we will discuss Omega notation.
  prefs: []
  type: TYPE_NORMAL
- en: Omega notation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Omega notation (Ω) describes an asymptotic lower bound on algorithms, similar
    to the way in which Big O notation describes an upper bound. Omega notation computes
    the best-case runtime complexity of the algorithm. The Ω notation (*Ω*(*F*(*n*))
    is pronounced as omega of F of n), is a set of functions in such a way that there
    are positive constants n[0] and c such that for all values of n greater than n[0],
    *T*(*n*) always lies on or above a function to *c***F*(*n*).
  prefs: []
  type: TYPE_NORMAL
- en: '*T*(*n*) = *Ω* (*F*(*n*))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Iff constants n[0] and c are present, then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_015.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.4* shows the graphical representation of the omega (Ω) notation.
    It can be observed from the figure that the value of *T*(*n*) always lies above
    cF(n) for values of *n* greater than n[0].'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: The graphical representation of Ω notation'
  prefs: []
  type: TYPE_NORMAL
- en: If the running time of an algorithm is *Ω*(*F*(*n*)), it means that the running
    time of the algorithm is at least a constant multiplier of *F*(*n*) for sufficiently
    large values of input size (n). The Ω notation gives a lower bound on the best-case
    running time complexity of a given algorithm. It means that the running time for
    a given algorithm will be at least *F*(*n*) without depending upon the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to understand the Ω notation and how to compute the lower bound on
    the best-case runtime complexity of an algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Find *F*(*n*) for the function *T*(*n*) =2n² +3 such that *T*(*n*) = *Ω*(*F*(*n*)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Solution**: Using the Ω notation, the condition for the lower bound is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c*F(n) ≤ T(n)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This condition holds true for all values of n greater than 0, and c=1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 0 ≤ cn² ≤ 2n² +3, for all n ≥ 0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2n² +3 = Ω(n²)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: F(n)=n²
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Find the lower bound for T(n) = 3n².
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Solution**: Using the Ω notation, the condition for the lower bound is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c*F(n) ≤ T(n)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Consider 0 ≤ cn² ≤ 3n². The condition for Ω notation holds true for all values
    of n greater than 1, and c=2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: cn² ≤ 3n² (for c = 2 and n[0] = 1)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3n² = Ω(n²)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Prove that 3n = Ω(n).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Solution**: Using the Ω notation, the condition for the lower bound is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c*F(n) ≤ T(n)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Consider 0 ≤ c*n≤ 3n. The condition for Ω notation holds true for all values
    of n greater than 1, and c=1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: cn² ≤ 3n² ( for c = 2 and n[0] = 1)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3n = Ω(n)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Ω notation is used to describe that at least a certain amount of running
    time will be taken by an algorithm for a large input size. In the next section,
    we will discuss amortized analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Amortized analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the amortized analysis of an algorithm, we average the time required to execute
    a sequence of operations with all the operations of the algorithm. This is called
    amortized analysis. Amortized analysis is important when we are not interested
    in the time complexity of individual operations but we are interested in the average
    runtime of sequences of operations. In an algorithm, each operation requires a
    different amount of time to execute. Certain operations require significant amounts
    of time and resources while some operations are not costly at all. In amortized
    analysis, we analyze algorithms considering both the costly and less costly operations
    in order to analyze all the sequences of operations. So, an amortized analysis
    is the average performance of each operation in the worst case considering the
    cost of the complete sequence of all the operations. Amortized analysis is different
    from average-case analysis since the distribution of the input values is not considered.
    An amortized analysis gives the average performance of each operation in the worst
    case.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three commonly used methods for amortized analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aggregate analysis**. In aggregate analysis, the amortized cost is the average
    cost of all the sequences of operations. For a given sequence of n operations,
    the amortized cost of each operation can be computed by dividing the upper bound
    on the total cost of n operations with n.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The accounting method**. In the accounting method, we assign an amortized
    cost to each operation, which may be different than their actual cost. In this,
    we impose an extra charge on early operations in the sequence and save “credit
    cost,” which is used to pay expensive operations later in the sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The potential method**. The potential method is like the accounting method.
    We determine the amortized cost of each operation and impose an extra charge to
    early operations that may be used later in the sequence. Unlike the accounting
    method, the potential method accumulates the overcharged credit as “potential
    energy” of the data structure as a whole instead of storing credit for individual
    operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we had an overview of amortized analysis. Now we will discuss
    how to compute the complexity of different functions with examples in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Composing complexity classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normally, we need to find the total running time of complex operations and algorithms.
    It turns out that we can combine the complexity classes of simple operations to
    find the complexity class of more complex, combined operations. The goal is to
    analyze the combined statements in a function or method to understand the total
    time complexity of executing several operations. The simplest way to combine two
    complexity classes is to add them. This occurs when we have two sequential operations.
    For example, consider the two operations of inserting an element into a list and
    then sorting that list. Assuming that inserting an item occurs in O(n) time, and
    sorting in O(nlogn) time, then we can write the total time complexity as O(n +
    nlogn); that is, we bring the two functions inside the O(…), as per Big O computation.
    Considering only the highest-order term, the final worst-case complexity becomes
    O(nlogn).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we repeat an operation, for example in a `while` loop, then we multiply
    the complexity class by the number of times the operation is carried out. If an
    operation with time complexity O(*f*(*n*)) is repeated O(*n*) times, then we multiply
    the two complexities: O(*f*(*n*) * O(*n*)) = O(*nf*(*n*)). For example, suppose
    the function *f*(*n*) has a time complexity of O(*n*²) and it is executed *n*
    times in a `for` loop, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The time complexity of the above code then becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: O(*n*²) x O(*n*) = O(*n* x *n*²) = O(*n*³)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are multiplying the time complexity of the inner function by the number
    of times this function executes. The runtime of a loop is at most the runtime
    of the statements inside the loop multiplied by the number of iterations. A single
    nested loop, that is, one loop nested inside another loop, will run *n*² times,
    such as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If each execution of the statements takes constant time, *c*, i.e. O(1), executed
    *n* x *n* times, we can express the running time as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*c* x *n* x *n* = *c* x *n*² = O(*n*²)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For consecutive statements within nested loops, we add the time complexities
    of each statement and multiply by the number of times the statement is executed—as
    in the following code, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be written as: *c*¹*n + c*² **n*² *= O(n*²*)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define (base 2) logarithmic complexity, reducing the size of the problem
    by half, in constant time. For example, consider the following snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that `i` is doubling in each iteration. If we run this code with n =
    10, we see that it prints out four numbers: 2, 4, 8, and 16\. If we double n,
    we see it prints out five numbers. With each subsequent doubling of n, the number
    of iterations is only increased by 1\. If we assume that the loop has k iterations,
    then the value of n will be 2^n. We can write this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17217_02_016.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17217_02_017.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17217_02_018.png)'
  prefs: []
  type: TYPE_IMG
- en: From this, the worst-case runtime complexity of the above code is equal to O(log(n)).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have seen examples to compute the running time complexity
    of different functions. In the next section, we will take examples to understand
    how to compute the running time complexity of an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the running time complexity of an algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To analyze an algorithm with respect to the best-, worst-, and average-case
    runtime of the algorithm, it is not always possible to compute these for every
    given function or algorithm. However, it is always important to know the upper-bound
    worst-case runtime complexity of an algorithm in practical situations; therefore,
    we focus on computing the upper-bound Big O notation to compute the worst-case
    runtime complexity of an algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the worst-case runtime complexity of the following Python snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Solution**: The runtime for a loop, in general, takes the time taken by all
    statements in the loop, multiplied by the number of iterations. Here, total runtime
    is defined as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: T(n) = constant time (c) * n = c*n = O(n)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Find the time complexity of the following Python snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Solution**: O(n²). The `print` statement will be executed n² times, n times
    for the inner loop, and, for each iteration of the outer loop, the inner loop
    will be executed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Find the time complexity of the following Python snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Solution**: The worst-case complexity will be O(n) since the `print` statement
    will run *n* times because the inner loop executes only once due to a `break`
    statement.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Find the time complexity of the following Python snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Solution**: Here, the `print` statements will execute *n* times in the first
    loop and *n*² times for the second nested loop. Here, the total time required
    is defined as the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: T(n) = constant time (c[1]) * n + c[2]*n*n
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c[1] n + c[2] n² = O(n²)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Find the time complexity of the following Python snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Solution**: O(n). Here, the worst-case runtime complexity will be the time
    required for the execution of all the statements; that is, the time required for
    the execution of the `if-else` conditions, and the `for` loop. The time required
    is defined as the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: T(n) = c[1] + c[2] n = O(n)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Find the time complexity of the following Python snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Solution**: O(![](img/Eqn_02_notation4.png)). The loop will terminate based
    on the value of `i`; the loop will iterate based on the condition:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/Eqn_02_notation5.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: T(n) = O(![](img/Eqn_02_notation41.png))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Find the time complexity of the following Python snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Solution**: Here, the outer loop will execute n/2 times, the middle loop
    will also run n/2 times, and the innermost loop will run for log(n) time. So,
    the total running time complexity will be O(n*n*logn):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: O(n²logn)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have looked at an overview of algorithm design. The study
    of algorithms is important because it trains us to think very specifically about
    certain problems. It is conducive to increasing our problem-solving abilities
    by isolating the components of a problem and defining the relationships between
    them. In this chapter, we discussed different methods for analyzing algorithms
    and comparing algorithms. We also discussed asymptotic notations, namely: Big
    Ο, Ω, and θ notation.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss algorithm design techniques and strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Find the time complexity of the following Python snippets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers: [https://packt.link/MEvK4](https://packt.link/MEvK4)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1421249772551223062.png)'
  prefs: []
  type: TYPE_IMG
