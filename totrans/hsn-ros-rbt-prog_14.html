<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Applying Machine Learning in Robotics</h1>
                </header>
            
            <article>
                
<p>This chapter provides a hands-on introduction to <strong>machine learning </strong>(<strong><span>ML</span></strong>) in robotics. Although we assume that you have not yet worked in such a field, it will be helpful to have some background in statistics and data analytics. In any case, this chapter intends to be a gentle introduction to the topic, favoring intuition instead of complex mathematical formulations, and putting the focus on understanding the common concepts used in the field of ML.</p>
<p>Throughout this chapter, we will devote the discussion to such concepts by providing specific examples of robots. This is somewhat original because most references and books on ML give examples oriented to data science. Hence, as you become more familiar with robotics, it should be easier for you to understand the concepts this way.</p>
<p>With the explanations about deep learning, you will understand how crucial this technique is for the robot to acquire knowledge of its surroundings through the processing of raw data coming from the robot's camera (2D and/or 3D) and specific distance sensors. With the specific example of object recognition explained in this chapter, you will learn how raw image data is processed in order to build robot's knowledge in the robot, making it capable to take smart actions.</p>
<p><span>The following topics will be covered in this chapter:</span></p>
<ul>
<li>Setting up the system for TensorFlow</li>
<li style="font-weight: 400">How ML is being applied in Robotics</li>
<li style="font-weight: 400">The ML pipeline</li>
<li style="font-weight: 400">A methodology to programmatically apply ML in robotics</li>
<li style="font-weight: 400">Deep learning applied to robotics— computer vision</li>
</ul>
<p><span>The concrete application we will do for GoPiGo3 deals with computer vision, the most common perception task in robotics. Equipped with this capability, the robot should be aware of the objects around itself, making it capable to interact with them. As a result of this chapter, we expect you to develop the basic insight of when and how to apply deep learning in robotics.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>For the examples in this chapter, we will use <strong>TensorFlow</strong> (<a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>), the ML framework open-sourced by Google in 2015, which has become the big brother in the data science community because of all of the people involved as active developers or end users.</p>
<p>The main TensorFlow API is developed in Python and is the one we are going to use. To install it, we need to have the well-known <kbd>pip</kbd> Python package manager in our system. Even though it comes bundled with the Ubuntu OS, we provide the instructions for installing it. Later, we will cover the TensorFlow installation process.</p>
<p>Let's first provide the path for the code of this chapter, and then describe the step-by-step procedure to configure your laptop with TensorFlow.</p>
<p class="mce-root"><span>In this chapter, we will make use of the code located in the</span> <kbd>Chapter10_Deep_Learning_</kbd> <span>fold</span><span>er at</span><span> </span><a href="https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter10_Deep_Learning_">https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter10_Deep_Learning_</a><span>.</span></p>
<p>Copy its files to the <strong>ROS</strong>(short for<strong> <span>Robot Operating System</span></strong><span>)</span> workspace to have them available and leave the rest outside the <kbd>src</kbd> folder:</p>
<pre><strong>$ cp -R ~/Hands-On-ROS-for-Robotics-Programming/Chapter10_Deep_Learning_ ~/catkin_ws/src</strong></pre>
<p><span>This way, you will have a cleaner ROS environment. </span><span>As usual, you need to rebuild the workspace on the laptop:</span></p>
<div>
<pre><strong>$ cd ~/catkin_ws</strong><br/><strong>$ catkin_make</strong><br/><strong>$ source ~/catkin_ws/devel/setup.bash</strong></pre></div>
<div>
<p><span>Then, let's start with the setup for TensorFlow.</span></p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the system for TensorFlow</h1>
                </header>
            
            <article>
                
<p>First, we will set up <kbd>pip</kbd>, the Python package manager and afterward the framework for performing ML, that is, TensorFlow. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing pip</h1>
                </header>
            
            <article>
                
<p>Ubuntu distributions typically ship with <kbd>pip</kbd> preinstalled. Unless a Python library requests you to upgrade, you can stay with the same version. In any case, we recommend working with the latest one, as explained in the following.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing the latest version</h1>
                </header>
            
            <article>
                
<p>This section applies to the case in which you need to install <kbd>pip</kbd> or upgrade it:</p>
<ol>
<li>First, remove the previous version if there is one:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo apt remove python-pip</strong></pre>
<p style="padding-left: 60px">We do this because the Ubuntu repository may not have the latest version of <kbd>pip</kbd>. In the next step, you will access the original source to get all of the updates.</p>
<ol start="2">
<li>Download the installation script and execute it:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>$ sudo apt update<br/></span><span>$ curl "https://bootstrap.pypa.io/get-pip.py" -o "get-pip.py"<br/></span><span>$ sudo python get-pip.py</span></strong></pre>
<ol start="3">
<li>Check which version is installed:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ pip --version</strong><br/><strong>    pip 19.3.1 from /usr/local/lib/python2.7/site-packages/pip (python 2.7)</strong><span><br/></span></pre>
<p style="padding-left: 60px">If it was already present in your system, you can easily upgrade using <kbd>pip</kbd> itself:</p>
<pre style="padding-left: 60px"><strong><span>$ sudo pip install --upgrade pip</span></strong></pre>
<p>Now, you are ready to proceed with the installation of the ML environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing TensorFlow and other dependencies</h1>
                </header>
            
            <article>
                
<p>OpenCV, the well-known and open source computer vision library (<a href="https://opencv.org/">https://opencv.org/</a>), brings to ROS the capability of image processing. It is used by TensorFlow to deal with images that you will obtain from the robot camera. To install it in your system, you need the <kbd>pip</kbd> package manager that we explained earlier:</p>
<pre><strong><span>$ pip install opencv-python --user</span></strong></pre>
<p><span>The <kbd>--user</kbd> </span><span>flag</span><span> </span><span>ensures that the package is installed locally to the user home folder at </span><kbd>~/.local/lib/python2.7/site-packages</kbd><span>. Otherwise, it should be installed system-wide at the </span><kbd>/usr/local/lib/python2.7/dist-packages</kbd><span> </span><span>path</span><span>, as is the case of</span> <kbd>pip</kbd><span> (in such cases, you should perform the installation with </span><kbd>sudo</kbd><span>).</span></p>
<p>The OpenCV ROS bridge<span> (</span><a href="http://wiki.ros.org/cv_bridge">http://wiki.ros.org/cv_bridge</a>)<span> </span>ships with the full-stack installation of ROS. If, for some reason, the package is missing in your environment, you can easily install it with this line:</p>
<pre><strong>$ sudo apt update &amp;&amp; sudo apt install ros-&lt;ROS_VERSION&gt;-cv-bridge</strong></pre>
<p>For the <kbd>&lt;ROS_VERSION&gt;</kbd> <span>tag,</span><span> </span><span>use the</span> <kbd>kinetic</kbd><span> </span><span>value</span><span> </span><span>or <kbd>melodic</kbd> depending on the ROS distribution you have.</span></p>
<p>Finally, install TensorFlow as follows:</p>
<pre><strong><span>$ pip install --upgrade tensorflow </span><span>--user</span></strong></pre>
<p><span>The <kbd>--upgrade</kbd> </span><span>flag </span><span>gives you the advantage to update the package if it is already installed. </span><span>If you are working in Ubuntu 16.04, TensorFlow V2 will throw compatibility issues. In such a case, install TensorFlow V1 as follows:</span></p>
<pre><strong><span>$ pip install --upgrade tensorflow==1.14 </span><span>--user</span></strong></pre>
<p>In Ubuntu 18.04, you will be ready with the upgraded version of TensorFlow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Achieving better performance using the GPU</h1>
                </header>
            
            <article>
                
<p class="mce-root">Alternatively, you could use the GPU version of TensorFlow to take advantage of this hardware on your laptop. The <strong>GPU</strong> (short for <strong>Graphical Processing</strong> <strong>Unit</strong>) card of your laptop is primarily used to power the display output on the screen. Therefore, it is very good at image processing.</p>
<p class="mce-root">As the kinds of calculations we need to do in ML are very similar (that is, floating-point, vector, and matrix operations), you can speed up the training and usage of your ML models by using the GPU for calculations instead of the CPU.</p>
<p class="mce-root">By using the GPU, you may achieve at least a factor of 10 in speed calculation for using the CPU, even in the case of the cheapest GPU cards. Hence, the choice of GPU is worth it. The command to install the corresponding TensorFlow library in Ubuntu 18.04 is pretty simple:</p>
<pre><strong>$ <span>pip install --upgrade tensorflow-gpu </span><span>--user</span></strong></pre>
<p><span>As before, if you are working i</span>n Ubuntu 16.04, install TensorFlow V1 to avoid compatibility issues:</p>
<pre><strong><span>$ pip install --upgrade tensorflow-gpu==1.14 </span><span>--user</span></strong></pre>
<p>With TensorFlow installed, being the normal version or the GPU-performant one, you are ready to use ML within ROS.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ML comes to robotics</h1>
                </header>
            
            <article>
                
<p>ML has its roots in statistical science. Remember when you have a cloud of points on an x-y frame and try to find the straight line that best fits all of them at the same time? This is what we call a linear regression and can be solved with a simple analytical formula. <strong>Regression</strong> is the first algorithm that you typically study when getting started with ML.</p>
<p>To acquire perspective, be aware that, before 1980, artificial intelligence and ML were part of the same corpora of knowledge. Then, artificial intelligence researchers focused their efforts on using logical, knowledge-based approaches, and ML kept the algorithmic approach, <em>regression</em> being the most basic and having neural network-based algorithms as its main bundle. Hence, this fact favored that ML evolved as a separated discipline.</p>
<p><span>Following path of the traditional research in neural networks in the '60s and '70s, ML kept on developing in this field. Then, its </span>first golden age came in the '90s.</p>
<p>However, 25 years ago, the computer resources that a neural network required were not within the reach of normal PCs, since a huge amount of data needed to be processed to obtain accurate results. It was more than one decade later that computing capacity was available to everyone, and then problem-solving based on neural network algorithms finally became a commodity.</p>
<p>This fact brings us to the present boom of ML, where functionalities such as content recommendation (shops, films, and music) and facial/ object recognition (camera-based apps) are used ubiquitously in most modern smartphones.</p>
<p class="mce-root">On the other side, robots started their path in the industry by 1950, being<span> at the beginning</span> just mechanical devices that performed repetitive motions. As artificial intelligence and its accompanying discipline, ML, developed in parallel, practical results in these fields could be transferred, since robots were also powered by similar CPUs to those with which ML problems were solved. Then, robots gradually acquired the capability to better accomplish actions by being aware of their effects in the environment. Data came from the robot's camera and sensors provides feedback to the <em>learning system</em> that allowed it to perform better every time. This learning system is just an ML pipeline.</p>
<p>And how different is robot learning from human learning? Well, our brain is far more efficient. To recognize for the first time whether an animal is a dog, a kid just needs four or five samples, while an ML algorithm needs hundreds to be accurate in its answers. This is the underlying reason why ML models used by robots need to be pretrained with lots of data so that the robot can respond—both <strong>accurately</strong> and in <strong>real time</strong><span>—</span>with a smart action, that is, by picking an object from one location and moving it to another previously marked as the target (a typical problem in the logistics industry).</p>
<p>This task of identifying objects is what we will do in the practical example of this chapter. We will supply the robot with a trained model able to recognize different kinds of common objects (balls, mouses, keyboards, and so on) and will observe the response when putting it in front of several of these objects. <span>Hence, let's keep on explaining the following concepts surrounding this practical example regarding the recognition of several kinds of objects.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Core concepts in ML</h1>
                </header>
            
            <article>
                
<p><span>Before going into the use case of object recognition in images, let's take a much simpler exampl</span>e, the prediction of the price of a house as a function of several independent variables: area, number of rooms, distance to the center of the city, population density, and more<span>.</span></p>
<p>First, to have a working ML algorithm, we need an underlying model that, when fed with input data, can produce a prediction. The data has to be supplied according to the features, that is, independent variables, that we have selected for our model. Then, establishing the correspondence with our simple example, we can explain the several concepts involved in an ML problem:</p>
<ul>
<li>The algorithm is the computation as a whole, specified as a sequence of instructions or steps that are to be followed to produce a result. All of the instructions have to be unambiguous and the actor that is running the algorithm does not have to make any additional decision; all of them are covered by the algorithm, which specifies what to do at a certain point if a condition needs to be evaluated. Then, you can easily infer that an algorithm is something that can be programmed in a computer, no matter which language is used. In the case of the example of the prediction of the price of a house, the algorithm consists of applying the sequence of instructions given sample data<span>—</span>that is, area, number of rooms, and so on—to obtain a prediction of its price.</li>
<li>The<span> </span>model provides an assumption of the analytical function to apply to the input data to obtain a prediction. For example, we can say that the model for the price of the house is a linear function of the inputs, that is, given an increment in the percentage of the area of the house leads to the same percentual increment in its predicted price. For the rest of the independent variables, the same <span>reasoning</span> would apply because we have assumed a linear dependence. The model is applied in some steps of the algorithm.</li>
<li>The features are the independent variables of our model, that is to say, the available data that you have to predict the price of a house. In our example, these are area, number of rooms, distance to the center of the city, and population density.</li>
<li>The dataset is a structured data collection providing values for each of the selected features for a large number of items. In our example, the dataset should be a table in which each row contains the available data of a concrete house, and each column contains the values of each selected feature, that is, a surface column, number of rooms column, distance to the center of the city column, population density column, and so on.</li>
</ul>
<p>When facing a new problem, the data scientist has to decide for all of these three elements: the algorithm, the model, and the features. The last topic, feature selection, is where there's the added value that a human provides to solving an ML problem; the rest of the tasks are automated and accomplished by a computer. The next subsection explains in detail what features are and emphasizes the importance of their selection to obtain accurate predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selecting features in ML</h1>
                </header>
            
            <article>
                
<p>Features in ML constitute a set of characteristics that have to be selected by the user, and it is this selection upon which the dataset is built. The expertise for making a good feature selection is more a question of experience and insight than a structured process. Hence, a good data scientist is one who understands the problem and can decompose it in its essential parts to find what the relevant features are. These act as the independent variables from which accurate predictions can be made.</p>
<p>To solve an ML problem, it is crucial to perform the right feature selection. If you do not detect the relevant features, no matter how much data you put in the solver, you will never get a good prediction. As shown in the following diagram, we will feed the ML algorithm with a data collection to obtain a result, that is, a prediction:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4835e387-64c0-47ef-b82e-3487f14183ae.png" style="width:32.50em;height:6.50em;"/></p>
<p><span>Data collection has been built according to the selected features. For example, if you decide to build the model for price prediction of the houses in a given city based on three feature</span>s<span>—</span>area, number of rooms, and distance to the center of the city—for every new house you want to predict the price of, you will have to feed the algorithm with the specific values of such features, for example, 85 square meters, 4 rooms, and 1.5 kilometers to the cent<span>er of the city.</span></p>
<p>Next, it is crucial to understand how the values of these features are combined to obtain the prediction.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The ML pipeline</h1>
                </header>
            
            <article>
                
<p>Problem-solving is split into two parts. The first is training the model according to the pipeline shown in this diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0e8f3cab-07f2-491d-99e1-16ab3d6c723a.png" style="width:31.42em;height:6.42em;"/></p>
<p>Since we are assuming a simple model where the output depends linearly on the values of the features, the goal of training consists of determining the weights to be applied to each of them to obtain the prediction. Let's explain it with this mathematical formulation:</p>
<p class="CDPAlignCenter CDPAlign"><em>Price = W1 * area + W2 * nº rooms + W3 * distance</em></p>
<p>As you may infer, the weights, <em>W1</em>, <em>W2</em>, and <em>W3</em>, are the coefficients that multiply each feature. After making the sum of the three products, we obtain the predicted price. So, the training phase consists of finding the set of weights that best fit the dataset we have available. In the training set, the data contains both the features and the actual prices. Hence, by applying the algorithm of least square regression (<a href="https://www.statisticshowto.datasciencecentral.com/least-squares-regression-line/">https://www.statisticshowto.datasciencecentral.com/least-squares-regression-line/</a>), we determine the set of values for <em>W1</em>, <em>W2</em>, and <em>W3</em> th<span>at best fit all of the actual prices supplied. This algorithm guarantees that the resulting equation is the one that provides the minimum global error for all of the items used for the training.</span></p>
<p><span>But we do not want to best fit only the supplied data since we already know these prices. We wish that the resulting equation </span><span>also</span><span> </span><span>be the best fit for any other house for which we do not</span> <span>know the price. So, the way to validate such an equation is by using a different</span> <span>dataset</span><span>, called the test set, from the one we used for training. The programmatic way to do this is by splitting the available data before performing the training. The typical approach is to make two random sets: one containing 70%-90% of the data for training and another with the remaining 30-10% to perform the validation. This way, the training set provides us with the provisional best-fit weights, </span><em>W1</em><span>, </span><em>W2</em><span>, and </span><em>W3</em><span>, and the validation set is used to</span> <span>estimate how well our ML model is operationally defined as the least square error.</span></p>
<p>The second part corresponds to the prediction itself, that is, when our ML algorithm is put in production in a real application. In the prediction (production) phase, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/57329b5c-ad0c-4a9a-9fb9-8bfdbef7228b.png" style="width:32.33em;height:6.50em;"/></p>
<p>The process of ML, in reality, is more a circular one than a linear one because, as we get more data for training, we can improve the calculation of the weights, and then rewrite the equation with the new set of coefficients, <em>W1</em>, <em>W2</em>, and <em>W3</em>. This way, ML is an iterative process that can improve the accuracy of predictions as more data is available and the model is retrained again and again.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From ML to deep learning</h1>
                </header>
            
            <article>
                
<p>In this section, you will understand what deep learning <span>is</span><span> </span><span>and how it relates to ML. And the most straightforward way to get this insight is by giving a quick overview of the most commonly </span><span>used</span> <span>algorithms. Then, from that perspective, you could appreciate why deep learning is the most active area of research nowadays.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ML algorithms</h1>
                </header>
            
            <article>
                
<p>As pointed out in the preceding diagram and explanations, the algorithm is the central part of ML problem-solving. A data scientist has also to select which one to apply depending on the kind of problem they are facing. So, let's have a quick overview of the most commonly used algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression</h1>
                </header>
            
            <article>
                
<p><span>Regression tries to find the curve that best fits a cloud of points, and it has been described in detail with the case of the prediction of house prices. In such a case, we have been talking about a linear dependency, but the algorithm can be generalized to any kind of curve that can be represented as a sum of dot products between coefficients (weights) and independent variables (features), that is, a polynomial. A common case is that of a term that is the square of a feature. In this case, the curve is a parabola and, mathematically, can be expressed as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><em>y = W1 * x + W2 * x² + W3 * 1</em></p>
<p>Let's review this with a real-life example. Given an independent variable, the years of experience of a candidate, we wish to predict what their salary will be when applying for a job opportunity. You can easily understand that the dependence of the salary, at least during the first years of experience, does not follow a linear dependence, that is, a candidate with 2 years will not get twice the salary with respect to when he/she had one year of experience. Percentual increments in salary will be gradually higher as he/she accumulates more experience. This kind of relationship can be modeled as a parabola. Then, from the independent variable, <em>x</em>, and the salary, we define two features: <em>x</em> and <em>x²</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression</h1>
                </header>
            
            <article>
                
<p>Logistic regression is used in classification problems, a very common type in ML. In this case, we try to predict a binary classification such as pass/fail, win/lose, alive/dead, or healthy/sick. This algorithm can be understood as a special case of regression, where the predicted variable is categorical, that is, it can only take a finite set of values (two if it is a binary classification). The underlying model is a probability function and, given a value of the independent variable, if the resulting probability is greater than 50%, we predict pass, win, alive, or healthy, and if lower, the prediction is the other category, that is, fail, lose, dead, o<span>r sick</span>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Product recommendation</h1>
                </header>
            
            <article>
                
<p>Product recommendation is the most used functionality in the consumer sector, for example, shopping, watching films, and readings books, taking as input user characteristics and well-rated items by other users with similar characteristics. There are several algorithms to implement this functionality such as collaborative filtering or featurized matrix factorization. If you are interested in this field, we provide good introduction references in the <em>Further reading</em> section at the end of this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering</h1>
                </header>
            
            <article>
                
<p>Clustering<strong> </strong>is a scenario where we have many items and we want to group them by similarity. In this case, items are unlabeled and we ask the algorithm to do two things:</p>
<ul>
<li>Make a group of similar items.</li>
<li>Label these groups so that new items are both classified and labeled by the algorithm.</li>
</ul>
<p>As an example, think of a collection of texts about many topics and you wish the algorithm to group similar texts and identify the main topic of each group, that is, label them: history, science, literature, philosophy, and so on. One of the classical algorithms for this scenario is the nearest neighbor method, where you define a metric, calculate it for each pair of items, and group together those pairs that are close enough (based on the defined metric). It can be though as a distance-like function that is computed between each set of two points.</p>
<p>A <strong>multiclassification scenario</strong>, where there are more than two categories<span>—</span>let's say n—is addressed by solving n logistic regressions where each one performs a binary classification for each of the possible categories. For example, if we want to detect the dominant color in an image (of four possible categories: red, green, blue, or yellow), we can build a classifier consisting of four logistic regressions, as follows:</p>
<ul>
<li>Red/NOT red</li>
<li>Green/NOT green</li>
<li>Blue/NOT blue</li>
<li>Yellow/NOT yellow</li>
</ul>
<p>There could be a fifth category, which we can call <em>unknown</em>, for cases in which the image is not classified in any of the red, green, blue, or yellow colors. Finally, this type of multi-logistic regression applied to images is the entrance door to the last algorithm, deep learning, on which we will focus from now until the end of this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning</h1>
                </header>
            
            <article>
                
<p>Deep learning is the most active research field in ML nowadays. The underlying model of this algorithm is a neural network whose way of working tries to mimic what the human brain does. Each neuron in the model performs a regression from its input with a special function, called <strong>sigmoid</strong>, that provides a sharp but continuous probability distribution of the output event. This function is the same as that of the probability function used in <strong>logistic regression,</strong> as described earlier. In this particular case of a neuron, if the resulting probability is greater than 50%, the neuron is activated and feeds another neuron or neurons downstream. If lower than 50%, the neuron is not active and hence it has negligible influence downstream.</p>
<p>Next, we are going to provide more details about how deep learning works so that when you perform the practical exercise with GoPiGo3, you know what it is going on under the hood in ROS.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning and neural networks</h1>
                </header>
            
            <article>
                
<p>From now on, we will base our explanations on the practical example of the<span> recognition of objects in images, which, in the case of the robot, will be supplied by the Raspberry Pi camera. </span><span>In the following diagram, you can see a representation of a neural network that differentiates the three kinds of layers that there can be:</span></p>
<ul>
<li>The input layer is where we feed the dataset. Remember that such data has to be structured according to the selected features, that is, one neuron per feature. We will later discuss this particular and very common case of image datasets.</li>
<li>The hidden layer(s)—one or more—are the intermediate steps in the deep learning pipeline that extract more features so that the algorithm is more capable of discriminating between objects. These hidden features are implicit, and the end user does not necessarily need to know about them because their extraction is intrinsic (automatic) to the network structure itself.</li>
<li>The output layer provides the prediction. Each neuron provides a logical 1 if activated (a probability greater than 50%) or a 0 if not activated (lower than 50%). So, the resulting probability in the output layer will be the answer with a certain probability:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1412 image-border" src="assets/251eae31-f607-45ba-a251-52ce7c9e502e.png" style="width:26.75em;height:16.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">License <span>CC-BY-SA-2.5 s</span>ource: https://commons.wikimedia.org/wiki/File:Neural_Network.gif</div>
<p>Following a sequential approach, let's explain how a neural network works by covering what each layer makes on the supplied input data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The input layer</h1>
                </header>
            
            <article>
                
<p><span>This is the first step of the deep learning pipeline, and the most common structure of this layer is to have as many input neurons (features) as three times the number of pixels the image has:</span></p>
<ul>
<li>For images of a size of 256 x 256 <span>pixels</span>, this means 65.536 pixels.</li>
<li>In general, we will deal with color images, so each pixel will have three channels: red, blue, and green; each value stands for the intensity ranging from 0 to 255 for 8 bits of color depth.</li>
<li>Then, the number of features is <em>65.536 x 3 = 196.608</em> and the value of each feature will be a number between 0 and 255. Each feature is represented with one neuron in the input layer.</li>
</ul>
<p>Afterwards, the neural network is asked to answer this question: is there a cat in the picture? And the goal of the next layers is to extract the essential aspects of the image to answer that question.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The hidden layer(s)</h1>
                </header>
            
            <article>
                
<p>For understanding how this layer works, let's go back to the regression algorithm we explain earlier. There, we expressed the predicted variable as a linear combination of features—area, number of rooms, and distance to the center multiplied by weights, respectively, <em>W1</em>, <em>W2</em>, and <em>W3</em>. Establishing the analogy with our neural network, the features would apply to the neurons and the weights to the edges that connect each pair of neuro<span>ns:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1413 image-border" src="assets/5b9b28ae-5bc2-4294-90c5-a3a1156d488f.png" style="width:21.00em;height:15.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Source: https://commons.wikimedia.org/wiki/File:Artificial_neural_network_pso.png, Cyberbotics Ltd.<br/>
CC BY-SA 3.0 https://creativecommons.org/licenses/by-sa/3.0</div>
<p><span>The value of e</span>ach feature would be processed with the sigmoid function of its neuron (input layer; <em>j</em> neurons) to produce a probability value, <em>Sij</em>, which is then multiplied by the weight, <em>Wij</em>, of the edge that connects it to each neuron downstream (hidden layer ; <em>i</em> neurons). Hence, the feature input to this neuron, <em>i</em>, in the hidden layer is a sum of products, there being as many terms as neurons are connected to it upstream (input layer ; <em>j</em> neurons).</p>
<p>Such a result is the sum over <em>j</em> of all of the terms, <em>Sij</em>, with the index, <em>j</em>, which is an iterator that ranges over all of the neurons connected to <em>i</em> neurons in the input layer. The weights <em>Wij</em> of the edges connecting pairs of neurons are more properly called <strong>hyperparameters</strong>.</p>
<p>The neural structure of the hidden layers provides what we call intrinsic features, which are inherent properties of the network and do not have to be selected by the user (they are established by the designer of the neural network). What the user has to do is to train the network to obtain the best set of weights, <em>Wij</em>, that makes the network to as predictive as possible with the available dataset. Here is where<span> </span><span>the magic of deep learning</span><span> resides because a well-designed architecture of layers can provide a very accurate predictive model. The downside is that you need a lot of data to get a well-trained network.</span></p>
<p>Recapping from the beginning, given an input image, you can calculate the feature input to the neurons of each layer, <em>Fi</em>, based on the probabilities from the previous layer, <em>Sij</em>, and the weights, <em>Wij</em>, of the edges connecting to neuron <em>i</em>:</p>
<p class="CDPAlignCenter CDPAlign"><em>Fi = (sum over j) [Sij * Wij]</em></p>
<p>Proceeding downstream layer by layer, you can finally obtain the probabilities of the neurons of the output layer and, therefore, answer with the prediction of what the analyzed image contains.</p>
<p>As was mentioned earlier, and given that complexity of the network structure, you may guess that, for training such a model, you would need much more data than for traditional ML algorithms such as regression. More specially, what you have to calculate are the values of how many hyperparameters as edges connecting pairs of neurons there are. Once you achieve this milestone, you get a trained network that can be applied to unlabeled images to predict its content.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The output layer</h1>
                </header>
            
            <article>
                
<p><span>For the question of our example, that is, there is a cat in the picture? yes if the image shows a cat, or not if it doesn't</span><span>. So we only need a neuron in the output layer, as shown in</span> the <span>diagram below. Then, if trained with many photos of cats, </span><span>this network</span> <span>could</span><span> classify an image to say whether it contains a cat (1) or not (0). An important point here is that the model should be able to identify </span><span>the cat whatever position it occupies in the image, center, left, right, top, down, and so on:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/38dc3fe0-59c4-4718-9f10-86034d2c1d88.png" style="width:14.92em;height:10.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Source: https://commons.wikimedia.org/wiki/File:NeuralNetwork.png</div>
<p><span>If we need to classify 10 kinds of objects (several types of pets, for example), we would need an output layer with 10 neurons. The result of the computation of the network would be a vector with 10 probabilities—each one linked to each neuron, and the one that provides the largest value (the closest to 100%) would tell us what kind of pet there is in the input image with more probability.</span></p>
<p>Of course, you can make<span> the network </span>more complex and add more output neurons (and possibly more hidden layers) to obtain more details of the images. Consider the following:</p>
<ul>
<li>Identify whether there is one cat or two or more.</li>
<li>Identify characteristics of the face, such as whether the eyes and/or mouth are open or closed:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/58274728-a20d-4abb-bcfd-cb5a5a82d307.png" style="width:24.00em;height:11.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Source: https://www.flickr.com/photos/55855622@N06/5173363938 by jeici1, License: CC BY 2.0</div>
<p><span>This is a quite complex topic and beyond of the scope of this introductory chapter, whose goal is just to provide a descriptive understanding of what deep learning is and how it works. Anyway, the reader is encouraged to delve deeper into the topic, and for that, two didactic references are included in</span> the <em>Further reading</em> secti<span>on at the end of this chapter: </span><span><em>Intuitive Deep Learning Parts 1 and 2</em>.</span></p>
<p>From this point, we move to the practical part and start by stating a general methodology to tackle ML problems in robotics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A methodology to programmatically apply ML in robotics</h1>
                </header>
            
            <article>
                
<p>A specific aspect of ML is that robot respon<span><span>ses</span></span> have to happen in real time, without delays, so that the actions taken are effective. For example, if it finds an obstacle crossing the path it is following, we expect that it avoids it. To do so, obstacle identification has to occur as it appears in the robot's field of view. Hence, the subsequent action of avoiding the obstacle has to be taken immediately to avoid a crash.</p>
<p>We will support our methodology description with an end-to-end example that covers all that GoPiGo3 can do up to this point. Then, with this example, we expect that GoPiGo3 can carry a load on top of its chassis from its current location to a target location (a common case in garbage collector robots).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A general approach to application programming</h1>
                </header>
            
            <article>
                
<p>The steps involved in solving this challenge are as follows:</p>
<ol>
<li>Determine what high-level tasks are involved.</li>
<li>List the atomic tasks that, put together, are capable of accomplishing the high-level tasks. This is the level at which we create our program in ROS, writing node scripts and launch files.</li>
<li>Program the robot application by adapting the algorithms of the high-level tasks to the specific situation we are trying to solve.</li>
</ol>
<p><span>Next, we provide a breakdown of each of these steps so that we can implement the functionality in the real robot:</span></p>
<ol>
<li>These are the high-level tasks to be carried out:
<ul>
<li><strong>SLAM</strong>: This is <strong>Simultaneous Localization and Mapping </strong>(<strong>SLAM</strong>) to build a map of the actual environment.</li>
<li><strong>Navigation</strong>: Setting a target pose, GoPiGo3 can move autonomously until achieving it.</li>
<li><strong>Visual recognition</strong>: GoPiGo3 can identify where it has to be placed so that the garbage it carries can be collected.</li>
</ul>
</li>
<li>List the atomic tasks that are involved in the example. Let's say that, to be successful, GoPiGo3 has to be able to do the following:
<ol>
<li>Load a map of the environment.</li>
<li>Calculate an optimum path to achieve the target location given the information from the map.</li>
<li>Start navigating toward the goal.</li>
<li>Avoid obstacles found along the path.</li>
<li>Stop if unexpected conditions are found in the environment that do not let it advance anymore. Then, ask for help.</li>
<li>After receiving help, resume the path to the target location.</li>
<li>Recognize the garbage store entrance and stop at the exact position where a hoist will hook the loaded garbage.</li>
</ol>
</li>
<li><span>Program t</span>he robot application. Ea<span>ch of the preceding atomic tasks will correspond to a ROS node script, which can be expressed as a launch file with just one <kbd>&lt;node&gt;</kbd> tag. Then, you have to put these seven nodes on a ROS graph and draw the edges that should connect pairs using topics:</span>
<ul>
<li><span>For each published topic</span>, y<span>ou should determine which frequency the topic should be published with so that the robot can react quickly enough. For example, since the typical speed of GoPiGo3 is</span> 1 m/s, we wish the scan distance to be updated 10 times every 1 m traveled. This means that the robot will receive a perception update every 10 cm(=0.1 m) traveled and will be able to detect the presence of obstacles outside of a circumference of 0.1 m radius. The minimum publishing rate so that the robot can react to avoid the obstacle is calculated with this simple <span>formula: </span><em>(1 m/s) /0.1 m = 10 Hz</em><span><span>.</span></span></li>
<li>For each topic a node is subscribed to the code should trigger a robot action that allows it to successfully adapt to such conditions in the environment. For example, given the topic providing distances around GoPiGo3, when its value is below a threshold, 20 cm, for example (you will see now where this number comes from), GoPiGo3 recalculates the local path to avoid the obstacle. We should select this threshold according to the 10 Hz rate of publishing we decided previously; remember that this rate came from the fact that the robot will receive a perception update every 10 cm traveling. Taking a safety factor of 2, the threshold is simply <em>10 cm * 2 = 20 cm</em>, providing room and time so that it avoids the obstacle.</li>
</ul>
</li>
</ol>
<p><span>There's no need for ML currently now for atomic tasks 1 through 6. But when it comes to aligning with the garbage stop entrance, GoPiGo3 needs to know not only its pose but also its relative position to the entrance, so that the hoist can successfully hook the loaded garbage. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integrating an ML task</h1>
                </header>
            
            <article>
                
<p>This node of step 7 formulates its functionality as<em> recognize the garbage store entrance and stop at the exact position where a hoist will hook the loaded garbage</em>. Hence, the Pi camera comes to the rescue and image recognition capability has to be included in the logic programming of this node. This logic can be briefly expressed as publishing the <kbd>cmd_vel</kbd> messages to robot differential drives that allow GoPiGo3 to be put right in place. So, it is a feedback mechanism between visual perception, that is, entrance shape alignment in the image or not, and a motion command to correct and center:</p>
<ul>
<li>If the entrance is shifted to the left in the image, the robot should rotate left.</li>
<li>And if deviated to the right, it <span>should rotate right an angle proportional to the distance from the entrance to the center of the image.</span></li>
</ul>
<p>And your very first question should be: how can we integrate such an ML task with our robotic application? And the answer comes to enlighten how the ROS publish/subscribe mechanism is both powerful and simple at the same time. Its neutral nature allows us to integrate any kind of task that can be packaged into a black box by adhering to the following two rules:</p>
<ul>
<li>Input is supplied via a subscribed topic.</li>
<li>Output is delivered using a published topic.</li>
</ul>
<p>In the concrete case of ML applied to center the robot in the entrance door, we have the following:</p>
<ul>
<li>Input to the ML node (subscribed topic) is the image feed from the Pi camera.</li>
<li>Output from<span> the ML node (</span>published topic) is the horizontal distance from the shape of the door to the center of the image.</li>
</ul>
<p>Then, the GoPiGo3 drive node takes that output topic as the data to determine which <kbd>cmd_vel</kbd> command should be sent to the motors. This establishes a feedback loop with the ML node that makes it possible that the robot position converges to get finally centered in the entrance door:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b1e991b3-3f41-4469-b7b5-6eb06bbf72e2.png" style="width:29.92em;height:7.17em;"/></p>
<p>The ML published topic, <kbd>object_position</kbd>, is an integer that provides the distance in pixels from the centroid of the object (entrance door) to the center of the image frame.</p>
<p>Although it is out of the scope of this chapter, it is good to know at this point that ROS provides other interaction mechanisms between nodes, and the programmer's choice about which one to use depends on the specific <span>functionality to be implemented:</span></p>
<ul>
<li>A ROS service is the classical implementation of the server/client architecture. The client node (<em>drive node</em>) makes a request to the server node <span>(</span><em>ML node</em><span>)</span> and this performs the calculation (t<span>he distance in pixels from the entrance door to the center of the image frame). Then, the response is sent back to the client. The key difference with the publish/ subscribe mechanism is that this is not expecting to receive requests; it publishes messages at the rate set within the code of the node, independently, whether other nodes are listening or not.</span></li>
<li>A ROS action is similar to a ROS service, that is, it provides a response to a request from a node, with the difference that, in this case, the client node does not block the execution (until it receives the answer). That is to say, it keeps executing other code instructions and, when it receives the response, the client triggers the <span>programmed</span> action (rotates the robot for alignment). This behavior is called asynchronous, unlike a ROS service, which is synchronous in nature, that is, it blocks the node execution until the response is received.</li>
</ul>
<p>So, let's dive into how to make GoPiGo3 aware of the objects it has around, and we will do this in the final section of this chapter where we will build a general ML node that is able to detect a wide range of object types.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning applied to robotics – computer vision</h1>
                </header>
            
            <article>
                
<p>The practical part of this chapter consists of operationally implementing the ML node described earlier. What we represented there as a black box is developed now as a ROS package that you may integrate with the functionalities you discovered in previous chapters:</p>
<ul>
<li>The remote control in <a href="0653ab6b-8710-41e7-9c01-5024865e3e27.xhtml" target="_blank">Chapter 7</a>, <em>Robot Control and Simulation</em><em>,</em> for both the virtual robot in Gazebo and the physical GoPiGo3</li>
<li>Robot navigation for a virtual robot in <a href="25ac032c-5bfe-47ff-aa5a-f178dbff7c57.xhtml" target="_blank">Chapter 8</a>, <em>Virtual SLAM and Navigation Using Gazebo</em>, and the physical GoPiGo3 in <a href="7b6ae4e6-2cd1-44e5-8f11-459b83987f42.xhtml" target="_blank">Chapter 9</a>, <em>SLAM for Robot Navigation</em></li>
</ul>
<p>So, we divide this section into two parts:</p>
<ul>
<li>The first section, <em>Object recognition in Gazebo</em>, provides you with the tools to integrate the ML node for image recognition in Gazebo so that, after finishing the practice, you may let your creativity fly to combine object recognition with any of the drive nodes from <strong>remote control</strong> or <strong>robot navigation</strong> and make the virtual robot smarter.</li>
<li><span>The second section, </span><em>Object recognition in the real world</em>, provides the <span>same integration with the physical GoPiGo3 and you will discover the ML node black box is the same no matter where the images come from, that is, objects in Gazebo or the real world. The choice is made by you when linking the ML node subscription to images of any of those scenarios.</span></li>
</ul>
<p>This procedure also gives an operational way to test a new robot application. Start with the validation in Gazebo, where you will mainly check that the developed code has no significant bugs and the robot works as expected; then, proceed with it to the real world—understand how all of the external variables that are not present in Gazebo act on the robot, see how it responds, and then decide which code refinements you need to make to get it to work.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object recognition in Gazebo</h1>
                </header>
            
            <article>
                
<p>To get the code, follow the instructions we provided at the beginning of this chapter under the section, <em>Technical requirements</em>. The exercise in Gazebo is going to be pretty simple and very effective at the same time. You will check how the virtual GoPiGo3 can recognize a common <em>tennis ball</em> from the image feed coming from the robot's camera:</p>
<ol>
<li>Let's start by spawning a model of the ball in Gazebo:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>T1 $ </span><span>roslaunch tf</span><span>_gopigo gopigo3_</span><span>world.launch</span></strong></pre>
<ol start="2">
<li>Then, launch a <kbd>rqt_image_view</kbd> node to watch the subjective view as perceived from the robot's camera:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>T2 $ rosrun rqt</span><span>_image_</span><span>view </span><span>rqt</span><span>_image_</span><span>view</span></strong></pre>
<p style="padding-left: 60px">Click on the top-left empty box, and select ;<kbd>/gopigo/camera1/image_raw</kbd><span> </span><span>topic</span><span>. Then, you will see the subjective view of the robot as acquired by its front camera.</span></p>
<ol start="3">
<li>Next, spawn a model of the ball in Gazebo:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>T3 $ sudo -s<br/></span><span>   $ roslaunch models</span><span>_spawn_</span><span>library spawn_tennisball.launch</span></strong></pre>
<p style="padding-left: 60px">Bear in mind that the <kbd><span>models</span><span>_spawn_library</span></kbd><span> </span><span>package requires you to execute the launch file as superuser. As soon as the ball is spawned in Gazebo, the process finishes and</span> <kbd>T3</kbd> is <span>released.</span></p>
<ol start="4">
<li>Then, launch the remote control node so that you can control GoPiGo with the keyboard as usual:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>T4 $ rosrun key</span><span>_teleop key_</span><span>teleop.py /key</span><span>_vel:=/cmd_</span><span>vel</span></strong></pre>
<div class="packt_infobox">This package was installed in <a href="0653ab6b-8710-41e7-9c01-5024865e3e27.xhtml" target="_blank">Chapter 7</a>, <em>Robot Control and Simulation</em>. If you did not install it, do so now. The source of this ROS package is at <a href="https://github.com/ros-teleop/teleop_tools">https://github.com/ros-teleop/teleop_tools</a>.</div>
<ol start="5">
<li>Finally, launch the image recognition node and watch the screen output. Use <kbd>T3</kbd> where you already have <kbd>sudo</kbd> enabled:</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>T3 $ sudo -s<br/>   $ </span><span>roslaunch </span><span>tf</span><span>_gopigo start_</span><span>image_recognition.launch</span></strong></pre></div>
<p style="padding-left: 60px">You can get a more condensed feed by subscribing to the <kbd>/result</kbd> topic, which provides just the name of the recognized objects:</p>
<pre style="padding-left: 60px"><strong><span>T6 $ rostopic echo /result</span></strong></pre>
<p>See the composition of the following screenshots showing how the tennis ball is recognized in the Terminal window (bottom-left side):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4ac87985-ba8e-41f4-9353-8d06e7198b33.png" style="font-size: 1em;text-align: center;"/></p>
<p>Is it easy to replicate? We expect so. Now,let's proceed to repeat the process with the physical robot.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object recognition in the real world</h1>
                </header>
            
            <article>
                
<p class="mce-root">First, remember to point the ROS master URI to the robot as usual:</p>
<pre class="mce-root"><strong>$ export ROS_MASTER_URI=http://gopigo3.local:11311</strong></pre>
<p class="mce-root">Apply this for every new Terminal in the laptop, or include the line in the <kbd>.bashrc</kbd><span> </span><span>file</span><span>. The physical robot configuration is as shown here, with GoPiGo3 in front of a</span> small yellow ball:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ce2fdd35-516e-4f94-ab6f-48371dbc66bd.png" style="width:38.92em;height:25.75em;"/></p>
<p class="mce-root">Run the following two commands in two independent Terminals in the Raspberry Pi:</p>
<pre class="mce-root"><strong>r1 $ roslaunch mygopigo gopigo3.launch</strong><br/><strong>r2 $ roslaunch raspicam_node camerav2_410x308_30fps.launch</strong></pre>
<p>The packages you are using in the preceding are the ones in <a href="0b20bdff-f1dc-42e8-ae83-fc290da31381.xhtml" target="_blank"/><a href="0b20bdff-f1dc-42e8-ae83-fc290da31381.xhtml" target="_blank">Chapter 6</a>, <em>Programming in ROS- Commands and Tools</em>. So make sure you did not delete them, and if so, get them back. In the laptop is where you run the new packages to perform image recognition:</p>
<pre class="mce-root"><strong>T1 $ rosrun image_transport republish compressed in:=/raspicam_node/image out:=/raspicam_node/image_raw</strong></pre>
<p><span>The <kbd>image_transport-</kbd></span><span> </span><span>package</span><span> (you can find its ROS wiki page at <a href="http://wiki.ros.org/image_transport">http://wiki.ros.org/image_transport</a>) is commonly used in ROS to provide transparent support for transmitting images in low-bandwidth compressed format</span><span>s.</span></p>
<p><span>Then, </span><kbd>T1</kbd><span> ma</span><span>kes <kbd>raspicam_node/image</kbd>—output from</span> <kbd>r2</kbd><span><span>—</span></span><span>available in raw format, that is, the <kbd>/raspicam_node/image_raw</kbd></span><span> </span><span>topic</span><span>, the output of</span> <kbd>T1</kbd><span>.</span> <span>This facilitates the image feed, which</span><span> can then be processed later by <kbd>start_image_recognition.launch</kbd>. At this point, it is very useful to look at</span><span> the ROS graph:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1414 image-border" src="assets/3b6c683c-4a22-47ca-afb4-6c0501ae747f.png" style="width:48.25em;height:11.67em;"/></p>
<p>Remember that this visualization is launched <span>with the</span><span> </span><kbd>rqt_graph</kbd><span> command in another Terminal. Find that the transport operation is carried out by the <kbd>image_republisher_157...</kbd></span><span> </span><span>node</span><span>. </span><span>Then, launch a </span><kbd>rqt_image_view</kbd><span> node to watch the subjective view as perceived through the Pi camera:</span></p>
<pre class="mce-root"><strong>T2 $ <span>rosrun rqt</span><span>_image_</span><span>view </span><span>rqt</span><span>_image_</span><span>view</span></strong></pre>
<p><span>In the pop-up window, you have to select the <kbd>/raspicam_node/image_raw</kbd></span><span> </span><span>topic</span><span> to get the subjective view from the Pi camera.</span></p>
<p><span>Finally, as we did in simulation, launch the image recognition node and subscribe to the <kbd>/result</kbd> topic:</span></p>
<pre class="mce-root"><strong>T3 $ sudo -s</strong><br/><strong>   $ roslaunch tf_gopigo start_image_recognition.launch rgb_image_topic:=/raspicam_node/image_raw</strong><br/><strong><span>T4 $ rostopic echo /result</span></strong></pre>
<p>The only difference for the Gazebo scenario is that you have to remap the topic supplied by the Pi camera with <kbd>raspicam_node</kbd>, to the topic named <kbd>rgb_image_topic</kbd>, which is the one accepted by the image recognition node.</p>
<p>We have presented three different objects to the robot successively: the yellow ball, the mouse, and the monitor. Find out how the three of them are recognized by the robot in real time. Is it surprising?</p>
<p class="mce-root">The yellow ball can be seen here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1415 image-border" src="assets/c83af262-0045-4c88-8622-397ac4e6c7d3.png" style="width:48.75em;height:15.00em;"/></p>
<p>Then, the mouse can be seen here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1416 image-border" src="assets/4e7577bb-ef5c-4d6a-81c9-297f98f9988e.png" style="width:46.83em;height:18.92em;"/></p>
<p>And, finally, the monitor can be seen here:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8e976bec-8aaf-4ae4-b05e-19596bdbc209.png" style="width:45.92em;height:20.42em;"/></p>
<p class="mce-root">If you have arrived at this point, you are in a good position to start creating advanced applications in ROS that integrate object recognition as an ability that uses GoPiGo3 to execute smart actions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter provided a quick introduction to ML in robotics. We expect you to have acquired insight into what ML and deep learning are, qualitatively understood how a neural network processes images to recognize objects, and can operationally implement the algorithm in a simulated and/or physical robot.</p>
<p>ML is a very wide field and you should not expect nor really need to get an expert in the field. What you need to assimilate is the knowledge to integrate deep learning capabilities in your robots.</p>
<p>As you have seen in the practical case, we have used a pretrained model that covers common objects. Then, we have simply used this model and have not needed additional training. There are plenty of trained models on the web shared by data science companies and open source developers. You should spend time looking for these models, and only go to train your own models when the scenario that the robot is facing is so specific that general-purpose ML models do not cover it with decent accuracy.</p>
<p>In the final two chapters, we will focus on reinforcement learning, a task that is complementary to the deep learning technique described in this chapter. With the latter, the robot gets the perception of the environment, and with the former, it chains several actions oriented to a goal.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the task for solving ML that requires more experience and insight from the data scientist?</li>
</ol>
<p style="padding-left: 60px">A) The algorithm selection<br/>
B) <span>The feature selection<br/></span><span>C) The model </span></p>
<ol start="2">
<li>What is the relationship between ML and deep learning?</li>
</ol>
<p style="padding-left: 60px">A) ML covers many algorithms and deep learning only <span>algorithms to find deep features.<br/></span>B) <span>Deep learning is a subset of ML.<br/></span>C) Deep learning deals with all of the ML algorithms except neural networks.</p>
<ol start="3">
<li>How should you integrate an ML task with a ROS application?</li>
</ol>
<p style="padding-left: 60px">A) You should train the model outside and then provide ROS with a file of results.<br/>
B) <span>You have the choice of using publish/subscribe, a ROS service, or an action server.<br/></span><span>C) You have to use the specific communication protocol of the ML model.</span></p>
<ol start="4">
<li>What is the main difference between the publish/subscribe mechanism and the ROS service mechanism?</li>
</ol>
<p style="padding-left: 60px">A) ROS service is synchronous while <span>publish/subscribe is asynchronous.<br/></span>B) <span>ROS service is asynchronous while </span><span>publish/subscribe is synchronous.<br/></span>C) <span>P</span><span>ublish/subscribe does not need to receive requests from other nodes in order to publish messages.</span></p>
<ol start="5">
<li>If the practical example explained in the <em>Deep learning applied to robotics – computer vision</em> section was carried out with a red ball instead of a yellow one, will the prediction with the same model we are using?</li>
</ol>
<p style="padding-left: 60px">A) Yes, the color is not a feature for object shape recognition.<br/>
<span>B) Yes, and in addition to identifying a ball, it will also tell that it is red.<br/></span><span>C) It depends on whether the model was trained with balls of different colors.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>To delve deeper into the concepts explained in this chapter, you can check out the following references:</p>
<ul>
<li><em>A Brief History of ML:</em> <a href="https://www.dataversity.net/a-brief-history-of-machine-learning">https://www.dataversity.net/a-brief-history-of-machine-learning</a></li>
<li><em>A Brief History of Robotics Since 1950:</em> <a href="https://www.encyclopedia.com/science/encyclopedias-almanacs-transcripts-and-maps/brief-history-robotics-1950">https://www.encyclopedia.com/science/encyclopedias-almanacs-transcripts-and-maps/brief-history-robotics-1950</a></li>
<li><em>ML for Recommender systems -Part 1 (algorithms, evaluation and cold start):</em> <a href="https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-1-algorithms-evaluation-and-cold-start-6f696683d0ed">https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-1-algorithms-evaluation-and-cold-start-6f696683d0ed</a></li>
<li><em>ML for Recommender systems - Part 2 (Deep Recommendation, Sequence Prediction, AutoML, and Reinforcement Learning in Recommendation):</em> <a href="https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-2-deep-recommendation-sequence-prediction-automl-f134bc79d66b">https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-2-deep-recommendation-sequence-prediction-automl-f134bc79d66b</a></li>
<li><em>Intuitive Deep Learning Part 1a: Introduction to Neural Networks:</em> <a href="https://medium.com/intuitive-deep-learning/intuitive-deep-learning-part-1a-introduction-to-neural-networks-d7b16ebf6b99">https://medium.com/intuitive-deep-learning/intuitive-deep-learning-part-1a-introduction-to-neural-networks-d7b16ebf6b99</a></li>
<li><em>Intuitive Deep Learning Part 2: CNNs for Computer Vision:</em> <a href="https://medium.com/intuitive-deep-learning/intuitive-deep-learning-part-2-cnns-for-computer-vision-24992d050a27">https://medium.com/intuitive-deep-learning/intuitive-deep-learning-part-2-cnns-for-computer-vision-24992d050a27</a></li>
<li><em>Build your first Convolutional Neural Network to recognize images:</em> <a href="https://medium.com/intuitive-deep-learning/build-your-first-convolutional-neural-network-to-recognize-images-84b9c78fe0ce">https://medium.com/intuitive-deep-learning/build-your-first-convolutional-neural-network-to-recognize-images-84b9c78fe0ce</a></li>
</ul>


            </article>

            
        </section>
    </body></html>