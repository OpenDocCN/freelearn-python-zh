- en: Chapter 8. Rendering and Image Manipulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we looked mainly at the scripting aspects of the individual
    components that make up a Blender scene such as meshes, lamps, materials, and
    so on. In this chapter, we will turn to the rendering process as a whole. We will
    automate this rendering process, combine the resulting images in various ways,
    and even turn Blender into a specialized web server.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Automate the rendering process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create multiple views for product presentations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create billboards from complex objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulate images, including render results, by using the Python Imaging Library
    (PIL)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a server that creates on-demand images that may be used as CAPTCHA challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a contact sheet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A different view—combining multiple camera angles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you might expect that rendering can be automated as well, and you're
    quite right. The Blender Python API provides access to almost all parameters of
    the rendering process and lets you render individual frames as well as animations.
    This allows for automating many tasks that would be tedious to do by hand.
  prefs: []
  type: TYPE_NORMAL
- en: Say you have created an object and want to create a single image that shows
    it from different angles. You could render these out separately and combine them
    in an external application, but we will write a script that not only renders these
    views but also combines them in a single image by using Blender's image manipulation
    capabilities and an external module called PIL. The effect we try to achieve is
    shown in the illustration of Suzanne, showing her from all of her best sides.
  prefs: []
  type: TYPE_NORMAL
- en: '![A different view—combining multiple camera angles](img/0400-08-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Blender is an excellent tool that provides you not only with modeling, animating,
    and rendering options but has compositing functionality as well. One area that
    it does not excel in is "image manipulation". It does have an UV-editor/Image
    window of course, but that is very specifically engineered to manipulate UV maps
    and to view images rather than to manipulate them. The Node editor is also capable
    of sophisticated image manipulation but it has no documented API so it can't be
    configured from a script.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, Blender cannot do everything and surely it doesn't try to compete
    with packages such as **GIMP** ([www.gimp.org](http://www.gimp.org)), but some
    built-in image manipulation functions would have been welcomed. (Each image can
    be manipulated on the pixel level but this would be fairly slow on large images
    and we still would have to implement high-level functionality, such as alpha blending
    or rotating images.)
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, we can access any image generated by Blender from Python, and in
    Python it is fairly simple to add additional packages that do provide the extra
    functionality and use them from our scripts. The only drawback is that any script
    that uses these additional libraries is not automatically portable so users would
    have to check that the relevant libraries are available to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Python Imaging Library** (**PIL**) that we will be using is freely available
    and easy to install. Therefore, it should pose no problem for the average user.
    However, as it is possible to implement the simple pasting functionality (we will
    see below) using just Blender''s `Image` module, we do provide in the full code
    a minimalist module `pim` that implements just the bare minimum to be able to
    use our example without the need to install PIL. This independence comes at a
    price: our `paste()` function is almost 40 times slower than the one provided
    by PIL and the resulting image can be saved only in TARGA (`.tga`) format. But
    you probably won''t notice that as Blender can display TARGA files just fine.
    The full code is equipped with some trickery to use PIL (if it''s available) and
    our replacement module if it isn''t. (This is not shown in the book.)'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**The Python Imaging Library (PIL)**'
  prefs: []
  type: TYPE_NORMAL
- en: PIL is an open source package available for free from [http://www.pythonware.com/products/pil/index.htm](http://www.pythonware.com/products/pil/index.htm).
    It consists of a number of Python modules and a core library that comes precompiled
    for Windows (and is easy enough to compile on Linux or might even be available
    in the distribution already). Just follow the instructions on the site to install
    it (just remember to use the correct python version to install PIL; if you have
    more than one version of Python installed, use the one Blender uses as well to
    install).
  prefs: []
  type: TYPE_NORMAL
- en: Code outline—combine.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What steps do we have to take to create our combined image? We will have to:'
  prefs: []
  type: TYPE_NORMAL
- en: Create cameras if needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Frame the cameras on the subject.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Render views from all cameras.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the rendered images to a single image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code starts off by importing all of the necessary modules. From the PIL
    package we need the `Image` module, but we import it under a different name (`pim`)
    to prevent name clashes with Blender''s `Image` module, which we will be using
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first utility function that we encounter is `paste()`. This function will
    combine four images into one. The images are passed as filenames and the result
    is saved as `result.png` unless another output filename is specified. We assume
    all four images to have the same dimensions, which we determine by opening the
    first file as a PIL image and examining its `size` attribute (highlighted in the
    next code). The images will be separated and bordered by a small line with a solid
    color. The width and color are hardcoded as the `edge` and `edgecolor` variables,
    although you might consider passing them as arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create an empty image big enough to hold the four images with the
    appropriate borders. We will not be drawing any borders specifically, but just
    defining the new image with a solid color onto which the four images will be pasted
    at a fitting offset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We already opened the top image so all we have to do is paste it in the upper-left
    quadrant of our combined image offset in both the horizontal and vertical directions
    by the border width:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Pasting the three other images follows the same line: open the image and paste
    it at the correct position. Finally, the combined image is saved (highlighted).
    The file type of the saved image is determined by its extension (for example,
    `png`) but might have been overridden had we passed a format argument to the `save()`
    method. Note that there was no reason to specify a format for the input files
    as the image type is determined from its contents by the `open()` function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Our next function renders the view from a specific camera and saves the result
    to a file. The camera to render is passed as the name of the Blender Object (that
    is, not the name of the underlying `Camera` object). The first line retrieves
    the `Camera` object and the current scene and makes the camera current in the
    scene—the one that will be rendered (highlighted below). `setCurrentCamera()`
    takes a Blender Object, not a `Camera` object, and that's the reason we passed
    the name of the object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As we might use this function in a **background** **process** we will be using
    the `renderAnim()` method of the rendering context rather than the `render()`
    method. This is because the `render()` method cannot be used in a background process.
    Therefore, we set the current frame and both the start and end frames to the same
    value to ensure that `renderAnim()` will render just a single frame. We also set
    `displayMode` to `0` to prevent an extra render window popping up (highlighted
    in the next code snippet):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `renderAnim()` method renders frames to files so our next task is to retrieve
    the filename of the frame that we just rendered. The exact format of the filename
    may be specified by the user in the **User** **Preferences** window, but by calling
    `getFrameFilename()` explicitly we ensure that we get the right one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As the frame number will be the same for each camera view that we render, we
    will have to rename this file otherwise it would be overwritten. Therefore, we
    create a suitable new name consisting of the path of the frame we just rendered
    and the name of the camera. We use portable path manipulation functions from Python's
    `os.path` module so everything will work just as well under Windows as on Linux,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'As our script may have been used already, we try to remove any existing file
    with the same name because renaming a file to an existing filename will fail under
    Windows. Of course, there might not be a file yet—a situation we guard against
    in the `try` block. Finally, our function returns the name of the newly created
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The next important task is to frame the cameras, that is, to choose a suitable
    **camera angle** for all of the cameras in such a way that the subject fits the
    available area in the picture in an optimal way. We want the camera angle to be
    the same for all cameras to provide the viewer with a consistent perspective from
    all viewing angles. Of course, this could be done manually, but this is tedious
    so we define a function to do the work for us.
  prefs: []
  type: TYPE_NORMAL
- en: The way we do this is to take the **bounding box** of our subject and determine
    the viewing angle of the camera by assuming that this bounding box must just fill
    our view. Because we can calculate the distance of the camera to the center of
    the bounding box, the viewing angle must be the same as the acute angle of the
    triangle formed by the bounding box and the camera distance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Code outline—combine.py](img/0400-08-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We calculate this angle for all of the cameras and then set the camera angle
    for each camera to the widest angle calculated to prevent unwanted clipping of
    our subject. Note that this algorithm may fail if the cameras are too close to
    the subject (or equivalently, if the subject is too large), in which case some
    clipping may occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is pretty heavy on the math, so we start off by importing the necessary
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The function itself will take a list of names of Blender objects (the cameras)
    and a bounding box (a list of vectors, one for each corner of the bounding box).
    It starts off by determining the minimum and maximum extents of the bounding box
    for all three axes and the widths. We assume that our subject is centered on the
    origin. `maxw` will hold the largest width along any axis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we get the world space coordinates for each `Camera` object to calculate
    the distance `d` to the midpoint of the bounding box (highlighted in the next
    code). We store the quotient of maximum width and distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We take the largest quotient calculated (as this will amount to the widest angle)
    and determine the angle by calculating the arc sinus and finish by setting the
    `lens` attribute of the `Camera` object. The relation between camera's viewing
    angle and the value of the `lens` attribute in Blender is complex and scarcely
    documented (`lens` holds an approximation of the focal length of an ideal lens).
    The formula shown is the one taken from Blender's source code (highlighted).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Another convenience function is the one that defines four cameras and puts
    them into the scene suitably arranged around the origin. The function is straightforward
    in principle but is a little bit complicated because it tries to reuse existing
    cameras with the same name to prevent unwanted proliferation of cameras if the
    script is run more than once. The `cameras` dictionary is indexed by name and
    holds a list of positions, rotations, and lens values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For each camera in the `cameras` dictionary we check if it already exists as
    a Blender object. If it does, we check whether the Blender object has a `Camera`
    object associated with it. If the latter is not true we create a perspective camera
    with the same name as the top-level object (highlighted) and associate it with
    the top-level object by way of the `link()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If there wasn''t a top-level object present already we create one and associate
    a new perspective `Camera` object with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We end by setting the `location`, `rotation`, and `lens` attributes. Note that
    the rotation angles are in radians so we convert them from the more intuitive
    degrees that we used in our table (highlighted). We end by calling `Redraw()`
    to make the changes show up in the user interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define a `run()` method that strings all components together. It
    determines the active object and then cycles through a list of camera names to
    render each view and add the resulting filename to a list (highlighted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We will put the combined pictures in the same directory as the individual views
    and call it `result.png`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We then call our `paste()` function, passing the list of component filenames
    expanded as individual arguments by the asterisk (`*`) operator and end with a
    finishing touch of loading the result file as a Blender image and showing it in
    the image editor window (highlighted below). The `reload` is necessary to ensure
    that a previous image of the same name is refreshed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `run()` function deliberately did not create any cameras because the user
    might want to do that himself. The final script itself does take care of creating
    the cameras, but this might be changed quite easily and is as usual quite simple.
    After the check to see if it runs standalone it just creates the cameras and calls
    the `run` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The full code is available as `combine.py` in `combine.blend`.
  prefs: []
  type: TYPE_NORMAL
- en: Workflow—how to showcase your model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The script can be used in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: Put your subject at the origin (position (0, 0, 0)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create suitable lighting conditions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `combine.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The script may be loaded into the text editor to run with *Alt + P* but you
    may also put the script in Blender's `scripts` directory to make it available
    from the **Scripts | Render** menu.
  prefs: []
  type: TYPE_NORMAL
- en: Now, strip—creating a film strip from an animation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fitting multiple camera views to a single image is just one example where multiple
    images might be effectively combined to a single image. Another example is when
    we would like to show frames from an animation where we don't have access to facilities
    to replay the animation. In such situations we would like to show something resembling
    a film strip where we combine a small rendition of, for example, every tenth frame
    to a single sheet of images. An example is shown in the following illustration.
  prefs: []
  type: TYPE_NORMAL
- en: Although there are more images to combine than in the multiple camera view,
    the code to create such a film strip is fairly similar.
  prefs: []
  type: TYPE_NORMAL
- en: '![Now, strip—creating a film strip from an animation](img/0400-08-03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first function that we develop is `strip()` that takes a list of filenames
    of images to combine and an optional `name` that will be given to the combined
    image. A third optional argument is `cols`, which is the number of columns in
    the combined image. The default is four, but for long sequences it might be more
    natural to print on landscape paper and use a higher value here. The function
    will return a Blender `Image` object containing the combined image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We again use the `pim` module, which is either an alias for the PIL module
    if it''s available or will refer to our bare bones implementation if PIL is not
    available. The important difference with our previous image combination code is
    highlighted. The first highlighted part shows how to calculate the dimensions
    of the combined image based on the number of rows and columns plus the amount
    of pixels needed for the colored edges around images. The second highlighted line
    shows where we paste an image in the destination image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `render()` function that we define here will take the number of frames
    to skip as an argument and will render any number of frames between the start
    and end frames. These start and end frames may be set by the user in the render
    buttons. The render buttons also contain a step value, but this value is not provided
    to the Python API. This means that our function is a little bit more verbose than
    we like as we have to create a loop that renders each frame ourselves (highlighted
    in the next code) instead of just calling `renderAnim()`. We therefore have to
    manipulate the `startFrame` and `endFrame` attributes of the render context (as
    before) but we take care to restore those attributes before returning a list of
    filenames of the rendered images. If we did not need any programmatic control
    of setting the `skip` value, we could have simply replaced a call to `render()`
    by a call to `renderAnim()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'With these functions defined the script itself now simply calls `render()`
    to create the images and `strip()` to combine them. The resulting Blender image
    is reloaded to force an update if an image with the same name was already present
    and all windows are prompted to redraw themselves (highlighted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The full code is available as `strip.py` in `combine.blend`.
  prefs: []
  type: TYPE_NORMAL
- en: Workflow—using strip.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating a strip of animated frames can now be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create your animation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `strip.py` from the text editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The combined image will show up in the UV-editor/image window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the image with a name of your choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rendering billboards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Realism in scenes is often accomplished by providing lots of detail, especially
    in natural objects. However, this kind of realism comes with a price as detailed
    models often contain many faces and these faces consume memory and take time to
    render. A realistic tree model may contain as much as half a million faces so
    a forest of these would be almost impossible to render, even more so, if this
    forest is part of the scenery in a fast-paced game.
  prefs: []
  type: TYPE_NORMAL
- en: Blender comes with a number of tools to reduce the amount of memory needed when
    rendering many copies of an object; different `Mesh` objects may refer to the
    same mesh data as may **DupliVerts.** (Child objects that are replicated at the
    position of each vertex of a parent object. See [http://wiki.blender.org/index.php/Doc:Manual/Modeling/Objects/Duplication/DupliVerts](http://wiki.blender.org/index.php/Doc:Manual/Modeling/Objects/Duplication/DupliVerts)
    for more information.) Duplication of objects in particle systems also allows
    us to create many instances of the same object without actually duplicating all
    the data. These techniques may save huge amounts of memory but detailed objects
    still may take a lot of CPU power to render because the details are still there
    to be rendered.
  prefs: []
  type: TYPE_NORMAL
- en: '**Billboards** are a technique used to apply a picture of a complex object
    to a simple object, such as a single square face, and replicate this simple object
    as many times as needed. The picture must have suitable transparency otherwise
    each object may occlude the others in unrealistic ways. Apart from that, this
    technique is quite simple and may save a lot of rendering time and it will give
    fairly realistic results for objects placed in the middle distance or farther
    away. Blender''s particle systems may use billboards either as simple squares
    with images applied or by applying an image to a simple object ourselves and using
    that as a duplicating object. The latter also holds for dupliverted objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The trick is to generate an image with suitable lighting to be used as an image
    that can be applied to a square. Actually, we want to create two images: one shot
    from the front, one from the right, and construct an object consisting of two
    square faces perpendicular to each other with the two images applied. Such an
    object will give us a limited amount of freedom later in the placement of the
    camera in our scene as they do not have to be seen from just one direction. This
    works well only for objects with a roughly cylindrical symmetry, such as trees
    or high-rises, but then it is quite effective.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow for constructing such objects is complex enough to warrant automation:'
  prefs: []
  type: TYPE_NORMAL
- en: Position two cameras front and right of the detailed object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Frame both cameras to capture all of the object with the same angle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Render the transparent images with alpha premultiplied and without sky.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a simple object of two perpendicular squares.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply each rendered image to a square.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hide the detailed object from rendering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optionally, replicate the simple object in a particle system (the user may opt
    not to automate this part but place the simple objects manually).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The "premultiplication" mentioned in the third step may need some clarification.
    Obviously, the rendered images of our complex object need not show any background
    sky as their replicated clones may be positioned anywhere and may show different
    parts of the sky through their transparent parts. As we will see, this is simple
    enough to accomplish but when we simply render a transparent image and overlay
    it later on some background the image may have unsightly glaring edges.
  prefs: []
  type: TYPE_NORMAL
- en: The way to avoid this is to adjust the rendered colors by multiplying them with
    the alpha value and the render context has the necessary attributes to indicate
    this. We should not forget to mark the images produced as "premultiplied" when
    using them as textures, otherwise they will look too dark. The difference is illustrated
    in the following screenshot where we composited and enlarged a correctly premultiplied
    half on the left and a sky rendered half on the right. The trunk of the tree shows
    a light edge on the right. (Refer to Roger Wickes' excellent book "Foundation
    Blender Compositing" for more details.)
  prefs: []
  type: TYPE_NORMAL
- en: '![Rendering billboards](img/0400-08-04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The beech tree (used in these and the following illustrations) is a highly-detailed
    model (over 30,000 faces) created by Yorik van Havre with the free plant-modeling
    package **ngPlant**. (See his website for more fine examples: [http://yorik.uncreated.net/greenhouse.html](http://yorik.uncreated.net/greenhouse.html))
    The following first set of images shows the beech tree from the front and the
    resulting front facing render of the two billboards on the left. (slightly darker
    because of the premultiplication).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rendering billboards](img/0400-08-05_NEW.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The next set of screenshots shows the same beech tree rendered from the right
    together with a right-facing render of the billboard on the left. As can be seen,
    the rendition is certainly not perfect from this angle and this closeup, but a
    reasonable three-dimensional aspect is retained.
  prefs: []
  type: TYPE_NORMAL
- en: '![Rendering billboards](img/0400-08-07_NEW.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To give an impression of the construction of the billboards the next screenshot
    shows the two faces with the rendered images applied. The transparency is deliberately
    lessened to show the individual faces.
  prefs: []
  type: TYPE_NORMAL
- en: '![Rendering billboards](img/0400-08-09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Our first challenge is to reuse some of the functions that we wrote for the
    generation of our contact sheet. These functions are in a text buffer called `combine.py`
    and we did not save this to an external file. We will create our `cardboard.py`
    script as a new text buffer in the same `.blend` file as `combine.py` and would
    like to refer to the latter just like some external module. Blender will make
    this possible for us as it searches for a module in the current text buffers if
    it cannot find an external file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because internal text buffers have no information on when they were last modified,
    we have to make sure that the latest version is loaded. That is what the `reload()`
    function will take care of. If we didn''t do this Blender would not detect if
    `combine.py` had changed, which could lead to us using an older compiled version
    of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We will not reuse the `render()` function from `combine.py` because we have
    different requirements for the rendered images that we will apply to the billboards.
    As explained, we have to make sure that we won''t get any bright edges at points
    where we have partial transparency so we have to premultiply the alpha channel
    in advance (highlighted). We reset the rendering context to ''rendering the sky''
    again just before we return from this function because it''s easy to forget to
    turn this on again manually and you may waste time wondering where your sky has
    gone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Each rendered image will have to be converted to a suitable material to apply
    to a UV-mapped square. The function `imagemat()` will do just that; it will take
    a Blender `Image` object as an argument and will return a `Material` object. This
    material will be made completely transparent (highlighted) but this transparency
    and the color will be modified by the texture we assign to the first texture channel
    (second highlighted line). The textures type is set to `Image` and because we
    rendered these images with a premultiplied alpha channel, we use the `setImageFlags()`
    method to indicate that we want to use this alpha channel and set the `premul`
    attribute of the image to `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Each face that we will apply a material to will have to be UV-mapped. In this
    case, this will be the simplest mapping possible as the square face will be mapped
    to match a rectangular image exactly once. This is often called **reset mapping**
    and therefore the function we define is called `reset()`. It will take a Blender
    `MFace` object that we assume to be a quad and set its `uv` attribute to a list
    of 2D vectors, one for each vertex. These vectors map each vertex to a corner
    of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cardboard()` function takes care of constructing an actual `Mesh` object
    from the two `Image` objects passed as arguments. It starts off by constructing
    two square faces that cross each other along the z-axis. The next step is to add
    an UV-layer (highlighted) and make it the active one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we construct suitable materials from both images and assign these materials
    to the `materials` attribute of the mesh. Then, we reset the UV coordinates of
    both faces and assign the materials to them (highlighted). We update the mesh
    to make the changes visible before we return it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To replace the mesh of the duplication object of a particle system we implement
    a utility function `setmesh()`. It takes the name of the object with an associated
    particle system and a `Mesh` object as arguments. It locates the Object by name
    and retrieves the first particle system (highlighted in the next code snippet).
    The duplication object is stored in the `duplicateObject` attribute. Note that
    this is a *read-only* attribute so currently there is no possibility of replacing
    the object from Python. But we can replace the *data* of the object and that is
    what we do by passing the `Mesh` object to the `link()` method. Both the emitter
    object and the particle system''s duplication object are changed so we ensure
    that the changes are visible by calling the `makeDisplayList()` method on both
    of them before initiating a redraw of all Blender''s windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `run()` function encapsulates all the work that needs to be done to convert
    the active object to a set of billboards and assign them to a particle system.
    First, we retrieve a reference to the active object and make sure that it will
    be visible when rendered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to make the rest of the objects in the scene invisible before
    we render the billboards. Some object may have been made invisible by the user,
    therefore, we have to remember the states so that we can restore them later. Also,we
    do not alter the state of lamps or cameras as making these invisible would leave
    us with all black images (highlighted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Once everything is set up to render just the active object, we render front
    and right images with suitably framed cameras, just like we did in the `combine.py`
    script. In fact, here we reuse the `frame()` function (highlighted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we restore the previous visibility of all the objects in the scene before
    we construct a new mesh from the two images. We finish by making the active object
    invisible for rendering and replacing the mesh of the duplication object in a
    designated particle system by our new mesh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The final lines of code create the cameras necessary to render the billboards
    (if those cameras are not already present) by calling the `createcams()` function
    from the `combine` module before calling `run()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The full code is available as `cardboard.py` in `combine.blend`.
  prefs: []
  type: TYPE_NORMAL
- en: Workflow—using cardboard.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming that you have a high poly object that you would like to convert to
    a set of billboards, a possible work flow would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an object called `CardboardP`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign a particle system to this object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dummy cube.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign the dummy cube as the duplicate object on the first particle system of
    the `CarboardP` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select (make active) the object to be rendered as a set of billboards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `cardboard.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the original camera and render the scene.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Of course, the script might be changed to omit the automated replacement of
    the duplication objects mesh if that is more suitable. For example, if we would
    like to use dupliverted objects instead of particles we would simply generate
    the cardboard object and assign its mesh to the dupliverted object. If we do use
    a particle system we probably do not want all replicated objects to be oriented
    in exactly the same way. We might, therefore, randomize their rotation, an example
    setup to accomplish that is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Workflow—using cardboard.py](img/0400-08-10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next screenshot illustrates the application of billboards created from
    a tree model and used in a particle system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Workflow—using cardboard.py](img/0400-08-11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Generating CAPTCHA challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many situations such as blogs, forums, and online polls (to name a few) website
    operators want to guard against automated postings by spambots without wanting
    to burden human visitors with registration and authentication. In such situations
    it has become common to provide the visitor with a so-called CAPTCHA challenge
    ([http://en.wikipedia.org/wiki/Captcha](http://en.wikipedia.org/wiki/Captcha)).
    A **CAPTCHA** **challenge** (or just **Captcha**) in its simplest form is a picture
    that should be hard to recognize for a computer, yet simple to decipher by a human
    as it is, typically a distorted or blurred word or number.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, no method is foolproof and certainly Captchas are neither without
    their flaws nor immune to the ever-growing computing power available, but they
    still remain quite effective. Although the current consensus is that simple blurring
    and coloring schemes are not up to the task, computers still have a hard time
    separating individual characters in words when they slightly overlap where humans
    have hardly any problem doing that.
  prefs: []
  type: TYPE_NORMAL
- en: Given these arguments, this might be an excellent application of 3D rendering
    of text as presumably three-dimensional renditions of words in suitable lighting
    conditions (that is, harsh shadows) are even harder to interpret than two-dimensional
    text. Our challenge then is to design a server that will respond to requests to
    render three-dimensional images of some text.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will design our server as a web server that will respond to requests addressed
    to it as URLs of the form `http:<hostname>:<port>/captcha?text=<sometext>` and
    that will return a PNG image—a 3D rendition of that text. In this way it will
    be easy to integrate this server into an architecture where some software, such
    as a blog, can easily incorporate this functionality by simply accessing our server
    through `HTTP`. An example of a generated challenge is shown in the illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating CAPTCHA challenges](img/0400-08-12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Design of a CAPTCHA server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By making use of the modules available in a full Python distribution the task
    of implementing an `HTTP` server is not as daunting as is may seem. Our Captcha
    server will be based on the classes provided in Python''s `BaseHTTPServer` module
    so we start by importing this module along with some additional utility modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The `BaseHTTPServer` module defines two classes that together comprise a complete
    `HTTP` server implementation. The `BaseHTTPServer` class implements the basic
    server that will listen to incoming `HTTP` requests on some network port and we
    will use this class as is.
  prefs: []
  type: TYPE_NORMAL
- en: Upon receiving a valid `HTTP` request `BaseHTTPServer` will dispatch this request
    to a request handler. Our implementation of such a request handler based on the
    `BaseHTTPRequestHandler` is pretty lean as all it is expected to do is to field
    `GET` and `HEAD` requests for URIs of the form `captcha?text=abcd`. Therefore,
    all we have to do is override the `do_GET()` and `do_HEAD()` methods of the base
    class.
  prefs: []
  type: TYPE_NORMAL
- en: A `HEAD` request is expected to return only the headers of a requested object,
    not its content, to save time when the content isn't changed since the last request
    (something that can be determined by checking the `Last-Modified` header). We
    ignore such niceties; we will return just the headers when we receive a `HEAD`
    request but we will generate a completely new image nonetheless. This is something
    of a waste but does keep the code simple. If performance is important, another
    implementation may be devised.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our implementation starts off by defining a `do_GET()` method that just calls
    the `do_HEAD()` method that will generate a Captcha challenge and return the headers
    to the client. `do_GET()` subsequently copies the contents of the file object
    returned by `do_HEAD()` to the output file, such as object of the request handler
    (highlighted), which will in turn return this content to the client (the browser
    for example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `do_HEAD()` method first determines whether we received a valid request
    (that is, a URI of the form `captcha?text=abcd`) by calling the `gettext()` method
    (highlighted, defined later in the code). If the URI is not valid, `gettext()`
    will return `None` and `do_HEAD()` will return a **File not found** error to the
    client by calling the `send_error()` method of the base class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If a valid URI was requested, the actual image is generated by the `captcha()`
    method that will return the filename of the generated image. If this method fails
    for any reason an **Internal server** error is returned to the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything went well we open the image file, send a **200** response to
    the client (indicating a successful operation), and return a `Content-type` header
    stating that we will return a `png` image. Next, we use the `fstat()` function
    with the number of the open file handle as argument to retrieve the length of
    the generate image and return this as a `Content-Length` header (highlighted)
    followed by the modification time and an empty line signifying the end of the
    headers before returning the open file object `f`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `gettext()` method verifies that the request passed to our request handler
    in the path variable is a valid URI by matching it against a regular expression.
    The `match()` function from Python''s `re` module will return a `MatchObject`
    if the regular expression matches and `None` if it does not. If there actually
    is a match we return the contents of the first match group (the characters that
    match the expression between the parentheses in the regular expression, in our
    case the value of the `text` argument), otherwise we return `None`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Now we come to the Blender-specific task of actually generating the rendered
    3D text that will be returned as a `png` image. The `captcha()` method will take
    the text to render as an argument and will return the filename of the generated
    image. We will assume that the lights and camera in the `.blend` file we run `captcha.py`
    from are correctly set up to display our text in a readable way. Therefore, the
    `captcha()` method will just consider itself with configuring a suitable `Text3d`
    object and rendering it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Its first task is to determine the current scene and check whether there is
    an Object called `Text` that can be reused (highlighted). Note that it is perfectly
    valid to have other objects in the scene to obfuscate the display even more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If there was no reusable `Text3d` object, a new one is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to set the text of the `Text3d` object to the argument passed
    to the `captcha()` method and make it 3D by setting its extrude depth. We also
    alter the width of the characters and shorten the spacing between them to deteriorate
    the separation. Adding a small bevel will soften the contours of the characters
    what may add to the difficulty for a robot to discern the characters if the lighting
    is subtle (highlighted). We could have chosen to use a different font for our
    text that is even harder to read for a bot and this would be the place to set
    this font (see the following information box).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Something is missing**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Blender''s API documentation has a small omission: there seems to be no way
    to configure a different font for a `Text3d` object. There is an undocumented
    `setFont()` method, however, that will take a `Font` object as argument. The code
    to accomplish the font change would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fancyfont=Text3d.Load( ''/usr/share/fonts/ttf/myfont.ttf'')` `text_ob.setFont(fancyfont)`'
  prefs: []
  type: TYPE_NORMAL
- en: We have chosen not to include this code, however, partly because it is undocumented
    but mostly because the available fonts differ greatly from system to system. If
    you do have a suitable font available, by all means use it. Script type fonts
    which resemble handwriting for example may raise the bar even further for a computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step is to update Blender''s display list for this object so that
    our changes will be rendered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Once our `Text3d` object is in place our next task is to actually render an
    image to a file. First, we retrieve the rendering context from the current scene
    and set the `displayMode` to `0` to prevent an additional render window popping
    up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set the image size and indicate that we want a `png` image. By enabling
    RGBA and setting the alpha mode to `2` we ensure that there won''t be any sky
    visible and that our image will have a nice transparent background:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though we will render just a still image, we will use the `renderAnim()`
    method of the rendering context because otherwise the results will not be rendered
    to a file but to a buffer. Therefore, we set the start and end frames of the animation
    to 1 (just like the current frame) to ensure that we generate just a single frame.
    We then use the `getFrameFilename()` method to return the filename (with the complete
    path) of the rendered frame (highlighted). We then both store this filename and
    return it as a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The final part of the script defines a `run()` function to start the Captcha
    server and calls this function if the script is running standalone (that is, not
    included as a module). By defining a `run()` function this way we can encapsulate
    the often used server defaults, such as port number to listen on (highlighted),
    yet allow reuse of the module if a different setup is required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The full code is available as `captcha.py` in the file `captcha.blend` and
    the server may be started in a number of ways: from the text editor (with *Alt
    + P*) from the menu **Scripts | render | captcha** or by invoking Blender in *background*
    mode from the command line. To stop the server again it is necessary to terminate
    Blender. Typically, this can be done by pressing **Ctrl + C** in the console or
    DOSbox'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Warning**'
  prefs: []
  type: TYPE_NORMAL
- en: Note that as this server responds to requests from anybody it is far from secure.
    As a minimum it should be run behind a firewall that restricts access to it to
    just the server that needs the Captcha challenges. Before running it in any location
    that might be accessible from the Internet you should think thoroughly about your
    network security!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we automated the render process and learned how to perform
    a number of operations on images without the need for an external image editing
    program. We have learned:'
  prefs: []
  type: TYPE_NORMAL
- en: How to automate the rendering process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create multiple views for product presentations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create billboards from complex objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to manipulate images, including render results by using the Python Imaging
    Library (PIL)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create a server that creates on demand images that may be used as CAPTCHA
    challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the final chapter, we will look at some housekeeping tasks.
  prefs: []
  type: TYPE_NORMAL
