- en: Pure Python Optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the last chapter, one of the most effective ways of improving
    the performance of applications is through the use of better algorithms and data
    structures. The Python standard library provides a large variety of ready-to-use
    algorithms and data structures that can be directly incorporated in your applications.
    With the tools learned from this chapter, you will be able to use the right algorithm
    for the task and achieve massive speed gains.
  prefs: []
  type: TYPE_NORMAL
- en: Even though many algorithms have been around for quite a while, they are especially
    relevant in today's world as we continuously produce, consume, and analyze ever
    increasing amounts of data. Buying a larger server or microoptimizing can work
    for some time, but achieving better scaling through algorithmic improvement can
    solve the problem once and for all.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will understand how to achieve better scaling using standard
    algorithms and data structures. More advanced use cases will also be covered by
    taking advantage of third-party libraries. We will also learn about tools to implement
    caching, a technique used to achieve faster response times by sacrificing some
    space on memory or on disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of topics to be covered in this chapter is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to computational complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lists and deques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build an inverted index using a dictionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heaps and priority queues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing autocompletion using tries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-memory caching with the `functools.lru_cache` decorator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On-disk cache with `joblib.Memory`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast and memory-efficient loops with comprehensions and generators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful algorithms and data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithmic improvements are especially effective in increasing performance
    because they typically allow the application to scale better with increasingly
    large inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm running times can be classified according to their computational complexity,
    a characterization of the resources required to perform a task. Such classification
    is expressed through the Big-O notation, an upper bound on the operations required
    to execute the task, which usually depends on the input size.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, incrementing each element of a list can be implemented using a
    `for` loop, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If the operation does not depend on the size of the input (for example, accessing
    the first element of a list), the algorithm is said to take constant, or *O*(1),
    time. This means that, no matter how much data we have, the time to run the algorithm
    will always be the same.
  prefs: []
  type: TYPE_NORMAL
- en: In this simple algorithm, the `input[i] += 1` operation will be repeated 10
    times, which is the size of the input. If we double the size of the input array,
    the number of operations will increase proportionally. Since the number of operations
    is proportional to the input size, this algorithm is said to take *O*(*N*) time,
    where *N* is the size of the input array.
  prefs: []
  type: TYPE_NORMAL
- en: In some instances, the running time may depend on the structure of the input
    (for example, if the collection is sorted or contains many duplicates). In these
    cases, an algorithm may have different best-case, average-case, and worst-case
    running times. Unless stated otherwise, the running times presented in this chapter
    are considered to be average running times.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will examine the running times of the main algorithms and
    data structures that are implemented in the Python standard library, and understand
    how improving running times results in massive gains and allows us to solve large-scale
    problems with elegance.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code used to run the benchmarks in this chapter in the `Algorithms.ipynb` notebook,
    which can be opened using Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: Lists and deques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python lists are ordered collections of elements and, in Python, are implemented
    as resizable arrays. An array is a basic data structure that consists of a series
    of contiguous memory locations, and each location contains a reference to a Python
    object.
  prefs: []
  type: TYPE_NORMAL
- en: Lists shine in accessing, modifying, and appending elements. Accessing or modifying
    an element involves fetching the object reference from the appropriate position
    of the underlying array and has complexity *O*(1). Appending an element is also
    very fast. When an empty list is created, an array of fixed size is allocated
    and, as we insert elements, the slots in the array are gradually filled up. Once
    all the slots are occupied, the list needs to increase the size of its underlying
    array, thus triggering a memory reallocation that can take *O*(*N*) time. Nevertheless,
    those memory allocations are infrequent, and the time complexity for the append
    operation is referred to as amortized O(1) time.
  prefs: []
  type: TYPE_NORMAL
- en: The list operations that may have efficiency problems are those that add or
    remove elements at the beginning (or somewhere in the middle) of the list. When
    an item is inserted, or removed, from the beginning of a list, all the subsequent
    elements of the array need to be shifted by a position, thus taking *O*(*N*) time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, the timings for different operations on a list of size
    10,000 are shown; you can see how insertion and removal performances vary quite
    dramatically if performed at the beginning or at the end of the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Code** | ****N=10000 (******µs)** | ****N=20000 (******µs)** | ****N=30000
    (******µs)** | ****Time**** |'
  prefs: []
  type: TYPE_TB
- en: '| `list.pop()` | 0.50 | 0.59 | 0.58 | *O*(1) |'
  prefs: []
  type: TYPE_TB
- en: '| `list.pop(0)` | 4.20 | 8.36 | 12.09 | *O*(*N*) |'
  prefs: []
  type: TYPE_TB
- en: '| `list.append(1)` | 0.43 | 0.45 | 0.46 | *O*(1) |'
  prefs: []
  type: TYPE_TB
- en: '| `list.insert(0, 1)` | 6.20 | 11.97 | 17.41 | *O*(*N*) |'
  prefs: []
  type: TYPE_TB
- en: In some cases, it is necessary to efficiently perform insertion or removal of
    elements both at the beginning and at the end of the collection. Python provides
    a data structure with those properties in the `collections.deque` class. The word
    **deque** stands for double-ended queue because this data structure is designed
    to efficiently put and remove elements at the beginning and at the end of the
    collection, as it is in the case of queues. In Python, deques are implemented
    as doubly-linked lists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deques, in addition to `pop` and `append`, expose the `popleft` and `appendleft` methods that
    have *O*(1) running time:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Code** | ****N=10000 (******µs)** | ****N=20000 (******µs)** | ****N=30000
    (******µs)** | ****Time**** |'
  prefs: []
  type: TYPE_TB
- en: '| `deque.pop()` | 0.41 | 0.47 | 0.51 | *O*(1) |'
  prefs: []
  type: TYPE_TB
- en: '| `deque.popleft()` | 0.39 | 0.51 | 0.47 | *O*(1) |'
  prefs: []
  type: TYPE_TB
- en: '| `deque.append(1)` | 0.42 | 0.48 | 0.50 | *O*(1) |'
  prefs: []
  type: TYPE_TB
- en: '| `deque.appendleft(1)` | 0.38 | 0.47 | 0.51 | *O*(1) |'
  prefs: []
  type: TYPE_TB
- en: 'Despite these advantages, deques should not be used to replace regular lists
    in most cases. The efficiency gained by the `appendleft` and `popleft` operations
    comes at a cost: accessing an element in the middle of a deque is a O(N) operation,
    as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Code** | ****N=10000 (******µs)** | ****N=20000 (******µs)** | ****N=30000
    (******µs)** | ****Time**** |'
  prefs: []
  type: TYPE_TB
- en: '| `deque[0]` | 0.37 | 0.41 | 0.45 | *O*(1) |'
  prefs: []
  type: TYPE_TB
- en: '| `deque[N -  1]` | 0.37 | 0.42 | 0.43 | *O*(1) |'
  prefs: []
  type: TYPE_TB
- en: '| `deque[int(N / 2)]` | 1.14 | 1.71 | 2.48 | *O*(N) |'
  prefs: []
  type: TYPE_TB
- en: Searching for an item in a list is generally a *O*(*N*) operation and is performed
    using the `list.index` method. A simple way to speed up searches in lists is to
    keep the array sorted and perform a binary search using the `bisect` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `bisect` module allows fast searches on sorted arrays. The `bisect.bisect`
    function can be used on a sorted list to find the index to place an element while
    maintaining the array in sorted order. In the following example, we can see that
    if we want to insert the `3` element in the array while keeping `collection` in
    sorted order, we should put `3` in the third position (which corresponds to index
    2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This function uses the binary search algorithm that has *O*(*log*(*N*)) running
    time. Such a running time is exceptionally fast, and basically means that your
    running time will increase by a constant amount every time you *double* your input
    size. This means that if, for example, your program takes `1` second to run on
    an input of size `1000`, it will take `2` seconds to process an input of size
    `2000`, `3` seconds to process an input of size `4000`, and so on. If you had
    `100` seconds, you could theoretically process an input of size `10^(33)`, which
    is larger than the number of atoms in your body!
  prefs: []
  type: TYPE_NORMAL
- en: 'If the value we are trying to insert is already present in the list, the `bisect.bisect`
    function will return the location *after* the already present value.  Therefore,
    we can use the `bisect.bisect_left` variant, which returns the correct index in
    the following way (taken from the module documentation at [https://docs.python.org/3.5/library/bisect.html](https://docs.python.org/3.5/library/bisect.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following table, you can see how the running time of the `bisect` solution
    is barely affected at these input sizes, making it a suitable solution when searching
    through very large collections:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Code** | **N=10000 (****µs)** | **N=20000 (****µs)** | **N=30000 (****µs)**
    | **Time** |'
  prefs: []
  type: TYPE_TB
- en: '| `list.index(a)` | 87.55 | 171.06 | 263.17 | *O*(N) |'
  prefs: []
  type: TYPE_TB
- en: '| `index_bisect(list, a)` | 3.16 | 3.20 | 4.71 | *O*(log(N)) |'
  prefs: []
  type: TYPE_TB
- en: Dictionaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dictionaries are extremely versatile and extensively used in the Python language.
    Dictionaries are implemented as hash maps and are very good at element insertion,
    deletion, and access; all these operations have an average *O*(1) time complexity.
  prefs: []
  type: TYPE_NORMAL
- en: In Python versions up to 3.5, dictionaries are unordered collections. Since
    Python 3.6, dictionaries are capable of maintaining their elements by order of
    insertion.
  prefs: []
  type: TYPE_NORMAL
- en: 'A hash map is a data structure that associates a set of key-value pairs. The
    principle behind hash maps is to assign a specific index to each key so that its
    associated value can be stored in an array. The index can be obtained through
    the use of a `hash` function; Python implements hash functions for several data
    types. As a demonstration, the generic function to obtain hash codes is `hash`.
    In the following example, we show you how to obtain the hash code given the `"hello"` string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Hash maps can be tricky to implement because they need to handle collisions
    that happen when two different objects have the same hash code. However, all the
    complexity is elegantly hidden behind the implementation and the default collision
    resolution works well in most real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Access, insertion, and removal of an item in a dictionary scales as *O*(1) with
    the size of the dictionary. However, note that the computation of the hash function
    still needs to happen and, for strings, the computation scales with the length
    of the string. As string keys are usually relatively small, this doesn't constitute
    a problem in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'A dictionary can be used to efficiently count unique elements in a list. In
    this example, we define the `counter_dict` function that takes a list and returns
    a dictionary containing the number of occurrences of each value in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The code can be somewhat simplified using `collections.defaultdict`, which
    can be used to produce dictionaries where each new key is automatically assigned
    a default value. In the following code, the `defaultdict(int)` call produces a
    dictionary where every new element is automatically assigned a zero value, and
    can be used to streamline the counting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `collections` module also includes a `Counter` class that can be used for
    the same purpose with a single line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Speed-wise, all these ways of counting have the same time complexity, but the
    `Counter` implementation is the most efficient, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Code** | **N=1000 (****µs)** | **N=2000 (****µs)** | **N=3000 (****µs)**
    | **Time** |'
  prefs: []
  type: TYPE_TB
- en: '| `Counter(items)` | 51.48 | 96.63 | 140.26 | *O*(N) |'
  prefs: []
  type: TYPE_TB
- en: '| `counter_dict(items)` | 111.96 | 197.13 | 282.79 | *O*(N) |'
  prefs: []
  type: TYPE_TB
- en: '| `counter_defaultdict(items)` | 120.90 | 238.27 | 359.60 | *O*(N) |'
  prefs: []
  type: TYPE_TB
- en: Building an in-memory search index using a hash map
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dictionaries can be used to quickly search for a word in a list of documents,
    similar to a search engine. In this subsection, we will learn how to build an
    inverted index based on a dictionary of lists. Let''s say we have a collection
    of four documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple way to retrieve all the documents that match a query is to scan each
    document and test for the presence of a word. For example, if we want to look
    up the documents where the word `table` appears, we can employ the following filtering
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This approach is simple and works well when we have one-off queries; however,
    if we need to query the collection very often, it can be beneficial to optimize
    querying time. Since the per-query cost of the linear scan is *O*(*N*), you can
    imagine that a better scaling will allow us to handle much larger document collections.
  prefs: []
  type: TYPE_NORMAL
- en: A better strategy is to spend some time preprocessing the documents so that
    they are easier to find at query time. We can build a structure, called the **inverted
    index**, that associates each word in our collection with the list of documents
    where that word is present. In our earlier example, the word `"table"` will be
    associated to the `"the cat is under the table"` and `"the dog is under the table"` documents;
    they correspond to indices `0` and `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a mapping can be implemented by going over our collection of documents
    and storing in a dictionary the index of the documents where that term appears.
    The implementation is similar to the `counter_dict` function, except that, instead
    of accumulating a counter, we are growing the list of documents that match the
    current term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have built our index, doing a query involves a simple dictionary lookup.
    For example, if we want to return all the documents containing the term table,
    we can simply query the index, and retrieve the corresponding documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Since all it takes to query our collection is a dictionary access, the index
    can handle queries with time complexity *O*(1)! Thanks to the inverted index,
    we are now able to query any number of documents (as long as they fit in memory)
    in constant time. Needless to say, indexing is a technique widely used to quickly
    retrieve data not only in search engines, but also in databases and any system
    that requires fast searches.
  prefs: []
  type: TYPE_NORMAL
- en: Note that building an inverted index is an expensive operation and requires
    you to encode every possible query. This is a substantial drawback, but the benefits
    are great and it may be worthwhile to pay the price in terms of decreased flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sets are unordered collections of elements, with the additional restriction
    that the elements must be unique.  The main use-cases where sets are a good choice
    are membership tests (testing if an element is present in the collection) and,
    unsurprisingly, set operations such as union, difference, and intersection.
  prefs: []
  type: TYPE_NORMAL
- en: In Python, sets are implemented using a hash-based algorithm just like dictionaries;
    therefore, the time complexities for addition, deletion, and test for membership
    scale as *O*(1) with the size of the collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sets contain only unique elements. An immediate use case of sets is the removal
    of duplicates from a collection, which can be accomplished by simply passing the
    collection through the `set` constructor, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The time complexity for removing duplicates is *O*(*N*), as it requires to read
    the input and put each element in the set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sets expose a number of operations like union, intersection, and difference.
    The union of two sets is a new set containing all the elements of both the sets;
    the intersection is a new set that contains only the elements in common between
    the two sets, and the difference is a new set containing the element of the first
    set that are not contained in the second set. The time complexities for these
    operations are shown in the following table. Note that since we have two different
    input sizes, we will use the letter S to indicate the size of the first set (called
    `s`), and T to indicate the size of the second set (called `t`):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Code** | **Time** |'
  prefs: []
  type: TYPE_TB
- en: '| `s.union(t)` | *O*(*S* + *T*) |'
  prefs: []
  type: TYPE_TB
- en: '| `s.intersection(t)` | *O*(*min*(*S*, *T*)) |'
  prefs: []
  type: TYPE_TB
- en: '| `s.difference(t)` | *O*(*S*) |'
  prefs: []
  type: TYPE_TB
- en: An application of set operations are, for example, Boolean queries. Going back
    to the inverted index example of the previous subsection, we may want to support
    queries that include multiple terms. For example, we may want to search for all
    the documents that contain the words `cat` and `table`. This kind of a query can
    be efficiently computed by taking the intersection between the set of documents
    containing `cat` and the set of documents containing `table`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to efficiently support those operations, we can change our indexing
    code so that each term is associated to a set of documents (rather than a list).
    After applying this change, calculating more advanced queries is a matter of applying
    the right set operation. In the following code, we show the inverted index based
    on sets and the query using set operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Heaps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Heaps are data structures designed to quickly find and extract the maximum (or
    minimum) value in a collection. A typical use-case for heaps is to process a series
    of incoming tasks in order of maximum priority.
  prefs: []
  type: TYPE_NORMAL
- en: One can theoretically use a sorted list using the tools in the `bisect` module;
    however, while extracting the maximum value will take *O*(1) time (using `list.pop`),
    insertion will still take *O*(N) time (remember that, even if finding the insertion
    point takes *O*(*log*(*N*)) time, inserting an element in the middle of a list
    is still a *O*(*N*) operation). A heap is a more efficient data structure that
    allows for insertion and extraction of maximum values with *O*(*log*(*N*)) time
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, heaps are built using the procedures contained in the `heapq` module
    on an underlying list. For example, if we have a list of 10 elements, we can reorganize it
    into a heap with the `heapq.heapify` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform the insertion and extraction operations on the heap, we can use
    the `heapq.heappush` and `heapq.heappop` functions. The `heapq.heappop` function
    will extract the minimum value in the collection in *O*(*log*(*N*)) time and can
    be used in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you can push the integer `1`, with the `heapq.heappush` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Another easy-to-use option is the `queue.PriorityQueue` class that, as a bonus,
    is thread and process-safe. The `PriorityQueue` class can be filled up with elements
    using the `PriorityQueue.put` method, while `PriorityQueue.get` can be used to
    extract the minimum value in the collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If the maximum element is required, a simple trick is to multiply each element
    of the list by `-1`. In this way, the order of the elements will be inverted.
    Also, if you want to associate an object (for example, a task to run) to each
    number (which can represent the priority), one can insert tuples of the `(number,
    object)` form; the comparison operator for the tuple will be ordered with respect
    to its first element, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Tries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A perhaps less popular data structure, very useful in practice, is the trie
    (sometimes called prefix tree). Tries are extremely fast at matching a list of
    strings against a prefix. This is especially useful when implementing features
    such as search-as-you type and autocompletion, where the list of available completions
    is very large and short response times are required.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, Python does not include a trie implementation in its standard
    library; however, many efficient implementations are readily available through
    PyPI. The one we will use in this subsection is `patricia-trie`, a single-file,
    pure Python implementation of trie. As an example, we will use `patricia-trie`
    to perform the task of finding the longest prefix in a set of strings (just like
    autocompletion).
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, we can demonstrate how fast a trie is able to search through
    a list of strings. In order to generate a large amount of unique random strings,
    we can define a function, `random_string`. The `random_string` function will return
    a string composed of random uppercase characters and, while there is a chance
    to get duplicates, we can greatly reduce the probability of duplicates to the
    point of being negligible if we make the string long enough. The implementation
    of the `random_string` function is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can build a list of random strings and time how fast it searches for a prefix
    (in our case, the `"AA"` string) using the `str.startswith` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'List comprehension and `str.startwith` are already very optimized operations
    and, on this small dataset, the search takes only a millisecond or so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s try using a trie for the same operation. In this example, we will
    use the `patricia-trie` library that is installable through `pip`. The `patricia.trie` class
    implements a variant of the trie data structure with an interface similar to a
    dictionary. We can initialize our trie by creating a dictionary from our list
    of strings, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To query `patricia-trie` for a matching prefix, we can use the `trie.iter` method,
    which returns an iterator over the matching strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know how to initialize and query a trie, we can time the operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If you look closely, the timing for this input size is **60.1 µs**, which is
    about 30 times faster (1.76 ms = 1760 µs) than linear search! The speed up is
    so impressive because of the better computational complexity of the trie prefix
    search. Querying a trie has a time complexity *O*(*S*), where S is the length
    of the longest string in the collection, while the time complexity of a simple
    linear scan is *O*(*N*), where *N* is the size of the collection.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if we want to return all the prefixes that match, the running time
    will be proportional to the number of results that match the prefix. Therefore,
    when designing timing benchmarks, care must be taken to ensure that we are always
    returning the same number of results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scaling properties of a trie versus a linear scan for datasets of different
    sizes that contains ten prefix matches are shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | ****N=10000 (******µs)** | ****N=20000 (******µs)** | ****N=30000
    (******µs)** | ****Time**** |'
  prefs: []
  type: TYPE_TB
- en: '| Trie | 17.12 | 17.27 | 17.47 | *O*(*S*) |'
  prefs: []
  type: TYPE_TB
- en: '| Linear scan | 1978.44 | 4075.72 | 6398.06 | *O*(*N*) |'
  prefs: []
  type: TYPE_TB
- en: An interesting fact is that the implementation of `patricia-trie` is actually
    a single Python file; this clearly shows how simple and powerful a clever algorithm
    can be. For extra features and performance, other C-optimized trie libraries are
    also available, such as `datrie` and `marisa-trie`.
  prefs: []
  type: TYPE_NORMAL
- en: Caching and memoization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Caching is a great technique used to improve the performance of a wide range
    of applications. The idea behind caching is to store expensive results in a temporary
    location, called cache, that can be located in memory, on-disk, or in a remote
    location.
  prefs: []
  type: TYPE_NORMAL
- en: Web applications make extensive use of caching. In a web application, it often
    happens that users request a certain page at the same time. In this case, instead
    of recomputing the page for each user, the web application can compute it once
    and serve the user the already rendered page. Ideally, caching also needs a mechanism
    for invalidation so that if the page needs to be updated, we can recompute it
    before serving it again. Intelligent caching allows web applications to handle
    increasing number of users with less resources. Caching can also be done preemptively,
    such as the later sections of the video get buffered when watching a video online.
  prefs: []
  type: TYPE_NORMAL
- en: Caching is also used to improve the performance of certain algorithms. A great
    example is computing the Fibonacci sequence. Since computing the next number in
    the Fibonacci sequence requires the previous number in the sequence, one can store
    and reuse previous results, dramatically improving the running time. Storing and
    reusing the results of the previous function calls in an application is usually
    termed as **memoization**, and is one of the forms of caching. Several other algorithms
    can take advantage of memoization to gain impressive performance improvements,
    and this programming technique is commonly referred to as **dynamic programming**.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of caching, however, do not come for free. What we are actually
    doing is sacrificing some space to improve the speed of the application. Additionally,
    if the cache is stored in a location on the network, we may incur transfer costs
    and general time needed for communication. One should evaluate when it is convenient
    to use a cache and how much space we are willing to trade for an increase in speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the usefulness of this technique, the Python standard library includes
    a simple in-memory cache out of the box in the `functools` module. The `functools.lru_cache`
    decorator can be used to easily cache the results of a function. In the following
    example, we create a function, `sum2`, that prints a statement and returns the
    sum of two numbers. By running the function twice, you can see that the first
    time the `sum2` function is executed the `"Calculating ..."` string is produced,
    while the second time the result is returned without running the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `lru_cache` decorator also provides other basic features. To restrict the
    size of the cache, one can set the number of elements that we intend to maintain
    through the `max_size` argument. If we want our cache size to be unbounded, we
    can specify a value of `None`. An example usage of `max_size` is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In this way, as we execute `sum2` with different arguments, the cache will reach
    a maximum size of `16` and, as we keep requesting more calculations, new values will
    replace older values in the cache. The `lru` prefix originates from this strategy,
    which means least recently used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `lru_cache` decorator also adds extra functionalities to the decorated
    function. For example, it is possible to examine the cache performance using the
    `cache_info` method, and it is possible to reset the cache using the `cache_clear`
    method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As an example, we can see how a problem, such as computing the fibonacci series,
    may benefit from caching. We can define a `fibonacci` function and time its execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The execution takes 5.57 ms, which is very high. The scaling of the function
    written in this way has poor performance; the previously computed fibonacci sequences are
    not reused, causing this algorithm to have an exponential scaling of roughly *O*(*2^N*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Caching can improve this algorithm by storing and reusing the already-computed
    fibonacci numbers. To implement the cached version, it is sufficient to apply
    the `lru_cache` decorator to the original `fibonacci` function. Also, to design
    a proper benchmark, we need to ensure that a new cache is instantiated for every
    run; to do this, we can use the `timeit.repeat` function, as shown in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Even though we changed the algorithm by adding a simple decorator, the running
    time now is much less than a microsecond. The reason is that, thanks to caching,
    we now have a linear time algorithm instead of an exponential one.
  prefs: []
  type: TYPE_NORMAL
- en: The `lru_cache` decorator can be used to implement simple in-memory caching
    in your application. For more advanced use cases, third-party modules can be used
    for more powerful implementation and on-disk caching.
  prefs: []
  type: TYPE_NORMAL
- en: Joblib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A simple library that, among other things, provides a simple on-disk cache is
    `joblib`. The package can be used in a similar way as `lru_cache`, except that
    the results will be stored on disk and will persist between runs.
  prefs: []
  type: TYPE_NORMAL
- en: The `joblib` module can be installed from PyPI using the `pip install joblib` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `joblib` module provides the `Memory` class that can be used to memoize
    functions using the `Memory.cache` decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The function will behave similar to `lru_cache`, with the exception that the
    results will be stored on-disk in the directory specified by the `cachedir` argument
    during `Memory` initialization. Additionally, the cached results will persist over
    subsequent runs!
  prefs: []
  type: TYPE_NORMAL
- en: The `Memory.cache` method also allows to limit recomputation only when certain
    arguments change, and the resulting decorated function supports basic functionalities
    to clear and analyze the cache.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the best `joblib` feature is that, thanks to intelligent hashing algorithms,
    it provides efficient memoization of functions that operate on `numpy` arrays,
    and is particularly useful in scientific and engineering applications.
  prefs: []
  type: TYPE_NORMAL
- en: Comprehensions and generators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore a few simple strategies to speed up Python
    loops using comprehension and generators. In Python, comprehension and generator
    expressions are fairly optimized operations and should be preferred in place of
    explicit for-loops. Another reason to use this construct is readability; even
    if the speedup over a standard loop is modest, the comprehension and generator
    syntax is more compact and (most of the times) more intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see that both the list comprehension and generator
    expressions are faster than an explicit loop when combined with the `sum` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like lists, it is possible to use `dict` comprehension to build dictionaries
    slightly more efficiently and compactly, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Efficient looping (especially in terms of memory) can be implemented using
    iterators and functions such as `filter` and `map`. As an example, consider the
    problem of applying a series of operations to a list using list comprehension
    and then taking the maximum value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The problem with this approach is that for every list comprehension, we are
    allocating a new list, increasing memory usage. Instead of using list comprehension,
    we can employ generators. Generators are objects that, when iterated upon, compute
    a value on the fly and return the result.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the `map` function takes two arguments--a function and an iterator--and
    returns a generator that applies the function to every element of the collection.
    The important point is that the operation happens only *while we are iterating*,
    and not when `map` is invoked!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite the previous function using `map` and by creating intermediate
    generators, rather than lists, thus saving memory by computing the values on the
    fly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can profile the memory of the two solutions using the `memory_profiler`
    extension from an IPython session. The extension provides a small utility, `%memit`,
    that will help us evaluate the memory usage of a Python statement in a way similar
    to `%timeit`, as illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the memory used by the first version is `102.54 MiB`, while
    the second version consumes `0.00 MiB`! For the interested reader, more functions
    that return generators can be found in the `itertools` module, which provides
    a set of utilities designed to handle common iteration patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithmic optimization can improve how your application scales as we process
    increasingly large data. In this chapter, we demonstrated use-cases and running
    times of the most common data structures available in Python, such as lists, deques,
    dictionaries, heaps, and tries. We also covered caching, a technique that can
    be used to trade some space, in memory or on-disk, in exchange for increased responsiveness
    of an application. We also demonstrated how to get modest speed gains by replacing
    for-loops with fast constructs, such as list comprehensions and generator expressions.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent chapters, we will learn how to improve performance further
    using numerical libraries such as `numpy`, and how to write extension modules
    in a lower-level language with the help of *Cython*.
  prefs: []
  type: TYPE_NORMAL
