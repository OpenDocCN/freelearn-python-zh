<html><head></head><body>
<div><div><div><h1 id="_idParaDest-167"><em class="italic"><a id="_idTextAnchor158"/>Chapter 9</em>: Concurrent Web Requests</h1>
			<p>This chapter will focus on concurrently making web requests. Intuitively, making requests to a web page to collect information about it is independent of applying the same task to another web page. This means that concurrency, specifically threading in this case, can be a powerful tool that provides a significant speedup in this process. In this chapter, we will learn about the fundamentals of web requests and how to interact with websites using Python. We will also learn how concurrency can help us make multiple requests efficiently. Finally, we will look at several good practices regarding web requests.</p>
			<p>Overall, this chapter serves as a practical exercise for us to become more comfortable with concurrency in Python, which will help you tackle future concurrent programming projects with more confidence.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>The basics of web requests</li>
				<li>The requests module</li>
				<li>Concurrent web requests</li>
				<li>The problem with timeouts</li>
				<li>Good practices in making web requests</li>
			</ul>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor159"/>The basics of web requests</h1>
			<p>The worldwide capacity to generate data is estimated to double in size every 2 years. Even though <a id="_idIndexMarker746"/>there is an interdisciplinary field known as <strong class="bold">data science</strong> that is entirely dedicated to studying data, almost every programming task in <a id="_idIndexMarker747"/>software development also has something to do with collecting and <a id="_idIndexMarker748"/>analyzing data. A significant part of this is, of course, <strong class="bold">data collection</strong>. However, the data that we need for our applications is sometimes not stored nicely and cleanly in a database – sometimes, we need to collect the data we need from web pages.</p>
			<p>For example, <strong class="bold">web scraping</strong> is a data extraction method that automatically makes requests to <a id="_idIndexMarker749"/>web pages and downloads specific information. Web scraping allows us to comb through numerous websites and collect any data we need systematically and consistently. The collected data can be analyzed later by our applications or simply saved on our computers in various formats. An example of this would be Google, which maintains and runs numerous web scrapers of its own to find and index web pages for its search engines.</p>
			<p>The Python language itself provides several good options for applications of this kind. In this chapter, we will mainly work with the <code>requests</code> module to make client-side web requests from our Python programs. However, before we look into this module in more detail, we need to understand some web terminology to be able to effectively design our applications.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor160"/>HTML</h2>
			<p><code>.html</code> file extension. In an HTML <a id="_idIndexMarker751"/>document, text is surrounded and delimited by tags, written in angle brackets; that is, <code>&lt;p&gt;</code>, <code>&lt;img&gt;</code>, <code>&lt;i&gt;</code>, and so on. These tags typically consist of pairs – an opening tag and a closing tag – indicating the styling or the nature of the data included inside.</p>
			<p>It is also possible to include other forms of media in HTML code, such as images or videos. Numerous other tags are used in common HTML documents. Some specify a group of elements that share some common characteristics, such as <code>&lt;id&gt;&lt;/id&gt;</code> and <code>&lt;class&gt;&lt;/class&gt;</code>.</p>
			<p>The following is an example of HTML code:</p>
			<div><div><img src="img/Figure_9.1_B17499.jpg" alt="Figure 9.1 – Sample HTML code " width="1287" height="867"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Sample HTML code</p>
			<p>Fortunately, detailed knowledge of what each HTML tag accomplishes is not required for us to <a id="_idIndexMarker752"/>make effective web requests. As we will see later in this chapter, the more essential part of making web requests is the ability to interact with web pages efficiently.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor161"/>HTTP requests</h2>
			<p>In a <a id="_idIndexMarker753"/>typical communication process on the web, HTML text <a id="_idIndexMarker754"/>is the data that is to be saved and/or further processed. This kind of data needs to be collected from web pages, but how can we go about doing that? Most <a id="_idIndexMarker755"/>of the communication is done via the <a id="_idIndexMarker756"/>internet – more specifically, the <strong class="bold">World Wide Web</strong> (<strong class="bold">WWW</strong>) – and this utilizes the <strong class="bold">Hypertext Transfer Protocol</strong> (<strong class="bold">HTTP</strong>). In HTTP, request methods are used to convey the information of what data is being requested and should be sent back from a server.</p>
			<p>For example, when you type <code>packtpub.com</code> in your browser, the browser sends a request method via HTTP to the Packt website's main server, asking for data from the website. Now, if both your internet connection and Packt's server are working well, then your browser will receive a response from the server, as shown in the following diagram. This response will be in the form of an HTML document, which will be interpreted by your browser, and your browser will display the corresponding HTML output on the screen:</p>
			<div><div><img src="img/Figure_9.2_B17499.jpg" alt="Figure 9.2 – Diagram of HTTP communication " width="1389" height="416"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Diagram of HTTP communication</p>
			<p>Generally, request <a id="_idIndexMarker757"/>methods are defined as verbs that <a id="_idIndexMarker758"/>indicate the desired action to be performed while the HTTP client (web browsers) and the server communicate with each other: <code>GET</code>, <code>HEAD</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, and so on. Of these methods, <code>GET</code> and <code>POST</code> are two of the most common request methods that are used in web scraping applications; their functionality is described here:</p>
			<ul>
				<li>The <code>GET</code> method requests <a id="_idIndexMarker759"/>specific data from the server. This method only retrieves data and has no other effect on the server and its databases.</li>
				<li>The <code>POST</code> method sends <a id="_idIndexMarker760"/>data in a specific form that is accepted by the server. This data could be, for example, a message to a bulletin board, mailing list, or newsgroup, information to be submitted to a web form, or an item to be added to a database.</li>
			</ul>
			<p>All general-purpose HTTP servers that we commonly see on the internet are required to implement at least the <code>GET</code> (and <code>HEAD</code>) method, while the <code>POST</code> method is considered optional.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor162"/>HTTP status code</h2>
			<p>It is not always the case that when a web request is made and sent to a web server, the server will <a id="_idIndexMarker761"/>process the request and return the requested data without fail. Sometimes, the server might be completely down or already busy interacting <a id="_idIndexMarker762"/>with other clients and therefore unresponsive to a new request; sometimes, the client itself makes bad requests to a server (for example, incorrectly formatted or malicious requests).</p>
			<p>As a way to categorize these problems as well as provide the most information possible during the communication resulting from a web request, HTTP requires servers to respond to <a id="_idIndexMarker763"/>each request from its clients with an <strong class="bold">HTTP response status code</strong>. A status code is typically a three-digit number that indicates the specific characteristics of the response that the server sends back to a client.</p>
			<p>In total, there are five large categories of HTTP response status codes, indicated by the first digit of the code. They are as follows:</p>
			<ul>
				<li><strong class="bold">1xx (informational status code)</strong>: The request was received, and the server is processing it. For example, <strong class="bold">100</strong> means that the request header has been received <a id="_idIndexMarker764"/>and that the server is waiting for the request body; <strong class="bold">102</strong> indicates that the request is currently being processed (this is used for large requests and to prevent clients from timing out).</li>
				<li><strong class="bold">2xx (successful status code)</strong>: The request was successfully received, understood, and processed by the server. For example, <strong class="bold">200</strong> means the request was <a id="_idIndexMarker765"/>successfully fulfilled; <strong class="bold">202</strong> indicates that the request has been accepted for processing, but the processing itself is not complete.</li>
				<li><strong class="bold">3xx (redirectional status code)</strong>: Additional actions need to be taken so that the <a id="_idIndexMarker766"/>request can be successfully processed. For example, <strong class="bold">300</strong> means that there are multiple options regarding how the response from the server should be processed (for example, giving the client multiple video format options when a video file is to be downloaded); <strong class="bold">301</strong> indicates that the server has been moved permanently and all requests should be directed to another address (provided in the response from the server).</li>
				<li><strong class="bold">4xx (error status code for the client)</strong>: The request was incorrectly formatted by the client and could not be processed. For example, <strong class="bold">400</strong> means that <a id="_idIndexMarker767"/>the client sent in a bad request (for example, a syntax error or the size of the request is too large); <strong class="bold">404</strong> (arguably the most well-known status code) indicates that the request method is not supported by the server.</li>
				<li><strong class="bold">5xx (error status code for the server)</strong>: The request, although valid, could not <a id="_idIndexMarker768"/>be processed by the server. For example, <strong class="bold">500</strong> means that there is an internal server error in which an unexpected condition was encountered; <strong class="bold">504</strong> (Gateway Timeout) means that the server, which was acting as a gateway or a proxy, did not receive a response from the final server in time.</li>
			</ul>
			<p>A lot more can be said about these status codes, but it is already sufficient for us to keep the big <a id="_idIndexMarker769"/>five categories previously mentioned in mind when making web <a id="_idIndexMarker770"/>requests from Python. If you <a id="_idIndexMarker771"/>would like to find more specific information about these or other status codes, the <strong class="bold">Internet Assigned Numbers Authority</strong> (<strong class="bold">IANA</strong>) maintains the official registry of HTTP status codes. Now, let's start learning about making web requests in Python.</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor163"/>The requests module</h1>
			<p>The <code>requests</code> module allows its users to make and send HTTP request methods. In the applications <a id="_idIndexMarker772"/>that we will be considering, it is mainly used to make contact with the server of the web pages we want to extract data from and obtain the response for the server.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">According to the official documentation of the module, the use of Python 3 is <code>requests</code>.</p>
			<p>To install the module on your computer, run one of the following commands:</p>
			<pre>pip install requests
conda install requests</pre>
			<p>These commands <a id="_idIndexMarker773"/>should install <code>requests</code> and any other required dependencies (<code>idna</code>, <code>certifi</code>, <code>urllib3</code>, and so on) for you if your system does not have those already. After this, run <code>import requests</code> in a Python interpreter to confirm that the module has been installed successfully. Next, we will use <code>requests</code> to build the sequential, non-concurrent version of our program.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor164"/>Making a request in Python</h2>
			<p>Let's look <a id="_idIndexMarker774"/>at an example usage of the module, as shown in the following code:</p>
			<pre>import requests
url = 'http://www.google.com'
res = requests.get(url)
print(res.status_code)
print(res.headers)
with open('google.html', 'w') as f:
    f.write(res.text)
print('Done.')</pre>
			<p>In this example, we are using the <code>requests</code> module to download the HTML code of a web page; that is, <code>www.google.com</code>. The <code>requests.get()</code> method sends a <code>GET</code> request method to <code>url</code> and we store the response in the <code>res</code> variable. After checking the status and headers of the response by printing them out, we create a file called <code>google.html</code> and write the HTML code, which is stored in the response text, to the file.</p>
			<p>After running <a id="_idIndexMarker775"/>the program (assuming that your internet is working and that the Google server is not down), you should get the following output:</p>
			<pre>200
{'Date': 'Sat, 17 Nov 2018 23:08:58 GMT', 'Expires': '-1', 
'Cache-Control': 'private, max-age=0', 'Content-Type': 'text/
html; charset=ISO-8859-1', 'P3P': 'CP="This is not a P3P 
policy! See g.co/p3phelp for more info."', 'X-XSS-Protection': 
'1; mode=block', 'X-Frame-Options': 'SAMEORIGIN', 'Content-
Encoding': 'gzip', 'Server': 'gws', 'Content-Length': '4958', 
'Set-Cookie': '1P_JAR=2018-11-17-23; expires=Mon, 17-Dec-2018 
23:08:58 GMT; path=/; domain=.google.com, NID=146=NHT7fic3mjBO_
vdiFB3-gqnFPyGN1EGxyMkkNPnFMEVsqjGJ8S0EwrivDBWBgUS7hCPZGHbos
LE4uxz31shnr3X4adRpe7uICEiK8qh3Asu6LH_bIKSLWStAp8gMK1f9_GnQ0_
JKQoMvG-OLrT_fwV0hwTR5r2UVYsUJ6xHtX2s; expires=Sun, 19-May-2019 
23:08:58 GMT; path=/; domain=.google.com; HttpOnly'}
Done.</pre>
			<p>The response had a <code>200</code> status code, which we know means that the request has been completed. The header of the response, which is stored in <code>res.headers</code>, also contains further specific information regarding the response. For example, we can see the date and time the request was made, that the content of the response is text and HTML, and that the total length of the content is <code>4958</code>.</p>
			<p>The data that was sent from the server was also written to the <code>google.html</code> file. When you open this file in a text editor, you will be able to see the HTML code of the web page that we have downloaded using <code>requests</code>. On the other hand, if you use a web browser to open the file, you will see how <strong class="bold">most</strong> of the information from the original web page is now being displayed through a downloaded offline file.</p>
			<p>For example, the following is how Google Chrome interprets the HTML file on my system:</p>
			<div><div><img src="img/Figure_9.3_B17499.jpg" alt="Figure 9.3 – Downloaded HTML opened offline " width="907" height="430"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – Downloaded HTML opened offline</p>
			<p>There is other information that is stored on the server that the web pages of that server refer to. This means that not all of the information that an online web page provides can be downloaded via a <code>GET</code> request, and this is why offline HTML code sometimes fails to contain all <a id="_idIndexMarker776"/>of the information available on the online web page that it was downloaded from. (For example, the downloaded HTML code in the preceding screenshot does not display the Google icon correctly.)</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor165"/>Running a ping test</h2>
			<p>With the basic knowledge of HTTP requests and the <code>requests</code> module in Python in hand, we will, for the <a id="_idIndexMarker777"/>remaining portion of this chapter, tackle the central problem of running a <strong class="bold">ping test</strong>. A ping test is a procedure in which you test <a id="_idIndexMarker778"/>the communication between your system and specific web servers, simply by requesting each of the servers in question. By considering the HTTP response status code (potentially) returned by the server, the test is used to evaluate either the internet connection of your system or the availability of the servers.</p>
			<p>Ping tests are quite common among web administrators, who usually have to manage a large number of websites simultaneously. It is a good tool to quickly identify pages that are unexpectedly unresponsive or down. Many tools provide you with powerful options regarding ping tests and, in this chapter, we will be designing a ping test application that can concurrently send multiple web requests at the same time.</p>
			<p>To simulate different HTTP response status codes to be sent back to our program, we will be using <a href="http://httpstat.us">httpstat.us</a>, a website that can generate various status codes and is commonly used to test how applications that make web requests can handle varying responses. Specifically, to use a request that will return a <code>200</code> status code in a program, we can simply send the request <a href="http://httpstat.us/200">httpstat.us/200</a>; the same applies to other status codes. In our ping test program, we will have a list of <a href="http://httpstat.us">httpstat.us</a> URLs with different status codes.</p>
			<p>Let's take <a id="_idIndexMarker779"/>a look at the following code:</p>
			<pre>import requests
def ping(url):
    res = requests.get(url)
    print(f'{url}: {res.text}')
urls = [
    'http://httpstat.us/200',
    'http://httpstat.us/400',
    'http://httpstat.us/404',
    'http://httpstat.us/408',
    'http://httpstat.us/500',
    'http://httpstat.us/511'
]
for url in urls:
    ping(url)
print('Done.')</pre>
			<p>In this program, the <code>ping()</code> function takes in a URL and attempts to make a <code>GET</code> request to the site. Then, it prints out the content of the response returned by the server. In our main program, we have a list of different status codes that we mentioned earlier, each of which we will go through and call the <code>ping()</code> function on.</p>
			<p>The final output, after running the preceding example, should be as follows:</p>
			<pre>http://httpstat.us/200: 200 OK
http://httpstat.us/400: 400 Bad Request
http://httpstat.us/404: 404 Not Found
http://httpstat.us/408: 408 Request Timeout
http://httpstat.us/500: 500 Internal Server Error
http://httpstat.us/511: 511 Network Authentication Required
Done.</pre>
			<p>Here, we <a id="_idIndexMarker780"/>can see that our ping test program was able to obtain the corresponding responses from the server. However, our current program is purely sequential, and we would like to implement a concurrent version of it. We will do this in the next section.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor166"/>Concurrent web requests</h1>
			<p>In the context of concurrent programming, we can see that the process of making a request to <a id="_idIndexMarker781"/>a web server and obtaining the returned response is independent of the same procedure for a different web server. This is to say that we could apply concurrency and parallelism to our ping test application to speed up our execution.</p>
			<p>In the concurrent ping test applications that we are designing, multiple HTTP requests will be made to the server simultaneously and the corresponding responses will be sent back to our program, as shown in the following diagram: </p>
			<div><div><img src="img/Figure_9.4_B17499.jpg" alt="Figure 9.4 – Parallel HTTP requests " width="1225" height="593"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – Parallel HTTP requests</p>
			<p>As we mentioned previously, concurrency and parallelism have significant applications in web <a id="_idIndexMarker782"/>development, and most servers nowadays can handle a large number of requests at the same time.</p>
			<p>Now, let's see how we can make multiple web requests at the same time, with the help of <code>threading</code>.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor167"/>Spawning multiple threads</h2>
			<p>To apply concurrency, we can simply use the <code>threading</code> module that we have been discussing <a id="_idIndexMarker783"/>to create separate threads to handle different web requests. Let's take a look at the following code:</p>
			<pre>import threading
import requests
import time
def ping(url):
    res = requests.get(url)
    print(f'{url}: {res.text}')
urls = [
    'http://httpstat.us/200',
    'http://httpstat.us/400',
    'http://httpstat.us/404',
    'http://httpstat.us/408',
    'http://httpstat.us/500',
    'http://httpstat.us/524'
]
start = time.time()
for url in urls:
    ping(url)
print(f'Sequential: {time.time() - start : .2f} seconds')
print()
start = time.time()
threads = []
for url in urls:
    thread = threading.Thread(target=ping, args=(url,))
    threads.append(thread)
    thread.start()
for thread in threads:
    thread.join()
print(f'Threading: {time.time() - start : .2f} seconds')</pre>
			<p>In this example, we are including the sequential logic from the previous example to process our URL list so that we can compare the speed improvement when we apply threading <a id="_idIndexMarker784"/>to our ping test program. We are also creating a thread to ping each of the URLs in our URL list using the <code>threading</code> module; these threads will be executed independently from each other. The time it takes to process the URLs both sequentially and concurrently is also tracked using methods from the <code>time</code> module.</p>
			<p>If you run the program, your output should be similar to the following:</p>
			<pre>http://httpstat.us/200: 200 OK
http://httpstat.us/400: 400 Bad Request
http://httpstat.us/404: 404 Not Found
http://httpstat.us/408: 408 Request Timeout
http://httpstat.us/500: 500 Internal Server Error
http://httpstat.us/524: 524 A timeout occurred
Sequential: 0.82 seconds
http://httpstat.us/404: 404 Not Found
http://httpstat.us/200: 200 OK
http://httpstat.us/400: 400 Bad Request
http://httpstat.us/500: 500 Internal Server Error
http://httpstat.us/524: 524 A timeout occurred
http://httpstat.us/408: 408 Request Timeout
Threading: 0.14 seconds</pre>
			<p>While the specific time that the sequential logic and threading logic takes to process all the URLs might be different from system to system, there should still be a clear distinction between the two. Specifically, here, we can see that the threading logic was almost <a id="_idIndexMarker785"/>six times faster than the sequential logic (which corresponds to the fact that we had six threads processing six URLs in parallel). There is no doubt, then, that concurrency can provide a significant speedup for our ping test application and for the process of making web requests in general.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor168"/>Refactoring request logic</h2>
			<p>The current version of our ping test application works as intended, but we can improve its <a id="_idIndexMarker786"/>readability by refactoring the logic where we make web requests in a thread class. Consider the <code>MyThread</code> class:</p>
			<pre>import threading
import requests
class MyThread(threading.Thread):
    def __init__(self, url):
        threading.Thread.__init__(self)
        self.url = url
        self.result = None
    def run(self):
        res = requests.get(self.url)
        self.result = f'{self.url}: {res.text}'</pre>
			<p>In this example, <code>MyThread</code> inherits from the <code>threading.Thread</code> class and contains two additional attributes: <code>url</code> and <code>result</code>. The <code>url</code> attribute holds the URL that the thread instance should process; the response that's returned from the web server to that thread will be written to the <code>result</code> attribute (in the <code>run()</code> function).</p>
			<p>Outside of this class, we can simply loop through the URL list and create and manage the threads <a id="_idIndexMarker787"/>accordingly, while not having to worry about the request logic in the main program:</p>
			<pre>urls = [
    'http://httpstat.us/200',
    'http://httpstat.us/400',
    'http://httpstat.us/404',
    'http://httpstat.us/408',
    'http://httpstat.us/500',
    'http://httpstat.us/524'
]
start = time.time()
threads = [MyThread(url) for url in urls]
for thread in threads:
    thread.start()
for thread in threads:
    thread.join()
for thread in threads:
    print(thread.result)
print(f'Took {time.time() - start : .2f} seconds')
print('Done.')</pre>
			<p>Note that we are now storing the responses in the <code>result</code> attribute of the <code>MyThread</code> class, instead of directly printing them out, as we did in the old <code>ping()</code> function from the previous examples. This means that, after making sure that all the threads have finished, we will need to loop through the threads one more time and print out those responses.</p>
			<p>Refactoring <a id="_idIndexMarker788"/>the request logic should not greatly affect the performance of our current program; we are keeping track of the execution speed to see if this is the case. If you execute the program, you will obtain an output similar to the following:</p>
			<pre>http://httpstat.us/200: 200 OK
http://httpstat.us/400: 400 Bad Request
http://httpstat.us/404: 404 Not Found
http://httpstat.us/408: 408 Request Timeout
http://httpstat.us/500: 500 Internal Server Error
http://httpstat.us/524: 524 A timeout occurred
Took 0.14 seconds
Done.</pre>
			<p>Just as we expected, we are still achieving a significant speedup from the sequential version of the program with this refactored request logic. Again, our main program is now more readable, and further adjustments to the request logic (as we will see in the next section) can simply be directed to the <code>MyThread</code> class, without affecting the rest of the program.</p>
			<p>Our program can now make concurrent web requests to specific sites and display the returned status code. However, there is a problem common in working with web requests that our program cannot handle yet: timeouts. We will learn how to address this in the next section.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor169"/>The problem with timeouts</h1>
			<p>In this section, we will <a id="_idIndexMarker789"/>explore a potential improvement we can make to our ping test application: handling <strong class="bold">timeouts</strong>. Timeouts typically occur when the server takes an unusually long time to process a specific request, and the connection between the server and its client is terminated.</p>
			<p>In the context of a ping test application, we will be implementing a customized threshold for the timeout. Recall that a ping test is used to determine whether specific servers are <a id="_idIndexMarker790"/>still responsive, so we can specify in our program that, if a request takes more than our timeout threshold for the server to respond, we will categorize that specific server with a timeout.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor170"/>Support from httpstat.us and simulation in Python</h2>
			<p>In addition to different options for status codes, the <a href="http://httpstat.us">httpstat.us</a> website also provides us with <a id="_idIndexMarker791"/>a way to simulate a delay in its response when we send in requests. Specifically, we can customize the delay time (in milliseconds) with a query argument in our <code>GET</code> request. For example, <a href="http://httpstat.us/200?sleep=5000">httpstat.us/200?sleep=5000</a> will return a response after a 5-second delay.</p>
			<p>Now, let's see how a delay like this would affect the execution of our program. Consider the following program, which contains the current request logic of our ping test application but has a different URL list:</p>
			<pre>import threading
import requests
class MyThread(threading.Thread):
    def __init__(self, url):
        threading.Thread.__init__(self)
        self.url = url
        self.result = None
    def run(self):
        res = requests.get(self.url)
        self.result = f'{self.url}: {res.text}'
urls = [
    'http://httpstat.us/200',
    'http://httpstat.us/200?sleep=20000',
    'http://httpstat.us/400'
]
threads = [MyThread(url) for url in urls]
for thread in threads:
    thread.start()
for thread in threads:
    thread.join()
for thread in threads:
    print(thread.result)
print('Done.')</pre>
			<p>Here, we have a URL that will take around 20 seconds to return a response. Considering that we <a id="_idIndexMarker792"/>will block the main program until all the threads finish their execution (with the <code>join()</code> method), our program will most likely appear to be hanging for 20 seconds before any response is printed out.</p>
			<p>Run the program to experience this for yourself. A 20-second delay will occur (which will make the execution take significantly longer to finish) and we will obtain the following output:</p>
			<pre>http://httpstat.us/200: 200 OK
http://httpstat.us/200?sleep=20000: 200 OK
http://httpstat.us/400: 400 Bad Request
Took 22.60 seconds
Done.</pre>
			<p>Let's say <a id="_idIndexMarker793"/>that 20 seconds is too long of a response time, and we cannot afford to wait for a request that long. So, we would like to implement some logic that can handle long waiting times like this.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor171"/>Timeout specifications</h2>
			<p>Overall, an efficient ping test application should not be waiting for responses from its websites <a id="_idIndexMarker794"/>for a long time; it should have a set threshold for a timeout that, if a server fails to return a response under that threshold, the application will deem that server non-responsive. So, we need to implement a way to keep track of how much time has passed since a request has been sent to a server. We will do this by counting down from the timeout threshold. Once that threshold has been passed, all the responses (whether they've returned yet or not) will be printed out.</p>
			<p>Additionally, we will also be keeping track of how many requests are still pending and have not had their responses returned. We will be using the <code>is_alive()</code> method from the <code>threading.Thread</code> class to indirectly determine whether a response has been returned for a specific request. If, at one point, the thread that's processing a specific request is alive, we can conclude that that specific request is still pending.</p>
			<p>Let's consider the following <code>process_requests()</code> function first:</p>
			<pre>import time
UPDATE_INTERVAL = 0.01
def process_requests(threads, timeout=5):
    def alive_count():
        alive = [1 if thread.is_alive() else 0 for thread \
          in threads]
        return sum(alive)
    while alive_count() &gt; 0 and timeout &gt; 0:
        timeout -= UPDATE_INTERVAL
        time.sleep(UPDATE_INTERVAL)
    for thread in threads:
        print(thread.result)</pre>
			<p>This function takes in a list of threads that we have been using to make web requests in the previous examples, as well as an optional argument specifying the timeout threshold. Inside this function, we have an inner function, <code>alive_count()</code>, which returns the count of the threads that are still alive at the time of the function call.</p>
			<p>In the <code>process_requests()</code> function, so long as there are threads that are currently alive and <a id="_idIndexMarker795"/>processing requests, we will allow the threads to continue with their execution (this is done in the <code>while</code> loop with the double condition). The <code>UPDATE_INTERVAL</code> variable, as you can see, specifies how often we check for this condition. If either condition fails (if there are no alive threads left or if the threshold timeout is passed), then we will proceed with printing out the responses (even if some might have not been returned).</p>
			<p>Let's turn our attention to the new <code>MyThread</code> class:</p>
			<pre>import threading
import requests
class MyThread(threading.Thread):
    def __init__(self, url):
        threading.Thread.__init__(self)
        self.url = url
        self.result = f'{self.url}: Custom timeout'
    def run(self):
        res = requests.get(self.url)
        self.result = f'{self.url}: {res.text}'</pre>
			<p>This class is almost identical to the one we considered in the previous example, except that the initial value for the <code>result</code> attribute is a message indicating a timeout. In the case that <a id="_idIndexMarker796"/>we discussed earlier, where the timeout threshold specified in the <code>process_requests()</code> function is passed, this initial value will be used when the responses are printed out.</p>
			<p>Finally, let's consider our main program in <code>example6.py</code>:</p>
			<pre>urls = [
    'http://httpstat.us/200',
    'http://httpstat.us/200?sleep=4000',
    'http://httpstat.us/200?sleep=20000',
    'http://httpstat.us/400'
]
start = time.time()
threads = [MyThread(url) for url in urls]
for thread in threads:
    thread.setDaemon(True)
    thread.start()
process_requests(threads)
print(f'Took {time.time() - start : .2f} seconds')
print('Done.')</pre>
			<p>Here, in our URL list, we have a request that would take 4 seconds and another that would take 20 seconds, aside from the ones that would respond immediately. As the timeout threshold that we are using is 5 seconds, theoretically, we should be able to see that the 4-second-delay request will successfully obtain a response, while the 20-second-delay one will not.</p>
			<p>There is another point to be made about this program: <code>process_requests()</code> function, if the timeout threshold is passed while there is still at least one <a id="_idIndexMarker797"/>thread being processed, then the function will proceed to print out the <code>result</code> attribute of each thread:</p>
			<pre>while alive_count() &gt; 0 and timeout &gt; 0:
    timeout -= UPDATE_INTERVAL
    time.sleep(UPDATE_INTERVAL)
for thread in threads:
    print(thread.result)</pre>
			<p>This means that we do not block our program until all of the threads have finished their execution by using the <code>join()</code> function, which means the program can simply move forward if the timeout threshold is reached. However, this also means that the threads themselves do not terminate at this point. The 20-second-delay request, specifically, will still most likely be running after our program exits out of the <code>process_requests()</code> function.</p>
			<p>If the thread that's processing this request is not a daemon thread (as we know, daemon threads execute in the background and never terminate), it will block the main program from finishing until the thread itself finishes. By making this thread, and any other thread, a daemon thread, we allow the main program to finish as soon as it executes the last line of its instructions, even if there are threads still running.</p>
			<p>Let's see this program in action. Execute this code; your output should be similar to the following:</p>
			<pre>http://httpstat.us/200: 200 OK
http://httpstat.us/200?sleep=4000: 200 OK
http://httpstat.us/200?sleep=20000: Custom timeout
http://httpstat.us/400: 400 Bad Request
Took 5.70 seconds
Done.</pre>
			<p>As you can see, it took around 5 seconds for our program to finish this time. This is because it spent 5 seconds waiting for the threads that were still running and, as soon as the 5-second threshold was passed, the program printed out the results. Here, we can see that the result from the 20-second-delay request was simply the default value of the <code>result</code> attribute of the <code>MyThread</code> class, while the rest of the requests were able to obtain the correct response from the server (including the 4-second-delay request since it had enough time to obtain the response).</p>
			<p>If you would <a id="_idIndexMarker798"/>like to see the effect of non-daemon threads, which we discussed earlier, simply comment out the corresponding line of code in our main program, as follows:</p>
			<pre>threads = [MyThread(url) for url in urls]
for thread in threads:
    #thread.setDaemon(True)
    thread.start()
process_requests(threads)</pre>
			<p>You will see that the main program will hang for around 20 seconds as the non-daemon thread processing the 20-second-delay request is still running, before being able to finish its execution (even though the output that's produced will be identical).</p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor172"/>Good practices in making web requests</h1>
			<p>There are a few aspects of making concurrent web requests that require careful consideration <a id="_idIndexMarker799"/>and implementation. In this section, we will be going over those aspects and some of the best practices that you should use when developing your applications.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor173"/>Consider the terms of service and data-collecting policies</h2>
			<p>Unauthorized <a id="_idIndexMarker800"/>data collection has been the <a id="_idIndexMarker801"/>topic of discussion in the technology world for the past few years, and it will continue to be for a long time – and for good reasons too. So, it is extremely important for developers who are making automated web requests in their applications to look for websites' policies on data collecting. You can find these policies in their terms of service or similar documents. When <a id="_idIndexMarker802"/>in doubt, it is generally a good rule of <a id="_idIndexMarker803"/>thumb to contact the website directly to ask for more details.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor174"/>Error handling</h2>
			<p>Errors are something that no one can easily avoid in the field of programming, and this is especially <a id="_idIndexMarker804"/>true when making web requests. Errors in these programs can include making bad requests (invalid requests or even bad internet connections), mishandling downloaded HTML code, or unsuccessfully parsing HTML code. So, it is important to make use of <code>try...except</code> blocks and other error-handling tools in Python to avoid crashing your application. Avoiding crashes is especially important if your code/applications are used in production and larger applications.</p>
			<p>Specifically, in concurrent web scraping, it might be possible for some threads to collect data successfully, while others fail. By implementing error-handling functionalities in multithreaded parts of your program, you can make sure that a failed thread will not be able to crash the entirety of your program and ensure that successful threads can still return their results.</p>
			<p>However, it is <a id="_idIndexMarker805"/>important to note that <code>try...except</code> block in our program that will catch all errors that occur in the program's execution, and no further information regarding the errors can be obtained; this practice is also known as error swallowing. It's highly recommended that you have some specific error handling code in a program so that not only can appropriate actions be taken with regards to that specific error, but other errors that have not been taken into account might also reveal themselves.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor175"/>Update your program regularly</h2>
			<p>It is quite <a id="_idIndexMarker806"/>common for websites to change their request-handling logic, as well as their displayed data, regularly. If a program that makes requests to a website has considerably inflexible logic to interact with the server of the website (for example, structuring its requests in a specific format, only handling one kind of response, and so on), then if and when the website alters the way it handles its client requests, the program will most likely stop functioning correctly. This situation happens frequently with web scraping programs that look for data in specific HTML tags; when the HTML tags are changed, these programs will fail to find their data.</p>
			<p>This practice is <a id="_idIndexMarker807"/>implemented to prevent automated data collecting programs from functioning. The only way to keep using a website that has recently changed its request-handling logic is to analyze the updated protocols and alter our programs accordingly.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor176"/>Avoid making a large number of requests</h2>
			<p>Each time one of the programs that we have been discussing runs, it makes HTTP requests <a id="_idIndexMarker808"/>to a server that manages the site that you'd like to extract data from. This process happens significantly more frequently and over a shorter amount of time in a concurrent program, where multiple requests are being submitted to that server.</p>
			<p>As we mentioned previously, servers nowadays can handle multiple requests simultaneously with ease. However, to avoid having to overwork and overconsume resources, servers are also designed to stop answering requests that come in too frequently. The websites of big tech companies, such as Amazon or Twitter, look for large amounts of automated requests that are made from the same IP address and implement different response protocols; some requests might be delayed, some might be refused a response, or the IP address might even be banned from making further requests for a specific amount of time.</p>
			<p>Interestingly, making <a id="_idIndexMarker809"/>repeated, heavy-duty requests to servers is a form of hacking a website. In <strong class="bold">Denial of Service</strong> (<strong class="bold">DoS</strong>) and <strong class="bold">Distributed Denial of Service</strong> (<strong class="bold">DDoS</strong>) attacks, a very large number of requests are made at the same time <a id="_idIndexMarker810"/>to the server, flooding the bandwidth of the targeted server with traffic. As a result, normal, non-malicious requests from other clients are denied because the servers are busy processing the concurrent requests, as illustrated in the following diagram:</p>
			<div><div><img src="img/Figure_9.5_B17499.jpg" alt="Figure 9.5 – Illustration of a DDoS attack " width="1192" height="509"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – Illustration of a DDoS attack</p>
			<p>So, it is important to space out the concurrent requests that your application makes to a server <a id="_idIndexMarker811"/>so that the application will not be considered an attacker and be potentially banned or treated as a malicious client. This could be as simple as limiting the maximum number of threads/requests that can be implemented at a time in your program or pausing the threading for a specific amount of time (for example, using the <code>time.sleep()</code> function) before sending a request to the server.</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor177"/>Summary</h1>
			<p>In this chapter, we learned about the basics of HTML and web requests. The two most common web requests are <code>GET</code> and <code>POST</code> requests. There are five main categories of HTTP response status codes, each indicating a different concept regarding the communication between the server and its client. By considering the status codes that are received from different websites, we can write a ping test application that effectively checks the responsiveness of those websites.</p>
			<p>Concurrency can be applied to the problem of making multiple web requests simultaneously via threading to provide a significant improvement in application speed. However, it is important to keep several considerations in mind when making concurrent web requests.</p>
			<p>All in all, the exercise we just went through in this chapter will prove useful in helping us approach the general problem of converting a sequential program into its concurrent version. The simple ping test that we have built could also be extended to have more complex behaviors and functionalities. In the next chapter, we will consider a similar procedure for the application of image processing.</p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor178"/>Questions</h1>
			<ol>
				<li>What is HTML?</li>
				<li>What are HTTP requests?</li>
				<li>What are HTTP response status codes?</li>
				<li>How does the <code>requests</code> module help with making web requests?</li>
				<li>What is a ping test and how is one typically designed?</li>
				<li>Why is concurrency applicable in making web requests?</li>
				<li>What are the considerations that need to be made while developing applications that make concurrent web requests?</li>
			</ol>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor179"/>Further reading</h1>
			<ul>
				<li><em class="italic">Automate the Boring Stuff with Python: Practical Programming for Total Beginners</em>, Al. Sweigart, No Starch Press, 2015</li>
				<li><em class="italic">Web Scraping with Python</em>, Richard Lawson, Packt Publishing Ltd, 2015</li>
				<li><em class="italic">Instant Web Scraping with Java</em>, Ryan Mitchell, Packt Publishing Ltd, 2013</li>
			</ul>
		</div>
	</div>
</div>
</body></html>