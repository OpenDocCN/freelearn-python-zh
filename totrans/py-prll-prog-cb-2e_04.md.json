["```py\nC:>pip install mpi4py\n```", "```py\nC:>conda install mpi4py\n```", "```py\nC:>mpiexec -n x python mpi4py_script_name.py \n```", "```py\nfrom mpi4py import MPI \n```", "```py\n comm = MPI.COMM_WORLD \n```", "```py\nrank = comm.Get_rank() \n```", "```py\nprint (\"hello world from process \", rank)  \n```", "```py\nC:>mpiexec -n 5 python helloworld_MPI.py \n```", "```py\nhello world from process  1 \nhello world from process  0 \nhello world from process  2 \nhello world from process  3\nhello world from process  4\n```", "```py\nfrom mpi4py import MPI\n```", "```py\ncomm=MPI.COMM_WORLD \n```", "```py\nrank = comm.rank \n```", "```py\nprint(\"my rank is : \" , rank) \n```", "```py\nif rank==0: \n    data= 10000000 \n    destination_process = 4 \n```", "```py\n    comm.send(data,dest=destination_process) \n    print (\"sending data %s \" %data + \\  \n           \"to process %d\" %destination_process) \n```", "```py\nif rank==1: \n    destination_process = 8 \n    data= \"hello\" \n    comm.send(data,dest=destination_process) \n    print (\"sending data %s :\" %data + \\  \n           \"to process %d\" %destination_process) \n```", "```py\nif rank==4: \n    data=comm.recv(source=0) \n```", "```py\n    print (\"data received is = %s\" %data) \n```", "```py\nif rank==8: \n    data1=comm.recv(source=1) \n```", "```py\n print (\"data1 received is = %s\" %data1) \n```", "```py\ncomm=MPI.COMM_WORLD \n```", "```py\nrank = comm.rank \n```", "```py\nif rank==0: \n    data= 10000000 \n    destination_process = 4 \n    comm.send(data,dest=destination_process) \n```", "```py\nif rank==4: \n    data=comm.recv(source=0) \n```", "```py\nif rank==1: \n    destination_process = 8 \n    data= \"hello\" \n    comm.send(data,dest=destination_process) \n```", "```py\nif rank==8: \n    data1=comm.recv(source=1) \n```", "```py\nC:>mpiexec -n 9 python pointToPointCommunication.py \n```", "```py\nmy rank is : 7\nmy rank is : 5\nmy rank is : 2\nmy rank is : 6\nmy rank is : 3\nmy rank is : 1\nsending data hello :to process 8\nmy rank is : 0\nsending data 10000000 to process 4\nmy rank is : 4\ndata received is = 10000000\nmy rank is : 8\ndata1 received is = hello \n```", "```py\nfrom mpi4py import MPI \n```", "```py\ncomm=MPI.COMM_WORLD \nrank = comm.rank \nprint(\"my rank is %i\" % (rank)) \n```", "```py\nif rank==1: \n    data_send= \"a\" \n    destination_process = 5 \n    source_process = 5 \n    data_received=comm.recv(source=source_process) \n    comm.send(data_send,dest=destination_process) \n    print (\"sending data %s \" %data_send + \\ \n           \"to process %d\" %destination_process) \n    print (\"data received is = %s\" %data_received) \n```", "```py\nif rank==5: \n    data_send= \"b\" \n```", "```py\n    destination_process = 1 \n    source_process = 1  \n    comm.send(data_send,dest=destination_process) \n    data_received=comm.recv(source=source_process) \n    print (\"sending data %s :\" %data_send + \\ \n           \"to process %d\" %destination_process) \n    print (\"data received is = %s\" %data_received) \n```", "```py\nC:\\>mpiexec -n 9 python deadLockProblems.py\n\nmy rank is : 8\nmy rank is : 6\nmy rank is : 7\nmy rank is : 2\nmy rank is : 4\nmy rank is : 3\nmy rank is : 0\nmy rank is : 1\nsending data a to process 5\ndata received is = b\nmy rank is : 5\nsending data b :to process 1\ndata received is = a\n```", "```py\nif rank==1: \n    data_send= \"a\" \n    destination_process = 5 \n    source_process = 5 \n    comm.send(data_send,dest=destination_process) \n    data_received=comm.recv(source=source_process) \n\n    print (\"sending data %s \" %data_send + \\\n           \"to process %d\" %destination_process)\n    print (\"data received is = %s\" %data_received)\n\nif rank==5: \n    data_send= \"b\" \n    destination_process = 1 \n    source_process = 1 \n    data_received=comm.recv(source=source_process) \n    comm.send(data_send,dest=destination_process) \n\n    print (\"sending data %s :\" %data_send + \\\n           \"to process %d\" %destination_process)\n    print (\"data received is = %s\" %data_received)\n```", "```py\nif rank==1: \n    data_send= \"a\" \n    destination_process = 5 \n    source_process = 5 \n    comm.send(data_send,dest=destination_process) \n    data_received=comm.recv(source=source_process) \n\nif rank==5: \n    data_send= \"b\" \n    destination_process = 1 \n    source_process = 1 \n    comm.send(data_send,dest=destination_process) \n    data_received=comm.recv(source=source_process) \n```", "```py\nC:\\>mpiexec -n 9 python deadLockProblems.py \n\nmy rank is : 4\nmy rank is : 0\nmy rank is : 3\nmy rank is : 8\nmy rank is : 6\nmy rank is : 7\nmy rank is : 2\nmy rank is : 1\nsending data a to process 5\ndata received is = b\nmy rank is : 5\nsending data b :to process 1\ndata received is = a \n```", "```py\nSendrecv(self, sendbuf, int dest=0, int sendtag=0, recvbuf=None, int source=0, int recvtag=0, Status status=None) \n```", "```py\nif rank==1: \n    data_send= \"a\" \n    destination_process = 5 \n    source_process = 5 \n    data_received=comm.sendrecv(data_send,dest=\\\n                                destination_process,\\ \n                                source =source_process) \nif rank==5: \n    data_send= \"b\" \n    destination_process = 1 \n    source_process = 1 \n    data_received=comm.sendrecv(data_send,dest=\\ \n                                destination_process,\\ \n                                source=source_process) \n```", "```py\nbuf = comm.bcast(data_to_share, rank_of_root_process) \n```", "```py\nfrom mpi4py import MPI \n```", "```py\ncomm = MPI.COMM_WORLD \nrank = comm.Get_rank() \n```", "```py\nif rank == 0: \n    variable_to_share = 100      \nelse: \n    variable_to_share = None \n```", "```py\nvariable_to_share = comm.bcast(variable_to_share, root=0) \nprint(\"process = %d\" %rank + \" variable shared  = %d \" \\   \n                               %variable_to_share) \n```", "```py\nif rank == 0: \n   variable_to_share = 100  \n```", "```py\nvariable_to_share = comm.bcast(variable_to_share, root=0) \n```", "```py\nprint(\"process = %d\" %rank + \" variable shared  = %d \" \\   \n                     %variable_to_share) \n```", "```py\nC:\\>mpiexec -n 10 python broadcast.py \nprocess = 0 \nvariable shared = 100 \nprocess = 8 \nvariable shared = 100 \nprocess = 2 variable \nshared = 100 \nprocess = 3 \nvariable shared = 100 \nprocess = 4 \nvariable shared = 100 \nprocess = 5 \nvariable shared = 100 \nprocess = 9 \nvariable shared = 100 \nprocess = 6 \nvariable shared = 100 \nprocess = 1 \nvariable shared = 100 \nprocess = 7 \nvariable shared = 100 \n```", "```py\nrecvbuf  = comm.scatter(sendbuf, rank_of_root_process) \n```", "```py\nfrom mpi4py import MPI \n```", "```py\ncomm = MPI.COMM_WORLD \nrank = comm.Get_rank() \n```", "```py\nif rank == 0: \n    array_to_share = [1, 2, 3, 4 ,5 ,6 ,7, 8 ,9 ,10]  \nelse: \n    array_to_share = None \n```", "```py\nrecvbuf = comm.scatter(array_to_share, root=0) \nprint(\"process = %d\" %rank + \" recvbuf = %d \" %recvbuf) \n```", "```py\narray_to_share = [1, 2, 3, 4 ,5 ,6 ,7, 8 ,9 ,10] \n```", "```py\nrecvbuf = comm.scatter(array_to_share, root=0)\n```", "```py\nC:\\>mpiexec -n 10 python scatter.py \nprocess = 0 variable shared  = 1 \nprocess = 4 variable shared  = 5 \nprocess = 6 variable shared  = 7 \nprocess = 2 variable shared  = 3 \nprocess = 5 variable shared  = 6 \nprocess = 3 variable shared  = 4 \nprocess = 7 variable shared  = 8 \nprocess = 1 variable shared  = 2 \nprocess = 8 variable shared  = 9 \nprocess = 9 variable shared  = 10 \n```", "```py\nC:\\> mpiexec -n 3 python scatter.py \nTraceback (most recent call last): \n  File \"scatter.py\", line 13, in <module> \n    recvbuf = comm.scatter(array_to_share, root=0) \n  File \"Comm.pyx\", line 874, in mpi4py.MPI.Comm.scatter \n  (c:\\users\\utente\\appdata\\local\\temp\\pip-build-h14iaj\\mpi4py\\\n  src\\mpi4py.MPI.c:73400) \n  File \"pickled.pxi\", line 658, in mpi4py.MPI.PyMPI_scatter \n  (c:\\users\\utente\\appdata\\local\\temp\\pip-build-h14iaj\\mpi4py\\src\\\n  mpi4py.MPI.c:34035) \n  File \"pickled.pxi\", line 129, in mpi4py.MPI._p_Pickle.dumpv \n  (c:\\users\\utente\\appdata\\local\\temp\\pip-build-h14iaj\\mpi4py\n  \\src\\mpi4py.MPI.c:28325) \n  ValueError: expecting 3 items, got 10 \n  mpiexec aborting job... \n\njob aborted: \nrank: node: exit code[: error message] \n0: Utente-PC: 123: mpiexec aborting job \n1: Utente-PC: 123 \n2: Utente-PC: 123 \n```", "```py\nbuf = [data, data_size, data_type] \n```", "```py\nrecvbuf  = comm.gather(sendbuf, rank_of_root_process) \n```", "```py\nfrom mpi4py import MPI \n```", "```py\ncomm = MPI.COMM_WORLD \nsize = comm.Get_size() \nrank = comm.Get_rank() \n```", "```py\ndata = (rank+1)**2 \n```", "```py\ndata = comm.gather(data, root=0) \n```", "```py\nif rank == 0: \n    print (\"rank = %s \" %rank +\\ \n          \"...receiving data to other process\") \n   for i in range(1,size): \n       value = data[i] \n       print(\" process %s receiving %s from process %s\"\\ \n            %(rank , value , i)) \n```", "```py\n    data = (rank+1)**2  \n```", "```py\nif rank == 0: \n    for i in range(1,size): \n        value = data[i] \n```", "```py\ndata = comm.gather(data, root=0) \n```", "```py\nC:\\>mpiexec -n 5 python gather.py\nrank = 0 ...receiving data to other process\nprocess 0 receiving 4 from process 1\nprocess 0 receiving 9 from process 2\nprocess 0 receiving 16 from process 3\nprocess 0 receiving 25 from process 4 \n```", "```py\nfrom mpi4py import MPI \nimport numpy \n```", "```py\ncomm = MPI.COMM_WORLD \nsize = comm.Get_size() \nrank = comm.Get_rank() \n```", "```py\nsenddata = (rank+1)*numpy.arange(size,dtype=int) \nrecvdata = numpy.empty(size,dtype=int) \n```", "```py\ncomm.Alltoall(senddata,recvdata) \n```", "```py\nprint(\" process %s sending %s receiving %s\"\\ \n      %(rank , senddata , recvdata)) \n```", "```py\nC:\\>mpiexec -n 5 python alltoall.py \nprocess 0 sending [0 1 2 3 4] receiving [0 0 0 0 0] \nprocess 1 sending [0 2 4 6 8] receiving [1 2 3 4 5] \nprocess 2 sending [ 0 3 6 9 12] receiving [ 2 4 6 8 10] \nprocess 3 sending [ 0 4 8 12 16] receiving [ 3 6 9 12 15] \nprocess 4 sending [ 0 5 10 15 20] receiving [ 4 8 12 16 20] \n```", "```py\ncomm.Reduce(sendbuf, recvbuf, rank_of_root_process, op = type_of_reduction_operation) \n```", "```py\nimport numpy \nfrom mpi4py import MPI  \n```", "```py\ncomm = MPI.COMM_WORLD  \nsize = comm.size  \nrank = comm.rank \n```", "```py\narray_size = 10 \n```", "```py\nrecvdata = numpy.zeros(array_size,dtype=numpy.int) \nsenddata = (rank+1)*numpy.arange(array_size,dtype=numpy.int) \n```", "```py\nprint(\" process %s sending %s \" %(rank , senddata)) \n```", "```py\ncomm.Reduce(senddata,recvdata,root=0,op=MPI.SUM) \n```", "```py\nprint ('on task',rank,'after Reduce:    data = ',recvdata) \n```", "```py\ncomm.Reduce(senddata,recvdata,root=0,op=MPI.SUM) \n```", "```py\nC:\\>mpiexec -n 10 python reduction.py \n  process 1 sending [ 0 2 4 6 8 10 12 14 16 18]\non task 1 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]\n process 5 sending [ 0 6 12 18 24 30 36 42 48 54]\non task 5 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]\n process 7 sending [ 0 8 16 24 32 40 48 56 64 72]\non task 7 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]\n process 3 sending [ 0 4 8 12 16 20 24 28 32 36]\non task 3 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]\n process 9 sending [ 0 10 20 30 40 50 60 70 80 90]\non task 9 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]\n process 6 sending [ 0 7 14 21 28 35 42 49 56 63]\non task 6 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]\n process 2 sending [ 0 3 6 9 12 15 18 21 24 27]\non task 2 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]\n process 8 sending [ 0 9 18 27 36 45 54 63 72 81]\non task 8 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]\n process 4 sending [ 0 5 10 15 20 25 30 35 40 45]\non task 4 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]\n process 0 sending [0 1 2 3 4 5 6 7 8 9]\non task 0 after Reduce: data = [ 0 55 110 165 220 275 330 385 440 495] \n```", "```py\ncomm.Create_cart((number_of_rows,number_of_columns))\n```", "```py\nfrom mpi4py import MPI \nimport numpy as np \n```", "```py\nUP = 0 \nDOWN = 1 \nLEFT = 2 \nRIGHT = 3 \n```", "```py\nneighbour_processes = [0,0,0,0] \n```", "```py\nif __name__ == \"__main__\": \n    comm = MPI.COMM_WORLD \n    rank = comm.rank \n    size = comm.size \n```", "```py\n    grid_rows = int(np.floor(np.sqrt(comm.size))) \n    grid_column = comm.size // grid_rows \n```", "```py\n    if grid_rows*grid_column > size: \n        grid_column -= 1 \n    if grid_rows*grid_column > size: \n        grid_rows -= 1\n```", "```py\n    if (rank == 0) : \n        print(\"Building a %d x %d grid topology:\"\\ \n              % (grid_rows, grid_column) ) \n\n    cartesian_communicator = \\ \n                           comm.Create_cart( \\ \n                               (grid_rows, grid_column), \\ \n                               periods=(False, False), \\\n                               reorder=True) \n    my_mpi_row, my_mpi_col = \\ \n                cartesian_communicator.Get_coords\\ \n                ( cartesian_communicator.rank )  \n\n    neighbour_processes[UP], neighbour_processes[DOWN]\\ \n                             = cartesian_communicator.Shift(0, 1) \n    neighbour_processes[LEFT],  \\ \n                               neighbour_processes[RIGHT]  = \\ \n                               cartesian_communicator.Shift(1, 1) \n    print (\"Process = %s\n    \\row = %s\\n \\ \n    column = %s ----> neighbour_processes[UP] = %s \\ \n    neighbour_processes[DOWN] = %s \\ \n    neighbour_processes[LEFT] =%s neighbour_processes[RIGHT]=%s\" \\ \n             %(rank, my_mpi_row, \\ \n             my_mpi_col,neighbour_processes[UP], \\ \n             neighbour_processes[DOWN], \\ \n             neighbour_processes[LEFT] , \\ \n             neighbour_processes[RIGHT])) \n```", "```py\ngrid_row = int(np.floor(np.sqrt(comm.size))) \ngrid_column = comm.size // grid_row \nif grid_row*grid_column > size: \n    grid_column -= 1 \nif grid_row*grid_column > size: \n    grid_rows -= 1\n```", "```py\ncartesian_communicator = comm.Create_cart( \\  \n    (grid_row, grid_column), periods=(False, False), reorder=True) \n```", "```py\nmy_mpi_row, my_mpi_col =\\ \n                cartesian_communicator.Get_coords(cartesian_communicator.rank ) \n```", "```py\n\nneighbour_processes[UP], neighbour_processes[DOWN] =\\            \n                                  cartesian_communicator.Shift(0, 1) \n\nneighbour_processes[LEFT],  neighbour_processes[RIGHT] = \\                                     \n                                    cartesian_communicator.Shift(1, 1) \n```", "```py\nC:\\>mpiexec -n 4 python virtualTopology.py\nBuilding a 2 x 2 grid topology:\nProcess = 0 row = 0 column = 0\n ---->\nneighbour_processes[UP] = -1\nneighbour_processes[DOWN] = 2\nneighbour_processes[LEFT] =-1\nneighbour_processes[RIGHT]=1\n\nProcess = 2 row = 1 column = 0\n ---->\nneighbour_processes[UP] = 0\nneighbour_processes[DOWN] = -1\nneighbour_processes[LEFT] =-1\nneighbour_processes[RIGHT]=3\n\nProcess = 1 row = 0 column = 1\n ---->\nneighbour_processes[UP] = -1\nneighbour_processes[DOWN] = 3\nneighbour_processes[LEFT] =0\nneighbour_processes[RIGHT]=-1\n\nProcess = 3 row = 1 column = 1\n ---->\nneighbour_processes[UP] = 1\nneighbour_processes[DOWN] = -1\nneighbour_processes[LEFT] =2\nneighbour_processes[RIGHT]=-1\n\n```", "```py\ncartesian_communicator = comm.Create_cart( (grid_row, grid_column),\\ \n                                 periods=(True, True), reorder=True) \n```", "```py\nC:\\>mpiexec -n 4 python virtualTopology.py\nProcess = 3 row = 1 column = 1\n---->\nneighbour_processes[UP] = 1\nneighbour_processes[DOWN] = 1\nneighbour_processes[LEFT] =2\nneighbour_processes[RIGHT]=2\n\nProcess = 1 row = 0 column = 1\n---->\nneighbour_processes[UP] = 3\nneighbour_processes[DOWN] = 3\nneighbour_processes[LEFT] =0\nneighbour_processes[RIGHT]=0\n\nBuilding a 2 x 2 grid topology:\nProcess = 0 row = 0 column = 0\n---->\nneighbour_processes[UP] = 2\nneighbour_processes[DOWN] = 2\nneighbour_processes[LEFT] =1\nneighbour_processes[RIGHT]=1\n\nProcess = 2 row = 1 column = 0\n---->\nneighbour_processes[UP] = 0\nneighbour_processes[DOWN] = 0\nneighbour_processes[LEFT] =3\nneighbour_processes[RIGHT]=3 \n```"]