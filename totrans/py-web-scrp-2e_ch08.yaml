- en: Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scrapy** is a popular web scraping and crawling framework utilizing high-level
    functionality to make scraping websites easier. In this chapter, we will get to
    know Scrapy by using it to scrape the example website, just as we did in [Chapter
    2](py-web-scrp-2e_ch02.html), *Scraping the Data*. Then, we will cover **Portia**,
    which is an application based on Scrapy which allows you to scrape a website through
    a point and click interface.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Scrapy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Spider
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing different spider types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crawling with Scrapy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual Scraping with Portia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated Scraping with Scrapely
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scrapy can be installed with the `pip` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Scrapy relies on some external libraries, so if you have trouble installing
    it there is additional information available on the official website at: [http://doc.scrapy.org/en/latest/intro/install.html](http://doc.scrapy.org/en/latest/intro/install.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If Scrapy is installed correctly, a `scrapy` command will now be available
    in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the following commands in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`startproject`: Creates a new project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`genspider`: Generates a new spider from a template'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crawl`: Runs a spider'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shell`: Starts the interactive scraping console'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For detailed information about these and other commands available, refer to
    [http://doc.scrapy.org/en/latest/topics/commands.html](http://doc.scrapy.org/en/latest/topics/commands.html)
  prefs: []
  type: TYPE_NORMAL
- en: Starting a project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that Scrapy is installed, we can run the `startproject` command to generate
    the default structure for our first Scrapy project.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, open the terminal and navigate to the directory where you want
    to store your Scrapy project, and then run `scrapy startproject <project name>`.
    Here, we will use `example` for the project name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the files generated by the `scrapy` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The important files for this chapter (and in general for Scrapy use) are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`items.py`: This file defines a model of the fields that will be scraped'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`settings.py`: This file defines settings, such as the user agent and crawl
    delay'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spiders/`: The actual scraping and crawling code are stored in this directory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, Scrapy uses `scrapy.cfg` for project configuration, `pipelines.py`
    to process the scraped fields and `middlewares.``py` to control request and response
    middleware, but they will not need to be modified for this example.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, `example/items.py` contains the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ExampleItem` class is a template which needs to be replaced with the details
    we''d like to extract from the example country page. For now, we will just scrape
    the country name and population, rather than all the country details. Here is
    an updated model to support this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Full documentation for defining items is available at [http://doc.scrapy.org/en/latest/topics/items.html](http://doc.scrapy.org/en/latest/topics/items.html)
  prefs: []
  type: TYPE_NORMAL
- en: Creating a spider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we can build the actual crawling and scraping code, known as a **spider**
    in Scrapy. An initial template can be generated with the `genspider` command,
    which takes the name you want to call the spider, the domain, and an optional
    template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We used the built-in `crawl` template which utilizes the Scrapy library's `CrawlSpider`.
    A Scrapy `CrawlSpider` has special attributes and methods available when crawling
    the web rather than a simple scraping spider.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the `genspider` command, the following code is generated in `example/spiders/country.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The initial lines import the required Scrapy libraries and encoding definition.
    Then, a class is created for the spider, which contains the following class attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: A string to identify the spider'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`allowed_domains`: A list of the domains that can be crawled -- if this isn''t set, any
    domain can be crawled'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_urls`: A list of URLs to begin the crawl.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rules`: This attribute is a tuple of `Rule` objects defined by regular expressions
    which tell the crawler what links to follow and what links have useful content
    to scrape'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will notice the defined `Rule` has a `callback` attribute which sets the
    callback to `parse_item`, the method defined just below. This method is the main
    data extraction method for `CrawlSpider` objects, and the generated Scrapy code
    within that method has an example of extracting content from the page.
  prefs: []
  type: TYPE_NORMAL
- en: Because Scrapy is a high-level framework, there is a lot going on here in only
    a few lines of code. The official documentation has further details about building
    spiders, and can be found at [http://doc.scrapy.org/en/latest/topics/spiders.html](http://doc.scrapy.org/en/latest/topics/spiders.html).
  prefs: []
  type: TYPE_NORMAL
- en: Tuning settings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before running the generated crawl spider, the Scrapy settings should be updated
    to avoid the spider being blocked. By default, Scrapy allows up to 16 concurrent
    downloads for a domain with no delay between downloads, which is much faster than
    a real user would browse. This behavior is easy for a server to detect and block.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in Chapter 1, the example website we are scraping is configured
    to temporarily block crawlers which consistently download at faster than one request
    per second, so the default settings would ensure our spider is blocked. Unless
    you are running the example website locally, I recommend adding these lines to
    `example/settings.py` so the crawler only downloads a single request per domain
    at a time with a reasonable 5 second delay between downloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You can also search and find those settings in the documentation, modify and
    uncomment them with the above values. Note that Scrapy will not use this precise
    delay between requests, because this would also make a crawler easier to detect
    and block. Instead, it adds a random offset within this delay between requests.
  prefs: []
  type: TYPE_NORMAL
- en: For details about these settings and the many others available, refer to [http://doc.scrapy.org/en/latest/topics/settings.html](http://doc.scrapy.org/en/latest/topics/settings.html).
  prefs: []
  type: TYPE_NORMAL
- en: Testing the spider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run a spider from the command line, the `crawl` command is used along with
    the name of the spider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The script runs to completion with no output. Take note of the `-s LOG_LEVEL=ERROR`
    flag-this is a Scrapy setting and is equivalent to defining `LOG_LEVEL = 'ERROR'`
    in the `settings.py` file. By default, Scrapy will output all log messages to
    the terminal, so here the log level was raised to isolate error messages. Here,
    no output means our spider completed without error -- great!
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to actually scrape some content from the pages, we need to add a few
    lines to the spider file. To ensure we can start building and extracting our items,
    we have to first start using our `CountryItem` and also update our crawler rules.
    Here is an updated version of the spider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In order to extract structured data, we should use our `CountryItem` class which
    we created. In this added code, we are importing the class and instantiating an
    object as the `i` (or item) in our `parse_item` method.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we need to add rules so our spider can find data and extract it.
    The default rule searched the url pattern `r'/Items'` which is not matched on
    the example site. Instead, we can create two new rules from what we know already
    about the site. The first rule will crawl the index pages and follow their links,
    and the second rule will crawl the country pages and pass the downloaded response
    to the `callback` function for scraping.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what happens when this improved spider is run with the log level
    set to `DEBUG` to show more crawling messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This log output shows the index pages and countries are being crawled and duplicate
    links are filtered, which is handy. We can also see our installed middlewares
    and other important information output when we first start the crawler.
  prefs: []
  type: TYPE_NORMAL
- en: However, we also notice the spider is wasting resources by crawling the login
    and register forms linked from each web page, because they match the `rules` regular
    expressions. The login URL in the preceding command ends with `_next=%2Findex%2F1`,
    which is a URL encoding equivalent to `_next=/index/1`, defining a post-login
    redirect. To prevent these URLs from being crawled, we can use the `deny` parameter
    of the rules, which also expects a regular expression and will prevent crawling
    every matching URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an updated version of code to prevent crawling the user login and registration
    forms by avoiding the URLs containing `/user/`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Further documentation about how to use the LinkExtractor class is available
    at [http://doc.scrapy.org/en/latest/topics/link-extractors.html](http://doc.scrapy.org/en/latest/topics/link-extractors.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'To stop the current crawl and restart with the new code, you can send a quit
    signal using *Ctrl* + *C* or *cmd* + *C*. You should then see a message similar
    to this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It will finish queued requests and then stop. You'll see some extra statistics
    and debugging at the end, which we will cover later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to adding deny rules to the crawler, you can use the `process_links`
    argument for the `Rule` object. This allow you to create a function which iterates
    through the found links and makes any modifications (such as removing or adding
    parts of query strings). More information about crawling rules is available in
    the documentation: [https://doc.scrapy.org/en/latest/topics/spiders.html#crawling-rules](https://doc.scrapy.org/en/latest/topics/spiders.html#crawling-rules)
  prefs: []
  type: TYPE_NORMAL
- en: Different Spider Types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this Scrapy example, we have utilized the Scrapy `CrawlSpider`, which is
    particularly useful when crawling a website or series of websites. Scrapy has
    several other spiders you may want to use depending on the site and your extraction
    needs. These spiders fall under the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Spider`: A normal scraping spider. This is usually used for just scraping
    one type of page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CrawlSpider`: A crawl spider; usually used for traversing a domain and scraping
    one (or several) types of pages from the pages it finds by crawling links.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XMLFeedSpider`: A spider which traverses an XML feed and extracts content
    from each node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CSVFeedSpider`: Similar to the XML spider, but instead can parse CSV rows
    within the feed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SitemapSpider`: A spider which can crawl a site with differing rules by first
    parsing the Sitemap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these spiders are included in your default Scrapy installation, so you
    can access them whenever you may want to build a new web scraper. In this chapter,
    we'll finish building our first crawl spider as a first example of how to use
    Scrapy tools.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping with the shell command
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that Scrapy can crawl the countries, we can define what data to scrape.
    To help test how to extract data from a web page, Scrapy comes with a handy command
    called `shell` which presents us with the Scrapy API via an Python or IPython
    interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can call the command using the URL we would like to start with, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can now query the `response` object to check what data is available.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Scrapy uses `lxml` to scrape data, so we can use the same CSS selectors as
    those in [Chapter 2](py-web-scrp-2e_ch02.html), *Scraping the Data*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The method returns a list with an `lxml` selector. You may also recognize some
    of the XPath syntax Scrapy and `lxml` use to select the item. As we learned in
    [Chapter 2](py-web-scrp-2e_ch02.html),  *Scraping the Data*, `lxml` converts all
    CSS Selectors to XPath before extracting content.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to actually get the text from this country row, we must call the `extract()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the output above, the Scrapy `response` object can be parsed
    using both `css` and `xpath`, making it very versatile for getting obvious and
    harder-to-reach content.
  prefs: []
  type: TYPE_NORMAL
- en: 'These selectors can then be used in the `parse_item()` method generated earlier
    in `example/spiders/country.py`. Note we set attributes of the `scrapy.Item` object
    using dictionary syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Checking results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the completed version of our spider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: To save the results, we could define a Scrapy pipeline or set up an output setting
    in our `settings.py` file. However, Scrapy also provides a handy `--output` flag
    to easily save scraped items automatically in CSV, JSON, or XML format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the results when the final version of the spider is run with the output
    to a CSV file and the log level is set to `INFO`, to filter out less important
    messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: At the end of the crawl, Scrapy outputs some statistics to give an indication
    of how the crawl performed. From these statistics, we know that 280 web pages
    were crawled and 252 items were scraped, which is the expected number of countries
    in the database, so we know the crawler was able to find them all.
  prefs: []
  type: TYPE_NORMAL
- en: You need to run Scrapy spider and crawl commands from within the generated folder
    Scrapy creates (for our project this is the `example/` directory we created using
    the `startproject` command). The spiders use the `scrapy.cfg` and `settings.py`
    files to determine how and where to scrape and to set spider paths for crawling
    or scraping use.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify these countries were scraped correctly we can check the contents
    of `countries.csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As expected this CSV contains the name and population for each country. Scraping
    this data required writing less code than the original crawler built in [Chapter
    2](py-web-scrp-2e_ch02.html), *Scraping the Data* because Scrapy provides high-level
    functionality and nice built-in features like built-in CSV writers.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section on Portia we will re-implement this scraper writing
    even less code.
  prefs: []
  type: TYPE_NORMAL
- en: Interrupting and resuming a crawl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes when scraping a website, it can be useful to pause the crawl and resume
    it at a later time without needing to start over from the beginning. For example,
    you may need to interrupt the crawl to reset your computer after a software update,
    or perhaps, the website you are crawling is returning errors and you want to continue
    the crawl later.
  prefs: []
  type: TYPE_NORMAL
- en: Conveniently, Scrapy comes with built-in support to pause and resume crawls
    without needing to modify our example spider. To enable this feature, we just
    need to define the `JOBDIR` setting with a directory where the current state of
    a crawl can be saved. Note separate directories must be used to save the state
    of multiple crawls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example using this feature with our spider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we see a `^C` in the line that says `Received SIG_SETMASK` which is the
    same *Ctrl* + *C* or *cmd* + *C* we used earlier in the chapter to stop our scraper.
    To have Scrapy save the crawl state, you must wait here for the crawl to shut
    down gracefully and resist the temptation to enter the termination sequence again
    to force immediate shutdown! The state of the crawl will now be saved in the data
    directory in `crawls/country`. We can see the saved files if we look in that directory
    (Note this command and directory syntax will need to be altered for Windows users):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The crawl can be resumed by running the same command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The crawl now resumes from where it paused and continues as normal. This feature
    is not particularly useful for our example website because the number of pages
    to download is manageable. However, for larger websites which could take months
    to crawl, being able to pause and resume crawls is quite convenient.
  prefs: []
  type: TYPE_NORMAL
- en: There are some edge cases not covered here that can cause problems when resuming
    a crawl, such as expiring cookies and sessions. These are mentioned in the Scrapy
    documentation available at [http://doc.scrapy.org/en/latest/topics/jobs.html](http://doc.scrapy.org/en/latest/topics/jobs.html).
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy Performance Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we check the initial full scrape of the example site and take a look at the
    start and end times, we can see the scrape took approximately 1,697 seconds. If
    we calculate how many seconds per page (on average), that is ~6 seconds per page.
    Knowing we did not use the Scrapy concurrency features and fully aware that we
    also added a delay of ~5 seconds between requests, this means Scrapy is parsing
    and extracting data at around 1s per page (Recall from [Chapter 2](py-web-scrp-2e_ch02.html),
    *Scraping the Data*, that our fastest scraper using XPath took 1.07s). I gave
    a talk at PyCon 2014 comparing web scraping library speed, and even then, Scrapy
    was massively faster than any other scraping frameworks I could find. I was able
    to write a simple Google search scraper that was returning (on average) 100 requests
    a second. Scrapy has come a long way since then, and I always recommend it for
    the most performant Python scraping framework.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to leveraging the concurrency Scrapy uses (via Twisted), Scrapy
    can be tuned to use things like page caches and other performance considerations
    (such as utilizing proxies to allow more concurrent requests to a single site).
    In order to install the cache, you should first read the cache middleware documentation
    ([https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpcache](https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpcache)).
    You might have already seen in the `settings.py` file, there are several good
    examples of how to implement the proper cache settings. For implementing proxies,
    there are some great helper libraries (as Scrapy only gives access to a simple
    middleware class). The current most popular and updated library is [https://github.com/aivarsk/scrapy-proxies](https://github.com/aivarsk/scrapy-proxies),
    which has Python3 support and is fairly easy to integrate.
  prefs: []
  type: TYPE_NORMAL
- en: As always, libraries and recommended setup can change, so reading the latest
    Scrapy documentation should always be your first stop when it comes to checking
    performance and making spider changes.
  prefs: []
  type: TYPE_NORMAL
- en: Visual scraping with Portia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Portia is a an open-source tool built on top of Scrapy that supports building
    a spider by clicking on the parts of a website which need to be scraped. This
    method can be more convenient than creating the CSS or XPath selectors manually.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Portia is a powerful tool, and it depends on multiple external libraries for
    its functionality. It is also relatively new, so currently, the installation steps
    are somewhat involved. In case the installation is simplified in future, the latest
    documentation can be found at [https://github.com/scrapinghub/portia#running-portia](https://github.com/scrapinghub/portia#running-portia).
    The current recommended way to run Portia is to use Docker (the open-source container
    framework). If you don't have Docker installed, you'll need to do so first by
    following the latest instructions ([https://docs.docker.com/engine/installation/](https://docs.docker.com/engine/installation/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once Docker is installed and running, you can pull the `scrapinghub` image
    and get started. First, you should be in the directory you''d like to create your
    new portia project and run the command like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the command, we created a new folder at `~/portia_projects`. If you'd rather
    have your portia projects stored elsewhere, change the `-v` command to point to
    the absolute file path where you would like to store your portia files.
  prefs: []
  type: TYPE_NORMAL
- en: These last few lines show that the Portia website is now up and running. The
    site will now be accessible in your web browser at `http://localhost:9001/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your initial screen should look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/portia_home.png)'
  prefs: []
  type: TYPE_IMG
- en: If you have problems during installation it's worth checking the Portia Issues
    page at [https://github.com/scrapinghub/portia/issues](https://github.com/scrapinghub/portia/issues),
    in case someone else has experienced the same problem and found a solution. In
    this book I have used the specific Portia image I used (`scrapinghub/portia:portia-2.0.7`),
    but you can also try using the latest official release `scrapinghub/portia`.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, I recommend always using the latest recommended instructions as
    documented in the README file and Portia documentation, even if they differ from
    the ones covered in this section. Portia is under active development and instructions
    could change after the publication of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Annotation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the Portia start page, the page prompts you to enter a project. Once you
    enter that text, then there is a textbox to enter the URL of the website you want
    to scrape, such as [http://example.webscraping.com](http://example.webscraping.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'When you''ve typed that, Portia will then load the project view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/portia_project.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you click the New Spider button, you will see the following Spider view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/portia_spider_view.png)'
  prefs: []
  type: TYPE_IMG
- en: You will start to recognize some of the fields from the Scrapy spider we already
    built earlier in this chapter (such as start pages and link crawling rules). By
    default, the spider name is set to the domain (example.webscraping.com), which
    can be modified by clicking on the labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, click on the "New Sample" button to start collecting data from the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/portia_select_area.png)'
  prefs: []
  type: TYPE_IMG
- en: Now when you roll over the different elements of the page, you will see them
    highlighted. You can also see the CSS selector in the Inspector tab to the right
    of the website area.
  prefs: []
  type: TYPE_NORMAL
- en: Because we want to scrape the population elements on the individual country
    pages, we first need to navigate from this homepage to the individual country
    pages. To do so, we first need to click "Close Sample" and then click on any country.
    When the country page loads, we can once again click "New Sample".
  prefs: []
  type: TYPE_NORMAL
- en: 'To start adding fields to our items for extraction, we can click on the population
    field. When we do, an item is added and we can see the extracted information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/portia_add_field.png)'
  prefs: []
  type: TYPE_IMG
- en: We can rename the field by using the left text field area and simply typing
    in the new name "population". Then, we can click the "Add Field" button. To add
    more fields, we can do the same for the country name and any other fields we are
    interested in by first clicking on the large + button and then selecting the field
    values in the same way. The annotated fields will be highlighted in the web page
    and you can see the extracted data in the extracted items section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/portia_extracted_items.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you want to delete any fields, you can simply use the red - sign next to
    the field name. When the annotations are complete, click on the blue "Close sample" button
    at the top. If you then wanted to download the spider to run in a Scrapy project,
    you can do so by clicking the link next to the spider name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/portia_download.png)'
  prefs: []
  type: TYPE_IMG
- en: You can also see all of your spiders and the settings in the mounted folder
    `~/portia_projects`.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Spider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are running Portia as a Docker container, you can run the `portiacrawl`
    command using the same Docker image. First, stop your current container using
    *Ctrl* + *C*. Then, you can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run -i -t --rm -v ~/portia_projects:/app/data/projects:rw -v <OUTPUT_FOLDER>:/mnt:rw
    -p 9001:9001 scrapinghub/portia portiacrawl /app/data/projects/<PROJECT_NAME>
    example.webscraping.com -o /mnt/example.webscraping.com.jl`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure to update the OUTPUT_FOLDER in an absolute path where you want to
    store your output files and PROJECT_NAME variables is the name you used when starting
    your project (mine was my_example_site). You should see output similar to output
    you notice when running Scrapy. You may notice error messages (this is due to
    not changing the download delay or parallel requests -- both of which can be done
    in the web interface by changing the project and spider settings). You can also
    pass extra settings to your spider when it is run using the `-s` flag. My command
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '``docker run -i -t --rm -v ~/portia_projects:/app/data/projects:rw -v ~/portia_output:/mnt:rw
    -p 9001:9001 scrapinghub/portia portiacrawl /app/data/projects/my_example_site example.webscraping.com
    -o /mnt/example.webscraping`.com.jl`-s CONCURRENT_REQUESTS_PER_DOMAIN=1 -s DOWNLOAD_DELAY=5``'
  prefs: []
  type: TYPE_NORMAL
- en: Checking results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When the spider is finished, you can check your results in the output folder
    you created:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ head ~/portia_output/example.webscraping.com.jl {"_type": "Example web scraping
    website1", "url": "http://example.webscraping.com/view/Antigua-and-Barbuda-10",
    "phone_code": ["+1-268"], "_template": "98ed-4785-8e1b", "country_name": ["Antigua
    and Barbuda"], "population": ["86,754"]} {"_template": "98ed-4785-8e1b", "country_name":
    ["Antarctica"], "_type": "Example web scraping website1", "url": "http://example.webscraping.com/view/Antarctica-9",
    "population": ["0"]} {"_type": "Example web scraping website1", "url": "http://example.webscraping.com/view/Anguilla-8",
    "phone_code": ["+1-264"], "_template": "98ed-4785-8e1b", "country_name": ["Anguilla"],
    "population": ["13,254"]} ...`'
  prefs: []
  type: TYPE_NORMAL
- en: Here are a few examples of the results of your scrape. As you can see, they
    are in JSON format. If you wanted to export in CSV format, you can simply change
    the output file name to end with `.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: With just a few clicks on a site and a few instructions for Docker, you've scraped
    the example website! Portia is a handy tool to use, especially for straightforward
    websites, or if you need to collaborate with non-developers. On the other hand,
    for more complex websites, you always have the option to develop the Scrapy crawler
    directly in Python or use Portia to develop the first iteration and expand it
    using your own Python skills.
  prefs: []
  type: TYPE_NORMAL
- en: Automated scraping with Scrapely
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For scraping the annotated fields Portia uses a library called **Scrapely ([https://github.com/scrapy/scrapely](https://github.com/scrapy/scrapely))**,
    which is a useful open-source tool developed independently from Portia. Scrapely
    uses training data to build a model of what to scrape from a web page. The trained
    model can then be applied to scrape other web pages with the same structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install it using pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example to show how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: First, Scrapely is given the data we want to scrape from the `Afghanistan` web
    page to train the model (here, the country name and population). This model is
    then applied to a different country page and Scrapely uses the trained model to
    correctly return the country name and population here as well.
  prefs: []
  type: TYPE_NORMAL
- en: This workflow allows scraping web pages without needing to know their structure,
    only the desired content you want to extract for the training case (or multiple
    training cases). This approach can be particularly useful if the content of a
    web page is static, but the layout is changing. For example, with a news website,
    the text of the published article will most likely not change, though the layout
    may be updated. In this case, Scrapely can then be retrained using the same data
    to generate a model for the new website structure. For this example to work properly,
    you would need to store your training data somewhere for reuse.
  prefs: []
  type: TYPE_NORMAL
- en: The example web page used here to test Scrapely is well structured with separate
    tags and attributes for each data type so Scrapely was able to correctly and easily
    train a model. For more complex web pages, Scrapely can fail to locate the content
    correctly. The Scrapely documentation warns you should "train with caution". As
    machine learning becomes faster and easier, perhaps a more robust automated web
    scraping library will be released; for now, it is still quite useful to know how
    to scrape a website directly using the techniques covered throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced Scrapy, a web scraping framework with many high-level
    features to improve efficiency when scraping websites. Additionally, we covered
    Portia, which provides a visual interface to generate Scrapy spiders. Finally,
    we tested Scrapely, the library used by Portia to scrape web pages automatically
    by first training a simple model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will apply the skills learned so far to some real-world
    websites.
  prefs: []
  type: TYPE_NORMAL
