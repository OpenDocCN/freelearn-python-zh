- en: 8 Running a Sanic Server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the time that I have been involved with the Sanic project—and specifically,
    in trying to assist other developers by answering their support questions—there
    is one topic that perhaps comes up more than any other: deployment. That one word
    is often bundled with a mixture of confusion and dread.'
  prefs: []
  type: TYPE_NORMAL
- en: Building a web application can be a lot of fun. I suspect that I am not alone
    in finding a tremendous amount of satisfaction in the build process itself. One
    of the reasons that I love software development in general—and web development
    in particular—is that I enjoy the almost puzzle-like atmosphere of fitting solutions
    to a given problem. When the build is done and it is time to launch, that is where
    the anxiety kicks in.
  prefs: []
  type: TYPE_NORMAL
- en: I cannot overemphasize this next point enough. One of Sanic’s biggest assets
    is its bundled web server. This is not just a gimmick, or some side feature to
    be ignored. The fact that Sanic comes bundled with its own web server truly does
    simplify the build process. Think about traditional Python web frameworks like
    Django or Flask, or about some of the newer ASGI frameworks. For them to become
    operational and connected to the web, you need a production-grade web server.
    Building the application is only one step. Deploying it requires knowledge and
    proficiency in another tool. Typically, the web server used to deploy your application
    built with one of those frameworks is not the same web server that you develop
    upon. For that, you have a development server. Not only is this an added complexity
    and dependency, but it also means you are not developing against the actual server
    that will be running your code in production. Is anyone else thinking what I am
    thinking? Bugs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at what is required to run Sanic. We will explore
    different ways to run Sanic both in development and production to make the deployment
    process as easy as possible. We will start by looking at the server lifecycle.
    Then, we will discuss setting up both a local and a production-grade scalable
    service. We will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Handling the server lifecycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring an application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running Sanic locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying to production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Securing your application with TLS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we are done, your days of deployment-induced anxiety should be a thing
    of the past.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will, of course, continue to build upon the tools and knowledge from previous
    chapters. Earlier in *Chapter 3*, *Routing and Intaking HTTP Requests*, we saw
    some implementations that used Docker. Specifically, we were using Docker to run
    an Nginx server for static content. While it is not required for deploying Sanic,
    knowledge of Docker and (to a lesser extent) Kubernetes will be helpful. In this
    Chapter, we will be exploring the usage of Docker with Sanic deployments. If you
    are not a black-belt Docker or Kubernetes expert, do not worry. There will be
    examples on the GitHub repository: [https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/08](https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/08).
    All that we hope and expect is some basic understanding and familiarity with these
    tools.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do not have these tools installed, you will need them to follow along:'
  prefs: []
  type: TYPE_NORMAL
- en: '`git`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doctl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling the server lifecycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this book, we have spent a lot of time talking about the lifecycle
    of an incoming HTTP request. In that time, we have seen how we can attach to modify,
    and run code at different points in that cycle. Well, the lifecycle of the application
    server as a whole is no different.
  prefs: []
  type: TYPE_NORMAL
- en: Whereas we had middleware and signals, the server lifecycle has what are called
    “listeners”. In fact, listeners are in effect (with one small exception) signals
    themselves. Before we look at how to use them, we will take a look at what listeners
    are available.
  prefs: []
  type: TYPE_NORMAL
- en: Server listeners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The basic premise of a **listener** is that you are attaching some function
    to an **event** in the server’s lifecycle. As the server progresses through the
    startup and shutdown process, Sanic will trigger these events, and therefore allow
    you to easily plug in your own functionality. Sanic triggers events at both the
    startup and shutdown phases. For any other event during the life of your server,
    you should refer to the *Leveraging signals for intra-worker communication* section
    on Signals in *Chapter 6*, *Outside the Response Cycle*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The order of the events is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`before_server_start`: This event naturally begins runs before the server is
    started. It is a great place to connect to a database, or perform any other operations
    that need to happen at the beginning of your application lifecycle. Anything that
    you might be inclined to do in the global scope would almost always be better
    off done here. The only caveat worth knowing about is that if you are running
    in ASGI mode, the server is already running by the time Sanic is even triggered.
    In that case, there is no difference between `before_server_start` and `after_server_start`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`after_server_start`: A common misconception about this event is that it could
    encounter a race condition where the event runs *while* your server begins responding
    to HTTP requests. That is *not* the case. What this event means is that there
    was an HTTP server created and attached to the OS. The infrastructure is in place
    to begin accepting requests, but it has not happened yet. Only once all of your
    listeners for `after_server_start` are complete will Sanic begin to accept HTTP
    traffic.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`before_server_stop`: This is a good place to start any cleanup you need to
    do. While you are in this location, Sanic is still able to accept incoming traffic,
    so anything that you might need to handle that should still be available (like
    database connections).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`after_server_stop`: Once the server has been closed, it is now safe to start
    any cleanup that is remaining. If you are in ASGI mode, like `before_server_start`,
    this event is not actually triggered after the server is off because Sanic does
    not control that. It will instead immediately follow any `before_server_stop`
    listeners to preserve their ordering.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are two more listeners that are available to you. However, these additional
    listeners are *only* available with the Sanic server since they are specific to
    the Sanic server lifecycle. This is due to how the server works. When you run
    Sanic with multiple workers, what happens is that there is the main process that
    acts as an orchestrator. It spins up multiple subprocesses for each of the workers
    that you have requested. If you want to tap into the lifecycle of each of those
    worker processes, then you already have the tools at your disposal with the four
    listeners we just saw.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, what if you wanted to run some bit of code not on each worker process,
    but once in the main process: that orchestrator? The answer is the Sanic server’s
    main process events. They are `main_process_start` and `main_process_stop`. Other
    than the fact that they run inside the main process and not the workers, they
    otherwise work like the other listeners. Remember how I said that the listeners
    are themselves signals, with an exception? This is that exception. These listeners
    are not signals in disguise. For all practical purposes, this distinction is not
    important.'
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth mentioning that even though these events are meant to allow
    code to be run in the main process and not the worker process when in multi-worker
    mode, they are still triggered when you are running with a single worker process.
    When this is the case, it will be run at the extreme beginning and extreme end
    of your lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'This raises an interesting and often seen mistake: double execution. Before
    continuing with listeners, we will turn our attention to mistakenly running code
    multiple times.'
  prefs: []
  type: TYPE_NORMAL
- en: Running code in the global scope
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When you are preparing your application to run, it is not uncommon to initialize
    various services, clients, interfaces, and so on. You likely will need to perform
    some operations on your application very early in the process before the server
    even begins to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s imagine that you are looking for a solution to help you
    better track your exceptions. You find a third-party service where you can report
    all of your exceptions and tracebacks to help you to better analyze, debug, and
    repair your application. To get started, the service provides some documentation
    to use their **software development kit** (**SDK**) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You get this setup and running in your multi-worker application, and you immediately
    start noticing that it is running multiple times, and not in your worker processes
    as expected. What is going on?
  prefs: []
  type: TYPE_NORMAL
- en: Likely, the issue is that you ran your initialization code in the global scope.
    By *global scope* in Python we mean something that is executing outside of a function
    or method. It runs on the outermost level in a Python file. In the above example,
    `init_error_reporting` runs in the global scope because it is not wrapped inside
    another function. The problem is that when multiple workers are running, you need
    to be aware of where and when that code is running. Since multiple workers mean
    multiple processes, and each process is likely to run your global scope, you need
    to be careful about what you put there.
  prefs: []
  type: TYPE_NORMAL
- en: As a very general rule, stick to putting *any* operable code inside a listener.
    This allows you to control the where and when and will operate in a more consistent
    and easily predictable manner.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up listeners
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Using listeners should look very familiar since they follow a similar pattern
    found elsewhere in Sanic. You create a listener handler (which is just a function)
    and then wrap it with a decorator. It should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: What we see here is something *incredibly important* in Sanic development. This
    pattern should be committed to memory because attaching elements to your application
    `ctx` object increases your overall flexibility in development. In this example,
    we set up our database client so that it can be accessed from anywhere that our
    application can be (which is literally anywhere in the code).
  prefs: []
  type: TYPE_NORMAL
- en: One important thing to know is that you can control the order in which the listeners
    execute depending upon when they are defined. For the “start” time listeners (`before_server_start`,
    `after_server_start`, and `main_process_start`), they are executed in the order
    in which they are declared.
  prefs: []
  type: TYPE_NORMAL
- en: For the *stop* time listeners (`before_server_stop`, `after_server_stop`, and
    `main_process_stop`) the opposite is true. They are run in the reverse order of
    declaration.
  prefs: []
  type: TYPE_NORMAL
- en: How to decide to use a *before* listener or an *after* listener
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As stated above, there persists a common misconception that logic must be added
    to `before_server_start` in the case you want to perform some operation before
    requests start. The fear is that using `after_server_start` might cause some kind
    of a race condition where some requests might hit the server in the moments before
    that event is triggered.
  prefs: []
  type: TYPE_NORMAL
- en: This is incorrect. Both `before_server_start` and `after_server_start` run to
    completion before any requests are allowed to come in.
  prefs: []
  type: TYPE_NORMAL
- en: So, then the question becomes when should you favor one over the other? There
    are, of course, some personal and application-specific preferences that could
    be involved. Generally, however, I like to use the `before_server_start` event
    to set up my application context. If I need to initialize some object and persist
    it to `app.ctx`, then I will reach for `before_server_start`. For any other use
    case (like performing any other types of external calls, or configuration, I like
    to use `after_server_start`. This is by no means a hard and fast rule, and I often
    break it myself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the lifecycle of the server, there is one more missing
    bit of information that we need before we can run the application: configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring an application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sanic tries to make some reasonable assumptions out of the box about your application.
    With this in mind, you can certainly spin up an application and it should already
    have some reasonable default settings in place. While this may be acceptable for
    a simple prototype, as soon as you start to build your application you will realize
    that you need to configure it.
  prefs: []
  type: TYPE_NORMAL
- en: And this is where Sanic’s configuration system comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuration comes in two main flavors: tweaking the Sanic runtime operation,
    and declaring a state of global constants to be used across your application.
    Both types of configuration are important, and both follow the same general principles
    for applying values.'
  prefs: []
  type: TYPE_NORMAL
- en: We will take a closer look at what the configuration object is, how we can access
    it, and how it can be updated or changed.
  prefs: []
  type: TYPE_NORMAL
- en: What is the Sanic configuration object?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When you create a Sanic application instance, it will create a configuration
    object. That object is really just a fancy `dict` type. As you will see, it does
    have some special properties. Do not let that fool you. You should remember: *it
    is a* `dict`. You can work with it like you would any other `dict` object. This
    will come in handy in a little bit when we explore how we can use the object.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do not believe me, then pop the following code into your application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that getting a configuration value with a default is no different
    than any other `dict` in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration object is—however—much more important than any other `dict`.
    It contains a lot of settings that are critical to the operation of your application.
    We have, of course, already seen in *Chapter 6* that we can use it to modify our
    default error handling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To understand the full scope of settings that you can tweak, you should take
    a look at the Sanic documentation: [https://sanicframework.org/en/guide/deployment/configuration.html#builtin-values.](https://sanicframework.org/en/guide/deployment/configuration.html#builtin-values.)'
  prefs: []
  type: TYPE_NORMAL
- en: How can an application’s configuration object be accessed?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The best way to access the configuration object is to first get access to the
    application instance. Depending upon the scenario you are tackling at the moment,
    there are three main ways to get access to an application instance:'
  prefs: []
  type: TYPE_NORMAL
- en: Access the application instance using a request object (`request.app`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing applications from a Blueprint instance (`bp.apps`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving an application instance from the application registry (`Sanic.get_app()`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perhaps the most common way to obtain the application instance (and therefore
    the configuration object by extension) is to grab it from the request object inside
    of a handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you are outside of a route handler (or middleware) where the request object
    is easily accessible, then the next best choice is probably to use the application
    registry. Rarely will it make sense to use the Blueprint `apps` property. It is
    a set of applications that the blueprint has been applied to. However, because
    it only exists *after* registration, and it could be ambiguous which application
    you need, I usually will not reach for that as a solution. It is, nonetheless,
    good to know that it exists.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have seen us using the third option already. As soon as an application
    is instantiated, it is part of a global registry that can be looked up using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Whenever I am not in a handler, this is the solution I usually reach for. The
    two caveats that you need to be aware of are:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that the application instance has already been instantiated. Using
    `app = Sanic.get_app()` in the global scope can be tricky if you are not careful
    with your import ordering. Later on, in *Chapter 11*, *A complete real-world example*
    when we build out a complete application I will show you a trick I use to get
    around this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you are building a runtime with multiple application instances, then you
    will need to differentiate them using the application name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once you have the object, you will usually just access the configuration value
    as a property, for example, `app.config.FOOBAR`. As shown previously, you can
    also use a variety of Python accessors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`app.config.FOOBAR`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`app.config.get("FOOBAR")`'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`app.config["FOOBAR"]`'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`getattr(app.config, "FOOBAR")`'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: How can the configuration object be set?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you go to the Sanic documentation, you will see that there are a bunch of
    default values already set. These values can be updated in a variety of methods
    as well. Of course, you can use the `object` and `dict` setters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You will usually set values like this right after creating your application
    instance. For example, throughout this book, I have repeatedly used `curl` to
    access endpoints that I created. The easiest method to see an exception is to
    use the text-based exception renderer. Therefore, in most cases, I have used the
    following pattern to make sure that when there is an exception it is easily formatted
    for display in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This is *not* usually ideal for a fully-built application. If you have been
    involved in web application development before, then you probably do not need
    me to tell you that configuration should be easily changeable depending upon your
    deployment environment. Therefore, Sanic will load environment variables as configuration
    values if they are prefixed with `SANIC_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the above `FALLBACK_ERROR_FORMAT` could also be set outside
    of the application with an environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The best method to do this will obviously depend upon your deployment strategy.
    We go deeper into those strategies later in this Chapter, and the specifics of
    how to set those variables will differ and are outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option that you may be familiar with is centralizing all of your configurations
    in a single location. Django does this with `settings.py`. While I am personally
    not a fan of this pattern, you might be. You can easily duplicate it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a `settings.py` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the configuration to the application instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Access the values as needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There is nothing special about the `settings.py` file name. You just need a
    module with a whole bunch of properties that are uppercased. In fact, you could
    replicate this with an object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Put all of your constants into an object now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Apply the configuration from that object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result will be the same.
  prefs: []
  type: TYPE_NORMAL
- en: Some general rules about configuration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'I have some general rules that I like to follow regarding configuration. I
    encourage you to adopt them since they have evolved from years of making mistakes.
    But, I just as strongly encourage you to break them when necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Use simple values*: If you have some sort of a complex object like a `datetime`,
    perhaps configuration is not the best location for it. Part of the flexibility
    of configuration is that it can be set in many different ways; including outside
    of your application in environment variables. While Sanic will be able to convert
    things like booleans and integers, everything else will be a string. Therefore,
    for the sake of consistency and flexibility, try to avoid anything but simple
    value types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Treat them as constants*: Yes, this is Python. That means everything is an
    object and everything is subject to runtime changes. But do not do this. If you
    have a value that needs to be changed *during* the running of your application,
    use `app.ctx` instead. In my opinion, once `before_server_start` has completed,
    your configuration object should be considered locked in stone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Don’t hardcode values*: Or, at least try really hard not to. When building
    out your application, you will undoubtedly find the need to create some sort of
    constant value. It is hard to guess the scenario that this might come up in without
    knowing your specific application. But when you realize that you are about to
    create a constant, or some value, ask yourself whether the configuration is more
    appropriate. Perhaps the most concrete example of this is the settings that you
    might use to connect to a database, a vendor integration, or any other third-party
    service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring your application is almost certainly something that will change
    over the lifetime of your application. As you build it, run it, and add new features
    (or fix broken features), it is not uncommon to return to configuration often.
    One marker of a professional-grade application is that it relies heavily upon
    this type of configuration. This is to provide you with the flexibility to run
    the application in different environments. You may, for example, have some features
    that are only beneficial in local development, but not in production. It may also
    be the other way around. Configuration is therefore almost always tightly coupled
    with the environment where you will be deploying your application.
  prefs: []
  type: TYPE_NORMAL
- en: We now turn our attention to those deployment options to see how Sanic will
    behave when running in development and production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Running Sanic locally
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We finally are at the point where it is time to run Sanic—well, locally that
    is. However, we also know we have been doing that all along since *Chapter 2,
    Organizing a project*. The Sanic CLI is already probably a fairly comfortable
    and familiar tool. But there are some things that you should know about it. Other
    frameworks have only development servers. Since we know that Sanic’s server is
    meant for both development and production environments, we need to understand
    how these environments differ.
  prefs: []
  type: TYPE_NORMAL
- en: How does running Sanic locally differ from production?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most common configuration change for local production is turning on debug
    mode. This can be accomplished in three ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It could be enabled directly on the application instance. You typically would
    see this inside of a factory pattern when Sanic is being run programmatically
    from a script (as opposed to the CLI). You can directly set the value as shown
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is perhaps more common to see it set as an argument of `app.run`. A common
    use case for this might be when reading environment variables to determine how
    Sanic should initialize. In the following example, an environment value is read
    and applied when Sanic server begins to run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final option is to use the Sanic CLI. This is generally my preferred solution,
    and if you have been following along with the book, it is the one that we have
    been using all along. This method is straightforward as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The reason that I prefer this final option is that I like to keep the operational
    aspects of the server distinct from other configurations.
  prefs: []
  type: TYPE_NORMAL
- en: For example, timeouts are configuration values that are closely linked to the
    operation of the framework and not the server itself. They impact how the framework
    responds to requests. Usually, these values are going to be the same regardless
    of where the application is deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Debug mode–on the other hand–is much more closely linked to the deployment environment.
    You will want to set it to `True` locally, but `False` in production. Therefore,
    since we will be controlling how Sanic is deployed with tools like Docker, controlling
    the server’s operational capacity outside of the application makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: “*Okay*”, you say, “*turning on debug mode is simple, but why should I?*” I’m
    glad that you asked. When you run Sanic in debug mode, it makes a couple of important
    changes. The most noticeable is that you begin to see debug logs and access logs
    dispatched from Sanic. This is, of course, very helpful to see while developing.
  prefs: []
  type: TYPE_NORMAL
- en: '**TIP**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'When I sit down to work on a web application, I always have three windows in
    my view at all times:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: My IDE
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An API client like Insomnia or Postman
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A terminal showing me my Sanic logs (in debug mode)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The terminal with debug level logging is your window into what is happening
    with your application as you build it.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Perhaps the biggest change that debug mode brings is that any exception will
    include its traceback in the response. In the next chapter, we will look at some
    examples of how you can make the most of this exception information.
  prefs: []
  type: TYPE_NORMAL
- en: This is hugely important and useful while you are developing. It is also a huge
    security issue to accidentally leave it on in production. *DO NOT leave debug
    mode on in a live web application*. This includes any instance of your application
    that is *not* on a local machine. So, for example, if you have a staging environment
    that is hosted somewhere on the Internet, it may not be your “production” environment.
    However, it still *MUST NOT* run debug mode. At best, it will leak details about
    how your application was built. At worst, it will make sensitive information available.
    Make sure to turn off debug mode in production.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of production, let’s move on over to what it takes to deploy Sanic
    into the wild world of production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have finally made it. After working your way through the application development
    process, there finally is a product to launch out into the ether of the World
    Wide Web. The obvious question then becomes: what are my options? There really
    are two sets of questions that need to be answered:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First question: which server should run Sanic? There are three options: Sanic
    server, an ASGI server, or Gunicorn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second question: where do you want to run the application? Some typical choices
    include: *bare metal* virtual machine, containerized image, **platform-as-a-service**
    (**PaaS**), or a self-hosted or fully managed orchestrated container cluster.
    Perhaps these choices might make more sense if we put some of the commonly used
    product names to them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Deployment type** | **Potential vendors** |'
  prefs: []
  type: TYPE_TB
- en: '| Virtual machine | Amazon EC2, Google Cloud, Microsoft Azure, Digital Ocean,
    Linode |'
  prefs: []
  type: TYPE_TB
- en: '| Container | Docker |'
  prefs: []
  type: TYPE_TB
- en: '| Platform as a service | Heroku |'
  prefs: []
  type: TYPE_TB
- en: '| Orchestrated cluster | Kubernetes |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 – Examples of common hosting providers and tools
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right server option
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we stated, there are three main ways to run Sanic: the built-in server,
    with an ASGI compatible server, or with Gunicorn. Before we decide which server
    to run, we will look take a brief look at the pros and cons for each option starting
    with the least performant option.'
  prefs: []
  type: TYPE_NORMAL
- en: Gunicorn
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you are coming to Sanic from the WSGI world, you may already be familiar
    with Gunicorn. Indeed, you may even be surprised to learn that Sanic can be run
    with Gunicorn since it is built for WSGI applications, not asynchronous applications
    like Sanic. Because of this, the biggest downside to running Sanic with Gunicorn
    is the *substantial* decrease in performance. Gunicorn effectively unravels much
    of the work done to leverage *concurrency* with the `asyncio` module. It is by
    far the slowest way to run Sanic, and in most use cases is not recommended.
  prefs: []
  type: TYPE_NORMAL
- en: It still could be a good choice in certain circumstances. Particularly, if you
    need a feature-rich set of configurations options, and cannot use something like
    Nginx, then this might be an approach. Gunicorn has a tremendous amount of options
    that can be leveraged for fine-tuning server operation. In my experience, however,
    I typically see people reaching for it out of habit and not out of necessity.
    People will use it simply because it is what they know. For people that are transitioning
    to Sanic from the Flash/Django world, they may be used to a particular deployment
    pattern that was centered around tools like Supervisor and Gunicorn. That’s fine,
    but it is a little old fashioned and should not be the go-to pattern for Sanic
    deployments.
  prefs: []
  type: TYPE_NORMAL
- en: For those people, I urge you to look at another option. You are building with
    a new framework, why not deploy it with a new strategy as well?
  prefs: []
  type: TYPE_NORMAL
- en: If, however, you do find yourself needing some of the more fine-tune controls
    offered by Gunicorn, I would recommend you take a look at Nginx, which has an
    equally (if not more) impressive set of features. Whereas Gunicorn would be set
    up to actually run Sanic, the Nginx implementation would rely upon Sanic running
    via one of the other two strategies and placing an Nginx proxy in front of it.
    More on Nginx proxying later in this Chapter. This option will allow you to retain
    a great deal of server control without sacrificing performance. It does, however,
    require some more complexity since you need to essentially run two servers instead
    of just one.
  prefs: []
  type: TYPE_NORMAL
- en: 'If in the end, you still decide the use Gunicorn, then the best way to do so
    is to use Uvicorn’s worker shim. Uvicorn is an ASGI server, which we will learn
    more about in the next section. In this context, however, it also ships with a
    worker class that allows Gunicorn to integrate with it. This effectively puts
    Sanic into ASGI mode. Gunicorn still runs as the webserver, but it will pass traffic
    off to Uvicorn, which will then reach into Sanic as if it were an ASGI application.
    This will retain much of the performance offered by Sanic and asynchronous programming
    (although still not as performant as the Sanic server by itself). You can accomplish
    this as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, make sure both Gunicorn and Uvicorn are installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, run the application like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should now have the full span of Gunicorn configurations at your fingertips.
  prefs: []
  type: TYPE_NORMAL
- en: ASGI server
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We visited ASGI briefly in *Chapter 1, Introduction to Sanic and async frameworks*.
    If you recall, **ASGI** stands for **Asynchronous Server Gateway Interface**,
    and it is a design specification for how servers and frameworks can communicate
    with each other asynchronously. It was developed as a replacement methodology
    for the older WSGI standard that is incompatible with modern asynchronous Python
    practices. This standard has given rise to three popular ASGI webservers: Uvicorn,
    Hypercorn, and Daphne. All three of them follow the ASGI protocol, and can therefore
    run any framework that adheres to that protocol. The goal, therefore, is to create
    a common language that allows one of these ASGI servers to run any ASGI framework.'
  prefs: []
  type: TYPE_NORMAL
- en: And this is where to discuss Sanic with regards to ASGI we must have a clear
    distinction in our mind of the difference between the server and the framework.
    *Chapter 1* discussed this difference in detail. As a quick refresher, the web
    server is the part of the application that is responsible for connecting to the
    operating system’s socket protocol and handling the translation of bytes into
    usable web requests. The framework takes the digested web requests and provides
    the application developer with the tools needed to respond and construct an appropriate
    HTTP response. The server then takes that response and sends the bytes back to
    the operating system for delivery back to the client.
  prefs: []
  type: TYPE_NORMAL
- en: Sanic handles this whole process, and when it does so, it operates outside the
    ASGI since that interface is not needed. However, it also has the ability to speak
    the language of an ASGI framework and thus can be used with any ASGI web server.
  prefs: []
  type: TYPE_NORMAL
- en: One of the benefits of running Sanic as an ASGI application is that it standardizes
    the run-time environment with a broader set of Python tools. There is, for example,
    a set of ASGI middleware that could be implemented to add a layer of functionality
    between the server and the application.
  prefs: []
  type: TYPE_NORMAL
- en: However, some of the standardization does come at the expense of performance.
  prefs: []
  type: TYPE_NORMAL
- en: Sanic server
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The default mechanism is to run Sanic with its built-in web server. It should
    come as no surprise that it is built with performance in mind. Therefore, what
    Sanic server gives up by forfeiting the standardization and interoperability of
    ASGI, it makes up in its ability to optimize itself as a single purpose server.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have touched on some of the potential downsides of using Sanic server. One
    of them was static content. No Python server will be able to match the performance
    of Nginx in handling static content. If you are already using Nginx as a proxy
    for Sanic, and you have a known location of static assets, then it might make
    sense to use it also for those assets. However, if you are not using it, then
    you need to determine whether the performance difference warrants the additional
    operational expense. In my opinion, if you can easily add this to your Nginx configuration:
    great. However, if it would take a lot of complicated effort, or you are exposing
    Sanic directly, then the benefit might not be as great as just leaving it as is
    and serving that content from Sanic. Sometimes, for example, the easiest thing
    to do is to run your entire frontend and backend from a single server. This is
    certainly a case where I would suggest learning the competing interests and making
    an appropriate decision and not trying to make a *perfect* decision.'
  prefs: []
  type: TYPE_NORMAL
- en: With this knowledge, you should now be able to decide which server is the right
    fit for your needs. We will assume for the remainder of this book that we are
    still deploying with the Sanic server, but since it is mainly a matter of changing
    the command line executable, the difference should not make a difference.
  prefs: []
  type: TYPE_NORMAL
- en: How to choose a deployment strategy?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last section laid out three potential web servers to use for Sanic applications.
    But that web server needs to run on a web host. But, before deciding on which
    web hosting company to use, there is still a very important missing component:
    how are you going to get your code from your local machine to the web host? In
    other words: how are you going to deploy your application? We will now look through
    some options for deploying Sanic applications.'
  prefs: []
  type: TYPE_NORMAL
- en: There is some assumed knowledge, so if some of the technologies or terms here
    are unfamiliar, please feel free to stop and go look them up.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual machine
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is perhaps the easiest option. Well, the easiest besides PAAS. Setting
    up a **virtual machine** (**VM**) is super simple these days. With just a few
    clicks of a button, you can have a custom configuration for a VM. The reason this
    then becomes a simple option is that you just need to run your Sanic application
    the same way you might on your local machine. This is particularly appealing when
    using the Sanic server since it literally means that you can run Sanic in production
    with the same commands that you use locally. However, getting your code to the
    VM, maintaining it once it is there, and then ultimately scaling it will make
    this option the hardest. To be blunt, I almost would never recommend this solution.
    It is appealing to new beginners since it looks so simple from the outside. But
    looks can be deceiving.
  prefs: []
  type: TYPE_NORMAL
- en: There may in fact be times when this is an appropriate solution. If that is
    the case, then what would deployment look like? Really, not that much different
    than running it locally. You run the server and bind it to an address and port.
    With the proliferation of cloud computing, service providers have made it such
    a trivial experience to stand up a virtual machine. I personally find platforms
    like Digital Ocean and Linode to be super user-friendly, and excellent choices.
    Other obvious choices include Amazon AWS, Google Cloud, and Microsoft Azure. In
    my opinion, however, they are a little less friendly to someone new to cloud computing.
    Armed with their good documentation, with Digital Ocean and Linode it is relatively
    inexpensive and painless to click a few buttons and get an instance running. Once
    they provide you with an IP address, it is now your responsibility for getting
    your code to the machine and running the application.
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking the simplest way to move your code to the server would
    be to use git. Then all you need to do is launch the application and you are done.
    But, what happens if you need more instances or redundancy? Yes, Sanic comes with
    the ability to spin up multiple worker processes. But what if that is not enough?
    Now you need another VM and some way to manage load balancing your incoming web
    traffic between them. How are you going to handle redeployments of bug patches
    or new features? What about changes to environment variables? These complexities
    could lead to a lot of sleepless nights if you are not careful.
  prefs: []
  type: TYPE_NORMAL
- en: This is also somewhat ignoring the other fact that not all environments are
    equal. VMs could be built with different dependencies, leading to wasteful time
    maintaining servers and packages.
  prefs: []
  type: TYPE_NORMAL
- en: That is not to say this cannot or should not be a solution. Indeed, it might
    be a great solution if you are creating a simple service for your own use. Perhaps
    you need a webserver for connecting to a smart home network. But it is certainly
    a case of *developer beware*. Running a webserver on a *baremetal* virtual machine
    is rarely as simple as it appears at first glance.
  prefs: []
  type: TYPE_NORMAL
- en: Containers with Docker
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One solution to the previous set of problems is using a Docker container. For
    those that have used Docker, you can probably skip to the next section because
    you already understand the power that it provides. If you are new to containers,
    then I highly recommend you learn about them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In brief, you write a simple manifest called a Dockerfile. That manifest describes
    an intended operating system and some instructions needed to build an ideal environment
    for running your application. An example manifest is available in the GitHub repository
    here: [https://github.com/PacktPublishing/Web-Development-with-Sanic/blob/main/chapters/08/k8s/Dockerfile](https://github.com/PacktPublishing/Web-Development-with-Sanic/blob/main/chapters/08/k8s/Dockerfile).'
  prefs: []
  type: TYPE_NORMAL
- en: This might include installing some dependencies (including Sanic), copying source
    code, and defining a command that will be used to run your application. With that
    in place, docker then builds a single image with everything needed to run the
    application. That image can be uploaded to a repository and used to run irrespective
    of the environment. You could, for example, opt to use this instead of managing
    all those separate VM environments. It is much simpler to bundle all that together
    and simply run it.
  prefs: []
  type: TYPE_NORMAL
- en: There is still some complexity involved in building our new versions and deciding
    where to run the image, but having consistent builds is a huge gain. This should
    really become a focal point of your deployment. So, although containers are part
    of the solution, there still is the problem of where to run it and the maintenance
    costs required to keep it running and up to date.
  prefs: []
  type: TYPE_NORMAL
- en: 'I almost *always* would recommend using Docker as part of your deployment practices.
    And if you know about Docker Compose, you might be thinking that is a great choice
    for managing deployments. I would agree with you, so long as we are talking about
    deployments on your local machine. Using Docker Compose for production is not
    something I would usually consider. The reason is simple: horizontal scaling.
    Just like the issue with running Sanic on a VM, or a single container on a VM,
    running Docker Compose on a single VM carries the same problem: horizontal scaling.
    The fix is orchestration.'
  prefs: []
  type: TYPE_NORMAL
- en: Container orchestration with Kubernetes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The problem with containers is that they only solve the environmental problems
    by creating a consistent and repeatable strategy for your application. They still
    suffer from scalability problem. Again, what happens when your application needs
    to scale past the resources that are available on a single machine? Container
    orchestrators like Kubernetes (aka “K8S”) are a dream come true for anyone that
    has done DevOps work in the past. By creating a set of manifests, you will describe
    to Kubernetes what your ideal application will look like: the number of replicas,
    the number of resources they need, how traffic should be exposed, and so on. That
    is it! All you need to do is describe your application with some YAML files. Kubernetes
    will handle the rest. It has the added benefit of enabling rolling deployments
    where you can rollout new code with zero downtime for your application.'
  prefs: []
  type: TYPE_NORMAL
- en: The downside, of course, is that this option is the most complex. It is suitable
    for more serious applications where the complexity is acceptable for the benefits
    added. It may, however, be overkill for a lot of projects. This is a go-to deployment
    strategy for any application that will have more than a trivial amount of traffic.
    Of course, the complexity and scale of a K8S cluster can expand based upon its
    needs. This dynamic quality is what makes it increasingly a standard deployment
    strategy that has been adopted by many industry professionals.
  prefs: []
  type: TYPE_NORMAL
- en: It is an ideal solution for platforms that consist of multiple services working
    together, or that require scaling beyond the boundaries of a single machine.
  prefs: []
  type: TYPE_NORMAL
- en: This does bring up an interesting question, however. We know that Sanic has
    the ability to scale horizontally on a single host by replicating its workers
    in multiple processes. Kubernetes is capable of scaling horizontally by spinning
    up replica **pods**. Let’s say you hypothetically have decided that you need four
    instances of your application to handle the load. Should you have two pods each
    running two workers or four pods each with one worker?
  prefs: []
  type: TYPE_NORMAL
- en: I have heard both put forth as *ideal* solutions. Some people say that you should
    maximize the resources per container. Other people say that you should have no
    more than one process per container. From a performance perspective, it is a dead
    heat. The solutions effectively perform the same. Therefore, it comes down entirely
    to the choice of the application builder. There is no right or wrong answer.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, we will take a closer look at what it takes to launch
    a Sanic application with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Platform as a service
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Heroku is probably one of the most well-known **PAAS** offering. It has been
    around for a while and has become an industry leader in these low-touch deployment
    strategies. Heroku is not the only provider, both Google and AWS have PAAS services
    in their respective cloud platforms, and Digital Ocean has also launched their
    own competing service. What makes PAAS super convenient is that all you need to
    do is write the code. There is no container management, environment handling,
    or deployment struggles. It is intended to be a super easy low-touch solution
    for deploying code. Usually, deploying an application is as simple as pushing
    code is to a git repository.
  prefs: []
  type: TYPE_NORMAL
- en: This simple option is, therefore, ideal for proof-of-concept applications or
    other builds you need to deploy super quickly. I also do know plenty of people
    that run more robust and scalable applications through these services, and they
    really can be a great alternative. The huge selling point of these services is
    that by outsourcing the deployment, scaling, and service maintenance to the service
    provider, you are freed up to focus on the application logic.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this simplicity, and ultimately flexibility, we will take a closer
    look at launching Sanic with a PAAS vendor later in this Chapter in the *Deployment
    examples* section. One of the things that is great about a PAAS is that it handles
    a lot of details like setting up a TLS certificate and enabling a `https://` address
    for your application. In the next section, however, we will learn what it takes
    to set up an `https://` address for your application in the absence of convenience
    from a PAAS.
  prefs: []
  type: TYPE_NORMAL
- en: Securing your application with TLS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are not encrypting traffic to your web application, you are doing something
    wrong. In order to protect information while it is in transit between the web
    browser and your application, it is an absolute necessity to add encryption. The
    international standard for doing that is known as TLS (which stands for Transport
    Layer Security). It is a protocol for how data can be encrypted between two sources.
    Often, however, it will be referred to as *SSL* (which is an earlier protocol
    that TLS replaces) or *HTTPS* (which is technically an implementation of TLS,
    not TLS itself). Since it is not important for us *how* it works, and we only
    care that it does what it needs to do, we will use these terms somewhat interchangeably.
    Therefore, it is safe for you to think about TLS and HTTPS as the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: So, what is it? The simple answer is that you request a pair of keys from some
    reputable source on the Internet. Your next step is to make them available to
    your web server, and expose your application over a secure port–typically, that
    is port 443\. After that, your web server should handle the rest, and you should
    now be able to access your application with an `https://` address instead of `http://`.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up TLS in Sanic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two common scenarios you should be familiar with. If you are exposing
    your Sanic application directly, or if you are placing Sanic behind a proxy. This
    will determine where you want to *terminate* your TLS connection. This simply
    means where you should set up your public-facing certificates. We will assume
    for now that Sanic is exposed directly. We also will assume that you already have
    certificates. If you do not know how to obtain them, don’t worry, we will get
    to a potential solution for you in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: All we need to do is to tell the Sanic server how to access those certificates.
    Also, since Sanic will default to port `8000`, we need to make sure to set it
    to `443`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, our new runtime command (in production) will be this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is largely the same operation if you are using `app.run` instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When you are exposing your Sanic application directly, and therefore terminating
    your TLS with Sanic, there is often a desire to add HTTP to HTTPS redirect. For
    your users’ convenience, you probably want them to always be directed to HTTPS
    and for this redirection to happen *magically* for them without having to think
    about it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Sanic User Guide provides us with a simple solution that involves running
    a second Sanic application inside our main app. Its only purpose will be to bind
    to port 80 (which is the default HTTP non-encrypted port) and redirect all traffic.
    Let’s quickly examine that solution and step through it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, in addition to our main application, we need a second that will be responsible
    for the redirects. So, we will set up two applications and some configuration
    details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We add only one endpoint to the `http_app` that will be responsible for redirecting
    all traffic to the `main_app`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To make running the HTTP redirect application easier, we will just piggyback
    off of the main application’s lifecycle so that there is not a need to create
    another executable. Therefore, when the main application starts up, it will also
    create and bind the HTTP application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should note how we are assigning that server to the `ctx` for our main application
    so we can use it again.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, when the main application shuts down, it will also be responsible
    for shutting down the HTTP application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this in place, any request to `http://example.com` should be automatically
    redirected to the `https://` version of the same page.
  prefs: []
  type: TYPE_NORMAL
- en: Back in Step 1 and Step 2, this example sort of skipped over the fact that you
    need to obtain actual certificate files to be used to encrypt your web traffic.
    This is largely because you need to bring your own certificates to the table.
    If you are not familiar with *how* to do that, the next section provides a potential
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: Getting and renewing a certificate from Let’s Encrypt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Back in the olden days of the Internet, if you wanted to add HTTPS protection
    to your web application, it was going to cost you. Certificates were not cheap,
    and they were somewhat cumbersome and complicated to manage. Actually, certificates
    are still not cheap if you are to buy one yourself, especially if you want to
    buy a certificate that covers your subdomains. However, this is no longer your
    only option since several players came together looking for a method to create
    a safer online experience. The solution: free TLS certificates. These free (and
    reputable) certificates are available from *Let’s Encrypt*, and are the reason
    that *every* production website should be encrypted. Expense is no longer an excuse.
    At this point in time, if I see a website still running `http://` in a live environment,
    a part of me cringes as I go running for the hills.'
  prefs: []
  type: TYPE_NORMAL
- en: If you do not currently have a TLS certificate for your application, head over
    to [https://letsencrypt.org](https://letsencrypt.org) to get one. The process
    to obtain a certificate from *Let’s Encrypt* requires you to follow some basic
    steps, and then prove that you own the domain. Because there are a lot of platform
    specifics, and it is outside the scope of this book, we will not really dive into
    the details of how to obtain one. Later on, this chapter does go through a step-by-step
    process to obtain a Let’s Encrypt certificate for use in a Kubernetes deployment
    in the *Kubernetes (as-a-service)* section.
  prefs: []
  type: TYPE_NORMAL
- en: I do, however, highly encourage you to use Let’s Encrypt if the budget for your
    project does not allow for you to go out and purchase a certificate.
  prefs: []
  type: TYPE_NORMAL
- en: With a certificate in hand, it is finally time to look at some actual code and
    decide which deployment strategy is right for your project.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Earlier when discussing the various choices for deployment strategies, two
    options rose above the others: PAAS, and Kubernetes. When deploying Sanic into
    production, I would almost always recommend one of these solutions. There is no
    hard and fast rule here, but I generally think of Kubernetes as being the go-to
    solution for platforms that will be running multiple services, have the need for
    more controlled deployment configurations, and have more resources and a team
    of developers. On the other hand, PAAS is more appropriate for single developer
    projects or projects that do not have resources to devote to maintaining a richer
    deployment pipeline. We will now explore what it takes to get Sanic running in
    these two environments.'
  prefs: []
  type: TYPE_NORMAL
- en: Platform-as-a-service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we stated before, Heroku is a well-known industry leader in deploying applications
    via PAAS. This is for good reason as they have been in business providing these
    services since 2007, and have played a critical role in popularizing the concept.
    They have made the process super simple for both new and experienced developers.
    However, in this section, we are going to instead take a look at deploying a Sanic
    application with Digital Ocean’s PAAS offering. The steps should be nearly identical
    and applicable to Heroku, or any of the other services that are out there:'
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to, of course, go to their website and signup for an account
    if you do not have one. Their PAAS is called **Apps**, which you can find on the
    left-hand side of their main dashboard once you are logged in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will next be taken through a series of steps that will ask you to connect
    a git repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will next need to configure the app through their UI. Your screen will
    look probably something like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.1 - Example settings for PAAS setup](img/file9.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.1 - Example settings for PAAS setup
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A very important thing to note here is that we have set the `--host=0.0.0.0`.
    This means that we are telling Sanic that it should bind itself to any IP address
    that Digital Ocean provides it. Sanic will bind itself to the `127.0.0.1` address
    without this configuration. As anyone that has done web development knows, the
    127.0.0.1 address maps to localhost on most computers. This means that Sanic will
    be accessible only to web traffic on that specific computer. This is no good.
    If you ever deploy an application and cannot access it, one of the first things
    to check is that the port and host are set up properly. One of the easiest options
    is to just use `0.0.0.0`, which is like the equivalent of a wildcard IP address.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, you will be asked to select a location for which data center it will live
    in. Usually, you want to pick one that will be close to where your intended audience
    will be to reduce latency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will then need to select an appropriate package. If you do not know what
    to choose, start small and then scale it up as needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The only thing left to do is to setup the files in our repository. There is
    a sample in GitHub for you to follow: [https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/08/paas](https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/08/paas).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we need a `requirements.txt` file that lists out our dependencies:
    sanic and a `server.py` just like every other build we have done so far.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once that is done, every time you push to the repository, your application should
    be rebuilt and available to you. One of the nice benefits of this is that you
    will get a TLS certificate with https out of the box. No configuration is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Seems simple enough? Let’s look at a more complex setup with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes (as-a-service)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are going to turn our attention to Kubernetes: one of the most widely adopted
    and utilized platforms for orchestrating the deployment of containers. You could,
    of course, spin up some virtual machines, install Kubernetes on them, and manage
    your own cluster. However, I find a much more worthwhile solution is to just take
    one of the Kubernetes-as-a-service solutions. You still have all of the power
    of Kubernetes (which we will use the common abbreviation: K8S), but none of the
    maintenance headaches.'
  prefs: []
  type: TYPE_NORMAL
- en: We will again look at Digital Ocean and use their platform for our example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our local directory we will need a few files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Dockerfile` to describe out docker container'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`app.yml` a K8S config file described below'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ingress.yml` a K8S config file described below'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load-balancer.yml` a K8S config file described below'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`server.py` which is again a Sanic applicationYou can follow along with the
    files in the GitHub repository: [https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/08/k8s](https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/08/k8s).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our Dockerfile is the set of instructions to build our container. We will take
    a shortcut and use one of the Sanic community’s base images that has both Python
    and Sanic pre-installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Just like we saw with the PAAS solution, we are binding to host `0.0.0.0` for
    the same reason. We are *not* adding multiple workers per container here. Again,
    this is something you could do if you prefer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will need to build an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s try running it locally to make sure it works
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Do not forget to clean up your environment, and remove the container when you
    are done, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And, you will of course need to push your container to some accessible repository.
    For ease of use and demonstration purposes, I will be pushing it to my public
    Docker Hub repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For this next part, we will interact with Digital Ocean through their CLI tool.
    If you do not have it installed, head to [https://docs.digitalocean.com/reference/doctl/how-to/install/](https://docs.digitalocean.com/reference/doctl/how-to/install/).
    You will want to make sure you login:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We need a Digital Ocean K8S cluster. Login to their web portal, click on **Kubernetes**
    on the main dashboard and set up a cluster. For now, default settings are fine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We next need to enable `kubectl` (the tool to interact with K8S) to be able
    to talk to our Digital Ocean K8S cluster. If `kubectl` is not installed, check
    out the instructions here: [https://kubernetes.io/docs/reference/kubectl/overview/](https://kubernetes.io/docs/reference/kubectl/overview/).
    The command you need will look something like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once your cluster is available and `kubectl` is set up, you can verify it by
    running:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Of course, we have not set up anything, so there should not be anything to see
    just yet.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When configuring Kubernetes, we need to start by running `kubectl apply` on
    our `app.yml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**TIP**'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Before going further, you will see a lot of online tutorials that use this
    style of command:'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`$ kubectl create ...`'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I generally try to avoid that in favor of this:'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`$ kubectl apply ...`'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: They essentially do the same thing, but the convenience is that resources that
    are created with apply can be continually modified by “applying” the same manifest
    over and over again.
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'What is in `app.yml`? Check out the GitHub repository for the full versions.
    It is rather lengthy and includes some boilerplate that is not relevant to the
    current discussion, so I will show only relevant snippets here. This goes for
    all of the K8S manifests in our example. The file should contain the Kubernetes
    primitives needed to run the application: a **service** and a **deployment**.The
    service should look something like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice how we are mapping port `7777` to `80`. This is because we will be terminating
    TLS in front of Sanic and our ingress controller will talk to Sanic over HTTP
    unencrypted. Because it is all in a single cluster, this is acceptable. Your needs
    might be more sensitive, and then you should look into encrypting that connection
    as well.The other thing in `app.yml` is the deployment, which should look something
    like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we are defining the number of replicas we want, as well as pointing the
    container to our docker image repository.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After creating that file, we will apply it, and you should see a result similar
    to this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now checkout to see that it worked:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will next use an off the shelf solution to create an NGINX ingress. This
    will be the proxy layer that terminates our TLS and feeds HTTP requests into Sanic.
    We will install it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note, at the time of writing, v1.0.0 is the latest. That probably is not true
    by the time you are reading this, so you may need to change that. You can find
    the latest version on their GitHub page: [https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we will setup our ingress. Create an `ingress.yml` following the pattern
    in our GitHub repository example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will notice there are *intentionally* some lines commented out. We will
    get to that in a minute. Let’s just quickly verify that it worked:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We should take a step back and jump over to the **Digital Ocean** dashboard.
    On the left is a tab called **Networking**. Go there and then in the tab for **Domains**
    follow the procedure to add your own domain there. In that example, in `ingress.yml`
    we added `example.com` as the ingress domain. Whatever domain you add to Digital
    Ocean’s portal is what should match your ingress. If you need to go back and update
    and re-apply the `ingress.yml` file with your domain, do that now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once that is all configured, we should be able to see our application working:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is of course not ideal because it is still on `http://`. We will now get
    a Let’s Encrypt certificate and set up TLS.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The easiest method for this is to set up a tool called `cert-manager`. It will
    do all of the interfacing we need with Let’s Encrypt. Start by installing it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Again, please check to see what the most up-to-date version is and update this
    command accordingly.We can verify its installation here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create the `load-balancer.yml` following the example in the GitHub repository.
    It should look something like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply that manifest and confirm that it worked:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Your K8S cluster will now start the process of obtaining a certificate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tip**'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'One thing that you might encounter is that the process gets stuck while requesting
    the certificate. If this happens to you, the solution is to turn on **Proxy Protocol**
    in your **Digital Ocean** dashboard. Go to the following setting and turn this
    on if you need to:'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Networking** > **Load Balancer** > **Manage Settings** > **Proxy Protocol**
    > **Enabled**'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: We’re almost there! Open up that `ingress.yml` file and uncomment those few
    lines that were previously commented out. Then `apply` the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Done! You should not automatically have a redirect from `http://` to `https://`,
    and your application is fully protected.
  prefs: []
  type: TYPE_NORMAL
- en: Better yet, you now have a deployable Sanic application with all the benefits,
    flexibility, and scalability that K8S container orchestration provides.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building a great Sanic application is only half of the job. Deploying it to
    make our application usable out in the wild is the other half. In this chapter,
    we explored some important concepts for you to consider. It is never too early
    to think about deployment as well. The sooner you know which server you will use,
    and where you will host your application, the sooner you can plan accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: There are of course many combinations of deployment options, and I only provided
    you with a small sampling. As always, you will need to learn what works for your
    project and team. Take what you have learned here and adapt it.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you were to ask me to boil all of this information down and ask
    my personal advice on how to deploy Sanic, I would tell you this:'
  prefs: []
  type: TYPE_NORMAL
- en: Run your applications using the built-in Sanic server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terminate TLS outside of your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For personal or smaller projects, or if you want a simpler deployment option,
    use a PAAS provider
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For larger projects that need to scale and have more developer-resources, use
    a hosted Kubernetes solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There you have it. You now should be able to build a Sanic application and run
    it on the Internet. Our time is done, right? You should have the skills and knowledge
    you need now to go out and build something great, so go ahead and do that now.
    In the remainder of this book, we will start to look at some more practical issues
    that arise while building web applications and look at some best-practice strategies
    in how to solve them.
  prefs: []
  type: TYPE_NORMAL
