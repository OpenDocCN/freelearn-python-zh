- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Solving Numerical, Symbolic, and Graphical Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservice architecture is not only used to build fine-grained, optimized,
    and scalable applications in the banking, insurance, production, human resources,
    and manufacturing industries. It is also used to develop scientific and computation-related
    research and scientific software prototypes for applications such as **laboratory
    information management systems** (**LIMSs**), weather forecasting systems, **geographical
    information systems** (**GISs**), and healthcare systems.
  prefs: []
  type: TYPE_NORMAL
- en: FastAPI is one of the best choices in building these granular services since
    they usually involve highly computational tasks, workflows, and reports. This
    chapter will highlight some transactions not yet covered in the previous chapters,
    such as symbolic computations using `sympy`, solving linear systems using `numpy`,
    plotting mathematical models using `matplotlib`, and generating data archives
    using `pandas`. This chapter will also show you how FastAPI is flexible when solving
    workflow-related transactions by simulating some Business Process Modeling Notation
    (BPMN) tasks. For developing big data applications, a portion of this chapter
    will showcase GraphQL queries for big data applications and Neo4j graph databases
    for graph-related projects with the framework.
  prefs: []
  type: TYPE_NORMAL
- en: The main objective of this chapter is to introduce the FastAPI framework as
    a tool for providing microservice solutions for scientific research and computational
    sciences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the projects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the symbolic computations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating arrays and DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing statistical analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating CSV and XLSX reports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting data models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulating a BPMN workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using GraphQL queries and mutations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing the Neo4j graph database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides the base skeleton of a `ch10` project.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The PCCS project has two versions: `ch10-relational`, which uses a PostgreSQL
    database with Piccolo ORM as the data mapper, and `ch10-mongo`, which saves data
    as MongoDB documents using Beanie ODM.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Piccolo ORM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`ch10-relational` uses a fast Piccolo ORM that can support both sync and async
    CRUD transactions. This ORM was not introduced in [*Chapter 5*](B17975_05.xhtml#_idTextAnchor107)*,
    Connecting to a Relational Database*, because it is more appropriate for computational,
    data science-related, and big data applications. The Piccolo ORM is different
    from other ORMs because it scaffolds a project containing the initial project
    structure and templates for customization. But before creating the project, we
    need to install the `piccolo` module using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterward, install the `piccolo-admin` module, which provides helper classes
    for the GUI administrator page of its projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create a project inside a newly created root project folder by
    running `piccolo asgi new`, a CLI command that scaffolds the Piccolo project directory.
    The process will ask for the API framework and application server to utilize,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Scaffolding a Piccolo ORM project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.01_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – Scaffolding a Piccolo ORM project
  prefs: []
  type: TYPE_NORMAL
- en: 'You must use FastAPI for the application framework and `uvicorn` is the recommended
    ASGI server. Now, we can add Piccolo applications inside the project by running
    the `piccolo app new` command inside the project folder. The following screenshot
    shows the main project directory, where we execute the CLI command to create a
    Piccolo application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Piccolo project directory'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.02_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – Piccolo project directory
  prefs: []
  type: TYPE_NORMAL
- en: 'The scaffolded project always has a default application called `home`, but
    it can be modified or even deleted. Once removed, the Piccolo platform allows
    you to replace `home` by adding a new application to the project by running the
    `piccolo app new` command inside the project folder, as shown in the preceding
    screenshot. A Piccolo application contains the ORM models, BaseModel, services,
    repository classes, and API methods. Each application has an auto-generated `piccolo_app.py`
    module where we need to configure an `APP_CONFIG` variable to register all the
    ORM details. The following is the configuration of our project’s survey application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For the ORM platform to recognize the new Piccolo application, its `piccolo_app.py`
    must be added to `APP_REGISTRY` of the main project’s `piccolo_conf.py` module.
    The following is the content of the `piccolo_conf.py` file of our `ch10-piccolo`
    project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `piccolo_conf.py` file is also the module where we establish the PostgreSQL
    database connection. Aside from PostgreSQL, the Piccolo ORM also supports SQLite
    databases.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the data models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Like in Django ORM, Piccolo ORM has migration commands to generate the database
    tables based on model classes. But first, we need to create model classes by utilizing
    its `Table` API class. It also has helper classes to establish column mappings
    and foreign key relationships. The following are some data model classes that
    comprise our database `pccs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: After creating the model classes, we can update the database by creating the
    migrations files. Migration is a way of updating the database of a project. In
    the Piccolo platform, we can run the `piccolo migrations new <app_name>` command
    to generate files in the `piccolo_migrations` folder. These are called migration
    files and they contain migration scripts. But to save time, we will include the
    `--auto` option for the command to let the ORM check the recently executed migration
    files and auto-generate the migration script containing the newly reflected schema
    updates. Check the newly created migration file first before running the `piccolo
    migrations forward <app_name>` command to execute the migration script. This last
    command will auto-create all the tables in the database based on the model classes.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the repository layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating the repository layer comes after performing all the necessary migrations.
    Piccolo’s CRUD operations are like those in the Peewee ORM. It is swift, short,
    and easy to implement. The following code shows an implementation of the `insert_respondent()`
    transaction, which adds a new respondent profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Like Peewee, Piccolo’s model classes can persist records, as shown by `insert_respondent()`,
    which implements an asynchronous `INSERT` transaction. On the other hand, `get_all_respondent()`
    retrieves all respondent profiles and has the same approach as Peewee, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The remaining Peewee-like `DELETE` and `UPDATE` respondent transactions are
    created in the project’s `/survey/repository/respondent.py` module.
  prefs: []
  type: TYPE_NORMAL
- en: The Beanie ODM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second version of the PCCS project, `ch10-mongo`, utilizes a MongoDB datastore
    and uses the Beanie ODM to implement its asynchronous CRUD transactions. We covered
    Beanie in [*Chapter 6*](B17975_06.xhtml#_idTextAnchor155)*, Using a Non-Relational
    Database*. Now, let us learn how to apply FastAPI in symbolic computations. We
    will be using the `ch10-piccolo` project for this.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing symbolic computations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`sympy` module using the `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Let us now start creating our first symbolic expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Creating symbolic expressions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One way of implementing the FastAPI endpoint that performs symbolic computation
    is to create a service that accepts a mathematical model or equation as a string
    and converts that string into a `sympy` symbolic expression. The following `substitute_eqn()`
    processes an equation in `str` format and converts it into valid linear or nonlinear
    bivariate equations with the `x` and `y` variables. It also accepts values for
    `x` and `y` to derive the solution of the expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Before converting a string equation into a `sympy` expression, we need to define
    the `x` and `y` variables as `Symbols` objects using the `symbols()` utility.
    This method accepts a string of comma-delimited variable names and returns a tuple
    of symbols equivalent to the variables. After creating all the needed `Symbols()`
    objects, we can convert our equation into `sympy` expressions by using any of
    the following `sympy` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sympify()`: This uses `eval()` to convert the string equation into a valid
    `sympy` expression with all Python types converted into their `sympy` equivalents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parse_expr()`: A full-fledged expression parser that transforms and modifies
    the tokens of the expression and converts them into their `sympy` equivalents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the `substitute_bivar_eqn()` service utilizes the `sympify()` method,
    the string expression needs to be sanitized from unwanted code before sympifying
    to avoid any compromise.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the `sympy` expression object has a `subs()` method to substitute
    values to derive the solution. Its resulting object must be converted into `str`
    format for `Response` to render the data. Otherwise, `Response` will raise `ValueError`,
    regarding the result as non-iterable.
  prefs: []
  type: TYPE_NORMAL
- en: Solving linear expressions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `sympy` module allows you to implement services that solve multivariate
    systems of linear equations. The following API service highlights an implementation
    that accepts two bivariate linear models in string format with their respective
    solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `solve_linear_bivar_eqns()` service accepts two bivariate linear equations
    and their respective outputs (or intercepts) and aims to establish a system of
    linear equations. First, it registers the `x` and `y` variables as `sympy` objects
    and then uses the `parser_expr()` method to transform the string expressions into
    their `sympy` equivalents. Afterward, the service needs to establish linear equality
    of these equations using the `Eq()` solver, which maps each `sympy` expression
    to its solution. Then, the API service passes all these linear equations to the
    `solve()` method to derive the `x` and `y` values. The result of `solve()` also
    needs to be rendered as a string, like in the substitution.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from the `solve()` method, the API also uses the `Poly()` utility to create
    a polynomial object from an expression to be able to access essential properties
    of an equation, such as `is_linear()`.
  prefs: []
  type: TYPE_NORMAL
- en: Solving non-linear expressions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous `solve_linear_bivar_eqns()` can be reused to solve non-linear
    systems. The tweak is to shift the validation from filtering the linear equations
    to any non-linear equations. The following script highlights this code change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Solving linear and non-linear inequalities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `sympy` module supports solving solutions for both linear and non-linear
    inequalities but on univariate equations only. The following is an API service
    that accepts a univariate string expression with its output or intercepts, and
    extracts the solution using the `solve()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `sympy` module has `Gt()` or `StrictGreaterThan`, `Lt()` or `StrictLessThan`,
    `Ge()` or `GreaterThan`, and `Le()` or `LessThan` solvers, which we can use to
    create inequality. But first, we need to convert the `str` expression into a `Symbols()`
    object using the `parser_expr()` method before passing them to these solvers.
    The preceding service uses the `GreaterThan` solver, which creates an equation
    where the left-hand side of the expression is generally larger than the left.
  prefs: []
  type: TYPE_NORMAL
- en: Most applications designed and developed for mathematical modeling and data
    science use `sympy` to create complex mathematical models symbolically, plot data
    directly from the `sympy` equation, or generate results based on datasets or live
    data. Now, let us proceed to the next group of API services, which deals with
    data analysis and manipulation using `numpy`, `scipy`, and `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating arrays and DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When numerical algorithms require some arrays to store data, a module called
    **NumPy**, short for **Numerical Python**, is a good resource for utility functions,
    objects, and classes that are used to create, transform, and manipulate arrays.
  prefs: []
  type: TYPE_NORMAL
- en: The module is best known for its n-dimensionalarrays or ndarrays, which consume
    less memory storage than the typical Python lists. An `ndarray` incurs less overhead
    when performing data manipulation than executing the list operations in totality.
    Moreover, `ndarray` is strictly heterogeneous, unlike Python’s list collections.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before we start our NumPy-FastAPI service implementation, we need to install
    the `numpy` module using the `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Our first API service will process some survey data and return it in `ndarray`
    form. The following `get_respondent_answers()` API retrieves a list of survey
    data from PostgreSQL through Piccolo and transforms the list of data into an `ndarray`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the size of the data retrieved, it would be faster if we apply
    the `ujson` or `orjson` serializers and de-serializers to convert `ndarray` into
    JSON data. Even though `numpy` has data types such as `uint`, `single`, `double`,
    `short`, `byte`, and `long`, JSON serializers can still manage to convert them
    into their standard Python equivalents. Our given API sample prefers `ujson` utilities
    to convert the array into a JSON-able response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from NumPy, `pandas` is another popular module that’s used in data analysis,
    manipulation, transformation, and retrieval. But to use pandas, we need to install
    NumPy, followed by the `pandas`, `matplotlib`, and `openpxyl` modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Let us now discuss about the ndarray in numpy module.
  prefs: []
  type: TYPE_NORMAL
- en: Applying NumPy’s linear system operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data manipulation in an `ndarray` is easier and faster, unlike in a list collection,
    which requires list comprehension and loops. The vectors and matrices created
    by `numpy` have operations to manipulate their items, such as scalar multiplication,
    matrix multiplication, transposition, vectorization, and reshaping. The following
    API service shows how the product between a scalar gradient and an array of survey
    data is derived using the `numpy` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the previous scripts, all `ndarray` instances resulting from any
    `numpy` operations can be serialized as JSON-able components using various JSON
    serializers. There are other linear algebraic operations that `numpy` can implement
    without sacrificing the performance of the microservice application. Let us take
    a look now on panda's DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the pandas module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this module, datasets are created as a `DataFrame` object, similar to in
    Julia and R. It contains rows and columns of data. FastAPI can render these DataFrames
    using any JSON serializers. The following API service retrieves all survey results
    from all survey locations and creates a DataFrame from these datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `DataFrame` object has a `to_json()` utility method, which returns a JSON
    object with an option to format the resulting JSON according to the desired type.
    On another note, `pandas` can also generate time series, a one-dimensional array
    depicting a column of a DataFrame. Both DataFrames and time series have built-in
    methods that are useful for adding, removing, updating, and saving the datasets
    to CSV and XLSX files. But before we discuss pandas’ data transformation processes,
    let us look at another module that works with `numpy` in many statistical computations,
    differentiation, integration, and linear optimizations: the `scipy` module.'
  prefs: []
  type: TYPE_NORMAL
- en: Performing statistical analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `scipy` module uses `numpy` as its base module, which is why installing
    `scipy` requires `numpy` to be installed first. We can use the `pip` command to
    install the module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Our application uses the module to derive the declarative statistics of the
    survey data. The following `get_respondent_answers_stats()` API service computes
    the mean, variance, skewness, and kurtosis of the dataset using the `describe()`
    method from `scipy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `describe()` method returns a `DescribeResult` object, which contains all
    the computed results. To render all the statistics as part of `Response`, we can
    invoke the `as_dict()` method of the `DescribeResult` object and serialize it
    using the JSON serializer.
  prefs: []
  type: TYPE_NORMAL
- en: Our API sample also uses additional utilities such as the `chain()` method from
    `itertools` to flatten the list of data and a custom converter, `ConvertPythonInt`,
    to convert NumPy’s `int32` types into Python `int` types. Now, let us explore
    how to save data to CSV and XLSX files using the `pandas` module.
  prefs: []
  type: TYPE_NORMAL
- en: Generating CSV and XLSX reports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `DataFrame` object has built-in `to_csv()` and `to_excel()` methods that
    save its data in CSV or XLSX files, respectively. But the main goal is to create
    an API service that will return these files as responses. The following implementation
    shows how a FastAPI service can return a CSV file containing a list of respondent
    profiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We need to create a `dict()` containing columns of data from the repository
    to create a `DataFrame` object. From the given script, we store each data column
    in a separate `list()`, add all the lists in `dict()` with keys as column header
    names, and pass `dict()` as a parameter to the constructor of `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: After creating the `DataFrame` object, invoke the `to_csv()` method to convert
    its columnar dataset into a text stream, `io.StringIO`, which supports Unicode
    characters. Finally, we must render the `StringIO` object through FastAPI’s `StreamResponse`
    with the `Content-Disposition` header set to rename the default filename of the
    CSV object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using the pandas `ExcelWriter`, our Online Survey application opted
    for another way of saving `DataFrame` through the `xlsxwriter` module. This module
    has a `Workbook` class, which creates a workbook containing worksheets where we
    can plot all column data per row. The following API service uses this module to
    render XLSX content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The given `create_respondent_report_xlsx()` service retrieves all the respondent
    records from the database and plots each profile record per row in the worksheet
    from the newly created `Workbook`. Instead of writing to a file, `Workbook` will
    store its content in a byte stream, `io.ByteIO`, which will be rendered by `StreamResponse`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pandas` module can also help FastAPI services read CSV and XLSX files
    for rendition or data analysis. It has a `read_csv()` that reads data from a CSV
    file and converts it into JSON content. The `io.StringIO` stream object will contain
    the full content, including its Unicode characters. The following service retrieves
    the content of a valid CSV file and returns JSON data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two ways to handle `multipart` file uploads in FastAPI:'
  prefs: []
  type: TYPE_NORMAL
- en: Use `bytes` to contain the file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `UploadFile` to wrap the file object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B17975_09.xhtml#_idTextAnchor266)*, Utilizing Other Advanced
    Features*, introduced the `UploadFile` class for capturing uploaded files because
    it supports more Pydantic features and has built-in operations that can work with
    coroutines. It can handle large file uploads without raising an change to - exception
    when the uploading process reaches the memory limit, unlike using the `bytes`
    type for file content storage. Thus, the given `read-csv()` service uses `UploadFile`
    to capture any CSV files for data analysis with `orjson` as its JSON serializer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to handle file upload transactions is through Jinja2 form templates.
    We can use `TemplateResponse` to pursue file uploading and render the file content
    using the Jinja2 templating language. The following service reads a CSV file using
    `read_csv()` and serializes it into HTML table-formatted content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Aside from `to_json()` and `to_html()`, the `TextFileReader` object also has
    other converters that can help FastAPI render various content types, including
    `to_latex()`, `to_excel()`, `to_hdf()`, `to_dict()`, `to_pickle()`, and `to_xarray()`.
    Moreover, the `pandas` module has a `read_excel()` that can read XLSX content
    and convert it into any rendition type, just like its `read_csv()` counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us explore how FastAPI services can plot charts and graphs and output
    their graphical result through `Response`.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting data models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the help of the `numpy` and `pandas` modules, FastAPI services can generate
    and render different types of graphs and charts using the `matplotlib` utilities.
    Like in the previous discussions, we will utilize an `io.ByteIO` stream and `StreamResponse`
    to generate graphical results for the API endpoints. The following API service
    retrieves survey data from the repository, computes the mean for each data strata,
    and returns a line graph of the data in PNG format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `plot_answers_mean()` service utilizes the `plot()` method of the `matplotlib`
    module to plot the app’s mean survey results per location on a linegraph. Instead
    of saving the file to the filesystem, the service stores the image in the `io.ByteIO`
    stream using the module’s `savefig()` method. The stream is rendered using `StreamResponse`,
    like in the previous samples. The following figure shows the rendered stream image
    in PNG format through `StreamResponse`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Line graph from StreamResponse'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.03_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Line graph from StreamResponse
  prefs: []
  type: TYPE_NORMAL
- en: 'The other API services of our app, such as `plot_sparse_data()`, create a bar
    chart image in JPEG format of some simulated or derived data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The approach is the same as our line graph rendition. With the same strategy,
    the following service creates a pie chart that shows the percentage of male and
    female respondents that were surveyed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The responses generated by the `plot_sparse_data()` and `plot_pie_gender()`
    services are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – The bar and pie charts generated by StreamResponse'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.04_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – The bar and pie charts generated by StreamResponse
  prefs: []
  type: TYPE_NORMAL
- en: This section will introduce an approach to creating API endpoints that produce
    graphical results using `matplotlib`. But there are other descriptive, complex,
    and stunning graphs and charts that you can create in less time using `numpy`,
    `pandas`, `matplotlib`, and the FastAPI framework. These extensions can even solve
    complex mathematical and data science-related problems, given the right hardware
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us shift our focus to the other project, `ch10-mongo`, to tackle topics
    regarding workflows, GraphQL, and Neo4j graph database transactions and how FastAPI
    can utilize them.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating a BPMN workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the FastAPI framework has no built-in utilities to support its workflows,
    it is flexible and fluid enough to be integrated into other workflow tools such
    as Camunda and Apache Airflow through extension modules, middleware, and other
    customizations. But this section will only focus on the raw solution of simulating
    BPMN workflows using Celery, which can be extended to a more flexible, real-time,
    and enterprise-grade approach such as Airflow integration.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the BPMN workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `ch10-mongo` project has implemented the following BPMN workflow design
    using Celery:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A sequence of service tasks that derives the percentage of the survey data
    result, as shown in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Percentage computation workflow design'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.05_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – Percentage computation workflow design
  prefs: []
  type: TYPE_NORMAL
- en: 'A group of batch operations that saves data to CSV and XLSX files, as shown
    in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Data archiving workflow design'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.06_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 – Data archiving workflow design
  prefs: []
  type: TYPE_NORMAL
- en: 'A group of chained tasks that operates on each location''s data independently,
    as shown in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Workflow design for stratified survey data analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.07_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.7 – Workflow design for stratified survey data analysis
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to implement the given design, but the most immediate solution
    is to utilize the Celery setup that we used in [*Chapter 7*](B17975_07.xhtml#_idTextAnchor190)*,
    Securing the REST APIs*.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Celery’s `chain()` method implements a workflow of linked task executions,
    as depicted in *Figure 10.5*, where every parent task returns the result to the
    first parameter of next task. The chained workflow works if each task runs successfully
    without encountering any exceptions at runtime. The following is the API service
    in `/api/survey_workflow.py` that implements the chained workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`compute_sum_results()`, `compute_avg_results()`, and `derive_percentile()`
    are bound tasks. Bound tasks are Celery tasks that are implemented to have the
    first method parameter allocated to the task instance itself, thus the `self`
    keyword appearing in its parameter list. Their task implementation always has
    the `@celery.task(bind=True)` decorator. The Celery task manager prefers bound
    tasks when applying workflow primitive signatures to create workflows. The following
    code shows the bound tasks that are used in the chained workflow design:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`compute_sum_results()` computes the total survey result per state, while `compute_avg_results()`consumes
    the sum computed by `compute_sum_results()` to derive the mean value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, `derive_percentile()` consumes the mean values produced
    by `compute_avg_results()` to return a percentage value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The given `derive_percentile()` consumes the mean values produced by `compute_avg_results()`
    to return a percentage value.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement the gateway approach, Celery has a `group()` primitive signature,
    which is used to implement parallel task executions, as depicted in *Figure 10.6*.
    The following API shows the implementation of the workflow structure with parallel
    executions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The workflow shown in *Figure 10.7* depicts a mix of grouped and chained workflows.
    It is common for many real-world microservice applications to solve workflow-related
    problems with a mixture of different Celery signatures, including `chord()`, `map()`,
    and `starmap()`. The following script implements a workflow with mixed signatures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The Celery signature plays an essential role in building workflows. A `signature()`
    method or `s()` that appears in the construct manages the execution of the task,
    which includes accepting the initial task parameter value(s) and utilizing the
    queues that the Celery worker uses to load tasks. As discussed in [*Chapter 7*](B17975_07.xhtml#_idTextAnchor190)*,
    Securing the REST APIs*, `apply_async()` triggers the whole workflow execution
    and retrieves the result.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from workflows, the FastAPI framework can also use the GraphQL platform
    to build CRUD transactions, especially when dealing with a large amount of data
    in a microservice architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Using GraphQL queries and mutations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GraphQL is an API standard that implements REST and CRUD transactions at the
    same time. It is a high-performing platform that’s used in building REST API endpoints
    that only need a few steps to set up. Its objective is to create endpoints for
    data manipulation and query transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the GraphQL platform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python extensions such as Strawberry, Ariadne, Tartiflette, and Graphene support
    GraphQL-FastAPI integration. This chapter introduces the use of the new Ariadne
    3.x to build CRUD transactions for this `ch10-mongo` project with MongoDB as the
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install the latest `graphene` extension using the `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Among the GraphQL libraries, Graphene is the easiest to set up, with fewer decorators
    and methods to override. It easily integrates with the FastAPI framework without
    requiring additional middleware and too much auto-wiring.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the record insertion, update, and deletion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data manipulation operations are always part of GraphQL’s mutation mechanism.
    This is a GraphQL feature that modifies the server-side state of the application
    and returns arbitrary data as a sign of a successful change in the state. The
    following is an implementation of a GraphQL mutation that inserts, deletes, and
    updates records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`CreateLoginData` is a mutation that adds a new login record to the data store.
    The inner class, `Arguments`, indicates the record fields that will comprise the
    new login record for insertion. These arguments must appear in the overridden
    `mutate()` method to capture the values of these fields. This method will also
    call the ORM, which will persist the newly created record.'
  prefs: []
  type: TYPE_NORMAL
- en: After a successful insert transaction, `mutate()` must return the class variables
    defined inside a mutation class such as `ok` and the `loginData` object. These
    returned values must be part of the mutation instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Updating a login attribute has a similar implementation to `CreateLoginData`
    except the arguments need to be exposed. The following is a mutation class that
    updates the `password` field of a login record that’s been retrieved using its
    `username`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the delete mutation class retrieves a record through an `id` and
    deletes it from the data store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can store all our mutation classes in an `ObjectType` class that exposes
    these transactions to the client. We assign field names to each `Field` instance
    of the given mutation classes. These field names will serve as the query names
    of the transactions. The following code shows the `ObjectType` class, which defines
    our `CreateLoginData`, `ChangeLoginPassword`, and `DeleteLoginData` mutations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Implementing the query transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GraphQL query transactions are implementations of the `ObjectType` base class.
    Here, `LoginQuery` retrieves all login records from the data store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The class must have a query field name, such as `get_login`, that will serve
    as its query name during query execution. The field name must be part of the `resolve_*()`
    method name for it to be registered under the `ObjectType` class. A class variable,
    such as `login_list`, must be declared for it to contain all the retrieved records.
  prefs: []
  type: TYPE_NORMAL
- en: Running the CRUD transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need a GraphQL schema to integrate the GraphQL components and register the
    mutation and query classes for the FastAPI framework before running the GraphQL
    transactions. The following script shows the instantiation of GraphQL’s `Schema`
    class with `LoginQuery` and `LoginMutations`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We set the `auto_camelcase` property of the `Schema` instance to `False` to
    maintain the use of the original field names with an underscore and avoid the
    camel case notation approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, we use the schema instance to create the `GraphQLApp()` instance.
    GraphQLApp is equivalent to an application that needs mounting to the FastAPI
    framework. We can use the `mount()` utility of FastAPI to integrate the `GraphQLApp()`
    instance with its URL pattern and the chosen GraphQL browser tool to run the API
    transactions. The following code shows how to integrate the GraphQL applications
    with Playground as the browser tool to run the APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the left-hand side panel to insert a new record through a JSON script
    containing the field name of the `CreateLoginData` mutation, which is `create_login`,
    along with passing the necessary record data, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Running the create_login mutation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.08_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 – Running the create_login mutation
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform query transactions, we must create a JSON script with a field name
    of `LoginQuery`, which is `get_login`, together with the record fields needed
    to be retrieved. The following screenshot shows how to run the `LoginQuery` transaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Running the get_login query transaction'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.09_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.9 – Running the get_login query transaction
  prefs: []
  type: TYPE_NORMAL
- en: GraphQL can help consolidate all the CRUD transactions from different microservices
    with easy setup and configuration. It can serve as an API Gateway where all GraphQLApps
    from multiple microservices are mounted to create a single façade application.
    Now, let us integrate FastAPI into a graph database.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the Neo4j graph database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For an application that requires storage that emphasizes relationships among
    data records, a graph database is an appropriate storage method to use. One of
    the platforms that use graph databases is Neo4j. FastAPI can easily integrate
    with Neo4j, but we need to install the `Neo4j` module using the `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Neo4j is a NoSQL database with a flexible and powerful data model that can manage
    and connect different enterprise-related data based on related attributes. It
    has a semi-structured database architecture with simple ACID properties and a
    non-JOIN policy that make its operations fast and easy to execute.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: ACID, which stands for atomicity, consistency, isolation, and durability, describes
    a database transaction as a group of operations that performs as a single unit
    with correctness and consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the Neo4j database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `neo4j` module includes `neo4j-driver`, which is needed to establish a
    connection with the graph database. It needs a URI that contains the `bolt` protocol,
    server address, and port. The default database port to use is `7687`. The following
    script shows how to create Neo4j database connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Creating the CRUD transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Neo4j has a declarative graph query language called Cypher that allows CRUD
    transactions of the graph database. These Cypher scripts need to be encoded as
    `str` SQL commands to be executed by its query runner. The following API service
    adds a new database record to the graph database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '`create_survey_loc()` adds new survey location details to the Neo4j database.
    A record is considered a node in the graph database with a name and attributes
    equivalent to the record fields in the relational databases. We use the connection
    object to create a session that has a `run()` method to execute Cypher scripts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The command to add a new node is `CREATE`, while the syntax to update, delete,
    and retrieve nodes can be added with the `MATCH` command. The following `update_node_loc()`
    service searches for a particular node based on the node’s name and performs the
    `SET` command to update the given fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, the delete transaction uses the `MATCH` command to search for the
    node to be deleted. The following service implements `Location` node deletion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'When retrieving nodes, the following service retrieves all the nodes from the
    database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The following service only retrieves a single node based on the node’s `id`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Our implementation will not be complete if we have no API endpoint that will
    link nodes based on attributes. Nodes are linked to each other based on relationship
    names and attributes that are updatable and removable. The following API endpoint
    creates a node relationship between the `Location` nodes and `Respondent` nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The FastAPI framework can easily integrate into any database platform. The previous
    chapters have proven that FastAPI can deal with relational database transactions
    with ORM and document-based NoSQL transactions with ODM, while this chapter has
    proven the same for the Neo4j graph database due to its easy configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the scientific side of FastAPI by showing that API services
    can provide numerical computation, symbolic formulation, and graphical interpretation
    of data via the `numpy`, `pandas`, `sympy`, and `matplotlib` modules. This chapter
    also helped us understand how far we can integrate FastAPI with new technology
    and design strategies to provide new ideas for the microservice architecture,
    such as using GraphQL to manage CRUD transactions and Neo4j for real-time and
    node-based data management. We also introduced the basic approach that FastAPI
    can apply to solve various BPMN workflows using Celery tasks. With this, we have
    started to understand the power and flexibility of the framework in building microservice
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will cover the last set of topics to complete our deep dive
    into FastAPI. We will cover some deployment strategies, Django and Flask integrations,
    and other microservice design patterns that haven’t been discussed in the previous
    chapters.
  prefs: []
  type: TYPE_NORMAL
