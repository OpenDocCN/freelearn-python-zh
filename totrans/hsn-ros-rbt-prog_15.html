<html><head></head><body>
        

                            
                    <h1 class="header-title">Machine Learning with OpenAI Gym</h1>
                
            
            
                
<p>In the previous chapter, we introduced you to the usage of deep learning in order to recognize objects based on a real-time image feed coming from the Raspberry Pi camera. Hence, this provides the robot the ability to take smart actions related to the recognized object. For example, if the object is a ball, the robot could collect it and put it apart so that nobody has an accident by stepping on the ball.</p>
<p>In this chapter, you will be introduced to <strong>reinforcement learning</strong> (<strong>RL</strong>), a field of machine learning that, nowadays, is a very active topic of research, having achieved the success of surpassing human performance in some scenarios, as shown in the recent case of the AlphaGo game (<a href="https://deepmind.com/blog/article/alphago-zero-starting-scratch">https://deepmind.com/blog/article/alphago-zero-starting-scratch</a>).</p>
<p>You will learn in a practical manner using the Python framework <strong>OpenAI Gym</strong>, which is a toolkit for developing and comparing RL algorithms. We will provide a conceptual approach to RL that will allow us to handle various problems within a programmatic way using Gym environments. In order to do this, we will differentiate between three main components: <em>scenarios</em>, <em>tasks</em>, and <em>agents</em>. Here, the scenario is the physical environment where the robot evolves, the task is the action(s) the robot is expected to learn, and the agent is the software program that makes the decisions for the action(s) to execute.</p>
<p>This segregation will allow you to decouple these components and reuse them in a different scope. For example, you could have trained an agent so that a two-wheeled drive robot, such as our GoPiGo3, would be able to carry a load from one point to a target location and use the same agent with a different robot, for example, a four-wheel drive such as Summit XL (<a href="https://www.robotnik.es/robots-moviles/summit-xl">https://www.robotnik.es/robots-moviles/summit-xl</a>). The code of the agent is the same because it abstracts the robot's concrete features.</p>
<p>Similarly, you could use different generated scenarios to test the same robot. This will show the ability of the trained agent to perform under different boundary conditions. With these ideas in mind, this chapter will teach you the basics of the OpenAI Gym API, how to integrate with an ROS environment, and how to follow the training process by representing the results graphically.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li style="font-weight: 400">An introduction to OpenAI Gym</li>
<li style="font-weight: 400">Running an environment</li>
<li style="font-weight: 400">Configuring an environment file</li>
<li style="font-weight: 400">Running the simulation and plotting the results</li>
</ul>
<p>In the first section, you will start using the base Gym API in its native Python environment. In the following ones, you will learn how to add the ROS wrappers to train robots in Gazebo. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>In this chapter, we will make use of the code in the <kbd>Chapter11_OpenAI_Gym</kbd> folder, located at <a href="https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter11_OpenAI_Gym">https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter11_OpenAI_Gym</a>. Copy the files of this chapter to the ROS workspace, putting them inside the <kbd>src</kbd> folder:</p>
<pre><strong>$ cp -R ~/Hands-On-ROS-for-Robotics-Programming/Chapter11_OpenAI_Gym</strong> <strong>~/catkin_ws/src/</strong></pre>
<p class="mce-root">Next, you will need to install Anaconda (<a href="https://www.anaconda.com">https://www.anaconda.com</a>). This is the Python distribution that has become the <em>de facto</em> open source standard for the Data Science community. It provides a complete Python environment for machine learning projects.</p>
<p>Visit the download section of the Anaconda website at <a href="https://www.anaconda.com/distribution/#linux">https://www.anaconda.com/distribution/#linux</a>, and select the Python 2.7 bundle. We select this package because the ROS Python client is focused on this version; however, you should be aware that it recently came to the end of life in December 2019.</p>
<p>Open Robotics intends to create a new ROS distribution in May 2020 targeting Python 3: <a href="https://discourse.ros.org/t/planning-future-ros-1-distribution-s/6538">https://discourse.ros.org/t/planning-future-ros-1-distribution-s/6538</a>.</p>
<p>After downloading Anaconda, go to the download directory and enter the following command to execute the code for the installation:</p>
<pre class="highlight"><strong>$ bash Anaconda2-2019.10-Linux-x86_64.sh</strong></pre>
<p>The filename marked in bold letters should match the name of the one you have downloaded. If this is not the case, then run the <kbd>bash</kbd> command, replacing the filename with the actual one you have.</p>
<p>The <kbd>$ conda init</kbd> command is optionally executed from the installation script, and, if successful, it will provide the following output:</p>
<pre><strong>==&gt; For changes to take effect, close and re-open your current shell. &lt;==</strong><br/><strong>If you'd prefer that conda's base environment not be activated on startup, </strong><br/><strong> set the auto_activate_base parameter to false:</strong><br/><br/><strong> $ conda config --set auto_activate_base false</strong></pre>
<p>Conda is the package manager that ships with Anaconda. It allows you to easily install, remove, list, and inspect the Python packages in your Anaconda installation.</p>
<p>After installing Anaconda, you will see these lines added to your <kbd>~/.bashrc</kbd> file:</p>
<pre># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;<br/># !! Contents within this block are managed by 'conda init' !!<br/><strong>__conda_setup</strong>="$('<strong>/home/${USER}/anaconda2/bin/conda</strong>' 'shell.bash' 'hook' 2&gt; /dev/n$<br/>if [ $? -eq 0 ]; then<br/> eval "$<strong>__conda_setup</strong>"<br/>else<br/> if [ -f "<strong>/home/${USER}/anaconda2/etc/profile.d/conda.sh</strong>" ]; then<br/> . "/home/${USER}/anaconda2/etc/profile.d/conda.sh"<br/> else<br/> export PATH="/home/${USER}/anaconda2/bin:$PATH"<br/> fi<br/>fi<br/><strong>unset __conda_setup</strong><br/># &lt;&lt;&lt; conda initialize &lt;&lt;&lt;</pre>
<p>So that the added configuration takes effect, source it in the Terminal prompt:</p>
<div><div><pre class="highlight"><strong>$ source ~/.bashrc</strong></pre></div>
</div>
<p>The preceding <kbd>.bashrc</kbd> line should take you into the (base) default Anaconda environment. We recommend that you do not activate Conda's base environment on startup. To ensure this, set the <kbd>auto_activate_base</kbd> parameter to <kbd>false</kbd>:</p>
<pre class="mce-root"><strong>$ conda config --set auto_activate_base false</strong></pre>
<p class="mce-root">If you wish to restore the default configuration, you can revert it by changing the value to <kbd>true</kbd>. Finally, you have the option to manually activate the default Anaconda environment, on demand, in a Terminal with this command:</p>
<pre class="mce-root">$ conda activate</pre>
<p>In this base environment, you are able to install Jupyter notebooks. You can use them to view, run, and modify Python notebooks:</p>
<pre><strong>(<em>base</em>) $ jupyter notebook</strong></pre>
<p>Remember that this is the user-friendly Python runtime you had preinstalled with DexterOS. It was used in <a href="7a2b1b82-c666-42df-9f10-9777eabe82df.xhtml" target="_blank">Chapter 2</a>, <em>Unit Testing of GoPiGo3</em>, to run most of the examples. To deactivate the virtual environment, just run the following:</p>
<pre class="mce-root">(<strong><em>base</em>) $ conda deactivate</strong></pre>
<p>You can find a useful cheatsheet for Conda Package Manager, which you should have at hand, at the following URL: <a href="https://kapeli.com/cheat_sheets/Conda.docset/Contents/Resources/Documents/index">https://kapeli.com/cheat_sheets/Conda.docset/Contents/Resources/Documents/index</a>.</p>
<p>At this point, we are ready to proceed with OpenAI Gym and its installation; this will be explained in a dedicated section next.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">An introduction to OpenAI Gym</h1>
                
            
            
                
<p>In the previous chapter, we provided a practical overview of what you can expect in RL when applied to robotics. In this chapter, we will provide a general view in which you will discover how RL is used to train smart <em>agents</em>.</p>
<p>First, we will need to install OpenAI Gym and OpenAI ROS on our laptop in preparation for the practical examples. Then, we will explain its concepts.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing OpenAI Gym</h1>
                
            
            
                
<p>As we did in the previous chapter, we are going to create a virtual environment for the Python setup of this chapter, which we will call <kbd>gym</kbd>. The following two commands allow for the creation and then the activation of <kbd>gym</kbd>:</p>
<div><div><pre class="highlight"><strong>$ conda create -n gym pip python=2.7</strong><br/><strong>$ conda activate gym</strong></pre></div>
</div>
<p>Following this, install the specific Python packages that we are going to need for the examples:</p>
<ul>
<li>The Keras package (<a href="https://keras.io/">https://keras.io/</a>), which is a high-level neural network API that it used within OpenAI Gym. Remember that it was also used in the previous chapter, but we need to install it again because this is a new <kbd>gym</kbd> environment. Keras will let us train an agent using the <strong>DQN</strong> (short for <strong>Deep Q-Network</strong>) algorithm, which is deep learning-based.</li>
<li>You also need TensorFlow since it is used as the backend for Keras.</li>
<li>Finally, you will need the Gym package (<a href="https://github.com/openai/gym">https://github.com/openai/gym</a>), which is the implementation in Python of OpenAI Gym.</li>
</ul>
<p>You can install three of the packages in a row, as follows:</p>
<pre class="highlight"><strong>(gym) $ pip install tensorflow keras gym box2d-py</strong></pre>
<p>Now check the version of <kbd>gym</kbd>:</p>
<pre class="highlight"><strong>(gym) $ pip show gym</strong></pre>
<p>The output should be as follows:</p>
<pre><strong>Name: gym</strong><br/><strong>Version: 0.15.4</strong><br/><strong>Summary: The OpenAI Gym: A toolkit for developing and comparing your reinforcement learning agents.</strong><br/><strong>Home-page: https://github.com/openai/gym</strong><br/><strong>Author: OpenAI</strong><br/><strong>Author-email: gym@openai.com</strong><br/><strong>License: UNKNOWN</strong><br/><strong>Location: ~/anaconda2/envs/gym/lib/python2.7/site-packages</strong><br/><strong>Requires: pyglet, cloudpickle, six, scipy, numpy, opencv-python, enum34</strong></pre>
<p>In addition to this, install the Jupyter notebooks, since some of the Python examples are explained in this friendly format:</p>
<pre class="highlight"><strong>(gym) $ conda install jupyter</strong></pre>
<p>Finally, install an optional library called <kbd>pybox2d</kbd>. This is a 2D physics engine for games and simple simulations, used by some of the premade environments that ship with Gym:</p>
<pre><strong>(gym) $ conda install -c https://conda.anaconda.org/kne pybox2d</strong></pre>
<p>The technical requirements end here. The following subsections are optional and are intended to increase your background of different ways of managing Python and Anaconda. We will show you how to install OpenAI Gym from the source and host the package in your working folder, which is a typical way of using a Python package in development mode.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Without Anaconda (optional)</h1>
                
            
            
                
<p>If you do not want to use Anaconda but instead keep working in the Python environment that ships with Ubuntu, you can install an in-home user directory by adding the <kbd>--user</kbd> flag:</p>
<pre class="highlight"><strong>$ pip install --user tensorflow keras tflearn gym</strong><br/></pre>
<p>This places the necessary packages in the <kbd>~/.local/lib/python2.7/site-packages</kbd> folder. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing gym in development mode (optional)</h1>
                
            
            
                
<p>This allows you to have the source code of OpenAI Gym in your working directory, change the files of the package, and see their effects instantly, without needing to reinstall the <kbd>gym</kbd> module:</p>
<pre class="highlight"><strong>(gym) $ conda deactivate<br/>$ cd ~/catkin_ws<br/>$ git clone https://github.com/openai/gym<br/>$ cd gym<br/>$ pip install --user -e . </strong></pre>
<p>The <kbd>-e</kbd> option allows this kind of installation, and it is suitable to be used as developer mode. The <kbd>--user</kbd> option performs the installation locally to the user at the <kbd>~/.local/lib/python2.7/site-packages</kbd> location.</p>
<p>To keep the environment clean, remove the package installation, keeping only the <kbd>gym</kbd> Python package in the Gym virtual environment:</p>
<pre><strong>$ rm -rf gym.egg-info</strong><br/><strong>$ ls ~/.local/lib/python2.7/site-packages | grep gym | xargs rm</strong></pre>
<p>This snippet lets you know how to manually remove a Python package from the system.</p>
<p>To reproduce the examples in this chapter, we will be following the former approach, that is, installing Gym as a system package within the Conda environment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing OpenAI ROS</h1>
                
            
            
                
<p>So that the code can run inside ROS, you have to install OpenAI ROS, which is built on top of OpenAI Gym. Execute the following command to clone the contributed ROS package and start the setup for ROS:</p>
<pre><strong>$ cd ~/catkin_ws/src</strong><br/><strong>$ git clone https://bitbucket.org/theconstructcore/openai_ros.git</strong><br/><strong>$ cd ~/ros_ws</strong><br/><strong>$ catkin_make</strong><br/><strong>$ source devel/setup.bash</strong><br/><strong>$ rosdep install openai_ros</strong></pre>
<p>Be aware that we had to install the ROS package from the source because the compiled binary is not available in Ubuntu. In particular, it is worth noting that the <kbd>rosdep install openai_ros</kbd> command does the equivalent of the general <kbd>sudo apt install &lt;package&gt;</kbd> command of Ubuntu; that is, every time you install a new component, it automatically includes the required dependencies. Remember that, for an ROS package, the dependencies are declared in the <kbd>package.xml</kbd> file located in the root folder of its source code.</p>
<p>Once the installation of OpenAI Gym is complete, we can go on to explain its concepts.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Agents, artificial intelligence, and machine learning</h1>
                
            
            
                
<p>The concept of an agent comes from the artificial intelligence field and is used to designate anything that makes decisions. Well, that is, it is what a typical computer program does with the conditional instructions of the <kbd>if ... then ... else ...</kbd> type. Put simply, an <em>agent</em> is a program that can take more elaborated decisions rather than using pure conditionals. For example, consider a video game: when you play against a machine, your opponent observes your actions and decides what to do next to win the game. What powers the opponent's decisions is an agent. Generalizing this idea, an agent can be used to solve many kinds of problems; for example, when to stop and start a heater to keep a room warm at a set temperature point.</p>
<p>When instead of using analytical formulas—as in the case of using a <strong>PID</strong> controller (acronym of <em>Proportional–Integral–Derivative</em>) to address the problem of temperature regulation mentioned previously—you use empirical data to tell the <em>agent</em> what to do in <em>hundreds or thousands of particular situations</em> (the outside temperature, the room temperature, the number of people in the room, the time of the day, and so on), you are training it so that it is able to generalize and respond correctly when faced with a broad range of conditions. And this training process is what we call machine learning in general, and RL for the particular scope of these last two chapters of the book.</p>
<p>At this point, you should also be aware that an <em>agent</em> that uses machine learning can make good decisions when the input conditions are in the range for which it has been trained. You cannot expect good decisions if one or more relevant conditions are outside of the training range. Hence the importance of the preceding paragraph in providing the empirical data of <em>hundreds or thousands of particular situations</em> for the training process.</p>
<p>OpenAI Gym is a structured framework to train agents based on RL techniques. Once this agent is trained, it can be reused in similar problems to power the decision capability. To illustrate these concepts, we are going to use a simple mechanism, the cart pole, which is also known as the inverted pendulum.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The cart pole example</h1>
                
            
            
                
<p>This is the classical control problem of the inverted pendulum that experiments with an unstable equilibrium (take a look at the following diagram). By applying lateral forces, <em>F</em>, you may compensate its tendency to fall down and get it to stay up (that is, when angle θ is close to zero):</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0a9bbe2f-0f25-4bbc-a1b5-56a0c9901cf1.png" style="width:22.58em;height:24.42em;"/></p>
<p>Source: https://de.wikipedia.org/wiki/Datei:Cart-pendulum.svg</p>
<p>We are going to solve this problem with OpenAI Gym, and the approach consists of starting with an <em>agent</em> that has no knowledge of the physics of the problem, that is, it does not have any idea of what lateral forces to apply so that the cart pole stays up. Following a trial-and-error strategy, the agent will learn which force directions and values are adequate for each angle of the pendulum. It is a quick problem to solve because you have only one degree of freedom—the angle θ—and one independent variable—the force, <em>F</em>.</p>
<p>This example is included with the code provided in the book repository, and we will take it as the base to explain the common concepts of the OpenAI Gym framework: environments, observations, and spaces.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Environments</h1>
                
            
            
                
<p>An environment is a scenario that models a problem (such as keeping a cart pole standing up) with a minimal interface that an agent can interact with. You can see the cart pole environment in action by running this snippet of code:</p>
<pre><strong>$ cd ~/catkin_ws/src/Chapter12_OpenAI_Gym/cart-pole</strong><br/><strong>$ conda activate gym</strong><br/><strong>(<em>gym</em>) $ python cart-pole_env.py</strong></pre>
<p>You should see the cart pole moving and rotating randomly, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/2f8cc531-5306-4eb0-8198-51889cf73587.png" style="width:27.42em;height:19.50em;"/></p>
<p>The content of the script is quite simple:</p>
<div><pre>import <strong>gym</strong><br/><br/><strong>env</strong> = gym.make('<strong>CartPole-v0</strong>')<br/>env.<strong>reset</strong>()<br/><br/>for _ in range(1000):<br/>    env.<strong>render</strong>()<br/>    env.<strong>step</strong>(env.action_space.<strong>sample()</strong>)<br/><br/>env.close()</pre></div>
<p>After importing the <kbd>gym</kbd> module, we set up the <kbd>env</kbd> variable to the predefined <kbd>CartPole-v0</kbd> environment. Then, in the next line, the <kbd>.reset()</kbd> method is applied to <kbd>env</kbd> so that the environment is initialized.</p>
<p>The body of the script is the <kbd>for</kbd> loop, which we set to 1,000 iterations. In each of these iterations, the script does two things:</p>
<ul>
<li>It renders the state of the cart pole with <kbd>env.render()</kbd>.</li>
<li>It takes a random action to execute a step of the simulation, something that is done with the line <kbd>env.step(env.action_space.sample())</kbd>. The <kbd>.sample()</kbd> method provides a random force, <em>F</em>, to act on the base of the cart pole.</li>
</ul>
<p>By chaining several steps together and letting the system evolve, the agent completes an episode, the end of which is defined by one of three possibilities:</p>
<ul>
<li>The pole angle is greater than ±12°.</li>
<li>The position of the cart with respect to the center of the track is more than ±2.4 units.</li>
<li>The episode length exceeds 200 steps.</li>
</ul>
<p>This definition is part of the environment specification that can be found at <a href="https://github.com/openai/gym/wiki/CartPole-v0">https://github.com/openai/gym/wiki/CartPole-v0</a>. Let's now review the definitions of observations and spaces.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Spaces</h1>
                
            
            
                
<p>Spaces describe valid actions (<em>F</em> forces) and observations (cart pole angle θ). This concept of observations will be covered in detail in the next subsection. Every environment has two spaces attached:</p>
<ul>
<li><strong>Action space</strong>: This is characterized by a set of state variables under the <kbd>env.action_space</kbd> <em>object. </em>This space defines the possible actions an agent is allowed to take. For the case of the cart pole, there is only one variable: to apply a lateral force, <em>F</em>.</li>
<li><strong>Observation space</strong>: This describes the physical state of the agent, that is, the angular position of the cart pole, θ. Operationally, it is a set of state variables under the <kbd>env.observation_space</kbd> object. </li>
</ul>
<p>Let's now describe the concept of observations in order to get a full understanding of how they support the learning process.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Observations</h1>
                
            
            
                
<p>Given an environment, an observation consists of a set of values that define a given state of the environment, the angle θ. It is like taking a snapshot of a scene. The environment's step function, <kbd>env.step</kbd>, returns the following:</p>
<ul>
<li class="mce-root">The current state; that is to say, it sets the current value of the state variable, <strong>θ</strong>. Operationally, it is an object type variable called <kbd>observation</kbd>.</li>
</ul>
<ul>
<li>The reward that the agent has obtained from the last action, force <em>F</em>, as was previously mentioned when describing the action space. The reward is like points in a game—a quantitative value that accumulates all the rewards (points) obtained from the actions performed in the current episode from the beginning, that is, the current score in the game analogy. If the applied force contributes to getting the cart pole to stay up, the reward is positive; if not, a negative reward (or penalty) is given. This variable is of the float type and is called <kbd>reward</kbd>.</li>
<li>Whether the current episode has finished, with a Boolean variable called <kbd>done</kbd>. When it finishes, the <kbd>env.reset</kbd> function is called to restart the environment, getting ready for the next episode.</li>
<li>Diagnostic information under the form of a Python dictionary object called <kbd>info</kbd>.</li>
</ul>
<p>Hence, the agent will try to maximize its score, which is calculated as the cumulative sum of rewards it receives for every force it applies. This maximization algorithm will make it learn to keep the cart pole up.</p>
<p>The preceding explanations should allow you to understand how the scripts work. Now we will run a training session of the cart pole so that you are able to see how the <em>good</em> actions are positively rewarded, encouraging the agent to build an effective strategy.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running the full cart pole example</h1>
                
            
            
                
<p>The first script that we ran, <kbd>cart-pole_env.py</kbd>, was intended to show a sequence of 1,000 random steps. The new script will provide feedback for every action that is taken by giving rewards for good actions:</p>
<pre><strong>$ cd ~/catkin_ws/src/Chapter12_OpenAI_Gym/cart-pole</strong><br/><strong>$ conda activate</strong><br/><strong>(<em>gym</em>) $ python CartPole-v0.py</strong></pre>
<div><p>The iterative block in the script includes the following lines:</p>
<div><pre>env.<strong>render</strong>()<br/><br/><strong>action </strong>= agent.act(state)<br/><strong>next_state</strong>, reward, done, _ = env.<strong>step</strong>(action)<br/><br/><strong>score </strong>+= reward<br/><br/>next_state = np.reshape(next_state, (1, 4))<br/>agent.<strong>remember</strong>(state, action, reward, next_state, done)<br/><br/><strong>state</strong> = next_state</pre></div>
<p>Following the order of the lines, this is what is done in each step:</p>
</div>
<ol>
<li>Render the environment. This is the window that shows you what is happening.</li>
<li>Choose the next action. This is where the accumulated experience is used to decide what to do, taking into account the current state. </li>
<li>Run the new step; that is to say, perform the selected action and observe what happens. The observation returns the new state of the agent, <kbd>next_state</kbd>, the reward the agent obtains, and the Boolean variable, <kbd>done</kbd>, telling you if the episode has finished.</li>
<li>The reward is added to the <kbd>score</kbd> variable, which accumulates all the rewards obtained from the beginning of the episode.</li>
<li>Next, the set (state, action, and reward) is stored in memory—<kbd>agent.remember</kbd>—so that the agent can take advantage of their past experience, promoting the actions in a given state that gave them more rewards.</li>
<li>Finally, we update the current <kbd>state</kbd> variable with the output of <em>step 3</em>, which is the value of <kbd>next_state</kbd>.</li>
</ol>
<p>When the training finishes, a curve representing the evolution of the score as a function of the episode is depicted:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/9122e7ca-773d-4b4c-9158-0fe32700455b.png" style="width:25.83em;height:18.67em;"/></p>
<p>You can see how after 40 episodes the agent starts getting good scores of around 200. What this means is that the agent has learned to keep the pole in equilibrium by applying the force in the direction that prevents it from falling down. This simple example takes a few minutes to achieve the target of getting 200 points in every new episode, so let's quickly understand how RL works. </p>
<p>Be aware that the cart pole problem is not representative of real scenarios. In actual cases, the state is defined by a set of many variables, and the possible actions an agent may take are also many. Real RL problems are very CPU-intensive and they take thousands and even millions of episodes to get a reasonably good performance.</p>
<p>In the next section, we will describe a more realistic problem where there are 500 states and six possible actions. The goal of this second example is to understand the score maximization algorithm in its most basic version, that is, through the Q-learning algorithm.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Q-learning explained – the self-driving cab example</h1>
                
            
            
                
<p>The problem we are going to solve consists of a self-driving cab that has to pick up passengers and drop them off to the right location. It must do so as quickly as possible while respecting the traffic rules. The graphics are based on ASCII characters and we are going to use real images to explain the goal. We will also follow a time-ordered sequence of frames:</p>
<ul>
<li>The first sequence represents the cab—the yellow square—in the start position. It can move up, down, left, or right except when it finds a vertical bar; that motion is not allowed. There are four possible taxi stands where the cab can pick up or drop off a passenger. They are marked with the letters <strong>R</strong>, <strong>G</strong>, <strong>B</strong>, and <strong>Y</strong>. The blue letter (<strong>R</strong>, in this case) is the pick-up location, and the purple letter (<strong>G</strong>) is the destination of where to transport the passenger to:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="img/07e86d35-1770-41cf-9c05-029469eb4236.png" style="width:7.33em;height:8.08em;"/></p>
<ul>
<li>This second sequence in the next diagram shows a step of the simulation where the passenger is inside the cab. They were picked up at location <strong>R</strong> and are being transported to the destination. This state is visually recognizable because the square representing the taxi is filled in green (when it does not carry any passenger, the color remains yellow):</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="img/6676cc83-c73e-4394-b489-560cdbb91b22.png" style="width:6.33em;height:7.08em;"/></p>
<ul>
<li>The final sequence corresponds to the scenario in which the taxi leaves the passenger at their destination; this is represented by the letter <strong>G</strong>, in this case. When this happens, the cab color changes to yellow again:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="img/707db954-8f85-47b9-a85e-1453b7da5c5b.png" style="width:6.75em;height:7.17em;"/></p>
<p>The goal of this problem is to train an RL agent to learn to drive the taxi following the shortest path for every trip. The shortest path is operationally implemented by a rewards policy, which gives the agent a predefined reward for every action it takes depending on its utility. Hence, the RL agent will try to maximize its total reward by associating states with <em>useful</em> actions. In this way, it will gradually discover a transport policy that will let it minimize (in global average) the time from traveling between pick-up locations to drop-off stands. The rewards policy will be detailed later in this section.</p>
<p>The advantage of using a simple example like this to explain Q-learning is that it allows you to apply a <strong>model-free RL algorithm</strong>. This is because it does not include any mathematical model to predict which will be the next state of the system based on the current state. To understand what the difference would be if there was a model, let's take the example of a robot at position <em>x</em> moving at the speed of <em>v</em>. After a time, <em>t</em>, the new position is expected to be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/66864c44-4bcf-481c-86e2-3420294c55b2.png" style="width:10.25em;height:1.42em;"/></p>
<p>Given that the state of the robot is represented by its position, <em>y</em>, the next state, <em>y'</em>, shall be a function of the speed, <em>v</em>, that is, <em>f(v)</em>. If the applied speed is doubled—<em>2v</em>—the robot will reach a different state, <em>x''</em>, because it will travel double the distance for the same time step, <em>t</em>. In this case, the set of speed values constitutes the action space. Based on this prediction, the robot is able to anticipate what reward it will obtain before executing the action. On the other hand, for the model-free case, it is not possible to anticipate the reward. The only thing the robot can do is to execute the action and see what happens.</p>
<p>Having this perspective, you are aware of the didactic reason to explain Q-learning using a model-free RL algorithm. The agent simply learns to select the most rewarded action in every state—it does not need to make any prediction in advance. And after many attempts, it will learn an optimal transport strategy.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to run the code for the self-driving cab</h1>
                
            
            
                
<p>The code is a Python file located in the <kbd>Chapter12_OpenAI_Gym/taxi</kbd> folder of the repository. As for the cart pole, the program is written in Python and the filename is <kbd>Taxi-v3.ipynb</kbd>. The <kbd>.ipynb</kbd> extension is the known Jupyter notebook extension. We have chosen this way of coding in Python so that the example can be understood by just following the notebook, because you have the code and the explanations in one place. </p>
<p>Jupyter notebooks were introduced in <a href="7a2b1b82-c666-42df-9f10-9777eabe82df.xhtml" target="_blank">Chapter 2</a>, <em>Unit Testing of GoPiGo3</em>. There, we have covered the practical explanations of the sensor and actuator using Python code in the notebook environment.</p>
<p>We suggest that you open the notebook, read it from the beginning until the end, and then come back here to complete your comprehension of the topic. To do so, follow these instructions:</p>
<ol>
<li>Activate the <kbd>gym</kbd> environment:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ conda activate gym</strong></pre>
<ol start="2">
<li>Move to the location of the example:</li>
</ol>
<pre style="padding-left: 60px"><strong>(gym) $ cd ~/catkin_ws/src/Chapter12_OpenAI_Gym/taxi</strong></pre>
<ol start="3">
<li>Launch the notebook server. This command will open a file explorer in a window of your default browser:</li>
</ol>
<pre style="padding-left: 60px"><strong>(gym) $ jupyter notebook</strong></pre>
<ol start="4">
<li>Click on the <kbd>Taxi-v3.ipynb</kbd> file and another window of the browser will open showing the notebook content.</li>
</ol>
<p>After having read it, we are ready to return to the spaces (action and state) and reward concepts that we introduced in the previous section, covering them in detail for the current example.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reward table</h1>
                
            
            
                
<p>This table specifies what reward the agent gets for every action it takes. A well-designed policy incentivizes the most desired actions with greater rewards. For the case of the cab example, the reward table is as follows:</p>
<ul>
<li>The agent receives +20 points for a successful drop-off.</li>
<li>It loses 1 point for every time step. This way, we encourage it to solve the environment as quickly as possible: all the time it is on the road it is consuming resources, such as fuel, so this negative reward can be understood as the fuel expense.</li>
<li>It is given a 10-point penalty for every illegal action it performs (during the pick-up or drop-off actions).</li>
</ul>
<p>Next, we proceed to describe the action and state spaces that the cab has to comply with to evolve in the environment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Action space</h1>
                
            
            
                
<p>The action space consists of the possible actions the agent can perform. For the case of the taxi example, these are as follows:</p>
<ul>
<li>The four possible moves: move (<em>S</em>) south, (<em>N</em>) north, (<em>E</em>) east, or (<em>W</em>) west</li>
<li>Picking up a passenger (<em>P</em>)</li>
<li>Dropoff (<em>D</em>)</li>
</ul>
<p>Hence, there are six possible actions in total, and we will call them <strong>action variables</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">State space</h1>
                
            
            
                
<p>The state space is composed of all the possible combinations of values of the <strong>state variables</strong> that define our problem. For the case of the taxi example, these variables are as follows:</p>
<ul>
<li>The current position is defined on the basis of rows and column numbers. These account for 5 rows x 5 columns = 25 cells (positions).</li>
<li>Four destinations: Marked with <strong>R</strong> (red; the color it shows in the accompanying Jupyter notebook), <strong>B </strong>(blue), <strong>Y</strong><strong> </strong>(yellow) and <strong>G</strong> (green).</li>
<li>Five possible passenger locations with regard to the taxi:
<ul>
<li>Pickup/drop-off in any of the four locations</li>
<li>Plus one for the passenger inside in any of the remaining cells (+1)</li>
</ul>
</li>
</ul>
<p>Hence, we have a total of 25 x 4 x 5 = 500 possible states. The following represents one of them:</p>
<pre><strong>                    +---------+
                    |R: | : :G|
                    | : | : : |   
                    | : : : : |
                    | |o: | : |
                    |Y| : |B: |
                    +---------+</strong></pre>
<p>The movement of hitting a wall is known to the encoded environment thanks to the <kbd>|</kbd> character. If the wall is to be crossed, when the environment tries to update the state, the cab could not move and will remain in the same cell. This is accomplished by keeping the state unchanged. Otherwise, the <kbd>:</kbd> character lets the cab move to the new cell. Bear in mind that there is no additional penalty for hitting a wall, just the -1 of the time step.</p>
<p>If you introduce this new rule, the training should be somewhat faster since the agent will implicitly learn where there is a wall and will not insist on moving in that direction after hitting it several times.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Self-driving cab example using the RL algorithm</h1>
                
            
            
                
<p class="inner_cell">As stated previously, we will explain the learning process using Q-learning because of its simplicity and physical sense. Bear in mind that the Q-learning algorithm lets the agent keep track of its rewards to learn the best action for every single state:</p>
<ul>
<li>Each time an action is taken from a given state, a reward is obtained according to P.</li>
<li>The reward associated with each pair (state, action) creates a q-table that is a 500 x 6 matrix.</li>
</ul>
<p class="inner_cell">The q-value for a concrete pair state-action stands for the <em>quality</em> of that action in that state. Hence, for a completely trained model, we will have a 500 x 6 matrix, that is, 3,000 q-values:</p>
<ul>
<li>Every row represents a state.</li>
<li>The maximum q-value in each row lets the agent know what action is the best to take in that state.</li>
</ul>
<p>In the first step, q-values are arbitrary. Then, when the agent receives rewards as it interacts with the environment, the q-value for every pair (<strong>state, action</strong>) is updated according to the following equation:</p>
<pre>Q(state,action) ← (1−α)Q(state,action) + α(reward + γ maxQ(next state,all actions))</pre>
<p>The preceding equation is described as follows:</p>
<ul>
<li>
<p>α, or alpha, is the learning rate (0&lt;α≤1) and is applied to the new information the agent discovers, that is, the first part of the second term of the sum in the formula, <kbd>α * reward</kbd>.</p>
</li>
<li>
<p>γ, or gamma, is the discount factor (0≤γ≤1) and determines how much importance we want to give to future rewards. If this factor is 0, it makes the agent consider only the immediate reward, making it behave in a greedy manner. Hence, this parameter ponders the utility of future actions. It applies to the second part of the second term of the sum: <kbd>γ * maxQ[next_state, all actions]</kbd>.</p>
</li>
</ul>
<p>Finally, we should also consider the trade-off between <strong>exploration</strong> and <strong>exploitation</strong>. Let's explain what these concepts mean:</p>
<ul>
<li>Exploration refers to the behavior where the robot executes an action from a given state for the first time. This will let it discover whether that state-action pair has a high reward or not. Operationally, it consists of taking a random action.</li>
<li>On the other hand, exploitation refers to the behavior of executing an action from a given state that was the more rewarded in the past. Operationally, it consists of taking the action with the maximum q-value for that state.</li>
</ul>
<p>We need to balance these two behaviors, and, for that, we introduce the ϵ (epsilon) parameter, which represents the percentage of actions that should be of the exploration type. This prevents the agent from following a single route, which may not necessarily be the best.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Evaluating the agent</h1>
                
            
            
                
<p>The script runs 100,000 episodes in less than one minute. Then, we evaluate the agent with 100 episodes and obtain these average values:</p>
<ul>
<li>Steps per episode: 12</li>
<li>Penalties per episode: 0</li>
</ul>
<p>In front of the brute-force approach (which you can find in the Jupyter notebook), you obtain routes that vary from hundreds to thousands of steps and more than 1,000 penalties (remember that a penalty of 1 was given for each illegal action incurred).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hyperparameters and optimization</h1>
                
            
            
                
<p>So, how can you choose the values of alpha, gamma, and epsilon? Well, this strategy has to be based on both intuition and trial and error. In any case, the three of them should decrease over time as the agent learns the best actions:</p>
<ul>
<li class="mce-root">Decreasing the need for learning, alpha, as the agent knows more about the environment and may trust in the acquired experience</li>
<li class="mce-root">Also decreasing the discount factor, gamma, as an agent develops an end-to-end strategy and not only focuses on the immediate reward</li>
<li class="mce-root">And, finally, decreasing the exploitation rate, epsilon, because the exploration gains lose priority as the environment is well known</li>
</ul>
<p>At this point, you should be ready to enter OpenAI ROS to train agents that power robots in Gazebo.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running an environment</h1>
                
            
            
                
<p>The goal of the rest of the chapter is to apply what you have learned about RL in general problems to a specific domain such as robotics. To easily transfer that knowledge, we will reproduce the simple cart pole example, modeling it as a robot in Gazebo. The code samples are in the <kbd>cart-pole_ROS</kbd> folder of the code repository of this chapter. Move to that location on your laptop:</p>
<pre><strong>$ cd ~/catkin_ws/src/Chapter12_OpenAI_Gym/cart-pole_ROS</strong></pre>
<p>Inside, you will find two ROS packages, each one giving its name to the folder:</p>
<ul>
<li><kbd>cartpole_description</kbd> contains the Gazebo simulation framework for the cart pole using ROS. The structure of this package is very similar to the one described in <a href="74284adc-e0d7-4e40-a54b-e2e447b8e2fe.xhtml" target="_blank">Chapter 5</a>, <em>Simulating Robot Behavior with Gazebo</em>. Hence, it is not necessary to dive into its details.</li>
<li><kbd>cartpole_dqn</kbd> contains the OpenAI Gym environment for the preceding Gazebo simulation. This is where the RL algorithm is introduced, and we will focus on this in the coming paragraphs.</li>
</ul>
<p>The package is quite similar. Let's enter through the launch file, <kbd>start_training.launch</kbd>:</p>
<pre>&lt;launch&gt;<br/>    &lt;<strong>rosparam</strong> command="load" file="$(find cartpole_dqn)/config/<strong>cartpole_dqn_params.yaml</strong>" /&gt;<br/>    &lt;!-- Launch the training system --&gt;<br/>    &lt;<strong>node</strong> pkg="cartpole_dqn" name="cartpole_dqn" type="<strong>cartpole_dqn.py</strong>" output="screen"/&gt;<br/>&lt;/launch&gt;<br/></pre>
<p> The line with the <kbd>&lt;rosparam&gt;</kbd> tag is the one that loads the configuration of the training process. We will explain this in the next section. This file is <kbd>cartpole_dqn_params.yaml</kbd> and it is hosted inside the <kbd>config</kbd> folder. </p>
<p>The other line, tagged with <kbd>&lt;node&gt;</kbd>, launches the single ROS <kbd>cartpole_dqn</kbd> node that implements the training process for the cart pole under the Python script, <kbd>cartpole_dqn.py</kbd>. What this code performs is briefly described in the following ordered points:</p>
<ol>
<li>It creates the Gym environment for the cart pole.</li>
<li>It loads the configuration from the ROS parameter server (this point is detailed in the following subsection).</li>
<li>Then, it initializes the learning algorithm with the loaded parameters.</li>
<li>Finally, it loops over the predefined number of episodes, each one composed of a fixed number of steps (both values are also part of the configuration file).</li>
</ol>
<ol start="5">
<li>For every episode, it performs the following:
<ul>
<li>Initialize the environment and get the first state of the robot.</li>
<li>For every step in the current episode, the agent chooses an action to run that will be one of random versus best action selection (depending on the epsilon exploration parameter):</li>
</ul>
</li>
</ol>
<pre style="padding-left: 120px">observation, reward, done, info = env.step(action)</pre>
<p style="padding-left: 120px">This is the key line of the loop, and you can see, it is the same that is used for the cart pole in pure Python and the taxi examples in the preceding section. Hence, the output of the steps are as follows:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><kbd>observation</kbd>: The new state of the environment resulting from applying the action</li>
<li><kbd>reward</kbd>: The value that indicates how effective the action taken is</li>
<li><kbd>done</kbd>: The Boolean variable telling you if the goal has been achieved</li>
</ul>
</li>
</ul>
<ol start="6">
<li>Finally, we let the algorithm learn from the result by following two subsequent steps:
<ul>
<li>Remember the running step: <kbd>self.remember(state, action, reward, next_state, done)</kbd></li>
<li>Replay to optimize the action selection: <kbd>self.replay(self.batch_size)</kbd></li>
</ul>
</li>
</ol>
<p>In this case, the algorithm that we use is somewhat different from the Q-learning we described in the <em>self-driving</em> <em>cab</em> example. It is called DQN and makes use of deep learning to select the best action for a given state. This is the algorithm that is more extensively used in RL problems, and if you want to deepen its formulation, you can do so by following the last reference in the <em>Further reading</em> section at the end of the chapter. In brief, this is what it performs:</p>
<ul>
<li>The remember process in every step of an episode saves what it is running and acts as the memory of the agent.</li>
<li>Then, the replay process takes a mini-batch of the last steps of the episode and applies for the improvement of the neural network. Such a network provides the agent with the <em>best</em> action to be carried out at every given state.</li>
</ul>
<p>Conceptually, what the agent does is to use its memory from previous experiences to guess which might be the most convenient action to maximize its total reward in the episode.</p>
<p>In the remaining sections, we will focus on the specific training and evaluation inside ROS with Gazebo.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Configuring the environment file</h1>
                
            
            
                
<p>In <em>step 2</em> of the preceding algorithmic description of the <kbd>start_training.py</kbd> script, ROS parameters are loaded into the model. Their definitions come from this line of the <kbd>start_training.launch</kbd> file:</p>
<div><pre>&lt;<strong>rosparam</strong> command="load" file="$(find cartpole_dqn)/config/<strong>cartpole_dqn_params.yaml</strong>" /&gt;</pre></div>
<p>When executing this part, the parameters in the <kbd>cartpole_dqn_params.yaml</kbd> file are loaded into memory and are available to the <kbd>cartpole_dqn.py</kbd> script. The more relevant are the following:</p>
<div><pre><strong>alpha</strong> = rospy.get_param('/cartpole_v0/alpha')<br/><strong>gamma</strong> = rospy.get_param('/cartpole_v0/gamma')<br/><strong>epsilon</strong> = rospy.get_param('/cartpole_v0/epsilon')</pre></div>
<p><kbd>cartpole_v0</kbd> is the namespace that is declared before the definitions in the <kbd>yaml</kbd> file. The meaning of every parameter was covered in the <em>Self driving cab example using RL algorithm</em> subsection. Although the DQN algorithm is more sophisticated than Q-learning, the conceptual meaning of <em>alpha</em>, <em>gamma</em>, and <em>epsilon</em> is equivalent to both. You can remember them by reviewing the preceding Q-learning algorithm section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running the simulation and plotting the results</h1>
                
            
            
                
<p class="mce-root">To run this simulation scenario, we follow the standard approach of first launching a Gazebo environment—part of the <kbd>cartpole_description</kbd> package with the model of the robot—and, afterward, we will start the training process:</p>
<pre><strong>T1 $ roslaunch cartpole_description main.launch</strong></pre>
<p>The result in the Gazebo window should be similar to the following screenshot. Although this is a 3D environment, the model itself behaves like a 2D model, since the cart pole can only slide along the direction of the guide:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d13622f9-529f-4d6a-900b-41045b1d5660.png" style="width:52.17em;height:27.75em;"/></p>
<p>For the training process, we have the launch file in the other ROS package, that is, <kbd>cartpole_v0_training</kbd>:</p>
<div><pre><strong>T2 $ conda activate gym<br/>T2 $ (gym) roslaunch cartpole_dqn start_training.launch</strong></pre>
<p>Be aware that before running the launch file, you have to activate the <kbd>gym</kbd> Python environment, which is where you installed OpenAI Gym.</p>
<p>You will see a live plot of the evolution of the training process that shows, in real time, the reward obtained in each episode. After the training concludes, you should obtain a graph similar to this one:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1418 image-border" src="img/5549d733-7555-4df1-999f-36ba7da50417.png" style="width:26.83em;height:17.75em;"/></p>
</div>
<p>Bear in mind that for every tick of the cart pole, a reward of +1 is given. Hence, the graph also represents the total number of rewards per episode. To have a measurement of the convergence it is more useful to plot—for every episode—the average number of ticks (= reward) over the last 100 episodes. For example, for episode 1,000 this means to take the reward (number of ticks) of episodes 901 to 1,000 and calculate the average of these 100 values. This result is the one plotted for episode 1,000 in the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1419 image-border" src="img/e251153c-6d2b-4f07-a5ff-24721c0a5b48.png" style="width:27.08em;height:18.42em;"/></p>
<p>In fact, the criteria for the convergence is to obtain an average reward greater than 800 over the last 100 episodes. You may check that the curve experiments with a boosting after 2,600 episodes and quickly reaches the criteria.</p>
<p>In the second part of this section, we will present a friendly way to access the ROS console log to follow the training process in detail.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Checking your progress with the logger</h1>
                
            
            
                
<p>As you may have observed, the log output is huge and runs at a very high speed. ROS provides another <kbd>rqt_tool</kbd> to easily follow the log of the session. To access it, launch it from a Terminal:</p>
<pre><strong>$ rqt_console</strong></pre>
<p>This should display a window that is similar to the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/3e81757f-4a25-442e-a8b5-15193f94b142.png"/></p>
<p>In the two boxes below the message feed, you can exclude or include messages based on your own criteria. If you want to change the log level of the node, run the <kbd>rqt_logger_level</kbd> utility:</p>
<pre><strong>$ rosrun rqt_logger_level rqt_logger_level</strong></pre>
<p>The following screenshot shows the log level of the node:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1420 image-border" src="img/8a04bd37-53d1-4dba-ad5d-31d89896e698.png" style="width:33.33em;height:25.08em;"/></p>
<p>The <kbd>rqt_console</kbd> tool allows you to both follow the log in real time and save it to a file for offline analysis. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter provided you with the theoretical background that you need to apply RL to real robots. By dissecting the simple example of the cart pole, you should now understand what happens under the hood in classical RL tasks.</p>
<p>Additionally, by doing this first with the native OpenAI Gym framework in Python, and afterward, inside ROS, you should have acquired the basic skills to perform an RL process with a real robot, our GoPiGo3. This is what you will learn to do in the final chapter of the book.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li>How does an agent learn following the RL approach?</li>
</ol>
<p style="padding-left: 60px">A) Via the experience that it gets from the reward it receives each time it executes an action.<br/>
B) By randomly exploring the environment and discovering the best strategy by trial and error.<br/>C) Via a neural network that gives as output a q-value as a function of the state of the system.</p>
<ol start="2">
<li>Does an agent trained with RL have to make predictions of the expected outcome of an action?</li>
</ol>
<p style="padding-left: 60px">A) Yes; this is a characteristic called model-free RL.<br/>
B) Only if it does not take the model-free RL approach.<br/>
C) No; by definition, RL methods only need to be aware of rewards and penalties to ensure the learning process.</p>
<ol start="3">
<li>If you run the Q-learning algorithm with a learning rate, alpha, of 0.7, what does this mean from the point of view of the learning process?</li>
</ol>
<p style="padding-left: 60px">A) That you keep the top 30% of the pair state-actions that provide the higher rewards.<br/>B) That you keep the 30% of the values of all the elements of the Q-matrix, and take the remaining 70% from the result of the new action.<br/>
C) That you keep the top 70% of the acquired knowledge from every iteration step to the next one.</p>
<ol start="4">
<li>If you run the Q-learning algorithm with a discount factor, gamma, of 1, what does this mean from the point of view of the learning process?</li>
</ol>
<p style="padding-left: 60px">A) That the agent will only be interested in the immediate reward.<br/>
B) That the agent will only be interested in the goal of the task.<br/>C) That the agent will only be interested in achieving the goal once.</p>
<ol start="5">
<li>If you run the Q-learning algorithm with an exploration rate, epsilon, of 0.5, what does this mean from the point of view of the learning process? </li>
</ol>
<p style="padding-left: 60px">A) That the behavior of the agent will be similar to that of an agent that selects random actions.<br/>B) That the agent will choose a random action in 50% of the steps of the episode.<br/>
C) That the agent will choose random actions in 50% of all the episodes.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Further reading</h1>
                
            
            
                
<p>To delve deeper into the concepts explained in this chapter, you can refer to the following sources:</p>
<ul>
<li><em>Reinforcement Learning: An Introduction</em>, Sutto R., Barto A. (2018), The MIT Press, licensed under the Creative Commons Attribution-NonCommercial-NoDeriv 2.0 Generic License (<a href="http://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">http://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf</a>)</li>
<li><em>Machine Learning Projects</em>, Chapter: <em>Bias-Variance for Deep Reinforcement Learning: How To Build a Bot for Atari with OpenAI Gym</em> (<a href="https://assets.digitalocean.com/books/python/machine-learning-projects-python.pdf">https://assets.digitalocean.com/books/python/machine-learning-projects-python.pdf</a>)</li>
<li>Reinforcement learning with ROS and Gazebo (<a href="https://ai-mrkogao.github.io/reinforcement%20learning/ROSRL">https://ai-mrkogao.github.io/reinforcement%20learning/ROSRL</a>)</li>
<li>Testing different OpenAI RL algorithms with ROS And Gazebo (<a href="https://www.theconstructsim.com/testing-different-openai-rl-algorithms-with-ros-and-gazebo/">https://www.theconstructsim.com/testing-different-openai-rl-algorithms-with-ros-and-gazebo/</a>)</li>
<li><em>Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo</em>, Zamora I., González N., Maoral V., Hernández A. (2016), arXiv:1608.05742 [cs.RO]</li>
<li><em>Deep Q-Learning with Keras and Gym</em> (<a href="https://keon.github.io/deep-q-learning">https://keon.github.io/deep-q-learning</a>)</li>
</ul>


            

            
        
    </body></html>