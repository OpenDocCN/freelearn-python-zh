<html><head></head><body><div><p>&#13;&#13;
    <h1 class="chapterNumber">8</h1>&#13;&#13;
    <h1 id="_idParaDest-156" class="chapterTitle">Advanced Event-Driven Structures</h1>&#13;&#13;
    <p class="normal">As we saw in the previous chapter, event-driven architectures are quite flexible and capable of creating complex scenarios. In this chapter, we will see what are the possible event-driven structures that cover more advanced use cases and how to deal with their complexities.</p>&#13;&#13;
    <p class="normal">We will see how some common applications like logs and metrics can be thought of as event-driven systems and use them to generate control systems that will feedback into the system producing the events.</p>&#13;&#13;
    <p class="normal">We will also discuss, with an example, how to create complex pipelines where different events are being produced and the system is coordinated. We will also move to a more general overview, introducing the bus as a concept to interconnect all the event-driven components.</p>&#13;&#13;
    <p class="normal">We will introduce some general ideas on further complex systems to describe some of the challenges that these kinds of big event-driven systems can produce, such as the need to use CQRS techniques to retrieve information that crosses multiple modules. Finally, we will give some notes on how to test the system, paying attention to the different levels of tests.</p>&#13;&#13;
    <p class="normal">In this chapter, we'll cover the following topics:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Streaming events</li>&#13;&#13;
      <li class="bullet">Pipelines</li>&#13;&#13;
      <li class="bullet">Defining a bus</li>&#13;&#13;
      <li class="bullet">More complex systems</li>&#13;&#13;
      <li class="bullet">Testing event-driven systems</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">We will start by describing streams of events.</p>&#13;&#13;
    <h1 id="_idParaDest-157" class="title">Streaming events</h1>&#13;&#13;
    <p class="normal">For some purposes, it can be good to just produce events that<a id="_idIndexMarker530"/> capture information and store it for later access. This structure is typical for instrumentation, for example, where we create an event every time there's an error. This event will contain information about things such as where the error was generated, debugging details to be able to understand it, and so on. The event is then sent, and the application continues recovering from the error.</p>&#13;&#13;
    <p class="normal">The same can<a id="_idIndexMarker531"/> be done for specific parts of the code. For example, to capture an access time to a database, the timing and related data (like the specific query) can be captured and sent in an event.</p>&#13;&#13;
    <p class="normal">All those events should be compiled into a location to allow them to be queried and aggregated.</p>&#13;&#13;
    <p class="normal">While usually not thought of as event-driven processes, this is pretty much how logs and metrics work. In the case of logs, the events are generally text strings that get fired whenever the code decides to create them. The logs are forwarded to a destination that allows us to search them later. </p>&#13;&#13;
    <div>&#13;&#13;
      <p class="Information-Box--PACKT-">Logs can be stored in different formats. It's also common to create them in JSON to allow better searching.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">These kinds of events are simple but can be very powerful by allowing us to discover what the program is executing in a live system.</p>&#13;&#13;
    <p class="normal">This instrumentation may also be used to enable controls or alerts when certain conditions are matched. A typical example of this is to alert us if the number of errors captured by logs crosses a certain threshold.</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_08_01.png" alt="" width="826" height="326"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 8.1: Monitoring events flow</p>&#13;&#13;
    <p class="normal">This can also be used to produce feedback systems, where the instrumentation monitoring the system<a id="_idIndexMarker532"/> can be used to determine whether to change something in the system itself. For example, capturing metrics to determine whether the system needs to scale up or scale down and change the number of servers available based on the amount of requests or other parameters.</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_08_02.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="371"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 8.2: Feedback of scaling events</p>&#13;&#13;
    <p class="normal">This is not the only way a system can be monitored, though. This method of operation can also be used as a way<a id="_idIndexMarker533"/> of detecting quotas, for example, short-circuiting the processing of incoming requests if a certain quota has been exceeded. </p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_08_03.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="265"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 8.3: Monitor to detect quotas and stop extra requests </p>&#13;&#13;
    <p class="normal">This structure is different from the upfront approach of setting a module that controls the system, relying instead<a id="_idIndexMarker534"/> on acting only when the threshold is breached, making the calculations in the background. This can reduce the amount of processing required upfront.</p>&#13;&#13;
    <p class="normal">For example, for a quota of a maximum number of requests per minute, the process will be something like the following pseudocode:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def process_request(request):&#13;&#13;
    # Search for the owner of the request&#13;&#13;
    owner = request.owner&#13;&#13;
    info = retrieve_owner_info_from_db(owner)&#13;&#13;
    if check_quota_info(info):&#13;&#13;
        return process_request(request)&#13;&#13;
    else:&#13;&#13;
        return 'Quota exceeded'&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal"><code class="Code-In-Text--PACKT-">check_quota_info</code> will be different in both cases. The upfront approach requires maintaining and storing information about the previous requests:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def check_quota_info(info):&#13;&#13;
    current_minute = get_current_minute()&#13;&#13;
 if current_minute != info.minute:&#13;&#13;
     # New minute, start the quota&#13;&#13;
     info.requests = 0&#13;&#13;
     info.minute = current_minute&#13;&#13;
 else:&#13;&#13;
     info.requests += 1&#13;&#13;
 # Update the information&#13;&#13;
 info.save()&#13;&#13;
 if info.requests &gt; info.quota:&#13;&#13;
     # Quota exceeded&#13;&#13;
     return False&#13;&#13;
 # Quota still valid&#13;&#13;
 return False&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">If the validation is done in an external system, based on the events generated, <code class="Code-In-Text--PACKT-">check_quota_info</code> doesn't need<a id="_idIndexMarker535"/> to store the information, rather just checking whether the quota has been exceeded:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def check_quota_info(info): &#13;&#13;
    # Generate the proper event for a new event&#13;&#13;
    generate_event('request', info.owner)&#13;&#13;
 if info.quota_exceeded:&#13;&#13;
     return False&#13;&#13;
 # Quota still valid&#13;&#13;
 return False&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The whole check is performed in the backend monitoring system, based on the generated events, and then stored in the info. This detaches the logic for whether to apply the quota from the check itself, decreasing the latency. The counterpart is that the detection of the quota having been exceeded may be delayed, allowing some requests to be processed even if they shouldn't be according to the quota.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">Ideally, the generated events should already be in use to monitor the requests received. This operation can be very useful as it reuses events generated for other uses, reducing the need to collect extra data.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">At the same time, the check can be more complex and doesn't need to be done as each new request comes along. For example, for an hourly quota when multiple requests are received every second, perhaps<a id="_idIndexMarker536"/> a check every minute is good enough to ensure the quota is respected. This can save a big deal of processing power compared to checking the conditions every time a request is received.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">This, of course, is highly dependent on the specific scales, characteristics, and requests involved in different systems. For some systems, upfront could be a better choice, as it's easier to implement and doesn't require a monitoring system. Always validate whether the options fit into your system before implementing.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">We will talk in more detail specifically about logs and metrics in <em class="chapterRef">Chapter 12</em>, <em class="italic">Logging</em>, and <em class="chapterRef">Chapter 13</em>, <em class="italic">Metrics</em>.</p>&#13;&#13;
    <h1 id="_idParaDest-158" class="title">Pipelines</h1>&#13;&#13;
    <p class="normal">The flow of events doesn't have to be contained<a id="_idIndexMarker537"/> in a single system. The receiving end of the system can produce its own events, directed to other systems. Events will cascade into multiple systems, generating a process.</p>&#13;&#13;
    <p class="normal">This is a similar situation to the one presented previously, but in this case it's a more deliberate process aimed at creating specific data pipelines where the flow between systems is triggered and processed.</p>&#13;&#13;
    <p class="normal">A possible example of this is a system to rescale videos into different sizes and formats. When a video is uploaded into the system, it needs to be converted into multiple versions to be used in different situations. A thumbnail should also be created to display the first frame of the video before playing it. </p>&#13;&#13;
    <p class="normal">We will do this in three steps. First, a queue will receive the event to start the processing. This will trigger two events in two different queues to process the resize and the thumbnail generation independently. This will be our pipeline.</p>&#13;&#13;
    <p class="normal">To store the input and output data, given that they are videos and images, we require external storage. We will use AWS S3, or more precisely, a mock for S3.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">AWS S3 is an object storage service provided by Amazon in the cloud, very popular for being both easy to use and very stable. We will use a mock of S3 that will allow us to start a local service that behaves like S3, which will simplify our example.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Here is a high-level diagram of the system:</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_08_04.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="267"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 8.4: Video and image queue</p>&#13;&#13;
    <p class="normal">To get started, we need to upload<a id="_idIndexMarker538"/> the source video to the mock S3 and start the task. We will also require some way of checking the results. For that, two scripts will be available.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">The code is available on GitHub at <a href="https://github.com/PacktPublishing/Python-Architecture-Patterns/tree/main/chapter_08_advanced_event_driven">https://github.com/PacktPublishing/Python-Architecture-Patterns/tree/main/chapter_08_advanced_event_driven</a>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Let's start with the setup configuration.</p>&#13;&#13;
    <h2 id="_idParaDest-159" class="title">Preparation</h2>&#13;&#13;
    <p class="normal">As outlined above, we have two<a id="_idIndexMarker539"/> key prerequisites: a queue backend and the mock S3 storage.</p>&#13;&#13;
    <p class="normal">For the queue backend, we will use Redis again. Redis is very easy to configure for multiple queues, and we'll see how later. To start the Redis queue, we will again use Docker to download and run the official image:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ docker run -d -p 6379:6379 redis&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This starts a Redis container exposed on the standard port <code class="Code-In-Text--PACKT-">6379</code>. Note the <code class="Code-In-Text--PACKT-">-d</code> option will keep the container running in the background.</p>&#13;&#13;
    <p class="normal">For the mock S3 service, we will use the same<a id="_idIndexMarker540"/> approach, starting a container that starts S3 Mock, a system that replicates the S3 API, but stores the files locally. This lets us avoid setting up a real S3 instance, which involves getting an AWS account, paying for our usage, and so on.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">S3 Mock is a great option for development testing for S3 storage without using a real connection to S3. We will see later how to connect to the mock with a standard module. The full documentation can be found at <a href="https://github.com/adobe/S3Mock">https://github.com/adobe/S3Mock</a>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">To start S3 Mock, we will also use Docker:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ docker run -d -p 9090:9090 -t adobe/s3mock&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The container exposes the endpoint on port <code class="Code-In-Text--PACKT-">9090</code>. We will direct the S3 requests toward this local port. We will use the <code class="Code-In-Text--PACKT-">videos</code> bucket for storing all the data.</p>&#13;&#13;
    <p class="normal">We will define three different Celery workers that will perform three different tasks: the base task, image task and video task. Each one will be pulling events from different queues.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">This distinction of specific tasks for different workers is done deliberately for explanation purposes. In this example, there's probably not a good reason to make this distinction, as all the tasks can run in the same worker, and new events can be reintroduced in the same queue, and this is recommended, as we saw in the previous chapter. Sometimes, though, there are other conditions that may require a change of approach.</p>&#13;&#13;
      <p class="Tip--PACKT-">For example, some of the tasks may require specific hardware for AI processing, use way more RAM or CPU power making it impractical to make all workers equal, or other reasons that will necessitate separating the workers. Still, be sure that there's a good reason to make the split. It will complicate the operation and performance of the system.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">We will also use some<a id="_idIndexMarker541"/> third-party libraries. This includes Celery, as we saw in the previous chapter, but also other libraries, like <code class="Code-In-Text--PACKT-">boto3</code>, <code class="Code-In-Text--PACKT-">click</code>, and <code class="Code-In-Text--PACKT-">MoviePy</code>. All the required libraries are available in the <code class="Code-In-Text--PACKT-">requirements.txt</code> file so they can be installed with the following command:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ pip3 install -r requirements.txt&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Let's start with the first stage of the process, the base task that will redirect to the other two.</p>&#13;&#13;
    <h2 id="_idParaDest-160" class="title">Base task</h2>&#13;&#13;
    <p class="normal">The main task will receive a path<a id="_idIndexMarker542"/> that contains the image. It will then create two tasks for the processing of the video resizing and the extraction of the thumbnail.</p>&#13;&#13;
    <p class="normal">Here's the code for <code class="Code-In-Text--PACKT-">base_tasks.py</code>:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">from celery import Celery&#13;&#13;
app = Celery(broker='redis://localhost/0')&#13;&#13;
images_app = Celery(broker='redis://localhost/1')&#13;&#13;
videos_app = Celery(broker='redis://localhost/2')&#13;&#13;
logger = app.log.get_default_logger()&#13;&#13;
@app.task&#13;&#13;
def process_file(path):&#13;&#13;
    logger.info('Stating task')&#13;&#13;
    logger.info('The file is a video, needs to extract thumbnail and '&#13;&#13;
                'create resized version')&#13;&#13;
    videos_app.send_task('video_tasks.process_video', [path])&#13;&#13;
    images_app.send_task('image_tasks.process_video', [path])&#13;&#13;
    logger.info('End task')&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Note that we are creating three different queues here:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">app = Celery(broker='redis://localhost/0')&#13;&#13;
images_app = Celery(broker='redis://localhost/1')&#13;&#13;
videos_app = Celery(broker='redis://localhost/2')&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Redis allows us to create different databases easily by referring to them with an integer. So, we create database <code class="Code-In-Text--PACKT-">0</code> for the base queue, database <code class="Code-In-Text--PACKT-">1</code> for the images queue, and database <code class="Code-In-Text--PACKT-">2</code> for the videos queue.</p>&#13;&#13;
    <p class="normal">We generate events<a id="_idIndexMarker543"/> in these queues with the <code class="Code-In-Text--PACKT-">.send_task</code> function. Note that on each queue we send the proper task. We include the path as a parameter.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Note that all parameters for the tasks are defined in the second parameter of <code class="Code-In-Text--PACKT-">.send_task</code>. This requires that the parameter is a list of arguments. In this case, we only have a single parameter that needs still to be described as a list with <code class="Code-In-Text--PACKT-">[path]</code>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">When the task is triggered, it will enqueue the next tasks. Let's take a look at the image task.</p>&#13;&#13;
    <h2 id="_idParaDest-161" class="title">Image task</h2>&#13;&#13;
    <p class="normal">To generate a thumbnail of the video, we need<a id="_idIndexMarker544"/> the help of two third-party modules:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet"><em class="italic">boto3</em>. This common library helps<a id="_idIndexMarker545"/> us connect to AWS services. In particular, we will use it to download and upload to our own mocked S3 service.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">You can check the<a id="_idIndexMarker546"/> whole <code class="Code-In-Text--PACKT-">boto3</code> documentation at <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html">https://boto3.amazonaws.com/v1/documentation/api/latest/index.html</a>. It can be used to control all AWS APIs.</p>&#13;&#13;
    </p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet"><em class="italic">MoviePy</em>. This is a library for working with video. We will extract<a id="_idIndexMarker547"/> the first frame as an independent file using this library.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">The full <code class="Code-In-Text--PACKT-">MoviePy</code> documentation<a id="_idIndexMarker548"/> is available at <a href="https://zulko.github.io/moviepy/">https://zulko.github.io/moviepy/</a>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Both libraries are included in the <code class="Code-In-Text--PACKT-">requirements.txt</code> file described earlier in the chapter and included in the GitHub repo. Let's take a look at <code class="Code-In-Text--PACKT-">image_tasks.py</code>:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">from celery import Celery&#13;&#13;
import boto3&#13;&#13;
import moviepy.editor as mp&#13;&#13;
import tempfile&#13;&#13;
MOCK_S3 = 'http://localhost:9090/'&#13;&#13;
BUCKET = 'videos'&#13;&#13;
videos_app = Celery(broker='redis://localhost/1')&#13;&#13;
logger = videos_app.log.get_default_logger()&#13;&#13;
@videos_app.task&#13;&#13;
def process_video(path):&#13;&#13;
    logger.info(f'Stating process video {path} for image thumbnail')&#13;&#13;
    client = boto3.client('s3', endpoint_url=MOCK_S3)&#13;&#13;
    # Download the file to a temp file&#13;&#13;
    with tempfile.NamedTemporaryFile(suffix='.mp4') as tmp_file:&#13;&#13;
        client.download_fileobj(BUCKET, path, tmp_file)&#13;&#13;
        # Extract first frame with moviepy&#13;&#13;
        video = mp.VideoFileClip(tmp_file.name)&#13;&#13;
        with tempfile.NamedTemporaryFile(suffix='.png') as output_file:&#13;&#13;
            video.save_frame(output_file.name)&#13;&#13;
            client.upload_fileobj(output_file, BUCKET, path + '.png')&#13;&#13;
    logger.info('Finish image thumbnails')&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Note that we define the Celery application with the correct database. We then describe the task. Let's divide<a id="_idIndexMarker549"/> it into different steps. We first download the source file defined in <code class="Code-In-Text--PACKT-">path</code> into a temporary file:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">client = boto3.client('s3', endpoint_url=MOCK_S3)&#13;&#13;
# Download the file to a temp file&#13;&#13;
with tempfile.NamedTemporaryFile(suffix='.mp4') as tmp_file:&#13;&#13;
    client.download_fileobj(BUCKET, path, tmp_file)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Note that we define the endpoint to connect with <code class="Code-In-Text--PACKT-">MOCK_S3</code>, which is our S3 Mock container, exposed on <code class="Code-In-Text--PACKT-">http://localhost:9090/</code> as we described before.</p>&#13;&#13;
    <p class="normal">Right after it we generate a temporary file to store the downloaded video. We define that the suffix of the temporary<a id="_idIndexMarker550"/> file to be <code class="Code-In-Text--PACKT-">.mp4</code> so later <code class="Code-In-Text--PACKT-">VideoPy</code> can detect properly that the temporary file is a video.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Note the next steps are all inside the <code class="Code-In-Text--PACKT-">with</code> block defining the temporary file. If it was defined outside of this block, the file would be closed and not available.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">The next step is to load the file in <code class="Code-In-Text--PACKT-">MoviePy</code> and then extract the first frame into another temporary file. This second temporary file has a suffix of <code class="Code-In-Text--PACKT-">.png</code> to label it as an image:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">video = mp.VideoFileClip(tmp_file.name)&#13;&#13;
with tempfile.NamedTemporaryFile(suffix='.png') as output_file:&#13;&#13;
    video.save_frame(output_file.name)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Finally, the file is uploaded to S3 Mock, adding <code class="Code-In-Text--PACKT-">.png</code> to the end of the original name:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">client.upload_fileobj(output_file, BUCKET, path + '.png')&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Once again, pay attention to the indentation to be sure that the temporary files are available at the different stages.</p>&#13;&#13;
    <p class="normal">The task to resize the video follows a similar pattern. Let's take a look.</p>&#13;&#13;
    <h2 id="_idParaDest-162" class="title">Video task</h2>&#13;&#13;
    <p class="normal">The video Celery worker pulls from the video<a id="_idIndexMarker551"/> queue and performs similar steps to the image task:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">from celery import Celery&#13;&#13;
import boto3&#13;&#13;
import moviepy.editor as mp&#13;&#13;
import tempfile&#13;&#13;
MOCK_S3 = 'http://localhost:9090/'&#13;&#13;
BUCKET = 'videos'&#13;&#13;
SIZE = 720&#13;&#13;
videos_app = Celery(broker='redis://localhost/2')&#13;&#13;
logger = videos_app.log.get_default_logger()&#13;&#13;
@videos_app.task&#13;&#13;
def process_video(path):&#13;&#13;
    logger.info(f'Starting process video {path} for image resize')&#13;&#13;
    client = boto3.client('s3', endpoint_url=MOCK_S3)&#13;&#13;
    # Download the file to a temp file&#13;&#13;
    with tempfile.NamedTemporaryFile(suffix='.mp4') as tmp_file:&#13;&#13;
        client.download_fileobj(BUCKET, path, tmp_file)&#13;&#13;
        # Resize with moviepy&#13;&#13;
        video = mp.VideoFileClip(tmp_file.name)&#13;&#13;
        video_resized = video.resize(height=SIZE)&#13;&#13;
        with tempfile.NamedTemporaryFile(suffix='.mp4') as output_file:&#13;&#13;
            video_resized.write_videofile(output_file.name)&#13;&#13;
            client.upload_fileobj(output_file, BUCKET, path + f'x{SIZE}.mp4')&#13;&#13;
    logger.info('Finish video resize')&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The only difference from the image task is the resizing of the video to a height of 720 pixels and uploading the result:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"># Resize with moviepy&#13;&#13;
video = mp.VideoFileClip(tmp_file.name)&#13;&#13;
video_resized = video.resize(height=SIZE)&#13;&#13;
with tempfile.NamedTemporaryFile(suffix='.mp4') as output_file:&#13;&#13;
     video_resized.write_videofile(output_file.name)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">But the general flow<a id="_idIndexMarker552"/> is very similar. Note that it's pulling from a different Redis database, corresponding to the video queue.</p>&#13;&#13;
    <h2 id="_idParaDest-163" class="title">Connecting the tasks</h2>&#13;&#13;
    <p class="normal">To test the system, we need to start<a id="_idIndexMarker553"/> all the different elements. Each one is started in a different terminal so we can see their different logs:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">    $ celery -A base_tasks worker --loglevel=INFO&#13;&#13;
    $ celery -A video_tasks worker --loglevel=INFO&#13;&#13;
    $ celery -A image_tasks worker --loglevel=INFO&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">To start the process, we need a video to be processed in the system. </p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">One possibility to find good, free, videos is to use <a href="https://www.pexels.com/">https://www.pexels.com/</a>, which has free stock content. For our example run, we will download the 4K video with URL <a href="https://www.pexels.com/video/waves-rushing-and-splashing-to-the-shore-1409899/">https://www.pexels.com/video/waves-rushing-and-splashing-to-the-shore-1409899/</a>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">We will use the following script<a id="_idIndexMarker554"/> to upload the video to the S3 Mock storage and start the task:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">import click&#13;&#13;
import boto3&#13;&#13;
from celery import Celery&#13;&#13;
celery_app = Celery(broker='redis://localhost/0')&#13;&#13;
   MOCK_S3 = 'http://localhost:9090/'&#13;&#13;
BUCKET = 'videos'&#13;&#13;
SOURCE_VIDEO_PATH = '/source_video.mp4'&#13;&#13;
@click.command()&#13;&#13;
@click.argument('video_to_upload')&#13;&#13;
def main(video_to_upload):&#13;&#13;
# Note the credentials are required by boto3, but we are using&#13;&#13;
# a mock S3 that doesn't require them, so they can be fake&#13;&#13;
    client = boto3.client('s3', endpoint_url=MOCK_S3,&#13;&#13;
                          aws_access_key_id='FAKE_ACCESS_ID',&#13;&#13;
                          aws_secret_access_key='FAKE_ACCESS_KEY')&#13;&#13;
    # Create bucket if not set&#13;&#13;
    client.create_bucket(Bucket=BUCKET)&#13;&#13;
    # Upload the file&#13;&#13;
    client.upload_file(video_to_upload, BUCKET, SOURCE_VIDEO_PATH)&#13;&#13;
    # Trigger the&#13;&#13;
    celery_app.send_task('base_tasks.process_file', [SOURCE_VIDEO_PATH])&#13;&#13;
if __name__ == '__main__':&#13;&#13;
    main()&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The start of the script describes the Celery queue, the base queue, that will be the start of the pipeline. We define several values related to the configuration, as we saw in the previous tasks. The only addition is <code class="Code-In-Text--PACKT-">SOURCE_VIDEO_PATH</code>, which will host the video in S3 Mock.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">In this script we use the same name to upload all files, overwriting it if the script is run again. Feel free to change this if it makes more sense to you to do it differently.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">We use the <code class="Code-In-Text--PACKT-">click</code> library<a id="_idIndexMarker555"/> to generate an easy <strong class="keyword">command-line interface</strong> (<strong class="keyword">CLI</strong>). The following lines generate<a id="_idIndexMarker556"/> a simple interface that requests the name of the video to upload as the parameter of the function.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">@click.command()&#13;&#13;
@click.argument('video_to_upload')&#13;&#13;
def main(video_to_upload):&#13;&#13;
           ….&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal"><code class="Code-In-Text--PACKT-">click</code> is a fantastic option to generate CLIs quickly. You can read more about it in its documentation here: <a href="https://click.palletsprojects.com/">https://click.palletsprojects.com/</a>.</p>&#13;&#13;
    <p class="normal">The content of the main function simply connects to our S3 Mock, creates the bucket if not set yet, uploads the file to <code class="Code-In-Text--PACKT-">SOURCE_VIDEO_PATH</code>, and then sends the task to the queue to start the process:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">    client = boto3.client('s3', endpoint_url=MOCK_S3)&#13;&#13;
    # Create bucket if not set&#13;&#13;
    client.create_bucket(Bucket=BUCKET)&#13;&#13;
    # Upload the file&#13;&#13;
    client.upload_file(video_to_upload, BUCKET, SOURCE_VIDEO_PATH)&#13;&#13;
    # Trigger the&#13;&#13;
    celery_app.send_task('base_tasks.process_file', [SOURCE_VIDEO_PATH])&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Let's run it and see the results.</p>&#13;&#13;
    <h2 id="_idParaDest-164" class="title">Running the task</h2>&#13;&#13;
    <p class="normal">The script can be run after<a id="_idIndexMarker557"/> adding the name of the video to upload. Remember that all the libraries in <code class="Code-In-Text--PACKT-">requirements.txt</code> need to be installed:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ python3 upload_video_and_start.py source_video.mp4&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">It will take a bit of time to upload the file to S3 Mock. Once called, the first worker to react is the base one. This worker will create two new tasks:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">[2021-07-08 20:37:57,219: INFO/MainProcess] Received task: base_tasks.process_file[8410980a-d443-4408-8f17-48e89f935325]&#13;&#13;
[2021-07-08 20:37:57,309: INFO/ForkPoolWorker-2] Stating task&#13;&#13;
[2021-07-08 20:37:57,660: INFO/ForkPoolWorker-2] The file is a video, needs to extract thumbnail and create resized version&#13;&#13;
[2021-07-08 20:37:58,163: INFO/ForkPoolWorker-2] End task&#13;&#13;
[2021-07-08 20:37:58,163: INFO/ForkPoolWorker-2] Task base_tasks.process_file[8410980a-d443-4408-8f17-48e89f935325] succeeded in 0.8547832089971052s: None&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The other two will start soon after. The image worker will display new logs, starting the image thumbnail creation:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">[2021-07-08 20:37:58,251: INFO/MainProcess] Received task: image_tasks.process_video[5960846f-f385-45ba-9f78-c8c5b6c37987]&#13;&#13;
[2021-07-08 20:37:58,532: INFO/ForkPoolWorker-2] Stating process video /source_video.mp4 for image thumbnail&#13;&#13;
[2021-07-08 20:38:41,055: INFO/ForkPoolWorker-2] Finish image thumbnails&#13;&#13;
[2021-07-08 20:38:41,182: INFO/ForkPoolWorker-2] Task image_tasks.process_video[5960846f-f385-45ba-9f78-c8c5b6c37987] succeeded in 42.650344008012326s: None&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The video worker will take longer as it needs to resize the video:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">[2021-07-08 20:37:57,813: INFO/MainProcess] Received task: video_tasks.process_video[34085562-08d6-4b50-ac2c-73e991dbb58a]&#13;&#13;
[2021-07-08 20:37:57,982: INFO/ForkPoolWorker-2] Starting process video /source_video.mp4 for image resize&#13;&#13;
[2021-07-08 20:38:15,384: WARNING/ForkPoolWorker-2] Moviepy - Building video /var/folders/yx/k970yrd11hb4lmrq4rg5brq80000gn/T/tmp0deg6k8e.mp4.&#13;&#13;
[2021-07-08 20:38:15,385: WARNING/ForkPoolWorker-2] Moviepy - Writing video /var/folders/yx/k970yrd11hb4lmrq4rg5brq80000gn/T/tmp0deg6k8e.mp4&#13;&#13;
[2021-07-08 20:38:15,429: WARNING/ForkPoolWorker-2] t:   0%|          | 0/528 [00:00&lt;?, ?it/s, now=None]&#13;&#13;
[2021-07-08 20:38:16,816: WARNING/ForkPoolWorker-2] t:   0%|          | 2/528 [00:01&lt;06:04,  1.44it/s, now=None]&#13;&#13;
[2021-07-08 20:38:17,021: WARNING/ForkPoolWorker-2] t:   1%|          | 3/528 [00:01&lt;04:17,  2.04it/s, now=None]&#13;&#13;
...&#13;&#13;
[2021-07-08 20:39:49,400: WARNING/ForkPoolWorker-2] t:  99%|#########9| 524/528 [01:33&lt;00:00,  6.29it/s, now=None]&#13;&#13;
[2021-07-08 20:39:49,570: WARNING/ForkPoolWorker-2] t:  99%|#########9| 525/528 [01:34&lt;00:00,  6.16it/s, now=None]&#13;&#13;
[2021-07-08 20:39:49,874: WARNING/ForkPoolWorker-2] t: 100%|#########9| 527/528 [01:34&lt;00:00,  6.36it/s, now=None]&#13;&#13;
[2021-07-08 20:39:50,027: WARNING/ForkPoolWorker-2] t: 100%|##########| 528/528 [01:34&lt;00:00,  6.42it/s, now=None]&#13;&#13;
[2021-07-08 20:39:50,723: WARNING/ForkPoolWorker-2] Moviepy - Done !&#13;&#13;
[2021-07-08 20:39:50,723: WARNING/ForkPoolWorker-2] Moviepy - video ready /var/folders/yx/k970yrd11hb4lmrq4rg5brq80000gn/T/tmp0deg6k8e.mp4&#13;&#13;
[2021-07-08 20:39:51,170: INFO/ForkPoolWorker-2] Finish video resize&#13;&#13;
[2021-07-08 20:39:51,171: INFO/ForkPoolWorker-2] Task video_tasks.process_video[34085562-08d6-4b50-ac2c-73e991dbb58a] succeeded in 113.18933968200872s: None&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">To retrieve the results, we will<a id="_idIndexMarker558"/> use the <code class="Code-In-Text--PACKT-">check_results.py</code> script, which downloads the contents of the S3 Mock storage:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">import boto3&#13;&#13;
MOCK_S3 = 'http://localhost:9090/'&#13;&#13;
BUCKET = 'videos'&#13;&#13;
client = boto3.client('s3', endpoint_url=MOCK_S3)&#13;&#13;
for path in client.list_objects(Bucket=BUCKET)['Contents']:&#13;&#13;
    print(f'file {path["Key"]:25} size {path["Size"]}')&#13;&#13;
    filename = path['Key'][1:]&#13;&#13;
    client.download_file(BUCKET, path['Key'], filename)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">By running it, we download the files into the local directory:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ python3 check_results.py&#13;&#13;
file /source_video.mp4         size 56807332&#13;&#13;
file /source_video.mp4.png     size 6939007&#13;&#13;
file /source_video.mp4x720.mp4 size 8525077&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">You can check the resulting files and confirm that they have been generated correctly. Note that <code class="Code-In-Text--PACKT-">source_video.mp4</code> will be the same as your input video.</p>&#13;&#13;
    <p class="normal">This example demonstrates<a id="_idIndexMarker559"/> how to set up a relatively complex pipeline where different queues and workers are triggered in a coordinated fashion. Note that while we directly used Celery to send the tasks to the queues, we could also have used Celery Flower and an HTTP request to do this.</p>&#13;&#13;
    <h1 id="_idParaDest-165" class="title">Defining a bus</h1>&#13;&#13;
    <p class="normal">While we talked about the queue backend system, this hasn't been truly expanded to the concept of a bus. The term <em class="italic">bus</em> originates from the hardware<a id="_idIndexMarker560"/> buses that transmit data between different components of a hardware system. This makes them a central, multisource, and multidestination part of the system.</p>&#13;&#13;
    <p class="normal">A software bus is a generalization of this concept that allows us to interconnect several logical components.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">In essence, a bus is a component specialized in the transmission of data. This is an ordered communication compared to the usual alternative of connecting directly to the services through a network, without any intermediate component.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">As the bus is in charge of data transmission, that means that the sender doesn't need to know much other than the message to transmit and the queue to send it to. The bus itself will transmit to the destination or destinations.</p>&#13;&#13;
    <p class="normal">The concept of a bus is closely related to that of the <em class="italic">message broker</em>. A message broker, though, typically includes more<a id="_idIndexMarker561"/> capacities than a pure bus, such as being able to transform messages along the way and use multiple protocols. Message brokers can be very complex and allow a huge amount of customization and decoupling of services. In general, most of the tools to support the usage of a bus will be labeled as message brokers, though some are more powerful than others.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Though we will use the term "bus", some of the capacities will be more closely related to features such as routing messages, which should require tools considered message brokers. Analyze the requirements of your specific use cases and use a tool that can fulfil them.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">The bus will be then defined<a id="_idIndexMarker562"/> as a central point where all the event-related communication will be directed to. This simplifies the configuration, as the events can be routed to the proper destination without requiring a different endpoint.</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_08_05.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="460"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 8.5: Message bussing</p>&#13;&#13;
    <p class="normal">Internally, though, the bus will contain different logical divisions that allow the proper routing of messages. These are the queues.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">The routing can be complicated, if the bus allows for it, which is the case here.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">In our example before, we used Redis as a bus. Though the connection URL is a little different, it can be refactored to make it a bit clearer:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"># Remember that database 0 is the base queue&#13;&#13;
BASE_BROKER = 'redis://localhost/0'&#13;&#13;
Base_app = Celery(broker=BROKER)&#13;&#13;
# Refactor for base&#13;&#13;
BROKER_ROOT = 'redis://localhost'&#13;&#13;
BROKER_BASE_QUEUE = 0&#13;&#13;
base_app = Celery(broker=f'{BASE_BROKER}/{BROKER_BASE_QUEUE}') &#13;&#13;
# To address the image queue&#13;&#13;
BROKER_ROOT = 'redis://localhost'&#13;&#13;
BROKER_IMAGE_QUEUE = 1&#13;&#13;
image_app = Celery(broker=f'{BASE_BROKER}/{BROKER_IMAGE_QUEUE}') &#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This central location<a id="_idIndexMarker563"/> makes the configuration of all the different services easy, both for pushing events to the queues and pulling from them.</p>&#13;&#13;
    <h1 id="_idParaDest-166" class="title">More complex systems</h1>&#13;&#13;
    <p class="normal">More complex systems can be created where the events pass through multiple stages and are even designed for easy plugin systems working from the same queue.</p>&#13;&#13;
    <p class="normal">This can create complicated setups where the data flows<a id="_idIndexMarker564"/> through complex pipelines and is processed by independent modules. These kinds of scenarios are typically seen on instrumentation that aims to analyze and process big quantities of data to try and detect patterns and behaviors.</p>&#13;&#13;
    <p class="normal">Imagine, for example, a system that makes bookings for a travel agency. There are a lot of searches and bookings requests that happen in the system, with associated purchases such as car rentals, luggage bags, food, and so on. Each of the actions produces a regular response (search, book, purchase, and so on), but an event describing the action will be introduced into a queue to be processed in the background. Different modules will analyze user behavior with different objectives in mind.</p>&#13;&#13;
    <p class="normal">For example, the following modules<a id="_idIndexMarker565"/> could be added to this system:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Aggregate economic results by time, to obtain a global view of how the service is working over time. This can involve details such as purchases per day, revenue, margins, and so on.</li>&#13;&#13;
      <li class="bullet">Analyze the behavior of regular users. Follow users to discover their patterns. What are they searching for before booking? Are they using offers? How often are they booking flights? How long is their average trip? Any outliers?</li>&#13;&#13;
      <li class="bullet">Be sure that there's enough inventory for purchases. Backorder any required elements, based on the items being purchased in the system. This includes also scheduling enough food for flights, based on pre-purchases.</li>&#13;&#13;
      <li class="bullet">Collect information about preferred destinations, based on searches. </li>&#13;&#13;
      <li class="bullet">Trigger alerts for things like full flights that could lead to scheduling more planes for those days.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">These modules are fundamentally about different things and present a different view on the system. Some are more oriented toward the behavior of users and marketing, while others are more related to logistics. Depending on the size of the system, it could be determined that the modules require a different, dedicated team to take care of each of them independently.</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_08_06.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="213"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 8.6: Bus from front end system to different modules</p>&#13;&#13;
    <p class="normal">Note that each system will likely have its own storage to allow it to store the information. This could also lead to the creation of their own APIs to access this information once collected.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">To query the information, the system needs to query the databases of the modules where the data is stored. This can be an independent service, but it will likely be the same system's front end, as it will typically contain all the external interface and permissions handling.</p>&#13;&#13;
      <p class="Information-Box--PACKT-">This makes it necessary for the front end system to access the stored information, either by directly accessing the database or by using some API to access it. The front end system should model access to the data, as we saw in <em class="chapterRef">Chapter 3</em>, <em class="italic">Data Modeling</em>, and will very likely require a model definition that abstracts the complex access to the data.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">The same event will be sent to the bus, and then the different services will receive it. To be able to do so, you'll need to get a bus that accepts subscriptions from several systems and delivers the same message to all subscribed systems.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">This pattern is called <em class="italic">publish/subscribe</em> or <em class="italic">pub/sub</em>. The consumers of the events need<a id="_idIndexMarker566"/> to subscribe to the <em class="italic">topic</em>, which is, in pub/sub parlance, is the equivalent of a queue. Most buses accept this system, though it may require some work to configure.</p>&#13;&#13;
      <p class="Information-Box--PACKT-">For example, there's a library to allow Celery to work under this system available at <a href="https://github.com/Mulugruntz/celery-pubsub">https://github.com/Mulugruntz/celery-pubsub</a>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Note that the workers in this case can create more events to be introduced. For example, any module<a id="_idIndexMarker567"/> will be able to create an alert, to which the alert system will be notified. For example, if the inventory is too low, it may require a quick alert at the same time it backorders, to be sure that action is taken quickly.</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_08_07.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="292"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 8.7: Note that communication between the modules and the alerts also is done through the bus</p>&#13;&#13;
    <p class="normal">Complex event-driven systems can help you distribute the work between different components. In this example, you can see how the immediate response (booking a flight) is completely independent of the further detailed analysis in the background that can be used for longer-term planning. If all the components were added while the request was served, it could interfere with performance. The backend components can be swapped and upgraded while the front end system is unaffected.</p>&#13;&#13;
    <p class="normal">To properly implement this kind of system, the event needs to use a standard format that's easy to adapt<a id="_idIndexMarker568"/> and extend, to ensure that any module that receives it can quickly scan through it and discard it if it's not necessary. </p>&#13;&#13;
    <p class="normal">A good idea is to use a simple JSON structure like the following:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">{&#13;&#13;
  "type": string defining the event type,&#13;&#13;
  "data": subevent content&#13;&#13;
}&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">For example, when a search is produced, an event like this will be created:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">{&#13;&#13;
  "type": "SEARCH",&#13;&#13;
  "data": {&#13;&#13;
    "from": "Dublin",&#13;&#13;
    "to": "New York",&#13;&#13;
    "depart_date": 2021-12-31,&#13;&#13;
    "return_date": null,&#13;&#13;
    "user": null&#13;&#13;
  }&#13;&#13;
}&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The <code class="Code-In-Text--PACKT-">type</code> field makes easy to discard the event if it's not of interest to any module. For example, the <code class="Code-In-Text--PACKT-">economic analysis</code> module will discard any <code class="Code-In-Text--PACKT-">SEARCH</code> event. Other modules may require further processing. For example, the <code class="Code-In-Text--PACKT-">user behavior</code> module will analyze <code class="Code-In-Text--PACKT-">SEARCH</code> events where the <code class="Code-In-Text--PACKT-">user</code> field in the <code class="Code-In-Text--PACKT-">data</code> is set.</p>&#13;&#13;
    <p class="normal">Keep in mind that an important element for event-driven systems is that the storage may not be common to all. Perhaps each independent module has its own database. You'll need to use the techniques for CQRS that we discussed in <em class="chapterRef">Chapter 3</em>,<em class="italic"> Data Modeling</em>, to model data in these modules. In essence, you'll need to ask differently to read and to save new data, as writing new data requires the generation of events; and you'll need to model them as a business unit. What's more, the model may need to merge information from multiple modules in some cases. For example, if there's a query in the system that requires obtaining some economic information for a user, it needs to query both the <code class="Code-In-Text--PACKT-">user behavior</code> module and the <code class="Code-In-Text--PACKT-">economic analysis</code> module, while presenting the information as a unique model of <code class="Code-In-Text--PACKT-">EconomicInfoUser</code>.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">When information is frequently accessed, it may make sense to duplicate it in several places. This goes against the single responsibility principle (that every feature should be the sole responsibility of a single module), but the alternative is to create complicated methods of access to get information that's commonly used. Be careful when designing and dividing the system to avoid these problems.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">The flexible data structure will allow for new events to be generated, adding more information and allowing for controlled changes across the modules by enforcing the backward compatibility<a id="_idIndexMarker569"/> of changes. Then the different teams can work in parallel, improving the system without stepping on each other's toes too much.</p>&#13;&#13;
    <p class="normal">But ensuring that they behave correctly can be complicated, as there are multiple parts that interact with each other.</p>&#13;&#13;
    <h1 id="_idParaDest-167" class="title">Testing event-driven systems</h1>&#13;&#13;
    <p class="normal">Event-driven systems are very flexible and, in certain situations, can be incredibly useful in detaching different<a id="_idIndexMarker570"/> elements. But this flexibility and detachment can make them difficult to test to ensure that everything works as expected.</p>&#13;&#13;
    <p class="normal">In general, unit tests are the fastest tests to generate, but the detached nature of event-driven systems makes them not very useful to properly test the reception of events. Sure, the events<a id="_idIndexMarker571"/> can be simulated, and the general behavior of receiving an event can be tested. But the problem is: how can we ensure that the event has been properly generated? And at the right moment?</p>&#13;&#13;
    <p class="normal">The only option is to use integration tests to check the behavior of the system. But these tests are more expensive to design and run.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">There's always an endless debate about naming tests, what exactly a unit test is compared to an integration test, system test, acceptance test, and so on. To avoid getting into too deep<a id="_idIndexMarker572"/> a discussion here, at it's not the objective of the book, we will use the term <em class="italic">unit test</em> to describe tests that can only be run in a single module, and <em class="italic">integration test </em>to refer to those that require two or more modules interacting<a id="_idIndexMarker573"/> with each other to be successful. Unit tests will mock any dependence, but integration tests will actually call the dependence to be sure that the connection between modules works correctly.</p>&#13;&#13;
      <p class="Tip--PACKT-">These two levels are significantly different in terms of the cost for each test written. Way more unit tests can be written and run than integration tests in the same period of time.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">For example, in our previous example, to test that a purchase of food correctly triggers an alert, we need to:</p>&#13;&#13;
    <ol>&#13;&#13;
      <li class="numbered">Generate a call to purchase a food item.</li>&#13;&#13;
      <li class="numbered">Produce the appropriate event.</li>&#13;&#13;
      <li class="numbered">Handle the event in the inventory control. The current inventory should be configured as low, which will produce an alert event.</li>&#13;&#13;
      <li class="numbered">Handle the alert event properly.</li>&#13;&#13;
    </ol>&#13;&#13;
    <p class="normal">All these steps require<a id="_idIndexMarker574"/> configuration to be done in three different systems (the front-end system, the inventory control module, and the alert module), along with setting up the bus to connect them. Ideally, this test will require the system to be able to start up with an automation system to automate the tests. That requires every module involved to be automatable.</p>&#13;&#13;
    <p class="normal">As we can see, this is a high bar in setting up and running tests, though it is still worth doing. To achieve a sane balance between integration and unit tests, we should grow them and apply some strategy to be sure that we have reasonable coverage for both.</p>&#13;&#13;
    <p class="normal">Unit tests are cheap, so every case should have healthy coverage by unit tests, where the external modules are mocked. This includes cases such as different input formats, different configurations, all flows, errors, and so on. Good unit tests should cover most possibilities from an isolation point of view, mocking the input of data and any sent event.</p>&#13;&#13;
    <p class="normal">For example, continuing the inventory control example, many unit tests can control the following requisites, all by changing the input request:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Purchase of an element with high inventory.</li>&#13;&#13;
      <li class="bullet">Purchase of an element with low inventory. This should produce an alert event.</li>&#13;&#13;
      <li class="bullet">Purchase of a non-existing element. This should generate an error.</li>&#13;&#13;
      <li class="bullet">Event with invalid format. This should generate an error.</li>&#13;&#13;
      <li class="bullet">Purchase of an element with zero inventory. This should generate an alert event.</li>&#13;&#13;
      <li class="bullet">More cases, such as different kinds of purchases, formats, and so on.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Integration tests, on the other hand, should have only a few tests, mostly covering the "happy path". The <em class="italic">happy path</em> means that a regular representative event is being sent and processed, but doesn't produce<a id="_idIndexMarker575"/> expected errors. The objective of an integration test is to confirm that all the parts are connecting and working as expected. Given that integration tests are more expensive to run and operate, aim to implement only the most important, and keep an eye out for any test that isn't worth maintaining and can be pruned.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">We described, in the above discussion on integration tests, a happy path scenario. The event triggers a handle in the inventory and generates an alert that's also handled. For integration tests, this is preferred over not generating an alert, as it stresses the system more.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Though it depends<a id="_idIndexMarker576"/> on the system, the ratio of unit to integration test should be heavily weighted toward unit tests, sometimes by 20 times or more (meaning 1 integration test for 20 unit tests).</p>&#13;&#13;
    <h1 id="_idParaDest-168" class="title">Summary</h1>&#13;&#13;
    <p class="normal">In this chapter, we have seen more event-driven systems with a variety of advanced and complex architectures that can be designed. We have presented some of the flexibility and power that event-driven design can bring to a design, but also the challenges attached to event-driven design.</p>&#13;&#13;
    <p class="normal">We started by presenting common systems such as logs and metrics as event-driven systems, as they are, and considered how looking at them in this way allows us to create alerting and feedback systems that can be used to control the source of the events.</p>&#13;&#13;
    <p class="normal">We also presented an example with Celery of a more complex pipeline, including the usage of multiple queues and shared storage to generate multiple coordinated tasks, such as resizing a video and extracting a thumbnail.</p>&#13;&#13;
    <p class="normal">We presented the idea of a bus, a shared access point for all events in the system, and looked at how we can generate more complex systems where events are delivered to multiple systems and cascade into complex actions. We also discussed the challenges of solving these complex interactions, both in terms of requiring the use of CQRS techniques to model information that can be read after the write is generated through events, and the demands in terms of testing at different levels with unit and integration tests.</p>&#13;&#13;
    <p class="normal">In the next chapter, we will see the two main architectures for complex systems: monolithic and microservices.</p>&#13;&#13;
  </div>&#13;&#13;
</div></body></html>