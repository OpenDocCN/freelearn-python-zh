<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
<meta charset="utf-8"/>
<meta name="generator" content="packt"/>
<title>8 Running a Sanic Server</title>



</head>
<body>

<h1 data-number="9">8 Running a Sanic Server</h1>
<p>In the time that I have been involved with the Sanic project—and specifically, in trying to assist other developers by answering their support questions—there is one topic that perhaps comes up more than any other: deployment. That one word is often bundled with a mixture of confusion and dread.</p>
<p>Building a web application can be a lot of fun. I suspect that I am not alone in finding a tremendous amount of satisfaction in the build process itself. One of the reasons that I love software development in general—and web development in particular—is that I enjoy the almost puzzle-like atmosphere of fitting solutions to a given problem. When the build is done and it is time to launch, that is where the anxiety kicks in.</p>
<p>I cannot overemphasize this next point enough. One of Sanic’s biggest assets is its bundled web server. This is not just a gimmick, or some side feature to be ignored. The fact that Sanic comes bundled with its own web server truly does simplify the build process. Think about traditional Python web frameworks like Django or Flask, or about some of the newer ASGI frameworks. For them to become operational and connected to the web, you need a production-grade web server. Building the application is only one step. Deploying it requires knowledge and proficiency in another tool. Typically, the web server used to deploy your application built with one of those frameworks is not the same web server that you develop upon. For that, you have a development server. Not only is this an added complexity and dependency, but it also means you are not developing against the actual server that will be running your code in production. Is anyone else thinking what I am thinking? Bugs.</p>
<p>In this chapter, we will look at what is required to run Sanic. We will explore different ways to run Sanic both in development and production to make the deployment process as easy as possible. We will start by looking at the server lifecycle. Then, we will discuss setting up both a local and a production-grade scalable service. We will cover:</p>
<ul>
<li>Handling the server lifecycle</li>
<li>Configuring an application</li>
<li>Running Sanic locally</li>
<li>Deploying to production</li>
<li>Securing your application with TLS</li>
<li>Deployment examples</li>
</ul>
<p>When we are done, your days of deployment-induced anxiety should be a thing of the past.</p>

<h2 data-number="9.1">Technical requirements</h2>
<p>We will, of course, continue to build upon the tools and knowledge from previous chapters. Earlier in <em>Chapter 3</em>, <em>Routing and Intaking HTTP Requests</em>, we saw some implementations that used Docker. Specifically, we were using Docker to run an Nginx server for static content. While it is not required for deploying Sanic, knowledge of Docker and (to a lesser extent) Kubernetes will be helpful. In this Chapter, we will be exploring the usage of Docker with Sanic deployments. If you are not a black-belt Docker or Kubernetes expert, do not worry. There will be examples on the GitHub repository: <a href="https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/08">https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/08</a>. All that we hope and expect is some basic understanding and familiarity with these tools.</p>
<p>If you do not have these tools installed, you will need them to follow along:</p>
<ul>
<li><code>git</code></li>
<li><code>docker</code></li>
<li><code>doctl</code></li>
<li><code>kubectl</code></li>
</ul>


<h2 data-number="9.2">Handling the server lifecycle</h2>
<p>Throughout this book, we have spent a lot of time talking about the lifecycle of an incoming HTTP request. In that time, we have seen how we can attach to modify, and run code at different points in that cycle. Well, the lifecycle of the application server as a whole is no different.</p>
<p>Whereas we had middleware and signals, the server lifecycle has what are called “listeners”. In fact, listeners are in effect (with one small exception) signals themselves. Before we look at how to use them, we will take a look at what listeners are available.</p>

<h3 data-number="9.2.1">Server listeners</h3>
<p>The basic premise of a <strong>listener</strong> is that you are attaching some function to an <strong>event</strong> in the server’s lifecycle. As the server progresses through the startup and shutdown process, Sanic will trigger these events, and therefore allow you to easily plug in your own functionality. Sanic triggers events at both the startup and shutdown phases. For any other event during the life of your server, you should refer to the <em>Leveraging signals for intra-worker communication</em> section on Signals in <em>Chapter 6</em>, <em>Outside the Response Cycle</em>.</p>
<p>The order of the events is as follows:</p>
<ol>
<li><code>before_server_start</code>: This event naturally begins runs before the server is started. It is a great place to connect to a database, or perform any other operations that need to happen at the beginning of your application lifecycle. Anything that you might be inclined to do in the global scope would almost always be better off done here. The only caveat worth knowing about is that if you are running in ASGI mode, the server is already running by the time Sanic is even triggered. In that case, there is no difference between <code>before_server_start</code> and <code>after_server_start</code>.</li>
<li><code>after_server_start</code>: A common misconception about this event is that it could encounter a race condition where the event runs <em>while</em> your server begins responding to HTTP requests. That is <em>not</em> the case. What this event means is that there was an HTTP server created and attached to the OS. The infrastructure is in place to begin accepting requests, but it has not happened yet. Only once all of your listeners for <code>after_server_start</code> are complete will Sanic begin to accept HTTP traffic.</li>
<li><code>before_server_stop</code>: This is a good place to start any cleanup you need to do. While you are in this location, Sanic is still able to accept incoming traffic, so anything that you might need to handle that should still be available (like database connections).</li>
<li><code>after_server_stop</code>: Once the server has been closed, it is now safe to start any cleanup that is remaining. If you are in ASGI mode, like <code>before_server_start</code>, this event is not actually triggered after the server is off because Sanic does not control that. It will instead immediately follow any <code>before_server_stop</code> listeners to preserve their ordering.</li>
</ol>
<p>There are two more listeners that are available to you. However, these additional listeners are <em>only</em> available with the Sanic server since they are specific to the Sanic server lifecycle. This is due to how the server works. When you run Sanic with multiple workers, what happens is that there is the main process that acts as an orchestrator. It spins up multiple subprocesses for each of the workers that you have requested. If you want to tap into the lifecycle of each of those worker processes, then you already have the tools at your disposal with the four listeners we just saw.</p>
<p>However, what if you wanted to run some bit of code not on each worker process, but once in the main process: that orchestrator? The answer is the Sanic server’s main process events. They are <code>main_process_start </code>and <code>main_process_stop</code>. Other than the fact that they run inside the main process and not the workers, they otherwise work like the other listeners. Remember how I said that the listeners are themselves signals, with an exception? This is that exception. These listeners are not signals in disguise. For all practical purposes, this distinction is not important.</p>
<p>It is also worth mentioning that even though these events are meant to allow code to be run in the main process and not the worker process when in multi-worker mode, they are still triggered when you are running with a single worker process. When this is the case, it will be run at the extreme beginning and extreme end of your lifecycle.</p>
<p>This raises an interesting and often seen mistake: double execution. Before continuing with listeners, we will turn our attention to mistakenly running code multiple times.</p>

<h4 data-number="9.2.1.1">Running code in the global scope</h4>
<p>When you are preparing your application to run, it is not uncommon to initialize various services, clients, interfaces, and so on. You likely will need to perform some operations on your application very early in the process before the server even begins to run.</p>
<p>For example, let’s imagine that you are looking for a solution to help you better track your exceptions. You find a third-party service where you can report all of your exceptions and tracebacks to help you to better analyze, debug, and repair your application. To get started, the service provides some documentation to use their <strong>software development kit</strong> (<strong>SDK</strong>) as follows:</p>
<pre><code>from third_party_sdk import init_error_reporting
init_error_reporting(app)</code></pre>
<p>You get this setup and running in your multi-worker application, and you immediately start noticing that it is running multiple times, and not in your worker processes as expected. What is going on?</p>
<p>Likely, the issue is that you ran your initialization code in the global scope. By <em>global scope</em> in Python we mean something that is executing outside of a function or method. It runs on the outermost level in a Python file. In the above example, <code>init_error_reporting</code> runs in the global scope because it is not wrapped inside another function. The problem is that when multiple workers are running, you need to be aware of where and when that code is running. Since multiple workers mean multiple processes, and each process is likely to run your global scope, you need to be careful about what you put there.</p>
<p>As a very general rule, stick to putting <em>any</em> operable code inside a listener. This allows you to control the where and when and will operate in a more consistent and easily predictable manner.</p>


<h4 data-number="9.2.1.2">Setting up listeners</h4>
<p>Using listeners should look very familiar since they follow a similar pattern found elsewhere in Sanic. You create a listener handler (which is just a function) and then wrap it with a decorator. It should look like this:</p>
<pre><code>@app.before_server_start
async def setup_db(app, loop):
    app.ctx.db = await db_setup()</code></pre>
<p>What we see here is something <em>incredibly important</em> in Sanic development. This pattern should be committed to memory because attaching elements to your application <code>ctx</code> object increases your overall flexibility in development. In this example, we set up our database client so that it can be accessed from anywhere that our application can be (which is literally anywhere in the code).</p>
<p>One important thing to know is that you can control the order in which the listeners execute depending upon when they are defined. For the “start” time listeners (<code>before_server_start</code>, <code>after_server_start</code>, and <code>main_process_start</code>), they are executed in the order in which they are declared.</p>
<p>For the <em>stop</em> time listeners (<code>before_server_stop</code>, <code>after_server_stop</code>, and <code>main_process_stop</code>) the opposite is true. They are run in the reverse order of declaration.</p>


<h4 data-number="9.2.1.3">How to decide to use a <em>before</em> listener or an <em>after</em> listener</h4>
<p>As stated above, there persists a common misconception that logic must be added to <code>before_server_start </code>in the case you want to perform some operation before requests start. The fear is that using <code>after_server_start </code>might cause some kind of a race condition where some requests might hit the server in the moments before that event is triggered.</p>
<p>This is incorrect. Both <code>before_server_start </code>and <code>after_server_start </code>run to completion before any requests are allowed to come in.</p>
<p>So, then the question becomes when should you favor one over the other? There are, of course, some personal and application-specific preferences that could be involved. Generally, however, I like to use the <code>before_server_start </code>event to set up my application context. If I need to initialize some object and persist it to <code>app.ctx</code>, then I will reach for <code>before_server_start</code>. For any other use case (like performing any other types of external calls, or configuration, I like to use <code>after_server_start</code>. This is by no means a hard and fast rule, and I often break it myself.</p>
<p>Now that we understand the lifecycle of the server, there is one more missing bit of information that we need before we can run the application: configuration.</p>




<h2 data-number="9.3">Configuring an application</h2>
<p>Sanic tries to make some reasonable assumptions out of the box about your application. With this in mind, you can certainly spin up an application and it should already have some reasonable default settings in place. While this may be acceptable for a simple prototype, as soon as you start to build your application you will realize that you need to configure it.</p>
<p>And this is where Sanic’s configuration system comes into play.</p>
<p>Configuration comes in two main flavors: tweaking the Sanic runtime operation, and declaring a state of global constants to be used across your application. Both types of configuration are important, and both follow the same general principles for applying values.</p>
<p>We will take a closer look at what the configuration object is, how we can access it, and how it can be updated or changed.</p>

<h4 data-number="9.3.0.1">What is the Sanic configuration object?</h4>
<p>When you create a Sanic application instance, it will create a configuration object. That object is really just a fancy <code>dict</code> type. As you will see, it does have some special properties. Do not let that fool you. You should remember: <em>it is a</em> <code>dict</code>. You can work with it like you would any other <code>dict </code>object. This will come in handy in a little bit when we explore how we can use the object.</p>
<p>If you do not believe me, then pop the following code into your application:</p>
<pre><code>app = Sanic(__name__)
assert isinstance(app.config, dict)</code></pre>
<p>This means that getting a configuration value with a default is no different than any other <code>dict</code> in Python:</p>
<pre><code>environment = app.config.get(&quot;ENVIRONMENT&quot;, &quot;local&quot;)</code></pre>
<p>The configuration object is—however—much more important than any other <code>dict</code>. It contains a lot of settings that are critical to the operation of your application. We have, of course, already seen in <em>Chapter 6</em> that we can use it to modify our default error handling:</p>
<pre><code>app.config.FALLBACK_ERROR_FORMAT = &quot;text&quot;</code></pre>
<p>To understand the full scope of settings that you can tweak, you should take a look at the Sanic documentation: <a href="https://sanicframework.org/en/guide/deployment/configuration.html#builtin-values.">https://sanicframework.org/en/guide/deployment/configuration.html#builtin-values.</a></p>


<h4 data-number="9.3.0.2">How can an application’s configuration object be accessed?</h4>
<p>The best way to access the configuration object is to first get access to the application instance. Depending upon the scenario you are tackling at the moment, there are three main ways to get access to an application instance:</p>
<ul>
<li>Access the application instance using a request object (<code>request.app</code>)</li>
<li>Accessing applications from a Blueprint instance (<code>bp.apps</code>)</li>
<li>Retrieving an application instance from the application registry (<code>Sanic.get_app()</code>)</li>
</ul>
<p>Perhaps the most common way to obtain the application instance (and therefore the configuration object by extension) is to grab it from the request object inside of a handler:</p>
<pre><code>@bp.route(&quot;&quot;)
async def handler(request):
environment = request.app.config.ENVIRONMENT</code></pre>
<p>If you are outside of a route handler (or middleware) where the request object is easily accessible, then the next best choice is probably to use the application registry. Rarely will it make sense to use the Blueprint <code>apps</code> property. It is a set of applications that the blueprint has been applied to. However, because it only exists <em>after</em> registration, and it could be ambiguous which application you need, I usually will not reach for that as a solution. It is, nonetheless, good to know that it exists.</p>
<p>You may have seen us using the third option already. As soon as an application is instantiated, it is part of a global registry that can be looked up using:</p>
<pre><code>from sanic import Sanic
app = Sanic.get_app()</code></pre>
<p>Whenever I am not in a handler, this is the solution I usually reach for. The two caveats that you need to be aware of are:</p>
<ol>
<li>Make sure that the application instance has already been instantiated. Using <code>app = Sanic.get_app()</code> in the global scope can be tricky if you are not careful with your import ordering. Later on, in <em>Chapter 11</em>, <em>A complete real-world example</em> when we build out a complete application I will show you a trick I use to get around this.</li>
<li><p>If you are building a runtime with multiple application instances, then you will need to differentiate them using the application name:</p>
<pre><code>main_app = Sanic(&quot;main&quot;)
side_app = Sanic(&quot;side&quot;)
assert Sanic.get_app(&quot;main&quot;) is main_app</code></pre></li>
</ol>
<p>Once you have the object, you will usually just access the configuration value as a property, for example, <code>app.config.FOOBAR</code>. As shown previously, you can also use a variety of Python accessors:</p>
<blockquote>
<p><code>app.config.FOOBAR</code></p>
<blockquote>
<p><code>app.config.get("FOOBAR")</code></p>
</blockquote>
<blockquote>
<p><code>app.config["FOOBAR"]</code></p>
</blockquote>
<blockquote>
<p><code>getattr(app.config, "FOOBAR")</code></p>
</blockquote>
</blockquote>


<h4 data-number="9.3.0.3">How can the configuration object be set?</h4>
<p>If you go to the Sanic documentation, you will see that there are a bunch of default values already set. These values can be updated in a variety of methods as well. Of course, you can use the <code>object</code> and <code>dict</code> setters:</p>
<pre><code>app.config.FOOBAR = 123
setattr(app.config, &quot;FOOBAR&quot;, 123)
app.config[&quot;FOOBAR&quot;] = 123
app.config.update({&quot;FOOBAR&quot;: 123})</code></pre>
<p>You will usually set values like this right after creating your application instance. For example, throughout this book, I have repeatedly used <code>curl</code> to access endpoints that I created. The easiest method to see an exception is to use the text-based exception renderer. Therefore, in most cases, I have used the following pattern to make sure that when there is an exception it is easily formatted for display in this book:</p>
<pre><code>app = Sanic(__name__)
app.config.FALLBACK_ERROR_FORMAT = &quot;text&quot;</code></pre>
<p>This is <em>not</em> usually ideal for a fully-built application. If you have been involved in web application development before, then you probably do not need me to tell you that configuration should be easily changeable depending upon your deployment environment. Therefore, Sanic will load environment variables as configuration values if they are prefixed with <code>SANIC_</code>.</p>
<p>This means that the above <code>FALLBACK_ERROR_FORMAT</code> could also be set outside of the application with an environment variable:</p>
<pre><code>$ export SANIC_FALLBACK_ERROR_FORMAT=text</code></pre>
<p>The best method to do this will obviously depend upon your deployment strategy. We go deeper into those strategies later in this Chapter, and the specifics of how to set those variables will differ and are outside the scope of this book.</p>
<p>Another option that you may be familiar with is centralizing all of your configurations in a single location. Django does this with <code>settings.py</code>. While I am personally not a fan of this pattern, you might be. You can easily duplicate it like this:</p>
<ol>
<li><p>Create a <code>settings.py</code> file.</p>
<pre><code>FOO = &quot;bar&quot;</code></pre></li>
<li><p>Apply the configuration to the application instance:</p>
<pre><code>import settings
app.update_config(settings)</code></pre></li>
<li><p>Access the values as needed:</p>
<pre><code>print(app.config.FOO)</code></pre>
<p>There is nothing special about the <code>settings.py</code> file name. You just need a module with a whole bunch of properties that are uppercased. In fact, you could replicate this with an object.</p></li>
<li><p>Put all of your constants into an object now:</p>
<pre><code>class MyConfig:
FOO = &quot;bar&quot;</code></pre></li>
<li><p>Apply the configuration from that object.</p>
<pre><code>app.update_config(MyConfig)</code></pre></li>
</ol>
<p>The result will be the same.</p>


<h4 data-number="9.3.0.4">Some general rules about configuration</h4>
<p>I have some general rules that I like to follow regarding configuration. I encourage you to adopt them since they have evolved from years of making mistakes. But, I just as strongly encourage you to break them when necessary:</p>
<ul>
<li><em>Use simple values</em>: If you have some sort of a complex object like a <code>datetime</code>, perhaps configuration is not the best location for it. Part of the flexibility of configuration is that it can be set in many different ways; including outside of your application in environment variables. While Sanic will be able to convert things like booleans and integers, everything else will be a string. Therefore, for the sake of consistency and flexibility, try to avoid anything but simple value types.</li>
<li><em>Treat them as constants</em>: Yes, this is Python. That means everything is an object and everything is subject to runtime changes. But do not do this. If you have a value that needs to be changed <em>during</em> the running of your application, use <code>app.ctx </code>instead. In my opinion, once <code>before_server_start </code>has completed, your configuration object should be considered locked in stone.</li>
<li><em>Don’t hardcode values</em>: Or, at least try really hard not to. When building out your application, you will undoubtedly find the need to create some sort of constant value. It is hard to guess the scenario that this might come up in without knowing your specific application. But when you realize that you are about to create a constant, or some value, ask yourself whether the configuration is more appropriate. Perhaps the most concrete example of this is the settings that you might use to connect to a database, a vendor integration, or any other third-party service.</li>
</ul>
<p>Configuring your application is almost certainly something that will change over the lifetime of your application. As you build it, run it, and add new features (or fix broken features), it is not uncommon to return to configuration often. One marker of a professional-grade application is that it relies heavily upon this type of configuration. This is to provide you with the flexibility to run the application in different environments. You may, for example, have some features that are only beneficial in local development, but not in production. It may also be the other way around. Configuration is therefore almost always tightly coupled with the environment where you will be deploying your application.</p>
<p>We now turn our attention to those deployment options to see how Sanic will behave when running in development and production environments.</p>



<h2 data-number="9.4">Running Sanic locally</h2>
<p>We finally are at the point where it is time to run Sanic—well, locally that is. However, we also know we have been doing that all along since <em>Chapter 2, Organizing a project</em>. The Sanic CLI is already probably a fairly comfortable and familiar tool. But there are some things that you should know about it. Other frameworks have only development servers. Since we know that Sanic’s server is meant for both development and production environments, we need to understand how these environments differ.</p>

<h3 data-number="9.4.1">How does running Sanic locally differ from production?</h3>
<p>The most common configuration change for local production is turning on debug mode. This can be accomplished in three ways:</p>
<ol>
<li><p>It could be enabled directly on the application instance. You typically would see this inside of a factory pattern when Sanic is being run programmatically from a script (as opposed to the CLI). You can directly set the value as shown here:</p>
<pre><code>def create_app(..., debug: bool = False) -&gt; Sanic:
    app = Sanic(__name__)
    app.debug = debug
    ...</code></pre></li>
<li><p>It is perhaps more common to see it set as an argument of <code>app.run</code>. A common use case for this might be when reading environment variables to determine how Sanic should initialize. In the following example, an environment value is read and applied when Sanic server begins to run.</p>
<pre><code>from os import environ
from path.to.somewhere import create_app

def main():
    app = create_app()

    debug = environ.get(&quot;RUNNING_ENV&quot;, &quot;local&quot;) != &quot;production&quot;

    app.run(..., debug=debug)</code></pre></li>
<li><p>The final option is to use the Sanic CLI. This is generally my preferred solution, and if you have been following along with the book, it is the one that we have been using all along. This method is straightforward as shown here:</p>
<pre><code>$ sanic path.to:app --debug</code></pre></li>
</ol>
<p>The reason that I prefer this final option is that I like to keep the operational aspects of the server distinct from other configurations.</p>
<p>For example, timeouts are configuration values that are closely linked to the operation of the framework and not the server itself. They impact how the framework responds to requests. Usually, these values are going to be the same regardless of where the application is deployed.</p>
<p>Debug mode–on the other hand–is much more closely linked to the deployment environment. You will want to set it to <code>True </code>locally, but <code>False </code>in production. Therefore, since we will be controlling how Sanic is deployed with tools like Docker, controlling the server’s operational capacity outside of the application makes sense.</p>
<p>“<em>Okay</em>”, you say, “<em>turning on debug mode is simple, but why should I?</em>” I’m glad that you asked. When you run Sanic in debug mode, it makes a couple of important changes. The most noticeable is that you begin to see debug logs and access logs dispatched from Sanic. This is, of course, very helpful to see while developing.</p>
<blockquote>
<p><strong>TIP</strong></p>
<p>When I sit down to work on a web application, I always have three windows in my view at all times:</p>
<p>My IDE</p>
<p>An API client like Insomnia or Postman</p>
<p>A terminal showing me my Sanic logs (in debug mode)</p>
<p>The terminal with debug level logging is your window into what is happening with your application as you build it.</p>
</blockquote>
<p>Perhaps the biggest change that debug mode brings is that any exception will include its traceback in the response. In the next chapter, we will look at some examples of how you can make the most of this exception information.</p>
<p>This is hugely important and useful while you are developing. It is also a huge security issue to accidentally leave it on in production. <em>DO NOT leave debug mode on in a live web application</em>. This includes any instance of your application that is <em>not</em> on a local machine. So, for example, if you have a staging environment that is hosted somewhere on the Internet, it may not be your “production” environment. However, it still <em>MUST NOT</em> run debug mode. At best, it will leak details about how your application was built. At worst, it will make sensitive information available. Make sure to turn off debug mode in production.</p>
<p>Speaking of production, let’s move on over to what it takes to deploy Sanic into the wild world of production environments.</p>



<h2 data-number="9.5">Deploying to production</h2>
<p>We have finally made it. After working your way through the application development process, there finally is a product to launch out into the ether of the World Wide Web. The obvious question then becomes: what are my options? There really are two sets of questions that need to be answered:</p>
<ul>
<li>First question: which server should run Sanic? There are three options: Sanic server, an ASGI server, or Gunicorn.</li>
<li>Second question: where do you want to run the application? Some typical choices include: <em>bare metal</em> virtual machine, containerized image, <strong>platform-as-a-service</strong> (<strong>PaaS</strong>), or a self-hosted or fully managed orchestrated container cluster. Perhaps these choices might make more sense if we put some of the commonly used product names to them:</li>
</ul>
<table>
<tbody>
<tr class="odd">
<td><strong>Deployment type</strong></td>
<td><strong>Potential vendors</strong></td>
</tr>
<tr class="even">
<td>Virtual machine</td>
<td>Amazon EC2, Google Cloud, Microsoft Azure, Digital Ocean, Linode</td>
</tr>
<tr class="odd">
<td>Container</td>
<td>Docker</td>
</tr>
<tr class="even">
<td>Platform as a service</td>
<td>Heroku</td>
</tr>
<tr class="odd">
<td>Orchestrated cluster</td>
<td>Kubernetes</td>
</tr>
</tbody>
</table>
Table 8.1 – Examples of common hosting providers and tools

<h3 data-number="9.5.1">Choosing the right server option</h3>
<p>As we stated, there are three main ways to run Sanic: the built-in server, with an ASGI compatible server, or with Gunicorn. Before we decide which server to run, we will look take a brief look at the pros and cons for each option starting with the least performant option.</p>

<h4 data-number="9.5.1.1">Gunicorn</h4>
<p>If you are coming to Sanic from the WSGI world, you may already be familiar with Gunicorn. Indeed, you may even be surprised to learn that Sanic can be run with Gunicorn since it is built for WSGI applications, not asynchronous applications like Sanic. Because of this, the biggest downside to running Sanic with Gunicorn is the <em>substantial</em> decrease in performance. Gunicorn effectively unravels much of the work done to leverage <em>concurrency</em> with the <code>asyncio</code> module. It is by far the slowest way to run Sanic, and in most use cases is not recommended.</p>
<p>It still could be a good choice in certain circumstances. Particularly, if you need a feature-rich set of configurations options, and cannot use something like Nginx, then this might be an approach. Gunicorn has a tremendous amount of options that can be leveraged for fine-tuning server operation. In my experience, however, I typically see people reaching for it out of habit and not out of necessity. People will use it simply because it is what they know. For people that are transitioning to Sanic from the Flash/Django world, they may be used to a particular deployment pattern that was centered around tools like Supervisor and Gunicorn. That’s fine, but it is a little old fashioned and should not be the go-to pattern for Sanic deployments.</p>
<p>For those people, I urge you to look at another option. You are building with a new framework, why not deploy it with a new strategy as well?</p>
<p>If, however, you do find yourself needing some of the more fine-tune controls offered by Gunicorn, I would recommend you take a look at Nginx, which has an equally (if not more) impressive set of features. Whereas Gunicorn would be set up to actually run Sanic, the Nginx implementation would rely upon Sanic running via one of the other two strategies and placing an Nginx proxy in front of it. More on Nginx proxying later in this Chapter. This option will allow you to retain a great deal of server control without sacrificing performance. It does, however, require some more complexity since you need to essentially run two servers instead of just one.</p>
<p>If in the end, you still decide the use Gunicorn, then the best way to do so is to use Uvicorn’s worker shim. Uvicorn is an ASGI server, which we will learn more about in the next section. In this context, however, it also ships with a worker class that allows Gunicorn to integrate with it. This effectively puts Sanic into ASGI mode. Gunicorn still runs as the webserver, but it will pass traffic off to Uvicorn, which will then reach into Sanic as if it were an ASGI application. This will retain much of the performance offered by Sanic and asynchronous programming (although still not as performant as the Sanic server by itself). You can accomplish this as shown next:</p>
<ol>
<li><p>First, make sure both Gunicorn and Uvicorn are installed:</p>
<pre><code>$ pip install gunicorn uvicorn</code></pre></li>
<li><p>Next, run the application like this:</p>
<pre><code>$ gunicorn \
    --bind 127.0.0.1:7777 \
    --worker-class=uvicorn.workers.UvicornWorker \
    path.to:app</code></pre></li>
</ol>
<p>You should now have the full span of Gunicorn configurations at your fingertips.</p>


<h4 data-number="9.5.1.2">ASGI server</h4>
<p>We visited ASGI briefly in <em>Chapter 1, Introduction to Sanic and async frameworks</em>. If you recall, <strong>ASGI</strong> stands for <strong>Asynchronous Server Gateway Interface</strong>, and it is a design specification for how servers and frameworks can communicate with each other asynchronously. It was developed as a replacement methodology for the older WSGI standard that is incompatible with modern asynchronous Python practices. This standard has given rise to three popular ASGI webservers: Uvicorn, Hypercorn, and Daphne. All three of them follow the ASGI protocol, and can therefore run any framework that adheres to that protocol. The goal, therefore, is to create a common language that allows one of these ASGI servers to run any ASGI framework.</p>
<p>And this is where to discuss Sanic with regards to ASGI we must have a clear distinction in our mind of the difference between the server and the framework. <em>Chapter 1</em> discussed this difference in detail. As a quick refresher, the web server is the part of the application that is responsible for connecting to the operating system’s socket protocol and handling the translation of bytes into usable web requests. The framework takes the digested web requests and provides the application developer with the tools needed to respond and construct an appropriate HTTP response. The server then takes that response and sends the bytes back to the operating system for delivery back to the client.</p>
<p>Sanic handles this whole process, and when it does so, it operates outside the ASGI since that interface is not needed. However, it also has the ability to speak the language of an ASGI framework and thus can be used with any ASGI web server.</p>
<p>One of the benefits of running Sanic as an ASGI application is that it standardizes the run-time environment with a broader set of Python tools. There is, for example, a set of ASGI middleware that could be implemented to add a layer of functionality between the server and the application.</p>
<p>However, some of the standardization does come at the expense of performance.</p>


<h4 data-number="9.5.1.3">Sanic server</h4>
<p>The default mechanism is to run Sanic with its built-in web server. It should come as no surprise that it is built with performance in mind. Therefore, what Sanic server gives up by forfeiting the standardization and interoperability of ASGI, it makes up in its ability to optimize itself as a single purpose server.</p>
<p>We have touched on some of the potential downsides of using Sanic server. One of them was static content. No Python server will be able to match the performance of Nginx in handling static content. If you are already using Nginx as a proxy for Sanic, and you have a known location of static assets, then it might make sense to use it also for those assets. However, if you are not using it, then you need to determine whether the performance difference warrants the additional operational expense. In my opinion, if you can easily add this to your Nginx configuration: great. However, if it would take a lot of complicated effort, or you are exposing Sanic directly, then the benefit might not be as great as just leaving it as is and serving that content from Sanic. Sometimes, for example, the easiest thing to do is to run your entire frontend and backend from a single server. This is certainly a case where I would suggest learning the competing interests and making an appropriate decision and not trying to make a <em>perfect</em> decision.</p>
<p>With this knowledge, you should now be able to decide which server is the right fit for your needs. We will assume for the remainder of this book that we are still deploying with the Sanic server, but since it is mainly a matter of changing the command line executable, the difference should not make a difference.</p>



<h3 data-number="9.5.2">How to choose a deployment strategy?</h3>
<p>The last section laid out three potential web servers to use for Sanic applications. But that web server needs to run on a web host. But, before deciding on which web hosting company to use, there is still a very important missing component: how are you going to get your code from your local machine to the web host? In other words: how are you going to deploy your application? We will now look through some options for deploying Sanic applications.</p>
<p>There is some assumed knowledge, so if some of the technologies or terms here are unfamiliar, please feel free to stop and go look them up.</p>

<h4 data-number="9.5.2.1">Virtual machine</h4>
<p>This is perhaps the easiest option. Well, the easiest besides PAAS. Setting up a <strong>virtual machine</strong> (<strong>VM</strong>) is super simple these days. With just a few clicks of a button, you can have a custom configuration for a VM. The reason this then becomes a simple option is that you just need to run your Sanic application the same way you might on your local machine. This is particularly appealing when using the Sanic server since it literally means that you can run Sanic in production with the same commands that you use locally. However, getting your code to the VM, maintaining it once it is there, and then ultimately scaling it will make this option the hardest. To be blunt, I almost would never recommend this solution. It is appealing to new beginners since it looks so simple from the outside. But looks can be deceiving.</p>
<p>There may in fact be times when this is an appropriate solution. If that is the case, then what would deployment look like? Really, not that much different than running it locally. You run the server and bind it to an address and port. With the proliferation of cloud computing, service providers have made it such a trivial experience to stand up a virtual machine. I personally find platforms like Digital Ocean and Linode to be super user-friendly, and excellent choices. Other obvious choices include Amazon AWS, Google Cloud, and Microsoft Azure. In my opinion, however, they are a little less friendly to someone new to cloud computing. Armed with their good documentation, with Digital Ocean and Linode it is relatively inexpensive and painless to click a few buttons and get an instance running. Once they provide you with an IP address, it is now your responsibility for getting your code to the machine and running the application.</p>
<p>You might be thinking the simplest way to move your code to the server would be to use git. Then all you need to do is launch the application and you are done. But, what happens if you need more instances or redundancy? Yes, Sanic comes with the ability to spin up multiple worker processes. But what if that is not enough? Now you need another VM and some way to manage load balancing your incoming web traffic between them. How are you going to handle redeployments of bug patches or new features? What about changes to environment variables? These complexities could lead to a lot of sleepless nights if you are not careful.</p>
<p>This is also somewhat ignoring the other fact that not all environments are equal. VMs could be built with different dependencies, leading to wasteful time maintaining servers and packages.</p>
<p>That is not to say this cannot or should not be a solution. Indeed, it might be a great solution if you are creating a simple service for your own use. Perhaps you need a webserver for connecting to a smart home network. But it is certainly a case of <em>developer beware</em>. Running a webserver on a <em>baremetal</em> virtual machine is rarely as simple as it appears at first glance.</p>


<h4 data-number="9.5.2.2">Containers with Docker</h4>
<p>One solution to the previous set of problems is using a Docker container. For those that have used Docker, you can probably skip to the next section because you already understand the power that it provides. If you are new to containers, then I highly recommend you learn about them.</p>
<p>In brief, you write a simple manifest called a Dockerfile. That manifest describes an intended operating system and some instructions needed to build an ideal environment for running your application. An example manifest is available in the GitHub repository here: <a href="https://github.com/PacktPublishing/Web-Development-with-Sanic/blob/main/chapters/08/k8s/Dockerfile">https://github.com/PacktPublishing/Web-Development-with-Sanic/blob/main/chapters/08/k8s/Dockerfile</a>.</p>
<p>This might include installing some dependencies (including Sanic), copying source code, and defining a command that will be used to run your application. With that in place, docker then builds a single image with everything needed to run the application. That image can be uploaded to a repository and used to run irrespective of the environment. You could, for example, opt to use this instead of managing all those separate VM environments. It is much simpler to bundle all that together and simply run it.</p>
<p>There is still some complexity involved in building our new versions and deciding where to run the image, but having consistent builds is a huge gain. This should really become a focal point of your deployment. So, although containers are part of the solution, there still is the problem of where to run it and the maintenance costs required to keep it running and up to date.</p>
<p>I almost <em>always</em> would recommend using Docker as part of your deployment practices. And if you know about Docker Compose, you might be thinking that is a great choice for managing deployments. I would agree with you, so long as we are talking about deployments on your local machine. Using Docker Compose for production is not something I would usually consider. The reason is simple: horizontal scaling. Just like the issue with running Sanic on a VM, or a single container on a VM, running Docker Compose on a single VM carries the same problem: horizontal scaling. The fix is orchestration.</p>


<h4 data-number="9.5.2.3">Container orchestration with Kubernetes</h4>
<p>The problem with containers is that they only solve the environmental problems by creating a consistent and repeatable strategy for your application. They still suffer from scalability problem. Again, what happens when your application needs to scale past the resources that are available on a single machine? Container orchestrators like Kubernetes (aka “K8S”) are a dream come true for anyone that has done DevOps work in the past. By creating a set of manifests, you will describe to Kubernetes what your ideal application will look like: the number of replicas, the number of resources they need, how traffic should be exposed, and so on. That is it! All you need to do is describe your application with some YAML files. Kubernetes will handle the rest. It has the added benefit of enabling rolling deployments where you can rollout new code with zero downtime for your application.</p>
<p>The downside, of course, is that this option is the most complex. It is suitable for more serious applications where the complexity is acceptable for the benefits added. It may, however, be overkill for a lot of projects. This is a go-to deployment strategy for any application that will have more than a trivial amount of traffic. Of course, the complexity and scale of a K8S cluster can expand based upon its needs. This dynamic quality is what makes it increasingly a standard deployment strategy that has been adopted by many industry professionals.</p>
<p>It is an ideal solution for platforms that consist of multiple services working together, or that require scaling beyond the boundaries of a single machine.</p>
<p>This does bring up an interesting question, however. We know that Sanic has the ability to scale horizontally on a single host by replicating its workers in multiple processes. Kubernetes is capable of scaling horizontally by spinning up replica <strong>pods</strong>. Let’s say you hypothetically have decided that you need four instances of your application to handle the load. Should you have two pods each running two workers or four pods each with one worker?</p>
<p>I have heard both put forth as <em>ideal</em> solutions. Some people say that you should maximize the resources per container. Other people say that you should have no more than one process per container. From a performance perspective, it is a dead heat. The solutions effectively perform the same. Therefore, it comes down entirely to the choice of the application builder. There is no right or wrong answer.</p>
<p>Later in this chapter, we will take a closer look at what it takes to launch a Sanic application with Kubernetes.</p>


<h4 data-number="9.5.2.4">Platform as a service</h4>
<p>Heroku is probably one of the most well-known <strong>PAAS</strong> offering. It has been around for a while and has become an industry leader in these low-touch deployment strategies. Heroku is not the only provider, both Google and AWS have PAAS services in their respective cloud platforms, and Digital Ocean has also launched their own competing service. What makes PAAS super convenient is that all you need to do is write the code. There is no container management, environment handling, or deployment struggles. It is intended to be a super easy low-touch solution for deploying code. Usually, deploying an application is as simple as pushing code is to a git repository.</p>
<p>This simple option is, therefore, ideal for proof-of-concept applications or other builds you need to deploy super quickly. I also do know plenty of people that run more robust and scalable applications through these services, and they really can be a great alternative. The huge selling point of these services is that by outsourcing the deployment, scaling, and service maintenance to the service provider, you are freed up to focus on the application logic.</p>
<p>Because of this simplicity, and ultimately flexibility, we will take a closer look at launching Sanic with a PAAS vendor later in this Chapter in the <em>Deployment examples</em> section. One of the things that is great about a PAAS is that it handles a lot of details like setting up a TLS certificate and enabling a <code>https://</code> address for your application. In the next section, however, we will learn what it takes to set up an <code>https://</code> address for your application in the absence of convenience from a PAAS.</p>




<h2 data-number="9.6">Securing your application with TLS</h2>
<p>If you are not encrypting traffic to your web application, you are doing something wrong. In order to protect information while it is in transit between the web browser and your application, it is an absolute necessity to add encryption. The international standard for doing that is known as TLS (which stands for Transport Layer Security). It is a protocol for how data can be encrypted between two sources. Often, however, it will be referred to as <em>SSL</em> (which is an earlier protocol that TLS replaces) or <em>HTTPS</em> (which is technically an implementation of TLS, not TLS itself). Since it is not important for us <em>how</em> it works, and we only care that it does what it needs to do, we will use these terms somewhat interchangeably. Therefore, it is safe for you to think about TLS and HTTPS as the same thing.</p>
<p>So, what is it? The simple answer is that you request a pair of keys from some reputable source on the Internet. Your next step is to make them available to your web server, and expose your application over a secure port–typically, that is port 443. After that, your web server should handle the rest, and you should now be able to access your application with an <code>https://</code> address instead of <code>http://</code>.</p>

<h3 data-number="9.6.1">Setting up TLS in Sanic</h3>
<p>There are two common scenarios you should be familiar with. If you are exposing your Sanic application directly, or if you are placing Sanic behind a proxy. This will determine where you want to <em>terminate</em> your TLS connection. This simply means where you should set up your public-facing certificates. We will assume for now that Sanic is exposed directly. We also will assume that you already have certificates. If you do not know how to obtain them, don’t worry, we will get to a potential solution for you in the next section.</p>
<p>All we need to do is to tell the Sanic server how to access those certificates. Also, since Sanic will default to port <code>8000</code>, we need to make sure to set it to <code>443</code>.</p>
<ol>
<li><p>With this in mind, our new runtime command (in production) will be this:</p>
<pre><code>$ sanic \
    --host=0.0.0.0 \
    --port=443 \
    --cert=/path/to/cert \
    --key=/path/to/keyfile \
    --workers=4 \
    path.to.server:app</code></pre></li>
<li><p>It is largely the same operation if you are using <code>app.run</code> instead:</p>
<pre><code>ssl = {&quot;cert&quot;: &quot;/path/to/cert&quot;, &quot;key&quot;: &quot;/path/to/keyfile&quot;}
app.run(host=&quot;0.0.0.0&quot;, port=443, ssl=ssl, workers=4)</code></pre></li>
</ol>
<p>When you are exposing your Sanic application directly, and therefore terminating your TLS with Sanic, there is often a desire to add HTTP to HTTPS redirect. For your users’ convenience, you probably want them to always be directed to HTTPS and for this redirection to happen <em>magically</em> for them without having to think about it.</p>
<p>The Sanic User Guide provides us with a simple solution that involves running a second Sanic application inside our main app. Its only purpose will be to bind to port 80 (which is the default HTTP non-encrypted port) and redirect all traffic. Let’s quickly examine that solution and step through it:</p>
<ol>
<li><p>First, in addition to our main application, we need a second that will be responsible for the redirects. So, we will set up two applications and some configuration details:</p>
<pre><code>main_app = Sanic(&quot;MyApp&quot;)
http_app = Sanic(&quot;MyHTTPProxy&quot;)

main_app.config.SERVER_NAME = &quot;example.com&quot;
http_app.config.SERVER_NAME = &quot;example.com&quot;</code></pre></li>
<li><p>We add only one endpoint to the <code>http_app</code> that will be responsible for redirecting all traffic to the <code>main_app</code>.</p>
<pre><code>@http_app.get(&quot;/&lt;path:path&gt;&quot;)
def proxy(request, path):
    url = request.app.url_for(
        &quot;proxy&quot;,
        path=path,
        _server=main_app.config.SERVER_NAME,
        _external=True,
        _scheme=&quot;https&quot;,
    )
    return response.redirect(url)</code></pre></li>
<li><p>To make running the HTTP redirect application easier, we will just piggyback off of the main application’s lifecycle so that there is not a need to create another executable. Therefore, when the main application starts up, it will also create and bind the HTTP application:</p>
<pre><code>@main_app.before_server_start
async def start(app, _):
    app.ctx.http_server = await http_app.create_server(
        port=80, return_asyncio_server=True
    )
    app.ctx.http_server.app.finalize()</code></pre>
<p>You should note how we are assigning that server to the <code>ctx</code> for our main application so we can use it again.</p></li>
<li><p>Finally, when the main application shuts down, it will also be responsible for shutting down the HTTP application:</p>
<pre><code>@main_app.before_server_stop
async def stop(app, _):
    await app.ctx.http_server.close()</code></pre></li>
</ol>
<p>With this in place, any request to <code>http://example.com</code> should be automatically redirected to the <code>https://</code> version of the same page.</p>
<p>Back in Step 1 and Step 2, this example sort of skipped over the fact that you need to obtain actual certificate files to be used to encrypt your web traffic. This is largely because you need to bring your own certificates to the table. If you are not familiar with <em>how</em> to do that, the next section provides a potential solution.</p>


<h3 data-number="9.6.2">Getting and renewing a certificate from Let’s Encrypt</h3>
<p>Back in the olden days of the Internet, if you wanted to add HTTPS protection to your web application, it was going to cost you. Certificates were not cheap, and they were somewhat cumbersome and complicated to manage. Actually, certificates are still not cheap if you are to buy one yourself, especially if you want to buy a certificate that covers your subdomains. However, this is no longer your only option since several players came together looking for a method to create a safer online experience. The solution: free TLS certificates. These free (and reputable) certificates are available from <em>Let’s Encrypt</em>, and are the reason that <em>every</em> production website should be encrypted. Expense is no longer an excuse. At this point in time, if I see a website still running <code>http://</code> in a live environment, a part of me cringes as I go running for the hills.</p>
<p>If you do not currently have a TLS certificate for your application, head over to <a href="https://letsencrypt.org">https://letsencrypt.org</a> to get one. The process to obtain a certificate from <em>Let’s Encrypt</em> requires you to follow some basic steps, and then prove that you own the domain. Because there are a lot of platform specifics, and it is outside the scope of this book, we will not really dive into the details of how to obtain one. Later on, this chapter does go through a step-by-step process to obtain a Let’s Encrypt certificate for use in a Kubernetes deployment in the <em>Kubernetes (as-a-service)</em> section.</p>
<p>I do, however, highly encourage you to use Let’s Encrypt if the budget for your project does not allow for you to go out and purchase a certificate.</p>
<p>With a certificate in hand, it is finally time to look at some actual code and decide which deployment strategy is right for your project.</p>



<h2 data-number="9.7">Deployment examples</h2>
<p>Earlier when discussing the various choices for deployment strategies, two options rose above the others: PAAS, and Kubernetes. When deploying Sanic into production, I would almost always recommend one of these solutions. There is no hard and fast rule here, but I generally think of Kubernetes as being the go-to solution for platforms that will be running multiple services, have the need for more controlled deployment configurations, and have more resources and a team of developers. On the other hand, PAAS is more appropriate for single developer projects or projects that do not have resources to devote to maintaining a richer deployment pipeline. We will now explore what it takes to get Sanic running in these two environments.</p>

<h3 data-number="9.7.1">Platform-as-a-service</h3>
<p>As we stated before, Heroku is a well-known industry leader in deploying applications via PAAS. This is for good reason as they have been in business providing these services since 2007, and have played a critical role in popularizing the concept. They have made the process super simple for both new and experienced developers. However, in this section, we are going to instead take a look at deploying a Sanic application with Digital Ocean’s PAAS offering. The steps should be nearly identical and applicable to Heroku, or any of the other services that are out there:</p>
<ol>
<li>First, you need to, of course, go to their website and signup for an account if you do not have one. Their PAAS is called <strong>Apps</strong>, which you can find on the left-hand side of their main dashboard once you are logged in.</li>
<li>You will next be taken through a series of steps that will ask you to connect a git repository.</li>
<li><p>You will next need to configure the app through their UI. Your screen will look probably something like this:</p>
<figure>
<img src="img/file9.png" alt="Figure 8.1 - Example settings for PAAS setup" /><figcaption aria-hidden="true">Figure 8.1 - Example settings for PAAS setup</figcaption>
</figure>
<p>A very important thing to note here is that we have set the <code>--host=0.0.0.0</code>. This means that we are telling Sanic that it should bind itself to any IP address that Digital Ocean provides it. Sanic will bind itself to the <code>127.0.0.1 </code>address without this configuration. As anyone that has done web development knows, the 127.0.0.1 address maps to localhost on most computers. This means that Sanic will be accessible only to web traffic on that specific computer. This is no good. If you ever deploy an application and cannot access it, one of the first things to check is that the port and host are set up properly. One of the easiest options is to just use <code>0.0.0.0</code>, which is like the equivalent of a wildcard IP address.</p></li>
<li>Next, you will be asked to select a location for which data center it will live in. Usually, you want to pick one that will be close to where your intended audience will be to reduce latency.</li>
<li>You will then need to select an appropriate package. If you do not know what to choose, start small and then scale it up as needed.</li>
<li>The only thing left to do is to setup the files in our repository. There is a sample in GitHub for you to follow: <a href="https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/08/paas">https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/08/paas</a>.</li>
<li>Finally, we need a <code>requirements.txt</code> file that lists out our dependencies: sanic and a <code>server.py</code> just like every other build we have done so far.</li>
</ol>
<p>Once that is done, every time you push to the repository, your application should be rebuilt and available to you. One of the nice benefits of this is that you will get a TLS certificate with https out of the box. No configuration is needed.</p>
<p>Seems simple enough? Let’s look at a more complex setup with Kubernetes.</p>


<h3 data-number="9.7.2">Kubernetes (as-a-service)</h3>
<p>We are going to turn our attention to Kubernetes: one of the most widely adopted and utilized platforms for orchestrating the deployment of containers. You could, of course, spin up some virtual machines, install Kubernetes on them, and manage your own cluster. However, I find a much more worthwhile solution is to just take one of the Kubernetes-as-a-service solutions. You still have all of the power of Kubernetes (which we will use the common abbreviation: K8S), but none of the maintenance headaches.</p>
<p>We will again look at Digital Ocean and use their platform for our example.</p>
<ol>
<li>In our local directory we will need a few files:
<ul>
<li><code>Dockerfile </code>to describe out docker container</li>
<li><code>app.yml</code> a K8S config file described below</li>
<li><code>ingress.yml</code> a K8S config file described below</li>
<li><code>load-balancer.yml</code> a K8S config file described below</li>
<li><code>server.py</code> which is again a Sanic application</li>
</ul>
You can follow along with the files in the GitHub repository: <a href="https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/08/k8s">https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/08/k8s</a>.</li>
<li><p>Our Dockerfile is the set of instructions to build our container. We will take a shortcut and use one of the Sanic community’s base images that has both Python and Sanic pre-installed:</p>
<pre><code>FROM sanicframework/sanic:3.9-latest

COPY . /srv
WORKDIR /srv
EXPOSE 7777

ENTRYPOINT [&quot;sanic&quot;, &quot;server:app&quot;, &quot;--port=7777&quot;, &quot;--host=0.0.0.0&quot;, ]</code></pre>
<p>Just like we saw with the PAAS solution, we are binding to host <code>0.0.0.0</code> for the same reason. We are <em>not</em> adding multiple workers per container here. Again, this is something you could do if you prefer.</p></li>
<li><p>Next, we will need to build an image:</p>
<pre><code>$ docker build -t admhpkns/my-sanic-example-app .</code></pre></li>
<li><p>Let’s try running it locally to make sure it works</p>
<pre><code>$ docker run -p 7000:7777 --name=myapp admhpkns/my-sanic-example-app </code></pre></li>
<li><p>Do not forget to clean up your environment, and remove the container when you are done, like this:</p>
<pre><code>$ docker rm myapp </code></pre></li>
<li><p>And, you will of course need to push your container to some accessible repository. For ease of use and demonstration purposes, I will be pushing it to my public Docker Hub repository:</p>
<pre><code>$ docker push admhpkns/my-sanic-example-app:latest</code></pre></li>
<li><p>For this next part, we will interact with Digital Ocean through their CLI tool. If you do not have it installed, head to <a href="https://docs.digitalocean.com/reference/doctl/how-to/install/">https://docs.digitalocean.com/reference/doctl/how-to/install/</a>. You will want to make sure you login:</p>
<pre><code>$ doctl auth init</code></pre></li>
<li>We need a Digital Ocean K8S cluster. Login to their web portal, click on <strong>Kubernetes</strong> on the main dashboard and set up a cluster. For now, default settings are fine.</li>
<li><p>We next need to enable <code>kubectl</code> (the tool to interact with K8S) to be able to talk to our Digital Ocean K8S cluster. If <code>kubectl</code> is not installed, check out the instructions here: <a href="https://kubernetes.io/docs/reference/kubectl/overview/">https://kubernetes.io/docs/reference/kubectl/overview/</a>. The command you need will look something like this:</p>
<pre><code>$ doctl kubernetes cluster kubeconfig save afb87d0b-9bbb-43c6-a711-638bc4930f7a</code></pre>
<p>Once your cluster is available and <code>kubectl</code> is set up, you can verify it by running:</p>
<pre><code>$ kubectl get pods</code></pre>
<p>Of course, we have not set up anything, so there should not be anything to see just yet.</p></li>
<li><p>When configuring Kubernetes, we need to start by running <code>kubectl apply</code> on our <code>app.yml</code>.</p>
<blockquote>
<p><strong>TIP</strong></p>
<p>Before going further, you will see a lot of online tutorials that use this style of command:</p>
<p><code>$ kubectl create ...</code></p>
<p>I generally try to avoid that in favor of this:</p>
<p><code>$ kubectl apply ...</code></p>
<p>They essentially do the same thing, but the convenience is that resources that are created with apply can be continually modified by “applying” the same manifest over and over again.</p>
</blockquote>
<p>What is in <code>app.yml</code>? Check out the GitHub repository for the full versions. It is rather lengthy and includes some boilerplate that is not relevant to the current discussion, so I will show only relevant snippets here. This goes for all of the K8S manifests in our example. The file should contain the Kubernetes primitives needed to run the application: a <strong>service</strong> and a <strong>deployment</strong>.The service should look something like the following:</p>
<pre><code>spec:
  ports:
    - port: 80
      targetPort: 7777
  selector:
    app: ch08-k8s-app</code></pre>
<p>Notice how we are mapping port <code>7777</code> to <code>80</code>. This is because we will be terminating TLS in front of Sanic and our ingress controller will talk to Sanic over HTTP unencrypted. Because it is all in a single cluster, this is acceptable. Your needs might be more sensitive, and then you should look into encrypting that connection as well.The other thing in <code>app.yml</code> is the deployment, which should look something like the following:</p>
<pre><code>spec:
  selector:
    matchLabels:
      app: ch08-k8s-app
  replicas: 4
  template:
    metadata:
      labels:
        app: ch08-k8s-app
    spec:
      containers:
        - name: ch08-k8s-app
          image: admhpkns/my-sanic-example-app:latest
          ports:
            - containerPort: 7777</code></pre>
<p>Here, we are defining the number of replicas we want, as well as pointing the container to our docker image repository.</p></li>
<li><p>After creating that file, we will apply it, and you should see a result similar to this:</p>
<pre><code>$ kubectl apply -f app.yml
service/ch08-k8s-app created
deployment.apps/ch08-k8s-app created</code></pre>
<p>You can now checkout to see that it worked:</p>
<pre><code>$ kubectl get pods
$ kubectl get svc</code></pre></li>
<li><p>We will next use an off the shelf solution to create an NGINX ingress. This will be the proxy layer that terminates our TLS and feeds HTTP requests into Sanic. We will install it as follows:</p>
<pre><code>$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.0/deploy/static/provider/do/deploy.yaml</code></pre>
<p>Note, at the time of writing, v1.0.0 is the latest. That probably is not true by the time you are reading this, so you may need to change that. You can find the latest version on their GitHub page: <a href="https://github.com/kubernetes/ingress-nginx">https://github.com/kubernetes/ingress-nginx</a>.</p></li>
<li><p>Next, we will setup our ingress. Create an <code>ingress.yml </code>following the pattern in our GitHub repository example.</p>
<pre><code>$ kubectl apply -f ingress.yml</code></pre>
<p>You will notice there are <em>intentionally</em> some lines commented out. We will get to that in a minute. Let’s just quickly verify that it worked:</p>
<pre><code>$ kubectl get pods -n ingress-nginx</code></pre></li>
<li>We should take a step back and jump over to the <strong>Digital Ocean</strong> dashboard. On the left is a tab called <strong>Networking</strong>. Go there and then in the tab for <strong>Domains</strong> follow the procedure to add your own domain there. In that example, in <code>ingress.yml </code>we added <code>example.com </code>as the ingress domain. Whatever domain you add to Digital Ocean’s portal is what should match your ingress. If you need to go back and update and re-apply the <code>ingress.yml</code> file with your domain, do that now.</li>
<li><p>Once that is all configured, we should be able to see our application working:</p>
<pre><code>$ curl http://example.com       
Hello from 141.226.169.179</code></pre>
<p>This is of course not ideal because it is still on <code>http://</code>. We will now get a Let’s Encrypt certificate and set up TLS.</p></li>
<li><p>The easiest method for this is to set up a tool called <code>cert-manager</code>. It will do all of the interfacing we need with Let’s Encrypt. Start by installing it:</p>
<pre><code>$ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.5.3/cert-manager.yaml</code></pre>
<p>Again, please check to see what the most up-to-date version is and update this command accordingly.We can verify its installation here:</p>
<pre><code>$ kubectl get pods --namespace cert-manager</code></pre></li>
<li><p>Next, create the <code>load-balancer.yml</code> following the example in the GitHub repository. It should look something like this:</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  annotations:
    service.beta.kubernetes.io/do-loadbalancer-hostname: example.com
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  type: LoadBalancer
  externalTrafficPolicy: Local
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/component: controller</code></pre></li>
<li><p>Apply that manifest and confirm that it worked:</p>
<pre><code>$ kubectl apply -f load-balancer.yml
service/ingress-nginx-controller configured</code></pre></li>
<li><p>Your K8S cluster will now start the process of obtaining a certificate.</p>
<blockquote>
<p><strong>Tip</strong></p>
<p>One thing that you might encounter is that the process gets stuck while requesting the certificate. If this happens to you, the solution is to turn on <strong>Proxy Protocol</strong> in your <strong>Digital Ocean</strong> dashboard. Go to the following setting and turn this on if you need to:</p>
<p><strong>Networking</strong> &gt; <strong>Load Balancer</strong> &gt; <strong>Manage Settings</strong> &gt; <strong>Proxy Protocol</strong> &gt; <strong>Enabled</strong></p>
</blockquote></li>
<li><p>We’re almost there! Open up that <code>ingress.yml</code> file and uncomment those few lines that were previously commented out. Then <code>apply</code> the file.</p>
<pre><code>$ kubectl apply -f ingress.yml</code></pre></li>
</ol>
<p>Done! You should not automatically have a redirect from <code>http://</code> to <code>https://</code>, and your application is fully protected.</p>
<p>Better yet, you now have a deployable Sanic application with all the benefits, flexibility, and scalability that K8S container orchestration provides.</p>



<h2 data-number="9.8">Summary</h2>
<p>Building a great Sanic application is only half of the job. Deploying it to make our application usable out in the wild is the other half. In this chapter, we explored some important concepts for you to consider. It is never too early to think about deployment as well. The sooner you know which server you will use, and where you will host your application, the sooner you can plan accordingly.</p>
<p>There are of course many combinations of deployment options, and I only provided you with a small sampling. As always, you will need to learn what works for your project and team. Take what you have learned here and adapt it.</p>
<p>However, if you were to ask me to boil all of this information down and ask my personal advice on how to deploy Sanic, I would tell you this:</p>
<ul>
<li>Run your applications using the built-in Sanic server</li>
<li>Terminate TLS outside of your application</li>
<li>For personal or smaller projects, or if you want a simpler deployment option, use a PAAS provider</li>
<li>For larger projects that need to scale and have more developer-resources, use a hosted Kubernetes solution</li>
</ul>
<p>There you have it. You now should be able to build a Sanic application and run it on the Internet. Our time is done, right? You should have the skills and knowledge you need now to go out and build something great, so go ahead and do that now. In the remainder of this book, we will start to look at some more practical issues that arise while building web applications and look at some best-practice strategies in how to solve them.</p>


</body>
</html>
