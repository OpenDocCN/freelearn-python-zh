- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Artificial Intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we saw a collection of scientific Python libraries that
    allow for really fast and easy processing of large data files. In this chapter,
    we will use some of these and a few others for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is a complex subject, and many completely distinct subjects
    within it are entire branches of research by themselves. This should not discourage
    you from diving in, however; many of the libraries mentioned in this chapter are
    really powerful and allow you to get started with a very reasonable amount of
    effort.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that there is a huge difference between applying a pre-trained
    model and generating your own. Applying a model is usually possible in a few lines
    of code and barely requires any processing power; building your own model usually
    takes many lines of code and hours or more to process. This makes the training
    of models outside of the scope of this book in all but the most trivial cases.
    In these cases, you will get an overview of what the library can do with some
    explanation of where this would be useful, without explicit examples.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence is the branch of computer science relating to the study
    of all types of machine learning, which includes neural networks and deep learning,
    Bayesian networks, evolutionary algorithms, computer vision, **natural language
    processing** (**NLP**), and **support-vector machines** (**SVM**s), among others.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to artificial intelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libraries for image processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libraries for NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libraries for neural networks and deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generic AI libraries and utilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to artificial intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we continue with this chapter, we need to establish a few definitions.
    Because **artificial intelligence** (**AI**) is such a broad subject, the lines
    tend to blur a bit, so we need to make sure that we are all talking about the
    same thing.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we define AI as *any algorithm with a human-like ability to solve
    problems*. While I admit that this statement is very broad, any narrower definition
    would exclude valid AI strategies. What is and is not AI is more a philosophical
    question than a technical one. While (almost) anyone would consider a neural network
    to be AI, once you get to algorithms such as (Bayesian) decision trees, not everyone
    agrees anymore.
  prefs: []
  type: TYPE_NORMAL
- en: With that broad definition in mind, here is a list of technologies and terms
    we are going to cover, with a short explanation of what they are and what they
    can do.
  prefs: []
  type: TYPE_NORMAL
- en: Types of AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Within the broad scope of AI, we have two major branches, **machine learning**
    (**ML**) and the rest. Machine learning covers any method that can learn by itself.
    You might wonder, is it even AI if it does not involve learning? This is a bit
    of a philosophical question, but I personally think that there are several non-learning
    algorithms that can still be considered AI because they can produce human-like
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within self-learning systems, we have further distinctions with their own goals
    and applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of one of these does not exclude the others from being used too, so
    many practical implementations use combinations of multiple methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-machine learning systems are quite a bit more diverse because they can
    mean just about anything, so here are a few examples of non-learning algorithms
    that can rival humans in some ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NLP**: It should be noted that NLP by itself does not use ML. Many NLP algorithms
    are still written by hand, because it is far easier for a human to explain to
    a machine how and why certain grammar and semantics work than to have a computer
    figure out the oddities and complexities of human languages. That field is changing
    very rapidly, however, and this might not be the case for much longer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expert systems**: This is the first type of AI that was actually successful
    in practice. The first expert systems were created in 1970 and they have been
    used ever since. These systems work by asking you a string of questions and narrowing
    down a list of potential solutions/answers based on those. You have certainly
    encountered many of these when going through problem-solving wizards at some point,
    perhaps in the FAQ on websites or when calling a helpdesk. These systems allow
    the capturing of expert information and compress it down into a simple system
    that can make decisions. Many of these have been used (and are still used today)
    in diagnosing medical issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we continue with actual AI implementations, it is a good idea to look
    at a few image processing libraries that are used as a basis in many of the AI
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As was the case with installing the scientific Python libraries in *Chapter
    15*, installing the packages in this chapter directly using `pip` can be troublesome
    in some cases. Using one of the Jupyter Docker Stacks or `conda` can be more convenient.
    Additionally, most of these projects have very well-documented installation instructions
    for many scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the neural networks portion of this chapter, it would be best to get a
    notebook stack that has most libraries available. I would recommend giving the
    `jupyter/tensorflow-notebook` stack a test: [https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html#jupyter-tensorflow-notebook](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html#jupyter-tensorflow-notebook).'
  prefs: []
  type: TYPE_NORMAL
- en: Image processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image processing is an essential part of many types of machine learning, such
    as **computer vision** (**CV**), so it is essential that we show you a few of
    the options and their possibilities here. These range from image-only libraries
    to libraries that have full machine learning capabilities while also supporting
    image inputs.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The scikit-image (`skimage`) library is part of the scikit project with the
    main project being scikit-learn (`sklearn`), covered later in this chapter. It
    offers a range of functions for reading, processing, transforming, and generating
    images. The library builds on `scipy.ndimage`, which provides several image processing
    options as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to talk about what an image is in terms of these Python libraries first.
    In the case of `scipy` (and consequently, `skimage`), an **image** is a `numpy.ndarray`
    object with 2 or more dimensions. The conventions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '2D grayscale: Row, column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2D color (for example, RGB): Row, column, color channel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3D grayscale: Plane, row, column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3D color: Plane, row, column, color channel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these are just conventions, however; you can shape your arrays in other
    ways as well. A multichannel image could also mean **CMYK** (**cyan, magenta,
    yellow, and key/black**) colors instead of **RGB** (**red, green, and blue**),
    or something completely different.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally you could have more dimensions as well, such as a dimension for time
    (in other words, video). Since the arrays are regular `numpy` arrays, you can
    manipulate them by slicing as usual.
  prefs: []
  type: TYPE_NORMAL
- en: Often you will not use the scikit-image library for machine learning directly,
    but rather for *pre-processing* image data before you feed it to your machine
    learning algorithms. In many types of detections, for example, color is not that
    relevant, which means you can make your machine learning system three times as
    fast by going from RGB to grayscale. Additionally, there are often fast algorithms
    available to pre-process the data so your machine learning system only needs to
    look at the relevant sections of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Installing scikit-image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The package is easily installable through `pip` for many platforms; I would
    suggest installing not just the base package but the `optional` extras as well,
    which add extra capabilities to scikit-image, such as parallel processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Edge detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s look at how we can display one of the built-in images and do some basic
    processing on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we are using the `coins` dataset that is bundled with `skimage`.
    It contains a few coins and we can use it to display some of the nice features
    of `skimage`. First, let’s look at the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/ph/3d51j84d2gg_pltczn6244q80000gn/T/com.microsoft.Word/Content.MSO/6A6F4AA2.tmp](img/B15882_16_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.1: scikit-image coins'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example of what kind of processing we can do, let’s do some edge detection
    using the Canny edge detection algorithm. This is a prime example of a non-ML
    algorithm that can be really useful for pre-processing your data before you feed
    it to your ML system. To display the results a bit better, first we will slice
    the image so only the top-right three coins are visible. In *Figure 16.1*, the
    numbers indicate the actual pixel indices for the *x* and *y* axes, which can
    be used to estimate where to slice. After that, we will apply the `canny()` function
    to detect the edges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown in the following image, where you can see the auto-detected
    edges of coins we have selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/ph/3d51j84d2gg_pltczn6244q80000gn/T/com.microsoft.Word/Content.MSO/748E410E.tmp](img/B15882_16_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: Coins after edge detection'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-image can do much more, but this is a nice and basic example of how you
    can do edge detection in a single line of code, which can make your data much
    more useful for ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: Face detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now use one of the examples from the fantastic scikit-image documentation:
    [https://scikit-image.org/docs/dev/auto_examples/applications/plot_face_detection.html](https://scikit-image.org/docs/dev/auto_examples/applications/plot_face_detection.html).'
  prefs: []
  type: TYPE_NORMAL
- en: This is a machine learning example that uses a pre-trained model to automatically
    detect faces. The specific model uses a multi-block **local binary pattern** (**LBP**).
    An LBP looks at points surrounding a center point and indicates whether these
    points are greater (lighter) or smaller (darker) than the center point. The multi-block
    part is an optional extension to this method and performs the LBP algorithm across
    multiple block sizes of 9 identically sized rectangles. The first iteration might
    look at a 3x3 pixel square; the second iteration could look at 6x6; the third
    9x9; and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The model was trained using the OpenCV cascade classifier training, which can
    train your model, generate samples, and run the detection. A cascade classifier
    concatenates the results of multiple classifiers to reach a combined model that
    is expected to perform better than the separate classifiers by themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the face detection, we will apply it to a photo of the NASA astronaut
    Eileen Collins. First, we will import the libraries, load the image, and tell
    `matplotlib` to draw it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the code above, you might notice a few magic numbers such as the
    `scale_factor`, `step_ratio`, `min_object_size`, and `max_object_size`. These
    parameters are ones that you will have to tune to your input image. These specific
    numbers are straight from the OpenCV documentation, but depending on your input
    you will need to experiment with these values until they suit your scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Since these parameters are somewhat arbitrary and dependent on your input, it
    can be a good idea to apply a bit of automation to find them. An evolutionary
    algorithm could be useful in helping you find effective parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to start the detection and illustrate what we found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the cascade, we run the model using the `detect_multi_scale`
    method. This method searches for matching objects (faces) with sizes varying from
    `min_size` to `max_size`, which is needed because we don’t know how large the
    subject (face) is. Once we have the matches, we draw a rectangle around them to
    indicate where they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15882_16_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.3: Face detected by scikit-image'
  prefs: []
  type: TYPE_NORMAL
- en: By itself, scikit-image does not have many machine learning features available,
    but the coupling with other libraries is what makes this library very useful for
    machine learning. In addition to the frontal face dataset we loaded above, you
    can also use pre-trained cascades from OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several pre-trained models are available in the OpenCV Git repository: [https://github.com/opencv/opencv/tree/master/data/lbpcascades](https://github.com/opencv/opencv/tree/master/data/lbpcascades).'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-image overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The scikit-image library can do much more than we have covered. Here’s a quick
    overview of a few of the available submodules:'
  prefs: []
  type: TYPE_NORMAL
- en: '`exposure`: Functions for analyzing and fixing photo exposure levels, which
    can be essential for cleaning data before you feed it to your AI system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature`: Feature detection such as the `canny()` edge detection function
    we used earlier. This allows for detecting objects, blobs of content, and more
    to pre-filter your input so you can reduce the processing time needed by your
    AI system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filters`: Image filtering functions, such as thresholding to automatically
    filter noise, and many others. Similar to the `exposure` functions, these can
    be very useful for cleanup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`morphology`: Many functions to sharpen edges, fill sections, find minima/maxima,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`registration`: Functions for calculating the optical flow in an image. With
    these functions, you can estimate what part of the image is moving, and how fast
    objects are moving. Given two images, this can help to calculate the intermediate
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`segmentation`: Functions for segmenting images. In the case of the coins above,
    the separate coins can be extracted and/or labeled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the scikit-image library offers an extensive list of image manipulation
    and processing functions. Additionally, it is well integrated into the scientific
    Python ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The big “competitor” to scikit-image is **OpenCV** (**Open Source Computer Vision
    library**). The OpenCV library is written in C/C++ but has bindings for several
    languages such as Python and Java. The reason I put “competitor” between quotes
    is that these libraries don’t have to compete; you can easily combine the strengths
    of both if you wish to do so, and it is something I have done myself in several
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: We will first look at installing the Python OpenCV package.
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenCV for Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `opencv-python` package comes in several variants depending on your needs.
    Besides the main OpenCV package, OpenCV also has many “contrib” and “extra” packages,
    which can be very useful. The contrib packages are mainly for following tutorials
    and trying examples, and the extra modules contain many useful additional algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of extra modules can be found in the documentation: [https://docs.opencv.org/5.x/](https://docs.opencv.org/5.x/).'
  prefs: []
  type: TYPE_NORMAL
- en: I strongly recommend installing the extra modules as well, since many very useful
    modules are part of the extra package.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have the following options if you are installing the package on a desktop
    machine where you will be using a GUI:'
  prefs: []
  type: TYPE_NORMAL
- en: '`opencv-python`: The main modules, the bare minimum'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`opencv-contrib-python`: The full package including the main modules from the
    `opencv-python` package, but also the contrib and extra modules'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For servers that are not running a GUI, you have these options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`opencv-python-headless`: Beyond not including any GUI output functions such
    as `cv2.imshow()`, this is identical to `opencv-python`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`opencv-contrib-python-headless`: As above, this is the headless version of
    `opencv-contrib-python`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have OpenCV installed, let’s see if we can replicate the Canny edge
    detection from scikit-image using OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Edge detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s look at how we can perform the Canny algorithm using OpenCV, similar
    to what we did in the scikit-image example earlier. The Canny algorithm is not
    part of the OpenCV core, so you need to install the `opencv-contrib-python` package
    for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the same coins image as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: At a first glance the code looks quite similar, but there are a few differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the `cv2.Canny()` function requires two extra parameters: `threshold_1`
    and `threshold_2`, or the lower and upper bounds. These parameters decide what
    should be considered noise and what parts are relevant for the edges. By increasing
    or decreasing these values, you can get finer details in the resulting edges,
    but doing so means the algorithm can also start wrongly detecting the background
    gradient as edges, which is already happening at the top right of the output image
    (*Figure 16.4*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'While you can pass these along to scikit-image if you wish, by default scikit-image
    automatically guesses some suitable parameters for you. With OpenCV you could
    easily do the same, but this is not included by default. The algorithm that scikit-image
    uses for this estimation can be seen in the source: [https://github.com/scikit-image/scikit-image/blob/main/skimage/feature/_canny.py](https://github.com/scikit-image/scikit-image/blob/main/skimage/feature/_canny.py)
    Second, OpenCV has no native support for Jupyter, so we are using `matplotlib`
    to render the output. Alternatively, we could also use the `IPython.display` module
    to display the image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The generated output is similar, however:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/ph/3d51j84d2gg_pltczn6244q80000gn/T/com.microsoft.Word/Content.MSO/9AF685DE.tmp](img/B15882_16_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.4: OpenCV Canny'
  prefs: []
  type: TYPE_NORMAL
- en: For more similar output, you could even use scikit-image to render the output
    from OpenCV. Since they both operate on `numpy` arrays, you can easily mix and
    match the functions if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the scikit-image face detection example, we were actually using an OpenCV-generated
    model, so we could easily use that model with `opencv-python` directly, with a
    few small changes:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of `skimage.feature.Cascade(filename)`, you need to use `cv2.CascadeClassifier(filename)`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of `cascade.detect_multi_scale()` the function is called `cascade.detectMultiScale()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This immediately illustrates one of the differences between scikit-image and
    `python-opencv`. Where scikit-image uses the Python convention of underscores
    between words in a function name, `opencv-python` uses the camelCase function
    names directly from the OpenCV source.
  prefs: []
  type: TYPE_NORMAL
- en: With OpenCV we can easily go beyond the simple cascades we used for face detection;
    this time we will use a **DNN** (**deep neural network**) instead.
  prefs: []
  type: TYPE_NORMAL
- en: The network we will be using is called **YOLOv3** (**You Only Look Once, version
    3**) and is able to detect many types of objects such as cars, animals, fruit,
    and many more. Naturally this model is far larger as well. The face detection
    model was only about 50 KiB, while the YOLOv3 network is nearly 5000 times larger,
    at 237 MiB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can start, we need to download a few files for the YOLO network to
    be fully functional:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model (237 MiB): [https://pjreddie.com/media/files/yolov3.weights](https://pjreddie.com/media/files/yolov3.weights)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The YOLO configuration file: [https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg](https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The names for the objects: [https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names](https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you have those files, we can demonstrate the YOLO network. First, we set
    up a few imports and variables, and then load the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the imports ready and the image converted to a blob that’s
    suitable for the model, we can load the model and show the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For brevity this example is very condensed, but it shows you how you can do
    something as advanced as object detection in just a few lines. If we look at the
    output, the deep neural network has correctly identified the astronaut as being
    a person:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/ph/3d51j84d2gg_pltczn6244q80000gn/T/com.microsoft.Word/Content.MSO/1F838D9C.tmp](img/B15882_16_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.5: Object detection on astronaut'
  prefs: []
  type: TYPE_NORMAL
- en: 'I highly encourage you to try the YOLOv3 network yourself with different images.
    For an old image of a street, I got the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15882_16_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.6: Applying YOLOv3 on an image of a street with cars'
  prefs: []
  type: TYPE_NORMAL
- en: Isn’t it amazing how easy it is to do object detection these days and how well
    it works? If you take a good look at the image, you might notice that it’s even
    detecting cars and people that are partially obstructed. Training a new deep neural
    network and doing the research for it is a completely different question, of course,
    but at least applying these networks has become child’s play and they execute
    well within a second, including the loading of the network.
  prefs: []
  type: TYPE_NORMAL
- en: The possibilities certainly don’t end here, and you could even use techniques
    like these to do real-time analysis of a video stream if you wanted. The OpenCV
    library really is an impressive bit of software.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV versus scikit-image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both scikit-image and OpenCV have their own advantages over the other. This
    is one of the cases where you don’t really have to choose, however; you can easily
    use both simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my opinion, OpenCV has three major advantages over scikit-image:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV has native support for processing using your GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since it is implemented in C++, you can do parallel processing in threads without
    having to worry about the GIL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCV has even more features than scikit-image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naturally, scikit-image has a few advantages as well:'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-image is written in Python so it is very easy to view (or modify) the
    algorithms right from your editor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-image is focused toward Python, so the naming conventions feel more natural.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As scikit-image is only for Python, all documentation is immediately relevant.
    With OpenCV, many of the examples you will find on the web (and in the documentation)
    are about the C++ interface, which is slightly different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need high performance for the live processing of video streams, then
    OpenCV would be my top recommendation because it has several methods built in
    to make that task a bit easier. If you simply need to read and modify some images
    and you can get away with scikit-image, then that would be my top recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: In either case, both libraries are great and I can confidently recommend both.
    If your needs span across both, use both.
  prefs: []
  type: TYPE_NORMAL
- en: Now it is finally time to start discussing the artificial intelligence libraries
    themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**NLP** is the process of parsing text and understanding its meaning. This
    can be used to extract knowledge from pieces of text, understand the differences
    between texts, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: There are several well-developed libraries available for this purpose that work
    quite well. Additionally, there are also hosted pre-trained networks available
    such as the **GPT-3** network, which can be accessed through the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: This network can generate text of such high quality that it is often indistinguishable
    from human-generated text.
  prefs: []
  type: TYPE_NORMAL
- en: NLTK – Natural Language Toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLTK is not really a machine learning library by itself like most of the other
    libraries here, but it’s the basis for many natural language processing libraries.
    The NLTK project started in 2001 with the purpose of understanding natural languages,
    and definitely deserves a place in this list.
  prefs: []
  type: TYPE_NORMAL
- en: The project comes bundled with a large collection of corpora and pre-trained
    models for many different languages.
  prefs: []
  type: TYPE_NORMAL
- en: Corpora are large collections of structured texts that can be used for training
    and testing models.
  prefs: []
  type: TYPE_NORMAL
- en: Using these corpora and models, it can do sentiment analysis, tokenize the text
    to find the relevant keywords, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install `nltk`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As a basic example, let’s use the pre-trained sentiment analysis capability
    to see how positive or negative a sentence is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We start by downloading the pre-trained model for sentiment analysis. After
    that, we can use the `SentimentIntensityAnalyzer` to detect if a sentence is negative,
    neutral, positive, or a combination.
  prefs: []
  type: TYPE_NORMAL
- en: The library can do much more, but this already gives you a nice indication of
    how easy it is to get started. If you need any basic human input parsing, make
    sure to give it a try as it offers very impressive results.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy – Natural language processing with Cython
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The spaCy library is a very impressive and extremely fast NLP library. It comes
    with many pre-trained neural network models for over 60 languages and does a very
    good job at text classification and named entity recognition.
  prefs: []
  type: TYPE_NORMAL
- en: The documentation is amazing and, while being fully open-source, it is developed
    by the company Explosion, which is doing a really good job of keeping up with
    the latest developments in NLP. If you want a high-level understanding of text,
    this library is one of your best options. If you only need basic text tokenization,
    then I would still recommend `NLTK` because it is faster and more effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we continue with the example, we need to install spaCy and download
    the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `en_core_web_sm` dataset is a small and fast English dataset. If you need
    a more thorough dataset, you can download `en_core_web_trf` instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install a different language, I recommend you visit the spaCy website: [https://spacy.io/usage#quickstart](https://spacy.io/usage#quickstart).
    For example, the Dutch dataset is called `nl_core_news_sm`, as opposed to `nl_core_web_sm`,
    which you might have been expecting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have that taken care of, let’s try to extract some information
    from a sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: After loading `spacy` and the `en_core_web_sm` model, we added the `merge_entities`
    pipe. This pipe automatically merges the tokens together so we get `"Guido van
    Rossum"` instead of `"Guido"`, `"van"`, and `"Rossum"` as separate tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Isn’t this an amazing result? It automatically understands that `"Guido van
    Rossum"` is a person, `"Stichting Mathematisch Centrum"` is an organisation, and
    `"Amsterdam"` is a geopolitical entity.
  prefs: []
  type: TYPE_NORMAL
- en: Gensim – Topic modeling for humans
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Gensim library ([https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/))
    takes care of NLP for you. It is similar to NLTK but more focused on the modern
    machine learning libraries. It is well documented and easy to use and can be used
    to calculate similarities between texts, analyze the topic of a piece of text,
    and more. While there is a large overlap between NLTK and Gensim, I would argue
    that Gensim is a bit of a higher-level library and slightly easier to get started
    with. NLTK, on the other hand, has existed for over 20 years and has a huge amount
    of documentation available in the wild because of that.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is the branch of artificial intelligence that can learn by
    itself. This can be fully autonomous learning, learning based on pre-labeled data,
    or a combination of these.
  prefs: []
  type: TYPE_NORMAL
- en: We need a little bit of background information before we can dive into the libraries
    and the examples for this subject. Feel free to gloss over this section and jump
    straight to the libraries if you are already familiar with the types of machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Types of machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have briefly covered in the introduction, machine learning roughly splits
    up into three different methodologies, but often uses a combination of several.
    To recap, we have the following three major branches:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naturally, there are many combinations of these, so we will discuss a few important
    distinct types of learning that are based on the branches above. The names themselves
    should already give you a hint about how they function, but we will dive deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of supervised learning, we provide the system with a lot of labeled
    data so the machine can learn the relationship between the input data and the
    labels. Once it has been trained on that data, we can test using new data to see
    if it works. If the results are not as expected, parameters or intermediate training
    steps are tuned until the results improve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of these are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification models where the models are trained on a large number of photos
    to recognize the objects in the photo. Or to answer a question such as: “Are we
    looking at a bird?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis of text. Is the person writing the message happy, sad, hostile,
    and so on?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weather prediction. Since we have a huge amount of historical weather data available,
    this is a perfect case for supervised learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have the data available, this will probably be your best option. In many
    cases, however, you either don’t have the data or you have data without high-quality
    labels. That is where the other learning methods come in.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reinforcement learning is similar to supervised learning, but instead of using
    labeled input/output pairs it uses a **scoring** or **reward function** to provide
    feedback. The parameter that has to be tuned with reinforcement learning is whether
    to re-use existing knowledge or to investigate a new solution. Leaning too heavily
    toward re-use will result in a “local optimum,” where you will never get the best
    (or even a good) result because you get stuck on your previously found solution.
    Leaning too much toward investigation/exploration of new solutions, however, results
    in never reaching an optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of these are:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating solvers/players for games. For a game such as Go or Chess, you could
    use win/lose as a scoring function. For a game such as Pong or Tetris, you could
    use the score as the reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robot navigation systems. As a scoring system, you could use “distance moved
    from origin” combined with “not hitting a wall.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swarm intelligence. These are systems with many (a swarm) of independent, self-organizing
    systems that need to reach a common goal. As an example, some online supermarkets
    use swarms of robots to automatically fetch and package groceries with this method.
    The swarm intelligence takes care of collision avoidance and automatically replacing
    defective robots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning is the next best option after supervised learning, because
    it doesn’t require a large amount of high-quality data. You can combine these
    methods quite well, though. Creating a good scoring function can be difficult,
    and you can easily verify your function by testing it on known good data.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By the name alone, you might be confused by unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: After all, how would an unsupervised system work if it has no idea when it has
    reached a useful solution? The point is that with unsupervised learning you don’t
    know what the solution will look like in the end, but you can declare how a solution
    *could* look.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the explanation of unsupervised learning is a bit vague, I hope some
    examples help:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms. With clustering, you feed the algorithm data with a lot
    of variables (for example, in the case of people, weight, height, gender, and
    so on) and tell the algorithm to find clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection. This is also an area where unsupervised learning can really
    shine. With anomaly detection, you never know what you are really looking for,
    but any patterns that are out of the ordinary could be important.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning is quite a different type of method from the other two
    machine learning methods we covered earlier because there is often no known target.
    However, that does not make it useless by any means. Finding patterns in seemingly
    random data can be really useful in uptime/stability monitoring or visitor analysis
    for e-commerce websites, among other things.
  prefs: []
  type: TYPE_NORMAL
- en: Now it’s time to look at combinations of the previous methods.
  prefs: []
  type: TYPE_NORMAL
- en: Combinations of learning methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AI development is as active as it has ever been and I expect the field to keep
    growing in the foreseeable future. That is why more and more variants of algorithms
    are being used, which causes these clear-cut definitions to become more flexible
    all the time.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, for example, you can get much better results by combining supervised
    and reinforcement learning together than you could by using either of these methods
    alone. That is why the lines between all of these methods can be extremely blurry,
    and if a method works for your goal, it is not wrong to combine them.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most effective examples of machine learning is deep learning. This
    type of machine learning has become extremely popular over the last few years
    because it has proven to be one of the most effective types of neural networks
    in practical applications, in some cases even outperforming human experts.
  prefs: []
  type: TYPE_NORMAL
- en: This type of network is called **deep** because the neural network has multiple
    (often many) hidden internal layers, while traditional neural networks usually
    only have a single or a few hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond that, it is just a regular neural network and can be supervised, unsupervised,
    reinforcement, or anything in between.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neural networks and deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When thinking about AI, most people will immediately think of **artificial neural
    networks** (**ANNs**). These networks are an attempt to mimic the workings of
    animal brains by having artificial neurons and connections between them similar
    to synapses.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few key differences, however. In an animal brain, a neuron can function
    both as input and output, whereas with an ANN there are usually a set of input
    neurons in an input layer, a set of neurons as an output layer, and the middle
    layer(s) that handles the processing.
  prefs: []
  type: TYPE_NORMAL
- en: Currently (in 2021; it was launched in June 2020) by far the most impressive
    ANN is the GPT-3 network, which has been trained for NLP. It has an incredible
    175 billion machine learning parameters and in some cases the text it generates
    is indistinguishable from human-written text.
  prefs: []
  type: TYPE_NORMAL
- en: This text is likely to be outdated quite soon, however. The GPT-3 network is
    already 100 times bigger than GPT-2, which was released in 2019\. GPT-4 has already
    been announced and is supposed to be about 500 times larger than GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that while ANNs (and especially deep learning) networks are
    very powerful and can be self-learning, many of them are static. After they have
    been trained once, they do not improve or update anymore.
  prefs: []
  type: TYPE_NORMAL
- en: The libraries in this section are made to build neural networks and to enable
    deep learning. Since this is an entirely distinct field in AI, it really deserves
    its own section. Note that you can still mix and match AI strategies if needed,
    of course.
  prefs: []
  type: TYPE_NORMAL
- en: Within Python, there are multiple large libraries for creating neural networks,
    but the biggest ones by far are **PyTorch** and **TensorFlow/Keras**. Until a
    few years ago, there was another large library with similar features called Theano.
    That library has since been discontinued and forked under a new name, Aesara.
    Neither of these is used very often these days, but Theano is considered to be
    the original Python neural network library. The TensorFlow library was actually
    created to replace Theano within Google.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The basis of an ANN is the tensor. Tensors are a mathematical representation
    for your data with descriptions of valid transformations that can be applied to
    this data. The actual story is much more complicated, of course, but for the purposes
    of the discussion here you can think of a tensor as a multi-dimensional array
    very similar to the `numpy.ndarray` object we have seen in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: When people talk about a 0-dimensional or 0D Tensor, they are effectively talking
    about a single number. Going up from that, a 1D tensor is an array or vector,
    and a 2D tensor is a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The big takeaway for now is that the difference between a regular number/array/matrix
    and a tensor is that the tensors specify what transformations are valid on them
    as well. It is basically the difference between a `list()` and a custom `class`
    that contains the data for the `list()` but has additional properties as well.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch – Fast (deep) neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch is a library developed by Facebook and focuses on building neural networks,
    such as deep learning networks, using tensors.
  prefs: []
  type: TYPE_NORMAL
- en: The tensors in PyTorch use a custom data structure (instead of `numpy.ndarray`)
    for performance reasons. The PyTorch library is heavily optimized for performance
    and it has built-in support for GPU acceleration for further speedups.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases you can use `torch.Tensor` as a drop-in replacement for `numpy.ndarray`
    to enable GPU acceleration. The `torch.Tensor` API is largely identical to the
    `numpy.ndarray` API.
  prefs: []
  type: TYPE_NORMAL
- en: The real strength of PyTorch (besides the performance) is the number of utility
    libraries included for different kinds of inputs. You can easily use it to process
    images, video, audio, and text using these APIs, and most processes can easily
    be run in parallel in a distributed fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a little overview of the most useful modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.distributed`: For parallel training across multiple GPUs in a single
    system or across multiple systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torchaudio`: For processing audio, either from pre-recorded files or straight
    from (multiple) microphones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torchtext`: For processing text; you can also combine this with NLP libraries
    such as NLTK.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torchvision`: For processing images and sequences of images (videos).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torchserve`: For setting up a server that hosts your models so you can build
    a service that runs your calculations. This is useful because starting a process
    and loading the model can be a slow and heavy task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.utils`: Contains many useful utility functions, but above all, TensorBoard.
    With TensorBoard, you can interactively (through a web interface) inspect your
    models and make changes to your model parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It’s time for a small example, but before we can get started we need to install
    both `pytorch` and `torchvision`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We will use the pre-trained **Mask R-CNN** model to do object recognition. This
    is a **region-based convolutional neural network** (**R-CNN**) that has been trained
    using a combination of images and labeled image masks (object outlines).
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are well suited for visual applications such as image classification and
    image segmentation. They can also be applied to other types of problems such as
    NLP as well.
  prefs: []
  type: TYPE_NORMAL
- en: The R-CNN is a specialized version of the CNN specifically for computer vision
    tasks such as object detection. R-CNN tasks are trained by specifying the **region
    of interest** (**ROI**) in a set of images. The Mask R-CNN is a specialization
    that specifies the ROI not as a rectangle but as a mask that only highlights the
    specific object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’ll do some object recognition using PyTorch. First, we load the photo
    and imports and convert the photo into a tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The conversion to a tensor can be done using the `ToTensor` transform operation.
    The `torchvision.transforms` module has many more operations available, such as
    resizing, cropping, and color normalization, to pre-filter the images before we
    send them to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up is the loading of the model and the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The label file is available on this book’s GitHub page.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the model itself is bundled with PyTorch. After loading the
    model and setting it to `eval` mode (as opposed to training), we can quickly apply
    the model to our image. The labels are unfortunately not bundled, so we need to
    fetch those ourselves. Now we need to display the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can display the matches and their bounding boxes to get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/ph/3d51j84d2gg_pltczn6244q80000gn/T/com.microsoft.Word/Content.MSO/B561F30E.tmp](img/B15882_16_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.7: Street in Amsterdam with objects labeled by PyTorch'
  prefs: []
  type: TYPE_NORMAL
- en: With just a few lines of code, we managed to create an object recognizer that
    correctly identified a few cars, bicycles, and a boat.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the model actually recognized far more objects in the image, but
    we filtered out small matches so the image is not too busy. It actually recognized
    seven more cars, four people, and two boats.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Lightning and PyTorch Ignite – High-level PyTorch APIs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The PyTorch Lightning and PyTorch Ignite libraries are convenient shortcuts
    for getting your network up and running with fewer steps and several useful features
    built in. You can do the same with PyTorch directly, but using the utility functions
    you can run several PyTorch steps at once, meaning less repetition while working.
  prefs: []
  type: TYPE_NORMAL
- en: These libraries were created independently, but serve roughly the same goal
    and are comparable in features. It depends on your personal preference as to which
    is the best for you. I would initially recommend you start with PyTorch directly,
    however. While these libraries are really great, it is important to understand
    the underlying principles before you start using shortcuts that you might not
    completely understand. The PyTorch documentation is quite easy to follow and largely
    identical in workings to PyTorch Ignite and PyTorch Lightning, besides being a
    bit more verbose.
  prefs: []
  type: TYPE_NORMAL
- en: Skorch – Mixing PyTorch and scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As was briefly mentioned, scikit-learn natively supports neural networks, but
    its performance is not good enough for large-scale networks. The Skorch library
    takes care of that; you can still use the scikit-learn API if you are familiar
    with that, but it runs on PyTorch internally to achieve great performance.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow/Keras – Fast (deep) neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The TensorFlow library is developed by Google and focuses on building deep neural
    networks very similar to PyTorch. The library is well documented and has a large
    number of pre-trained models available to use; you may never have to train your
    own models, which can be a big advantage.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to PyTorch, TensorFlow is also based on tensors for the actual calculations,
    and it is highly optimized for performance on many platforms, including mobile
    phones for deployment and dedicated **tensor processing units** (**TPU**s) or
    GPU hardware for training the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, we will run the Mask R-CNN we used with PyTorch earlier again.
    Since this model is not bundled with `tensorflow`, we need to install `tensorflow-hub`
    in addition to `tensorflow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will automatically install `tensorflow` with GPU support if available
    for your platform. Currently, that means either Windows or Ubuntu Linux. Now we
    can test some TensorFlow/Keras code. First, we import what we need, set some variables,
    and load the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the image is loaded, let’s load the model using `tensorflow_hub` and
    apply it on our image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we don’t have the labels available, so we read that from our `coco_labels.txt`
    file. However, once we load the model we can easily apply it to our image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to prepare the results for easy processing and display them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is largely similar to the PyTorch code because it uses the same pre-trained
    model. The notable differences are:'
  prefs: []
  type: TYPE_NORMAL
- en: We loaded the model using `tensorflow_hub`. This automatically downloads and
    executes pre-trained models from [https://tfhub.dev/](https://tfhub.dev/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The box points are from 0 to 1 instead of being relative to the image size.
    So, coordinate `10x5` in a `20x20` image results in `0.5x0.25`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output variable names are different. It should be noted that these are
    dependent on the model and can be found on TensorFlow Hub for this model: [https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1](https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The box points use the left, top, right, bottom order instead of top, left,
    bottom, right, as was the case with PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beyond those small changes, the code is effectively identical.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy compatibility
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The actual tensor objects in TensorFlow are slightly different from the PyTorch
    tensors. While the `pytorch.Tensor` API can be used as a `numpy.ndarray` alternative,
    with `tensorflow.Tensor` the API is a bit different.
  prefs: []
  type: TYPE_NORMAL
- en: There is a `tensorflow.Tensor.numpy()` method, which returns a `numpy.ndarray`
    of the data. It is important to note that this is *not* a reference, however;
    modifying the `numpy` array will *not* update the original tensor, so you will
    need to convert it back after your changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an alternative, TensorFlow does offer an experimental `numpy` API if you
    prefer that API. It can be enabled like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Usage is fairly straightforward, but it is by no means fully `numpy.ndarray`-compatible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Keras
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Keras submodule of TensorFlow is similar to what PyTorch Lightning and PyTorch
    Ignite are for PyTorch. It offers a high-level interface for TensorFlow, making
    it easier to use and get started with. As opposed to the aforementioned PyTorch
    libraries, Keras is quite suitable as a starting point as well. Knowing the underlying
    TensorFlow functions can be useful, but it is not a requirement for being able
    to use Keras effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Keras might be for you if you are just getting started with TensorFlow and want
    to apply some machine learning to your project without going down the rabbit hole.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow versus PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a few advantages and disadvantages to TensorFlow compared to PyTorch,
    so let’s list those before we continue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some reasons you might choose TensorFlow over PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow supports the execution of the pre-trained model in a web browser.
    While PyTorch does have a few libraries available to do this as well, they are
    either stale or far behind TensorFlow in terms of features and stability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow is largely language-agnostic. This means that it has bindings for
    multiple languages, whereas PyTorch is largely Python-only.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow, or more specifically, Keras, is a very high-level API that allows
    you to get started quickly. When comparing Keras to PyTorch Lightning/PyTorch
    Ignite, I personally feel that you can get a working result more quickly with
    TensorFlow. Keras has many utility functions and classes bundled that can save
    you some work while creating a model. Another big help is TensorFlow Hub, which
    offers many pre-trained models with example code for your convenience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow has a slightly bigger community and slightly more tutorials available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conversely:'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch was written around Python and has a much more Pythonic API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch offers more fine-grained control and easily gives you many parameters
    to tune.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While this is a personal opinion, I find that debugging PyTorch is much nicer
    (from Python, at least) than TensorFlow or Keras because the codebase has fewer
    layers and seems less complicated. Stepping through the execution of your model
    with a regular Python debugger works great and is easy to follow in the case of
    PyTorch. In my experience, regular Python debuggers do not work at all with TensorFlow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch is a bit faster than TensorFlow. This can be a huge help while developing
    and debugging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which of the libraries you should use depends on personal preference and factors
    such as pre-existing experience for you and/or the rest of your team. I can certainly
    recommend both of them.
  prefs: []
  type: TYPE_NORMAL
- en: Evolutionary algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evolutionary algorithms are a technique based on evolution in nature that improve
    by using a fitness function to determine quality, and by evolving the solution.
  prefs: []
  type: TYPE_NORMAL
- en: The most common implementation is the **genetic algorithm**, which commonly
    encodes the solution, or chromosome, into a string or an array that can be tested
    by the fitness function. This chromosome could be a list of functions to apply,
    a list of parameters to a function, or something else entirely. How you wish to
    encode the chromosome is up to you, as long as the fitness function can use it
    to calculate a fitness score.
  prefs: []
  type: TYPE_NORMAL
- en: 'The genetic algorithm will employ one of the following operations to try and
    improve the fitness score:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutation**: This could be a bit flip from 0 to 1, or a more complex mutation
    of replacing multiple bits. For example, if we have bit-string `0101`, then a
    mutation could result in `0111`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Selection**: Given a set of different tested chromosomes using the fitness
    function, only keep the best few.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Crossover**: Given a few different chromosomes, combine parts of them to
    try new solutions. For example, if we have two strings, AABB and DEFG, the crossover
    can split them and combine them; for instance, you could get AAFG, which combines
    the first two characters from the first string and the last two from the second
    string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The genetic algorithm takes a few parameters to control which strategy is employed
    at a given run. The **mutation rate** sets the probability of a mutation occurring;
    the **elitism parameter** decides how many results to keep in the selection process;
    and the **crossover rate** sets the probability of crossovers occurring. The difficult
    part is tuning these parameters to return a good and stable solution (in other
    words, one that does not change too much between runs), but not getting stuck
    in a local optimum where your solution appears the best but could be far better
    by attempting more genetic diversity.
  prefs: []
  type: TYPE_NORMAL
- en: There are many applications where genetic algorithms (or more generally, genetic
    programming) are the most feasible option to get a good solution to your problem.
    One of the prime examples of where genetic algorithms shine is the **traveling
    salesman problem** (**TSP**). With the TSP, you have a list of cities that you
    want to visit, and you want to find the shortest route that covers all of them.
    The standard brute-force solution has a time complexity of `O(n!)`, which means
    that for 10 cities you need about 3,628,800 steps to calculate. That is a lot,
    but still easily manageable. For 20 cities, however, the number grows to 2,432,902,008,176,640,000,
    or, 2 quintillion (2 billion billion), and that growth continues very rapidly.
    With genetic programming, the fitness problem will almost immediately eliminate
    parts of the solution space that are completely infeasible and gives you a good
    (but possibly not the best) solution relatively fast.
  prefs: []
  type: TYPE_NORMAL
- en: Even though evolutionary algorithms offer a lot of power, implementing them
    is relatively easy to do and often highly specific to your specific use case.
    This makes it a scenario where applications and libraries usually opt for writing
    their own implementation instead of using a library for this goal.
  prefs: []
  type: TYPE_NORMAL
- en: There is at least one notable Python library for genetic algorithms however.
    The PyGAD library can make it easily possible for you to use genetic algorithms
    in your project. It also comes with built-in support for Keras and PyTorch to
    save you some work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by installing PyGAD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will attempt to solve a problem you might encounter in the real world.
    Let’s say that you need a new floor and you want wooden floorboards. Due to bulk
    discounts, it can be cheaper to buy a large stack of boards instead of just a
    few separate boards, so let’s assume we have a few different bulk quantities and
    make our algorithm optimize for cost. First, we need to define our list of bulk
    sizes with the prices. We will also define the number of boards we are looking
    for. Lastly, we will define the fitness function to tell PyGAD how good (or bad)
    the solution is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The fitness functions for PyGAD optimize for the highest number; since we are
    looking for the lowest price, we can simply invert the price. Additionally, we
    can return minus infinity when we want to rule out “bad” solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get some intermediate results, we can optionally add a function that will
    show us the state at every generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it’s time to run the algorithm and show the output while doing so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This will run 1,000 generations for us with 100 solutions per generation. A
    single solution contains the number of stacks of wood to buy for each stack size.
    When we run this code, we should get something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot of our results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15882_16_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.8: Genetic algorithm fitness result plot'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, 495 is actually the optimal result; in most cases, though, you
    don’t know if you have reached the optimal result. This essentially means that
    you could keep your code running forever, which is why you should either configure
    a fixed number of generations, or tell PyGAD to stop once it has reached a steady
    state for a certain number of generations.
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, however, after about 50 generations we already had a great
    and very usable solution for our problem, whereas the optimal solution took roughly
    700 generations this run. In many of the other runs I did, it never even found
    the optimal solution. This shows you how quickly the genetic algorithm can give
    you a useful result.
  prefs: []
  type: TYPE_NORMAL
- en: Support-vector machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Support-vector machines** (**SVMs**) or support-vector networks are common
    models for supervised learning. Since it is a supervised learning method, it expects
    a dataset that is already labeled (for example, a list of photos with correct
    labels) to train on. Once the model has been trained, it can be used for classification
    and regression analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: In statistics, **regression analysis** is a way to show the relationship between
    variables. These can be used to fit lines, create predictors, detect outliers,
    and more. We have seen several examples of regression analysis in *Chapter 15*,
    *Scientific Python and Plotting*, as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification** refers to statistical classification and is a method of
    splitting data. For example, the question as to whether an email is spam or not
    is a form of binary classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bayesian networks are based on the idea that we have probabilities of an event
    occurring. This is usually expressed as `P(event)`, where `P(event)=1` is 100%
    probability of `event` occurring and `P(event)=0` is no probability at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'These can be used for all sorts of applications and are particularly useful
    for expert systems, which can make recommendations based on your given data. For
    example, given that there is a thunderstorm outside, we know that there is a larger
    probability of rain than if it is sunny outside. In Bayesian terms, we would describe
    it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Bayesian networks are often used for spam filters that look for certain keywords
    and calculate the odds of an email being spam. Another possible use case for Bayesian
    networks is text prediction when typing. If you train your network with many sentences,
    you can calculate the next most likely word to occur given the previous word or
    words.
  prefs: []
  type: TYPE_NORMAL
- en: As you have seen, there are many types of different machine learning models,
    and many more submodels that all have their own strengths and weaknesses. This
    list of examples is an extremely condensed and simplified list of available models,
    but it should give you at least some idea of the scenarios in which these different
    algorithms can do their magic.
  prefs: []
  type: TYPE_NORMAL
- en: Versatile AI libraries and utilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is by far the most popular language when it comes to developing AI systems.
    The result of this popularity is that there are a huge number of libraries available
    for every branch of AI you can think of. There is at least a single good library
    for nearly every AI technique, and often dozens.
  prefs: []
  type: TYPE_NORMAL
- en: In this section of this chapter, you will find a curated (and incomplete) list
    of useful AI libraries split into segments. There are many more that are not mentioned
    due to being too specific, too new, or simply because I have omitted them owing
    to the great number of libraries that are out there.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn – Machine learning in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The scikit-learn library is an extremely versatile machine learning library
    that covers many AI topics; for many of them, this should be your starting point.
    We have already seen the scikit-image library earlier, which is a part of the
    scikit-learn project, but there are many more options.
  prefs: []
  type: TYPE_NORMAL
- en: The complete list of possibilities is huge, so I will try to give you a very
    small list based on the scikit-learn modules that I have personally found useful.
    Many more methods are available, so make sure to read through the scikit-learn
    documentation if you are interested in anything specific.
  prefs: []
  type: TYPE_NORMAL
- en: This section is split between supervised and unsupervised options, since your
    dataset is the most important factor in deciding on an algorithm for your use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Starting with supervised learning, scikit-learn offers a host of different options
    in many different categories.
  prefs: []
  type: TYPE_NORMAL
- en: Linear models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First of all, scikit-learn offers dozens of different linear models for performing
    many types of regressions. It has functions for many specific use cases, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ordinary least squares** regression, as we have seen several times in the
    previous chapter as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ridge regression and classifier**, a function similar to the ordinary least
    squares method but more resistant to collinearity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **LASSO** (**least absolute shrinkage and selection operator**) **model**,
    which can be seen as the successor to the Ridge model for specific use cases.
    One of the advantages of the lasso model is that, in the case of machine learning,
    it can help filter out (usually irrelevant) features with very little data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial regression**: Methods such as the ordinary least squares method
    perform regression by creating a single straight line. In some cases, however,
    a straight line will never properly fit your data. In these cases, polynomial
    regression can help a lot since it can generate curved lines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are many more methods in this module, so make sure to take a look at
    the documentation: [https://scikit-learn.org/stable/modules/linear_model.html](https://scikit-learn.org/stable/modules/linear_model.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Support-vector machines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next up are support-vector machines. We already discussed SVMs briefly, but
    in short these can be used for classification, regression, and outlier detection.
    As opposed to the linear (2D) models above, these methods also function for higher-dimensional
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, scikit-learn supports these types of SVMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SVC`/`SVR`: Support-vector classification and regression based on the C `libsvm`
    library. For smaller datasets (a few thousand samples), this is the most useful
    and flexible SVM implementation in scikit-learn. This method can also handle support
    vectors, which can increase the precision of classifiers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NuSVC`/`NuSVR`: A modified version of SVC/SVR that introduces a parameter
    *v* (the Greek letter Nu) to approximate the fraction of training errors and support
    vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LinearSVC`/`LinearSVR`: A fast (faster than `SVC`/`SVR`) linear support vector
    classification and regression system. For large datasets (over 10,000 samples)
    this is the better alternative to `SVC`/`SVR`, but it does not handle separate
    support vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVMs are very robust prediction methods for higher-dimensional data while still
    maintaining decent execution speeds.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Decision trees** (**DTs**) also deserve special attention. While most of
    the machine learning models are still relatively expensive to use after training,
    with DTs you build a tree based on the training data to use in your classification
    or regression. If you are familiar with tree structures, you know that many lookups
    only take `O(log(n))` time to do. In addition to being really fast to calculate,
    it can also make it much easier to visualize your data, because `scikit-learn`
    can export the evaluated results to Graphviz, a tool for rendering graph structures.'
  prefs: []
  type: TYPE_NORMAL
- en: To supercharge the DTs, you can also combine a collection of them into a forest
    using a `RandomForestClassifier` or `RandomForestRegressor`, which results in
    reduced variance. To take this a step further, you can also use the **extremely
    randomized trees** methods `ExtraTreesClassifier` or `ExtraTreesRegressor`, which
    also randomize the specific thresholds between the trees, for further reduced
    variance over the normal forest methods.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using feature selection, you can input a large number of input parameters without
    specifying what they are for, and let the model figure out the most important
    features.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say that you have collected a large set of weather and geographical
    data, such as temperature, humidity, air pressure, altitude, and coordinates,
    and you want to know which of these play a role in answering the question of whether
    it will snow. The coordinates and air pressure are probably less important factors
    than temperature is in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit-learn library has several different options available for feature
    selection:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn.feature_selection.VarianceThreshold`: Excludes items with a small
    variance by satisfying the equation Var[X]=p(1-p)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn.feature_selection.SelectKBest`: Selects the k highest scoring features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn.feature_selection.SelectPercentile`: Selects the top nth percentile
    scoring features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn.feature_selection.SelectFromModel`: A special and very useful feature
    selector that can use previously generated models (such as an SVM) to filter features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are several other feature selection and feature filtering methods available,
    so make sure to check the documentation to see if there is a better method available
    for your specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Other models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In addition to these methods, there are many other methods supported by scikit-learn,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian networks**: Gaussian, multinomial, complement, Bernoulli, categorical,
    and out-of-core.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear and quadratic discriminant analysis**: These are similar to the linear
    models but also offer quadratic solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel ridge regression**: A combination of ridge regression and classification.
    This can be a faster alternative to SVR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent**: A very fast regression/classifier alternative
    to SVM for specific use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nearest neighbor**: These methods are useful for a range of different purposes
    and are at the core of many of the other functions in this library. At the very
    least, take a look at this section, because structures such as KD-trees have many
    applications outside of machine learning as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While there are several other options as well, these are probably the ones that
    are most useful to you. Note that even though scikit-learn does support neural
    networks such as multi-layer perceptrons, I would not recommend you use scikit-learn
    for this purpose. While the implementation works well, it does not have support
    for GPU (video card) acceleration, which makes a huge performance difference.
    For neural networks I recommend using TensorFlow, as discussed earlier in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the nature of unsupervised learning, it is a lot less versatile than
    supervised learning, but there are a few scenarios where unsupervised learning
    absolutely makes sense and is an easy solution. While the unsupervised learning
    portion of scikit-learn is smaller than the supervised portion, there are still
    several really useful functions available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering is the prime example of where unsupervised learning shines. This
    comes down to giving the algorithm a whole bunch of data and telling it to cluster
    (split) it into useful sections wherever it can find a pattern. To facilitate
    this, scikit-learn has a range of different algorithms. The documentation explains
    this very well: [https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A subsection of this documentation is given below:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method name** | **Scalability** | **Use case** |'
  prefs: []
  type: TYPE_TB
- en: '| K-Means | Very large `n_samples`, medium `n_clusters` with MiniBatch code
    | General-purpose, even cluster size, flat geometry, not too many clusters, inductive
    |'
  prefs: []
  type: TYPE_TB
- en: '| Affinity propagation | Not scalable with `n_samples` | Many clusters, uneven
    cluster size, non-flat geometry, inductive |'
  prefs: []
  type: TYPE_TB
- en: '| Mean-shift | Not scalable with `n_samples` | Many clusters, uneven cluster
    size, non-flat geometry, inductive |'
  prefs: []
  type: TYPE_TB
- en: '| Spectral clustering | Medium `n_samples`, small `n_clusters` | Few clusters,
    even cluster size, non-flat geometry, transductive |'
  prefs: []
  type: TYPE_TB
- en: '| Ward hierarchical clustering | Large `n_samples` and `n_clusters` | Many
    clusters, possibly connectivity constraints, transductive |'
  prefs: []
  type: TYPE_TB
- en: '| Agglomerative clustering | Large `n_samples` and `n_clusters` | Many clusters,
    possibly connectivity constraints, non-Euclidean distances, transductive |'
  prefs: []
  type: TYPE_TB
- en: '| DBSCAN | Very large `n_samples`, medium `n_clusters` | Non-flat geometry,
    uneven cluster sizes, transductive |'
  prefs: []
  type: TYPE_TB
- en: '| OPTICS | Very large `n_samples`, large `n_clusters` | Non-flat geometry,
    uneven cluster sizes, variable cluster density, transductive |'
  prefs: []
  type: TYPE_TB
- en: '| Gaussian mixtures | Not scalable | Flat geometry, good for density estimation,
    inductive |'
  prefs: []
  type: TYPE_TB
- en: '| BIRCH | Large `n_clusters` and `n_samples` | Large dataset, outlier removal,
    data reduction, inductive |'
  prefs: []
  type: TYPE_TB
- en: All of these methods have their own use cases and the scikit-learn documentation
    explains this much better than I could. In general, however, the K-Means algorithm,
    which we have also used in the previous chapter, is a very good starting point.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the clusters can also be used for learning features and the relationship
    between them. Once you have learned the features, you could use the feature selection
    in supervised learning to filter them for subselections.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, for the general machine learning cases, scikit-learn is probably
    your best bet. For special cases, there are often better libraries available;
    many of these are built on top of scikit-learn, however, so it is recommended
    that you familiarize yourself with the library if you plan to employ machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: auto-sklearn – Automatic scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The scikit-learn library can do so many things that it is often overwhelming
    to use. At the time of writing, there are 34 distinct regression functions and
    25 different classifiers, which can make it quite a challenge to select the right
    one for you.
  prefs: []
  type: TYPE_NORMAL
- en: This is where `auto-sklearn` can help. It can automatically select a classification
    function for you and fill in the parameters needed for it to work. If you’re just
    looking for something that just works, this is your best bet.
  prefs: []
  type: TYPE_NORMAL
- en: mlxtend – Machine learning extensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`mlxtend` is a library with a range of relatively easy and well-documented
    machine learning examples.'
  prefs: []
  type: TYPE_NORMAL
- en: It uses `scikit-learn`, pandas, and `matplotlib` internally to provide a more
    user-friendly interface for machine learning compared to scikit-learn. If you
    are starting out with machine learning (or scikit-learn), this can be a nice introduction,
    since it’s a bit less complicated than using scikit-learn directly.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-lego – scikit-learn utilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though scikit-learn already has a huge catalog of functions and features
    built in, there are still many things that it does not provide an easy interface
    for. This is where the scikit-lego library can help, it has many convenient functions
    for scikit-learn and pandas so you don’t need to repeat yourself too often.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we used the Penguins dataset a few times. Loading
    that dataset and plotting the distribution can be done in just a few lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/ph/3d51j84d2gg_pltczn6244q80000gn/T/com.microsoft.Word/Content.MSO/195F70F3.tmp](img/B15882_16_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.9: Penguin distribution'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-lego can automatically perform some conversions for us (the `return_X_y`
    parameter here) so we can easily plot the results. There are many more of these
    functions available, which make it really easy to play around with Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost – eXtreme Gradient Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XGBoost is a fast and efficient library for gradient boosting, a regression/classification
    technique that produces forests of decision trees. The main advantage of this
    technique compared to many other regression/classification algorithms is the **scalability**.
    With XGBoost, you can easily spread your workload along clusters of many computers,
    and it happily scales to billions of data points.
  prefs: []
  type: TYPE_NORMAL
- en: If you have very large datasets, XGBoost might be one of your best options.
  prefs: []
  type: TYPE_NORMAL
- en: Featuretools – Feature detection and prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Featuretools library makes it really easy to transform your datasets into
    aggregated feature matrices based on either time-based datasets or relational
    ones. Once the feature matrix is constructed, the library can be used for predictions
    about these features.
  prefs: []
  type: TYPE_NORMAL
- en: You could, for example, predict trip durations based on a collection of multiple
    trips, or predict when a customer will purchase from you again.
  prefs: []
  type: TYPE_NORMAL
- en: Snorkel – Improving your ML data automatically
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Snorkel is a library that attempts to make the training of your ML models much
    easier. Getting enough training data to properly train your models can be really
    difficult, and this library has several clever methods to make this easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The library has three core operations to help you build your datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: First, to help with labeling, the Snorkel library features several heuristic
    methods. While these labels will not be perfect, manually labeling all data can
    be a prohibitive task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second core operation is the transforming and augmenting of datasets. Once
    again, these use heuristic methods to (hopefully) improve your data quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last core operation is the slicing of data so you only get data that is
    relevant for your use case. This operation is also heuristics-based.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will not need this if you already have good-quality data available, but
    it is certainly worth looking at if your data could use some improvement. As is
    always the case with machine learning, care must be taken to avoid overfitting
    or underfitting data. Applying the Snorkel methods can quickly exacerbate problems
    in your dataset, since it uses the dataset as a source.
  prefs: []
  type: TYPE_NORMAL
- en: TPOT – Optimizing ML models using genetic programming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TPOT (tea-pot) is a library that optimizes your learning pipelines through genetic
    programming. We already covered evolutionary algorithms earlier, but to remind
    you, they are algorithms that improve by changing themselves or their parameters
    through evolution.
  prefs: []
  type: TYPE_NORMAL
- en: While genetic algorithms are relatively easy to implement by themselves, the
    complexity comes from the encoding of the solution so it is compatible with the
    genetic algorithm. This is what is very nice about the TPOT library; it makes
    it really easy to encode your features, cache parts of the pipeline, and even
    run the attempts in parallel using Dask.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, here is the code needed to tell TPOT to automatically optimize
    a scikit-learn classifier with its parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Once it is done trying multiple classifiers, it will write the optimized function
    calls to `optimized_classifier.py`. It is important to note that the classifier
    returned is also dependent on the optimizer results; it could be `sklearn.neighbors.KNeighborsClassifier`,
    but you could also get `sklearn.ensemble.RandomForestClassifier` or something
    else.
  prefs: []
  type: TYPE_NORMAL
- en: Do not assume that `TPOT` is a fast solution for finding your parameters, though;
    getting a good solution using genetic algorithms can take a long time, and it
    can be beneficial to reduce your test set before you apply this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: That was the last library, and it’s time to try things out for yourself in the
    *Exercises* section.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the nature of this chapter, all topics only cover the absolute basics
    of the mentioned libraries and they really do deserve much more. In this case,
    as an exercise, I recommend that you try and use some (or all) of the mentioned
    libraries and see if you can do something useful with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: Browse through TensorFlow Hub and apply some models to your own data. Perhaps
    you can apply object detection to your holiday photos.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After applying a model to your photos, try and improve the model by adding some
    new objects and finetuning it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to extract some data or information from this chapter’s summary by applying
    one of the NLP algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI is a complicated subject, and even simple example guides are often quite
    elaborate. Luckily, these days we can often immediately try examples online through
    Google Colab or by running a Jupyter Notebook. Dive in and don’t get discouraged;
    there is an incredible amount of high-quality information available online from
    field experts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example answers for these exercises can be found on GitHub: [https://github.com/mastering-python/exercises](Chapter_16.xhtml).
    You are encouraged to submit your own solutions and learn about alternative solutions
    from others.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter gave you a sample of some of the largest and most popular Python
    AI libraries, but there are many more (large) libraries around that could be useful
    for your particular use case. There are, for example, also many libraries available
    for specific topics such as astronomy, geographical information systems (GISes),
    protein folding, and neurological imaging.
  prefs: []
  type: TYPE_NORMAL
- en: After this chapter, you should have some idea of where to start searching for
    particular types of AI libraries. Additionally, you should know a little bit about
    when to apply a particular type of AI. For many use cases, you will need a combination
    of these methods to solve the problem in an efficient manner. A supervised ML
    system, for example, is a fantastic option if you have a vast amount of good-quality,
    labeled data. Often this is not the case, which is where the other algorithms
    come in.
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly enough, many of the current “AI” start-up companies don’t actually
    use AI for their recommendation systems but humans instead, hoping to upgrade
    to an effective AI somewhere in the future when they have gathered enough training
    data. Effectively, they are trying to solve the data requirement for supervised
    ML systems with brute force. Similarly, algorithms are only part of the reason
    that voice recognition systems such as Alexa, Google Assistant, or Siri have become
    possible. Another large part is the availability of training data over the last
    several years. Naturally, these systems are not built on one algorithm specifically
    but use a combination of multiple algorithms; the system not only tries to convert
    your voice to words, but also attempts to understand what you are likely to say
    by constantly cross-validating those results with what would be a logical sentence
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: The field of AI is improving and changing more rapidly with each year. With
    the increased processing power we have now, there are many more options than we
    had in the past. The currently used deep learning AI models were completely infeasible
    to build only 20 years ago, and in 10 years’ time the models will have far surpassed
    what is possible now. If there is no solution available for the issue you are
    facing today, the situation might be completely different a year from now.
  prefs: []
  type: TYPE_NORMAL
- en: It is also perfectly reasonable to skip this part of Python entirely. While
    AI is becoming a larger and larger portion of what is being done with Python,
    a big part of that is in academic settings and might not be interesting for your
    field of work. AI can be a great help, but it is often a much more complicated
    solution than is actually needed.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about creating extensions in C/C++ to increase
    performance and allow low-level access to memory and other hardware resources.
    While this can greatly help with performance, performance rarely comes free, as
    we will see.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers: [https://discord.gg/QMzJenHuJf](https://discord.gg/QMzJenHuJf)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code156081100001293319171.png)'
  prefs: []
  type: TYPE_IMG
