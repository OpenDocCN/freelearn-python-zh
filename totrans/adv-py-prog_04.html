<html><head></head><body>
<div><div><div><h1 id="_idParaDest-48"><em class="italic"><a id="_idTextAnchor047"/>Chapter 3</em>: Fast Array Operations with NumPy, Pandas, and Xarray</h1>
			<p>NumPy is the <em class="italic">de facto</em> standard for scientific computing in Python. It offers flexible multidimensional arrays that allow you to perform fast and concise mathematical calculations.</p>
			<p>NumPy provides common data structures and algorithms designed to express complex mathematical operations using a concise syntax. The multidimensional array, <code>numpy.ndarray</code>, is internally based on C arrays. Apart from the performance benefits, this choice allows NumPy code to easily interface with the existing C and FORTRAN routines; NumPy helps bridge the gap between Python and the legacy code written using those languages.</p>
			<p>In this chapter, we will learn how to create and manipulate NumPy arrays. We will also explore the NumPy broadcasting feature, which is used to rewrite complex mathematical expressions efficiently and succinctly.</p>
			<p><strong class="bold">pandas</strong> is a tool that relies heavily on NumPy and provides additional data structures and algorithms targeted toward data analysis. We will introduce the main pandas features and their usage. We will also learn how to achieve high performance using pandas data structures and vectorized operations. </p>
			<p>Both NumPy and pandas are insufficient in many use cases concerning labeled, multidimensional data. The xarray library combines the best features from the other two tools and offers further optimized data processing functionalities. We will discuss the motivation for this tool and study the performance improvements it offers via explicit examples.</p>
			<p>In this chapter, we will be covering the following topics:</p>
			<ul>
				<li>Getting started with NumPy</li>
				<li>Rewriting the particle simulator in NumPy</li>
				<li>Reaching optimal performance with numexpr</li>
				<li>Working with database-style data with pandas</li>
				<li>High-performance labeled data with xarray</li>
			</ul>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>Technical requirement</h1>
			<p>The code files for this chapter can be accessed by going to this book's GitHub repository at <a href="https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/Chapter03">https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/Chapter03</a>.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>Getting started with NumPy</h1>
			<p>The NumPy library revolves around its multidimensional array object, <code>numpy.ndarray</code>. NumPy arrays are collections<a id="_idIndexMarker146"/> of elements of the same data type; this fundamental restriction allows NumPy to pack the data in a way that allows for high-performance mathematical operations.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Creating arrays</h2>
			<p>Let's explore NumPy's functionalities<a id="_idIndexMarker147"/> by following these steps:</p>
			<ol>
				<li>You can create NumPy arrays using the <code>numpy.array</code> function. It takes a list-like object (or another array) as input and, optionally, a string expressing its data type. You can interactively test array creation using an IPython shell, as follows:<pre>    import numpy as np 
    a = np.array([0, 1, 2]) </pre></li>
				<li>Every NumPy array has an associated data type that can be accessed using the <code>dtype</code> attribute. If we inspect the <code>a</code> array, we will find that its <code>dtype</code> is <code>int64</code>, which stands for 64-bit integer:<pre>    a.dtype 
    # Result: 
    # dtype('int64') </pre></li>
				<li>We may decide to convert those integer numbers into the <code>float</code> type. To do this, we can either pass the <code>dtype</code> argument at array initialization or cast the array to another data type using the <code>astype</code> method. These two ways to select a data type are shown in the following code:<pre>    a = np.array([1, 2, 3], dtype='float32') 
    a.astype('float32') 
    # Result:
    # array([ 0.,  1.,  2.], dtype=float32) </pre></li>
				<li>To create an array<a id="_idIndexMarker148"/> with two dimensions (an array of arrays), we can perform the required initialization using a nested sequence, as follows:<pre>    a = np.array([[0, 1, 2], [3, 4, 5]]) 
    print(a) 
    # Output:
    # [[0 1 2]
    #  [3 4 5]] </pre></li>
				<li>An array that's created<a id="_idIndexMarker149"/> in this way has two dimensions, which are called <code>ndarray.shape</code> attribute:<pre>    a.shape 
    # Result:
    # (2, 3) </pre></li>
				<li>Arrays can also be reshaped, so long as the product of the shape dimensions is equal to the total number of elements in the array (that is, the total number of elements is conserved). For example, we can reshape an array containing 16 elements in the following ways: <code>(2, 8)</code>, <code>(4, 4)</code>, or <code>(2, 2, 4)</code>. To reshape an array, we can either use the <code>ndarray.reshape</code> method or assign a new value to the <code>ndarray.shape</code> tuple. The following code<a id="_idIndexMarker150"/> illustrates the use of the <code>ndarray.reshape</code> method:<pre>    a = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 
                  9, 10, 11, 12, 13, 14, 15]) 
    a.shape 
    # Output:
    # (16,)
    a.reshape(4, 4) # Equivalent: a.shape = (4, 4) 
    # Output: 
    # array([[ 0,  1,  2,  3],
    #        [ 4,  5,  6,  7],
    #        [ 8,  9, 10, 11],
    #        [12, 13, 14, 15]]) </pre></li>
			</ol>
			<p>Thanks to this property, you can freely add dimensions of size <code>1</code>. You can reshape an array with 16 elements to <code>(16, 1)</code>, <code>(1, 16)</code>, <code>(16, 1, 1)</code>, and so on. In the next section, we will extensively use this feature to implement complex operations through <em class="italic">broadcasting</em>. </p>
			<ol>
				<li value="7">NumPy provides convenience functions, as shown in the following code, to create arrays filled with zeros, ones, or with no initial value (in this case, their actual value is meaningless and depends on the memory state). Those functions take the array shape as a tuple and, optionally, its <code>dtype</code>:<pre>    np.zeros((3, 3)) 
    np.empty((3, 3)) 
    np.ones((3, 3), dtype='float32') </pre></li>
			</ol>
			<p>In our examples, we will use the <code>numpy.random</code> module to generate random floating-point numbers in the <code>(0, 1)</code> interval. <code>numpy.random.rand</code> will take a shape and return an array of random numbers with that shape:</p>
			<pre>    np.random.rand(3, 3) </pre>
			<ol>
				<li value="8">Sometimes, it is convenient to initialize arrays that have the same shape as that of some other array. For that purpose, NumPy provides some handy functions, such as <code>zeros_like</code>, <code>empty_like</code>, and <code>ones_like</code>. These functions can be used as follows:<pre>    np.zeros_like(a) 
    np.empty_like(a) 
    np.ones_like(a) </pre></li>
			</ol>
			<p>These functions will return arrays<a id="_idIndexMarker151"/> with the specified values, whose shapes match exactly with that of array <code>a</code>.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor051"/>Accessing arrays</h2>
			<p>The NumPy array interface is, at a shallow<a id="_idIndexMarker152"/> level, similar to that of Python lists. NumPy arrays can be indexed using integers and iterated using a <code>for</code> loop:</p>
			<pre>    A = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8]) 
    A[0] 
    # Result:
    # 0 
    [a for a in A] 
    # Result:
    # [0, 1, 2, 3, 4, 5, 6, 7, 8] </pre>
			<p>However, explicitly<a id="_idIndexMarker153"/> looping over an array is, most of the time, not the most efficient way to access its elements. In this section, we will learn how to take advantage of NumPy's API to fully utilize its efficiency.</p>
			<h3 id="_idParaDest-53">Indexing and slicing</h3>
			<p>Indexing and slicing refer to the act of accessing elements within an array that are at certain locations or satisfy<a id="_idIndexMarker154"/> some condition that we are<a id="_idIndexMarker155"/> interested in. In NumPy, array elements and sub-arrays<a id="_idIndexMarker156"/> can be conveniently accessed by using multiple values separated<a id="_idIndexMarker157"/> by commas inside the subscript operator, <code>[]</code>. Let's get started:</p>
			<ol>
				<li value="1">If we take a <code>(3,3)</code> array (an array containing three triplets) and we access the element with an index of <code>0</code>, we can obtain the first row, as follows:<pre>    A = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) 
    A[0] 
    # Result:
    # array([0, 1, 2]) </pre></li>
				<li>We can index the row again by adding another index separated by a comma. To get the second element of the first row, we can use the <code>(0, 1)</code> index. An important observation is that the <code>A[0, 1]</code> notation is shorthand for <code>A[(0, 1)]</code>; that is, we are indexing using a <em class="italic">tuple</em>! Both versions are shown in the following snippet:<pre>    A[0, 1] 
    # Result:
    # 1
    # Equivalent version using tuple
    A[(0, 1)]</pre></li>
				<li>NumPy allows you to slice arrays into multiple dimensions. If we slice on the first dimension, we can obtain a collection of triplets, as follows:<pre>    A[0:2] 
    # Result:
    # array([[0, 1, 2], 
    #        [3, 4, 5]]) </pre></li>
				<li>If we slice the array<a id="_idIndexMarker158"/> again on the second dimension with <code>0:2</code>, we are extracting<a id="_idIndexMarker159"/> the first two elements<a id="_idIndexMarker160"/> from the collection of triplets<a id="_idIndexMarker161"/> shown earlier. This results in an array whose shape is <code>(2, 2)</code>, as shown in the following code:<pre>    A[0:2, 0:2] 
    # Result:
    # array([[0, 1], 
    #        [3, 4]]) </pre></li>
				<li>Intuitively, you can update the values in the array using both <em class="italic">numerical indexes</em> and <em class="italic">slices</em>. An example of this is illustrated in the following code snippet:<pre>    A[0, 1] = 8 
    A[0:2, 0:2] = [[1, 1], [1, 1]]</pre></li>
				<li>Indexing with the slicing syntax is very fast because, unlike lists, it doesn't produce a copy of the array. In NumPy's terminology, it returns a <em class="italic">view</em> of the same memory area. If we take a slice of the original array, and then we change one of its values, the original array will be updated as well. The following code illustrates an example of this feature:<pre>    a= np.array([1, 1, 1, 1]) 
    a_view = a[0:2] 
    a_view[0] = 2 
    print(a) 
    # Output:
    # [2 1 1 1] </pre></li>
			</ol>
			<p>It is important to be extra careful when mutating NumPy arrays. Since views share data, changing the values of a view can result in hard-to-find bugs. To prevent side effects, you can set the <code>a.flags.writeable = False</code> flag, which will prevent the array or any of its views from being accidentally mutated.</p>
			<ol>
				<li value="7">Let's look at another example that shows how the slicing syntax can be used in a real-world setting. Let's define an <code>r_i</code> array, as shown in the following line of code, which contains a set of 10 coordinates (<em class="italic">x</em>, <em class="italic">y</em>). Its shape will be <code>(10, 2)</code>:<pre>    r_i = np.random.rand(10, 2)</pre></li>
			</ol>
			<p>If you have a hard time<a id="_idIndexMarker162"/> distinguishing arrays that differ in the axes order, for<a id="_idIndexMarker163"/> example, between an array of shape <code>(10, 2)</code> and shape <code>(2, 10)</code>, it is useful to think that every time<a id="_idIndexMarker164"/> you say the word <em class="italic">of</em>, you should introduce a new dimension. An array<a id="_idIndexMarker165"/> with 10 elements <em class="italic">of</em> size two will be <code>(10, 2)</code>. Conversely, an array with two elements <em class="italic">of</em> size 10 will be <code>(2, 10)</code>.</p>
			<p>A typical operation we may be interested in is extracting the <em class="italic">x</em> component from each coordinate. In other words, you want to extract the <code>(0, 0)</code>, <code>(1, 0)</code>, and <code>(2, 0)</code>, items, resulting in an array with a shape of <code>(10,)</code>. It is helpful to think that the first index is <em class="italic">moving</em> while the second one is <em class="italic">fixed</em> (at <code>0</code>). With this in mind, we will slice every index on the first axis (the moving one) and take the first element (the fixed one) on the second axis, as shown in the following line of code:</p>
			<pre>    x_i = r_i[:, 0] </pre>
			<p>On the other hand, the following expression will keep the first index fixed and the second index moving, returning the first (<em class="italic">x</em>, <em class="italic">y</em>) coordinate:</p>
			<pre>    r_0 = r_i[0, :] </pre>
			<p>Slicing all the indexes over the last<a id="_idIndexMarker166"/> axis is optional; using <code>r_i[0]</code> has the same effect as using <code>r_i[0, :]</code>.</p>
			<h3 id="_idParaDest-54">Fancy indexing</h3>
			<p>NumPy allows you<a id="_idIndexMarker167"/> to index an array using another NumPy array<a id="_idIndexMarker168"/> made up of either integer or Boolean values. This is a feature called <strong class="bold">fancy indexing</strong>:</p>
			<ol>
				<li value="1">If you index an array (say, <code>a</code>) with another array of integers (say, <code>idx</code>), NumPy will interpret the integers as indexes and will return an array containing their corresponding values. If we index an array containing 10 elements with <code>np.array([0, 2, 3])</code>, we obtain an array of shape <code>(3,)</code> containing the elements at positions <code>0</code>, <code>2</code>, and <code>3</code>. The following code illustrates this concept:<pre>    a = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) 
    idx = np.array([0, 2, 3]) 
    a[idx] 
    # Result:
    # array([9, 7, 6]) </pre></li>
				<li>You can use fancy indexing on multiple dimensions by passing an array for each dimension. If we want to extract the <code>(0, 2)</code> and <code>(1, 2)</code> elements, we have to pack all the indexes acting on the first axis in one array, and the ones acting on the second axis in another. This can be seen in the following code:<pre>    a = np.array([[0, 1, 2], [3, 4, 5], \
                  [6, 7, 8], [9, 10, 11]]) 
    idx1 = np.array([0, 1]) 
    idx2 = np.array([2, 2]) 
    a[idx1, idx2]</pre></li>
				<li>You can also use normal lists as index arrays, but not tuples. For example, the following two statements are equivalent:<pre>    a[np.array([0, 1])] # is equivalent to
    a[[0, 1]]</pre></li>
			</ol>
			<p>However, if you use a tuple, NumPy will interpret the following statement as an index on multiple dimensions:</p>
			<pre>    a[(0, 1)] # is equivalent to
    a[0, 1] </pre>
			<ol>
				<li value="4">The index arrays are not required to be one-dimensional; we can extract elements from<a id="_idIndexMarker169"/> the original array in any<a id="_idIndexMarker170"/> shape. For example, we can select elements from the original array to form a <code>(2,2)</code> array, as shown here:<pre>    idx1 = [[0, 1], [3, 2]] 
    idx2 = [[0, 2], [1, 1]] 
    a[idx1, idx2] 
    # Output: 
    # array([[ 0,  5],
    #        [10,  7]]) </pre></li>
				<li>The array slicing and fancy indexing features can be combined. This is useful, for instance, when we want to swap the <em class="italic">x</em> and <em class="italic">y</em> columns in a coordinate array. In the following code, the first index will be running over all the elements (a slice) and, for each of those, we extract the element in position <code>1</code> (the <em class="italic">y</em>) first and then the one in position <code>0</code> (the <em class="italic">x</em>):<pre>    r_i = np.random.rand(10, 2) 
    r_i[:, [0, 1]] = r_i[:, [1, 0]] </pre></li>
				<li>When the index array is of the <code>bool</code> type, the rules are slightly different. The <code>bool</code> array will act as a <em class="italic">mask</em>; every element corresponding to <code>True</code> will be extracted and put in the output array. This is shown in the following code:<pre>    a = np.array([0, 1, 2, 3, 4, 5]) 
    mask = np.array([True, False, True, False, \
      False, False]) 
    a[mask] 
    # Output:
    # array([0, 2]) </pre></li>
			</ol>
			<p>The same rules apply when dealing with multiple dimensions. Furthermore, if the index array has the same shape as the original array, the elements corresponding to <code>True</code> will be selected and put in the resulting array.</p>
			<ol>
				<li value="7">Indexing in NumPy is a reasonably fast operation. When speed is critical, you can use the slightly faster <code>numpy.take</code> and <code>numpy.compress</code> functions to squeeze out a little more performance. The first argument of <code>numpy.take</code> is the array we want<a id="_idIndexMarker171"/> to operate on, while the second<a id="_idIndexMarker172"/> is the list of indexes we want to extract. The last argument is <code>axis</code>; if this is not provided, the indexes will act on the flattened array; otherwise, they will act along the specified axis:<pre>    r_i = np.random.rand(100, 2) 
    idx = np.arange(50) # integers 0 to 50 
    %timeit np.take(r_i, idx, axis=0) 
    1000000 loops, best of 3: 962 ns per loop 
    %timeit r_i[idx] 
    100000 loops, best of 3: 3.09 us per loop </pre></li>
				<li>The similar but faster version for Boolean arrays is <code>numpy.compress</code>, which works in the same way. The use of <code>numpy.compress</code> is shown here:<pre>    In [51]: idx = np.ones(100, dtype='bool') # all 
      True values 
    In [52]: %timeit np.compress(idx, r_i, axis=0) 
    1000000 loops, best of 3: 1.65 us per loop 
    In [53]: %timeit r_i[idx] 
    100000 loops, best of 3: 5.47 us per loop </pre></li>
			</ol>
			<p>As we can see, <code>compress</code> gives us a slight speed<a id="_idIndexMarker173"/> improvement, which will prove<a id="_idIndexMarker174"/> useful if you are dealing with large-sized arrays.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor052"/>Broadcasting</h2>
			<p>The true power of NumPy lies in its fast mathematical operations. The approach that's used by NumPy is to avoid stepping into the Python interpreter by performing element-wise calculations using optimized C code. <strong class="bold">Broadcasting</strong> is a clever set of rules<a id="_idIndexMarker175"/> that enables fast array calculations<a id="_idIndexMarker176"/> for arrays of similar (but not equal!) shapes. Let's see how that goes.</p>
			<p>Whenever you perform an arithmetic operation on two arrays (such as a product), if the two operands have the same shape, the operation will be applied in an element-wise fashion. For example, upon multiplying two <code>(2,2)</code> arrays, the operation will be done between pairs of corresponding elements, producing another <code>(2, 2)</code> array, as shown in the following code:</p>
			<pre>    A = np.array([[1, 2], [3, 4]]) 
    B = np.array([[5, 6], [7, 8]]) 
    A * B 
    # Output:
    # array([[ 5, 12],           
    #        [21, 32]]) </pre>
			<p>If the shapes of the operands don't match, NumPy will attempt to match them using broadcasting rules. If one of the operands is a <em class="italic">scalar</em> (for example, a number), it will be applied to every element of the array, as shown in the following code:</p>
			<pre>    A * 2 
    # Output: 
    # array([[2, 4], 
    #        [6, 8]]) </pre>
			<p>If the operand is another array, NumPy will try to match the shapes starting from the last axis. For example, if we want to combine<a id="_idIndexMarker177"/> an array of shape <code>(3, 2)</code> with one of shape <code>(2,)</code>, the second array<a id="_idIndexMarker178"/> will be repeated three times to generate a <code>(3, 2)</code> array. In other words, the array is <em class="italic">broadcasted</em> along a dimension to match the shape of the other operand, as shown in the following diagram:</p>
			<div><div><img src="img/Figure_3.1_B17499.jpg" alt="Figure 3.1 – Illustration of array broadcasting " width="894" height="161"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Illustration of array broadcasting</p>
			<p>If the shapes don't match – for example, when combining a <code>(3, 2)</code> array with a <code>(2, 2)</code> array – NumPy will throw an exception.</p>
			<p>If one of the axis's sizes is 1, the array will be repeated over this axis until the shapes match. To illustrate this point, let's consider that we have an array of the following shape:</p>
			<pre>    5, 10, 2 </pre>
			<p>Now, let's consider that we want to broadcast it with an array of shape <code>(5, 1, 2)</code>; the array will be repeated on the second axis 10 times, as shown here:</p>
			<pre>    5, 10, 2 
    5,  1, 2 → repeated 
    - - - - 
    5, 10, 2 </pre>
			<p>Earlier, we saw that it is possible to freely reshape arrays to add axes of size 1. Using the <code>numpy.newaxis</code> constant while indexing will<a id="_idIndexMarker179"/> introduce an extra dimension. For instance, if we have a <code>(5, 2)</code> array and we<a id="_idIndexMarker180"/> want to combine it with one of shape <code>(5, 10, 2)</code>, we can add an extra axis in the middle, as shown in the following code, to obtain a compatible <code>(5, 1, 2)</code> array:</p>
			<pre>    A = np.random.rand(5, 10, 2) 
    B = np.random.rand(5, 2) 
    A * B[:, np.newaxis, :] </pre>
			<p>This feature can be used, for example, to operate on all possible combinations of the two arrays. One of these applications is the <em class="italic">outer product</em>. Let's consider that we have the following two arrays:</p>
			<pre>    a = [a1, a2, a3] 
    b = [b1, b2, b3] </pre>
			<p>The outer product is a matrix containing the product of all the possible combinations <em class="italic">(i, j)</em> of the two array elements, as shown in the following snippet:</p>
			<pre>    a x b = a1*b1, a1*b2, a1*b3 
            a2*b1, a2*b2, a2*b3 
            a3*b1, a3*b2, a3*b3 </pre>
			<p>To calculate this using NumPy, we will repeat the <code>[a1, a2, a3]</code> elements in one dimension, the <code>[b1, b2, b3]</code> elements in another dimension, and then take their element-wise product, as shown in the following diagram:</p>
			<div><div><img src="img/Figure_3.2_B17499.jpg" alt="Figure 3.2 – Illustration of an outer product " width="989" height="260"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Illustration of an outer product</p>
			<p>Using code, our strategy will be to transform the <code>a</code> array from shape <code>(3,)</code> into shape <code>(3, 1)</code>, and the <code>b</code> array from shape <code>(3,)</code> into shape <code>(1, 3)</code>. These two arrays are broadcasted in the two dimensions and get multiplied together using the following code:</p>
			<pre>    AB = a[:, np.newaxis] * b[np.newaxis, :] </pre>
			<p>This operation is very fast<a id="_idIndexMarker181"/> and extremely effective as it avoids Python loops and can process<a id="_idIndexMarker182"/> a high number of elements at speeds comparable with pure C or FORTRAN code.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor053"/>Mathematical operations</h2>
			<p>NumPy includes the most<a id="_idIndexMarker183"/> common mathematical operations available for broadcasting by default, ranging from simple algebra to trigonometry, rounding, and logic.</p>
			<p>For instance, to take the square root of every element in the array, we can use <code>numpy.sqrt</code>, as shown in the following code:</p>
			<pre>    np.sqrt(np.array([4, 9, 16])) 
    # Result:
    # array([2., 3., 4.]) </pre>
			<p>The comparison operators are useful when we're trying to filter certain elements based on a condition. Imagine that we have an array of random numbers from <code>0</code> to <code>1</code>, and we want to extract all the numbers greater than <code>0.5</code>. We can use the <code>&gt;</code> operator on the array to obtain a <code>bool</code> array, as follows:</p>
			<pre>    a = np.random.rand(5, 3) 
    a &gt; 0.3 
    # Result:
    # array([[ True, False,  True],
    #        [ True,  True,  True],
    #        [False,  True,  True],
    #        [ True,  True, False],
    #        [ True,  True, False]], dtype=bool) </pre>
			<p>The resulting <code>bool</code> array can then be reused as an index to retrieve the elements that are greater than <code>0.5</code>:</p>
			<pre>    a[a &gt; 0.5] 
    print(a[a&gt;0.5]) 
    # Output:
    # [ 0.9755  0.5977  0.8287  0.6214  0.5669  0.9553  
        0.5894  0.7196  0.9200  0.5781  0.8281 ] </pre>
			<p>NumPy also implements methods such as <code>ndarray.sum</code>, which takes the sum of all the elements on an axis. If we have an array of shape <code>(5, 3)</code>, we can use the <code>ndarray.sum</code> method to sum the elements<a id="_idIndexMarker184"/> on the first axis, the second axis, or over all the elements of the array, as illustrated in the following snippet:</p>
			<pre>    a = np.random.rand(5, 3) 
    a.sum(axis=0) 
    # Result:
    # array([ 2.7454,  2.5517,  2.0303]) 
    a.sum(axis=1) 
    # Result:
    # array([ 1.7498,  1.2491,  1.8151,  1.9320,  0.5814]) 
    a.sum() # With no argument operates on flattened array 
    # Result:
    # 7.3275 </pre>
			<p>Note that by summing the elements<a id="_idIndexMarker185"/> over an axis, we eliminate that axis. From the preceding example, the sum on axis <code>0</code> produces an array of shape <code>(3,)</code>, while the sum on axis <code>1</code> produces an array of shape <code>(5,)</code>.</p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor054"/>Calculating the norm</h2>
			<p>We can review the basic concepts illustrated in this section by calculating the <em class="italic">norm</em> of a set of coordinates. The norm<a id="_idIndexMarker186"/> of a pair of coordinates is an important concept in linear algebra and is often interpreted as the magnitude of the corresponding line segment. For a two-dimensional vector, the norm is defined as follows:</p>
			<pre>    norm = sqrt(x**2 + y**2) </pre>
			<p>Given an array of 10 coordinates (<em class="italic">x</em>, <em class="italic">y</em>), we want to find the norm of each coordinate. We can calculate the norm by performing these steps:</p>
			<ol>
				<li value="1">Square the coordinates, obtaining an array that contains <code>(x**2, y**2)</code> elements.</li>
				<li>Sum those with <code>numpy.sum</code> over the last axis.</li>
				<li>Take the square root, element-wise, with <code>numpy.sqrt</code>.</li>
			</ol>
			<p>The final expression can be compressed into a single line:</p>
			<pre>    r_i = np.random.rand(10, 2) 
    norm = np.sqrt((r_i ** 2).sum(axis=1)) 
    print(norm)
    # Output:
    # [ 0.7314  0.9050  0.5063  0.2553  0.0778   0.9143   
        1.3245  0.9486  1.010   1.0212] </pre>
			<p>We can verify that this method<a id="_idIndexMarker187"/> of calculating <code>norm</code> gives us the correct answer while having compact code.</p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor055"/>Rewriting the particle simulator in NumPy</h1>
			<p>In this section, we will<a id="_idIndexMarker188"/> optimize our particle simulator by rewriting some parts<a id="_idIndexMarker189"/> of it in NumPy. From the profiling we did in <a href="B17499_01_Final_SS_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Benchmarking and Profiling</em>, we found that the slowest part of our program is the following loop, which is contained in the <code>ParticleSimulator.evolve</code> method:</p>
			<pre>    for i in range(nsteps): 
      for p in self.particles: 
        norm = (p.x**2 + p.y**2)**0.5 
        v_x = (-p.y)/norm 
        v_y = p.x/norm 
        d_x = timestep * p.ang_vel * v_x 
        d_y = timestep * p.ang_vel * v_y 
        p.x += d_x 
        p.y += d_y </pre>
			<p>You may have noticed that the body of the loop acts solely on the current particle. If we had an array containing the particle positions and angular speed, we could rewrite the loop using a broadcasted operation. In contrast, the loop's steps depend on the previous step and cannot be parallelized in this way.</p>
			<p>So, it is natural to store<a id="_idIndexMarker190"/> all the array coordinates in an array<a id="_idIndexMarker191"/> of shape <code>(nparticles, 2)</code> and the angular speed in an array of shape <code>(nparticles,)</code>, where <code>nparticles</code> is the number of particles. We'll call those arrays <code>r_i</code> and <code>ang_vel_i</code>:</p>
			<pre>    r_i = np.array([[p.x, p.y] for p in self.particles]) 
    ang_vel_i = np.array([p.ang_vel for p in \
      self.particles]) </pre>
			<p>The velocity direction, which is perpendicular to the vector (<em class="italic">x</em>, <em class="italic">y</em>), was defined as follows:</p>
			<pre>    v_x = -y / norm 
    v_y = x / norm </pre>
			<p>The norm can be calculated using the strategy illustrated in the <em class="italic">Calculating the norm</em> section under the <em class="italic">Getting started with NumPy</em> heading:</p>
			<pre>    norm_i = ((r_i ** 2).sum(axis=1))**0.5 </pre>
			<p>For the (<em class="italic">-y</em>, <em class="italic">x</em>) components, we need to swap the <em class="italic">x</em> and <em class="italic">y</em> columns in <code>r_i</code> and then multiply the first column by <code>-1</code>, as shown in the following code:</p>
			<pre>    v_i = r_i[:, [1, 0]] / norm_i 
    v_i[:, 0] *= -1 </pre>
			<p>To calculate the displacement, we need to compute the product of <code>v_i</code>, <code>ang_vel_i</code>, and <code>timestep</code>. Since <code>ang_vel_i</code> is of shape <code>(nparticles,)</code>, it needs a new axis to operate with <code>v_i</code> of shape <code>(nparticles, 2)</code>. We will do that using <code>numpy.newaxis</code>, as follows:</p>
			<pre>    d_i = timestep * ang_vel_i[:, np.newaxis] * v_i 
    r_i += d_i </pre>
			<p>Outside the loop, we have to update the particle instances with the new coordinates, <em class="italic">x</em> and <em class="italic">y</em>, as follows:</p>
			<pre>    for i, p in enumerate(self.particles): 
      p.x, p.y = r_i[i] </pre>
			<p>To summarize, we will implement<a id="_idIndexMarker192"/> a method called <code>ParticleSimulator.evolve_numpy</code> and benchmark it against the pure Python<a id="_idIndexMarker193"/> version, renamed as <code>ParticleSimulator.evolve_python</code>:</p>
			<pre>    def evolve_numpy(self, dt): 
      timestep = 0.00001 
      nsteps = int(dt/timestep) 
      r_i = np.array([[p.x, p.y] for p in self.particles]) 
      ang_vel_i = np.array([p.ang_vel for p in \
        self.particles]) 
      for i in range(nsteps): 
        norm_i = np.sqrt((r_i ** 2).sum(axis=1)) 
        v_i = r_i[:, [1, 0]] 
        v_i[:, 0] *= -1 
        v_i /= norm_i[:, np.newaxis] 
        d_i = timestep * ang_vel_i[:, np.newaxis] * v_i 
        r_i += d_i 
        for i, p in enumerate(self.particles): 
          p.x, p.y = r_i[i] </pre>
			<p>We will also update the benchmark<a id="_idIndexMarker194"/> to conveniently change<a id="_idIndexMarker195"/> the number of particles and the simulation method, as follows:</p>
			<pre>    def benchmark(npart=100, method='python'): 
      particles = [Particle(uniform(-1.0, 1.0),     
                            uniform(-1.0, 1.0),
                            uniform(-1.0, 1.0))  
                            for i in range(npart)] 
      simulator = ParticleSimulator(particles) 
      if method=='python': 
        simulator.evolve_python(0.1) 
      elif method == 'numpy': 
        simulator.evolve_numpy(0.1) </pre>
			<p>Let's run the benchmark in an IPython session:</p>
			<pre>    from simul import benchmark 
    %timeit benchmark(100, 'python') 
    1 loops, best of 3: 614 ms per loop 
    %timeit benchmark(100, 'numpy') 
    1 loops, best of 3: 415 ms per loop </pre>
			<p>We have made some improvements, but it doesn't look like a huge speed boost. The power of NumPy is revealed when handling big arrays. If we increase the number of particles, we will note a more significant performance boost:</p>
			<pre>    %timeit benchmark(1000, 'python') 
    1 loops, best of 3: 6.13 s per loop 
    %timeit benchmark(1000, 'numpy') 
    1 loops, best of 3: 852 ms per loop </pre>
			<p>The plot in the following diagram was produced by running the benchmark with different particle numbers:</p>
			<div><div><img src="img/Figure_3.3_B17499.jpg" alt="Figure 3.3 – The running time growth of pure Python versus NumPy " width="797" height="391"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – The running time growth of pure Python versus NumPy</p>
			<p>The plot shows that both implementations scale linearly with particle size, but the runtime in the pure<a id="_idIndexMarker196"/> Python version grows much faster than the NumPy<a id="_idIndexMarker197"/> version; at greater sizes, we have a greater NumPy advantage. In general, when using NumPy, you should try to pack things into large arrays and group the calculations using the broadcasting feature.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor056"/>Reaching optimal performance with numexpr</h1>
			<p>When handling complex expressions, NumPy stores<a id="_idIndexMarker198"/> intermediate results in memory. David M. Cooke wrote a package called <code>numexpr</code>, which optimizes and compiles array expressions on the fly. It works by optimizing the usage of the CPU cache and by taking advantage of multiple processors.</p>
			<p>Its usage is generally straightforward and is based on a single function: <code>numexpr.evaluate</code>. The function takes a string<a id="_idIndexMarker199"/> containing an array expression as its first argument. The syntax is basically identical to that of NumPy. For example, we can calculate a simple <code>a + b * c</code> expression in the following way:</p>
			<pre>    a = np.random.rand(10000) 
    b = np.random.rand(10000) 
    c = np.random.rand(10000) 
    d = ne.evaluate('a + b * c') </pre>
			<p>The <code>numexpr</code> package increases performance in almost all cases, but to get a substantial advantage, you should use it with large arrays. An application that involves a large array is the calculation of a <em class="italic">distance matrix</em>. In a particle system, a distance matrix contains all the possible distances between the particles. To calculate it, we should calculate all the vectors connecting any two particles, <code>(i,j)</code>, as follows:</p>
			<pre>    x_ij = x_j - x_i 
    y_ij = y_j - y_i. </pre>
			<p>Then, we must calculate the length of this vector by taking its norm, as shown in the following code:</p>
			<pre>    d_ij = sqrt(x_ij**2 + y_ij**2) </pre>
			<p>We can write this in NumPy by employing the usual broadcasting rules (the operation is similar to the outer product):</p>
			<pre>    r = np.random.rand(10000, 2) 
    r_i = r[:, np.newaxis] 
    r_j = r[np.newaxis, :] 
    d_ij = r_j - r_i </pre>
			<p>Finally, we must calculate the norm over the last axis using the following line of code:</p>
			<pre>    d_ij = np.sqrt((d_ij ** 2).sum(axis=2)) </pre>
			<p>Rewriting the same expression using the <code>numexpr</code> syntax is extremely easy. The <code>numexpr</code> package (aliased <code>ne</code> in our following code) doesn't support slicing in its array expression; therefore, we first need to prepare the operands for broadcasting by adding an extra dimension, as follows:</p>
			<pre>    r = np.random.rand(10000, 2) 
    r_i = r[:, np.newaxis] 
    r_j = r[np.newaxis, :] </pre>
			<p>At this point, we should try to pack as many operations as possible into a single expression to allow significant optimization.</p>
			<p>Most of the NumPy mathematical functions<a id="_idIndexMarker200"/> are also available in <code>numexpr</code>. However, there is a limitation – the reduction operations (the ones that reduce an axis, such as <code>sum</code>) have to happen last. Therefore, we have to calculate the sum first, then step out of <code>numexpr</code>, and finally calculate the square root in another expression:</p>
			<pre>    import numexpr as ne
    d_ij = ne.evaluate('sum((r_j - r_i)**2, 2)') 
    d_ij = ne.evaluate('sqrt(d_ij)') </pre>
			<p>The <code>numexpr</code> compiler will avoid redundant memory allocation by not storing intermediate results. When possible, it will also distribute the operations over multiple processors. In the <code>distance_matrix.py</code> file, you will find two functions that implement the two versions – <code>distance_matrix_numpy</code> and <code>distance_matrix_numexpr</code>:</p>
			<pre>    from distance_matrix import (distance_matrix_numpy, \
       distance_matrix_numexpr) 
    %timeit distance_matrix_numpy(10000) 
    1 loops, best of 3: 3.56 s per loop 
    %timeit distance_matrix_numexpr(10000) 
    1 loops, best of 3: 858 ms per loop </pre>
			<p>By simply converting the expressions to use <code>numexpr</code>, we were able to obtain a 4.5x increase in performance over standard NumPy. The <code>numexpr</code> package can be used every time you need to optimize a NumPy expression that involves large arrays and complex operations, and you can do so while making minimal changes to the code.</p>
			<p>Overall, we have seen that NumPy, in combination with numexpr, offers powerful APIs when it comes to working with multidimensional data. However, in many use cases, data is only two-dimensional but is <em class="italic">labeled</em> in the sense that the data axes include explicit information about<a id="_idIndexMarker201"/> the type of data they contain. This is the case for data extracted from database tables. In such situations, pandas is the most popular and one of the best libraries in Python for this, as we will see next.</p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor057"/>Working with database-style data with pandas</h1>
			<p>pandas is a library that was originally<a id="_idIndexMarker202"/> developed by Wes McKinney. It was designed to analyze datasets in a seamless and performant way. In recent years, this powerful library has seen<a id="_idIndexMarker203"/> incredible growth and a huge<a id="_idIndexMarker204"/> adoption by the Python community. In this section, we will introduce the main concepts and tools provided in this library, and we will use them to increase the performance of various use cases that can't otherwise be addressed with NumPy's vectorized operations and broadcasting.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor058"/>pandas fundamentals</h2>
			<p>While NumPy deals mostly with arrays, pandas's main data structures are <code>pandas.Series</code>, <code>pandas.DataFrame</code>, and <code>pandas.Panel</code>. In the rest of this chapter, we will abbreviate <code>pandas</code> to <code>pd</code>.</p>
			<p>The main difference between a <code>pd.Series</code> object and an <code>np.array</code> is that a <code>pd.Series</code> object associates<a id="_idIndexMarker205"/> a specific <em class="italic">key</em> with each element of an array. Let's see how this works in practice with an example.</p>
			<p>Let's assume that we are trying to test a new blood pressure drug and we want to store, for each patient, whether the patient's blood pressure improved after administering the drug. We can encode this information by associating each subject ID (represented by an integer) with <code>True</code> if the drug was effective and <code>False</code> otherwise:</p>
			<ol>
				<li value="1">We can create a <code>pd.Series</code> object by associating an array of keys – the patients – to the array of values that represent the drug's effectiveness. This array of keys can<a id="_idIndexMarker206"/> be passed to the <code>Series</code> constructor using the <code>index</code> argument, as shown in the following snippet:<pre>    import pandas as pd
    patients = [0, 1, 2, 3]
    effective = [True, True, False, False]
    effective_series = pd.Series(effective, \
      index=patients)</pre></li>
				<li>Associating a set of integers from <em class="italic">0</em> to <em class="italic">N</em> with a set of values can technically be implemented with <code>np.array</code> since, in this case, the key will simply be the position of the element in the array. In pandas, keys are not limited to integers; they can also be strings, floating-point numbers, and generic (hashable) Python objects. For example, we can easily turn our IDs into strings with little effort, as shown in the following code:<pre>    patients = ["a", "b", "c", "d"]
    effective = [True, True, False, False]
    effective_series = pd.Series(effective, \
      index=patients)</pre></li>
			</ol>
			<p>An interesting observation is that, while NumPy arrays can be thought of as a contiguous collection of values similar to Python lists, the pandas <code>pd.Series</code> object can be thought of as a structure that maps keys to values, similar to Python dictionaries.</p>
			<ol>
				<li value="3">What if you want to store the initial and final blood pressure for each patient? In pandas, you can use a <code>pd.DataFrame</code> object to associate multiple data with each key.</li>
			</ol>
			<p><code>pd.DataFrame</code> can be initialized, similarly to a <code>pd.Series</code> object, by passing a dictionary of columns and an index. In the following example, we will see how<a id="_idIndexMarker207"/> to create <code>pd.DataFrame</code> containing four columns that represent the initial and final measurements of systolic and diastolic blood pressure for our patients:</p>
			<pre>    patients = ["a", "b", "c", "d"]
    columns = {
      "sys_initial": [120, 126, 130, 115],
      "dia_initial": [75, 85, 90, 87],
      "sys_final": [115, 123, 130, 118],
      "dia_final": [70, 82, 92, 87]
    }
    
    df = pd.DataFrame(columns, index=patients)</pre>
			<ol>
				<li value="4">Equivalently, you can think of <code>pd.DataFrame</code> as a collection of <code>pd.Series</code>. It is possible to directly initialize <code>pd.DataFrame</code> using a dictionary of <code>pd.Series</code> instances:<pre>    columns = {
      "sys_initial": pd.Series([120, 126, 130, 115], \
        index=patients),
      "dia_initial": pd.Series([75, 85, 90, 87], \
        index=patients),
      "sys_final": pd.Series([115, 123, 130, 118], \
        index=patients),
      "dia_final": pd.Series([70, 82, 92, 87], \
        index=patients)
    }
    df = pd.DataFrame(columns)</pre></li>
				<li>To inspect the content of a <code>pd.DataFrame</code> or <code>pd.Series</code> object, you can use the <code>pd.Series.head</code> and <code>pd.DataFrame.head</code> methods, which<a id="_idIndexMarker208"/> print the first few rows of the dataset:<pre>    effective_series.head()
    # Output:
    # a True
    # b True
    # c False
    # d False
    # dtype: bool
    df.head()
    # Output:
    #    dia_final  dia_initial  sys_final sys_initial
    # a         70           75        115          
    120
    # b         82           85        123          
    126
    # c         92           90        130          
    130
    # d         87           87        118          
    115</pre></li>
			</ol>
			<p>Just like a <code>pd.DataFrame</code> can be used to store a collection of <code>pd.Series</code>, you can use a <code>pd.Panel</code> to store a collection of <code>pd.DataFrames</code>. We will not cover the usage of <code>pd.Panel</code> as it is not used<a id="_idIndexMarker209"/> as often as <code>pd.Series</code> and <code>pd.DataFrame</code>. To learn more about <code>pd.Panel</code>, make sure that you refer to the excellent documentation at <a href="http://pandas.pydata.org/pandas-docs/stable/dsintro.html#panel">http://pandas.pydata.org/pandas-docs/stable/dsintro.html#panel</a>.</p>
			<h3 id="_idParaDest-62">Indexing Series and DataFrame objects</h3>
			<p>In many instances, we might want to access<a id="_idIndexMarker210"/> certain elements stored inside a <code>pd.Series</code> or a <code>pd.DataFrame</code> object. In the following steps, we will see<a id="_idIndexMarker211"/> how we can index these objects:</p>
			<ol>
				<li value="1">Retrieving data from a <code>pd.Series</code>, given its <em class="italic">key</em>, can be done intuitively by indexing the <code>pd.Series.loc</code> attribute:<pre>    effective_series.loc["a"]
    # Result:
    # True</pre></li>
				<li>It is also possible to access the elements, given their <em class="italic">position</em> in the underlying array, using the <code>pd.Series.iloc</code> attribute:<pre>    effective_series.iloc[0]
    # Result:
    # True</pre></li>
				<li>Indexing <code>pd.DataFrame</code> works similarly. For example, you can use <code>pd.DataFrame.loc</code> to extract a row by key, and you can use <code>pd.DataFrame.iloc</code> to extract a row by position:<pre>    df.loc["a"]
    df.iloc[0]
    # Result:
    # dia_final 70
    # dia_initial 75
    # sys_final 115
    # sys_initial 120
    # Name: a, dtype: int64</pre></li>
				<li>An important aspect is that the return type in this case is a <code>pd.Series</code>, where each column is a new key. To retrieve a specific row and column, you can use the following code. The <code>loc</code> attribute will index both<a id="_idIndexMarker212"/> the row and the column by<a id="_idIndexMarker213"/> key, while the <code>iloc</code> version will index the row and the column by an integer:<pre>    df.loc["a", "sys_initial"] # is equivalent to
    df.loc["a"].loc["sys_initial"]
    df.iloc[0, 1] # is equivalent to
    df.iloc[0].iloc[1]</pre></li>
				<li>Retrieving a column from a <code>pd.DataFrame</code> by name can be achieved by regular indexing or attribute access. To retrieve a column by position, you can either use <code>iloc</code> or use the <code>pd.DataFrame.column</code> attribute to retrieve the name of the column:<pre>    # Retrieve column by name
    df["sys_initial"] # Equivalent to
    df.sys_initial
    # Retrieve column by position
    df[df.columns[2]] # Equivalent to
    df.iloc[:, 2]</pre></li>
			</ol>
			<p>These methods also support more advanced indexing, similar to those of NumPy, such as <code>bool</code>, lists, and <code>int</code> arrays.</p>
			<p>Now, it's time for some performance considerations. There are some differences between an index in pandas and a dictionary. For example, while the keys of a dictionary cannot contain duplicates, pandas indexes can contain repeated elements. This flexibility, however, comes at a cost – if we try to access an element in a non-unique index, we may incur substantial performance loss – the access will be <em class="italic">O</em>(<em class="italic">N</em>), like a linear search, rather than <em class="italic">O</em>(1), like a dictionary.</p>
			<p>A way to mitigate<a id="_idIndexMarker214"/> this effect is to sort the index; this will allow pandas to use<a id="_idIndexMarker215"/> a binary search algorithm with a computational complexity of <em class="italic">O</em>(<em class="italic">log</em>(<em class="italic">N</em>)), which is much better. This can be accomplished using the <code>pd.Series.sort_index</code> function, as shown in the following code (the same applies for <code>pd.DataFrame</code>):</p>
			<pre>    # Create a series with duplicate index
    index = list(range(1000)) + list(range(1000))
    # Accessing a normal series is a O(N) operation
    series = pd.Series(range(2000), index=index)
    # Sorting the will improve look-up scaling to O(log(N))
    series.sort_index(inplace=True)</pre>
			<p>The timings for the different versions are summarized in the following table. If you'd like to rerun this benchmarking yourself, please refer to <code>Chapter03/Pandas.ipynb</code>:</p>
			<div><div><img src="img/Table__3.1_B17499.jpg" alt="Table 3.1 – Performance analysis for pandas indexing " width="1650" height="378"/>
				</div>
			</div>
			<p class="figure-caption">Table 3.1 – Performance analysis for pandas indexing</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor059"/>Database-style operations with pandas</h2>
			<p>You may have noted that the <em class="italic">tabular</em> data is similar to what is usually stored in a database. A database is usually indexed using a primary key, and the various columns can have different data types, just like in a <code>pd.DataFrame</code>. </p>
			<p>The efficiency of the index operations<a id="_idIndexMarker216"/> in pandas makes it suitable for<a id="_idIndexMarker217"/> database-style manipulations, such as counting, joining, grouping, and aggregations.</p>
			<h3 id="_idParaDest-64">Mapping</h3>
			<p>pandas supports<a id="_idIndexMarker218"/> element-wise operations, just like NumPy (after all, <code>pd.Series</code> stores their data using <code>np.array</code>).</p>
			<p>For example, it is possible to apply transformation very easily to both <code>pd.Series</code> and <code>pd.DataFrame</code>:</p>
			<pre>    np.log(df.sys_initial) # Logarithm of a series
    df.sys_initial ** 2    # Square a series
    np.log(df)             # Logarithm of a dataframe
    df ** 2                # Square of a dataframe</pre>
			<p>You can also perform element-wise operations between two <code>pd.Series</code> objects in a way similar to NumPy. An important difference is that the operands will be matched by key, rather than by position; if there is a mismatch in the index, the resulting value will be set to <code>NaN</code>. Both scenarios are exemplified in the following example:</p>
			<pre>    # Matching index
    a = pd.Series([1, 2, 3], index=["a", "b", "c"])
    b = pd.Series([4, 5, 6], index=["a", "b", "c"])
    a + b
    # Result: 
    # a 5
    # b 7
    # c 9
    # dtype: int64
    # Mismatching index
    b = pd.Series([4, 5, 6], index=["a", "b", "d"])
    a + b
    # Result:
    # a 5.0
    # b 7.0
    # c NaN
    # d NaN
    # dtype: float64</pre>
			<p>For added flexibility, pandas offers the <code>map</code>, <code>apply</code>, and <code>applymap</code> methods, which can be used to apply specific transformations.</p>
			<p>The <code>pd.Series.map</code> method can be used to execute a function for each value and return a <code>pd.Series</code> containing each<a id="_idIndexMarker219"/> result. In the following example, we can see how to apply the <code>superstar</code> function to each element of a <code>pd.Series</code>:</p>
			<pre>    a = pd.Series([1, 2, 3], index=["a", "b", "c"])
    def superstar(x):
        return '*' + str(x) + '*'
    a.map(superstar)
    # Result:
    # a *1*
    # b *2*
    # c *3*
    # dtype: object</pre>
			<p>The <code>pd.DataFrame.applymap</code> function is the equivalent of <code>pd.Series.map</code>, but for <code>DataFrames</code>:</p>
			<pre>    df.applymap(superstar)
    # Result:
    #    dia_final  dia_initial  sys_final  sys_initial
    # a       *70*         *75*      *115*        *120*
    # b       *82*         *85*      *123*        *126*
    # c       *92*         *90*      *130*        *130*
    # d       *87*         *87*      *118*        *115*</pre>
			<p>Finally, the <code>pd.DataFrame.apply</code> function can apply the passed function to each column or each row, rather<a id="_idIndexMarker220"/> than element-wise. This selection can be performed with the argument axis, where a value of <code>0</code> (the default) corresponds to columns and <code>1</code> corresponds to rows. Also, note that the return value of <code>apply</code> is a <code>pd.Series</code>:</p>
			<pre>    df.apply(superstar, axis=0)
    # Result:
    # dia_final *a 70nb 82nc 92nd 87nName: dia...
    # dia_initial *a 75nb 85nc 90nd 87nName: dia...
    # sys_final *a 115nb 123nc 130nd 118nName:...
    # sys_initial *a 120nb 126nc 130nd 115nName:...
    # dtype: object
    df.apply(superstar, axis=1)
    # Result:
    # a *dia_final 70ndia_initial 75nsys_f...
    # b *dia_final 82ndia_initial 85nsys_f...
    # c *dia_final 92ndia_initial 90nsys_f...
    # d *dia_final 87ndia_initial 87nsys_f...
    # dtype: object</pre>
			<p>pandas also supports efficient <code>numexpr</code>-style expressions with the convenient <code>eval</code> method. For example, if we want to calculate<a id="_idIndexMarker221"/> the difference between the final and initial blood pressure, we can write the expression as a string, as shown in the following code:</p>
			<pre>    df.eval("sys_final - sys_initial")
    # Result:
    # a -5
    # b -3
    # c 0
    # d 3
    # dtype: int64</pre>
			<p>It is also possible to create new columns using the assignment operator in the <code>pd.DataFrame.eval</code> expression. Note that if the <code>inplace=True</code> argument is used, the operation will be applied directly to the original <code>pd.DataFrame</code>; otherwise, the function will return a new DataFrame. In the following example, we are computing the difference between <code>sys_final</code> and <code>sys_initial</code>, and we store it in the <code>sys_delta</code> column:</p>
			<pre>df.eval("sys_delta = sys_final - sys_initial", \
  inplace=False)
# Result:
#     dia_final   dia_initial   sys_final   sys_initial   
   sys_delta
# a          70            75         115           120     
-5
# b          82            85         123           126     
  -3
# c          92            90         130           130     
  0
# d          87            87         118           115     
  3</pre>
			<h3 id="_idParaDest-65">Grouping, aggregations, and transforms</h3>
			<p>One of the most appreciated features of pandas is its simple and concise method of grouping, transforming, and aggregating data. To demonstrate this concept, let's extend our dataset<a id="_idIndexMarker222"/> by adding two new patients<a id="_idIndexMarker223"/> that we didn't administer<a id="_idIndexMarker224"/> the treatment to (this is usually called a <em class="italic">control group</em>). We will also include<a id="_idIndexMarker225"/> a column, <code>drug_admst</code>, which records whether the patient was administered the treatment:</p>
			<pre>    patients = ["a", "b", "c", "d", "e", "f"]
    columns = {
      "sys_initial": [120, 126, 130, 115, 150, 117],
      "dia_initial": [75, 85, 90, 87, 90, 74],
      "sys_final": [115, 123, 130, 118, 130, 121],
      "dia_final": [70, 82, 92, 87, 85, 74],
      "drug_admst": [True, True, True, True, False, False]
    }
    df = pd.DataFrame(columns, index=patients)</pre>
			<p>At this point, we may be interested<a id="_idIndexMarker226"/> to know how the blood pressure<a id="_idIndexMarker227"/> changed between the two groups. You can group the patients according to <code>drug_amst</code> using the <code>pd.DataFrame.groupby</code> function. The return value will be the <code>DataFrameGroupBy</code> object, which can be iterated to obtain a new <code>pd.DataFrame</code> for each value of the <code>drug_admst</code> column:</p>
			<pre>    df.groupby('drug_admst')
    for value, group in df.groupby('drug_admst'):
        print("Value: {}".format(value))
        print("Group DataFrame:")
        print(group)
# Output:
# Value: False
# Group DataFrame:
#    dia_final   dia_initial   drug_admst   sys_final   
   sys_initial
# e         85            90        False         130       
  150
# f         74            74        False         121       
  117
# Value: True
# Group DataFrame:
#    dia_final   dia_initial   drug_admst   sys_final   
   sys_initial
# a         70            75         True         115       
  120
# b         82            85         True         123       
  126
# c         92            90         True         130       
  130
# d         87            87         True         118       
  115</pre>
			<p>Iterating the <code>DataFrameGroupBy</code> object is rarely necessary because, thanks to method chaining, it is possible to calculate group-related properties directly. For example, we may want to<a id="_idIndexMarker228"/> calculate the mean, max, or standard deviation for each group. All those<a id="_idIndexMarker229"/> operations that summarize the data<a id="_idIndexMarker230"/> in some way are called aggregations and can be performed using the <code>agg</code> method. The result of <code>agg</code> is another <code>pd.DataFrame</code> that relates the grouping variables and the result of the aggregation, as illustrated in the following code:</p>
			<pre>df.groupby('drug_admst').agg(np.mean)
#              dia_final   dia_initial   sys_final   sys_in
  itial
# drug_admst 
# False            79.50         82.00       125.5        
  133.50
# True             82.75         84.25       121.5        
  122.75</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">It is also possible to perform processing on the DataFrame groups that do not represent a summarization. One common<a id="_idIndexMarker231"/> example of such an operation is filling in missing values. Those intermediate steps are called <strong class="bold">transforms</strong>.</p>
			<p>We can illustrate<a id="_idIndexMarker232"/> this concept with an example. Let's assume<a id="_idIndexMarker233"/> that we have a few missing<a id="_idIndexMarker234"/> values in our dataset, and we want to replace those values with the average of the other values in the same group. This can be accomplished using a transform, as follows:</p>
			<pre>df.loc['a','sys_initial'] = None
df.groupby('drug_admst').transform(lambda df: \
  df.fillna(df.mean())) 
#     dia_final    dia_initial   sys_final   sys_initial
# a          70             75         115    123.666667
# b          82             85         123    126.000000
# c          92             90         130    130.000000
# d          87             87         118    115.000000
# e          85             90         130    150.000000
# f          74             74         121    117.000000</pre>
			<h3 id="_idParaDest-66">Joining</h3>
			<p><code>H1</code>, <code>H2</code>, and <code>H3</code> labels, and we can store the address and identifier of the hospital in a <code>hospital</code> table:</p>
			<pre>    hospitals = pd.DataFrame(
      { "name" : ["City 1", "City 2", "City 3"],
        "address" : ["Address 1", "Address 2", "Address \
          3"],
        "city": ["City 1", "City 2", "City 3"] },
      index=["H1", "H2", "H3"])
    hospital_id = ["H1", "H2", "H2", "H3", "H3", "H3"]
    df['hospital_id'] = hospital_id</pre>
			<p>Now, we want to find the city where the measure was taken for each patient. We need to <em class="italic">map</em> the keys from the <code>hospital_id</code> column to the city stored in the <code>hospitals</code> table.</p>
			<p>This can be implemented in Python using dictionaries:</p>
			<pre>    hospital_dict = {
     "H1": ("City 1", "Name 1", "Address 1"),
     "H2": ("City 2", "Name 2", "Address 2"),
     "H3": ("City 3", "Name 3", "Address 3")
    }
    cities = [hospital_dict[key][0] 
             for key in hospital_id]</pre>
			<p>This algorithm runs efficiently with an <em class="italic">O</em>(<em class="italic">N</em>) time complexity, where <em class="italic">N</em> is the size of <code>hospital_id</code>. pandas allows you to encode the same operation using simple indexing; the advantage is that the<a id="_idIndexMarker237"/> join will be performed in heavily optimized Cython and with efficient hashing algorithms. The preceding simple Python expression can easily be converted into pandas in this way:</p>
			<pre>    cities = hospitals.loc[hospital_id, "city"]</pre>
			<p>More advanced joins can also be performed with the <code>pd.DataFrame.join</code> method, which will produce a new <code>pd.DataFrame</code> that will attach the hospital information for each patient:</p>
			<pre>    result = df.join(hospitals, on='hospital_id')
    result.columns
    # Result:
    # Index(['dia_final', 'dia_initial', 'drug_admst', 
    # 'sys_final', 'sys_initial',
    # 'hospital_id', 'address', 'city', 'name'],
    # dtype='object')</pre>
			<p>This concludes our discussion<a id="_idIndexMarker238"/> on pandas. In the next section, we will talk about xarray, the state-of-the-art tool for working with multidimensional labeled data in Python.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor060"/>High-performance labeled data with xarray</h1>
			<p>With NumPy, we can manipulate multidimensional numerical data and perform mathematical computations<a id="_idIndexMarker239"/> that are highly optimized by low-level C and FORTRAN<a id="_idIndexMarker240"/> code. On the other hand, we have seen that pandas allows us to work with labeled, categorical data that resembles data tables using database-like operations.</p>
			<p>These two tools complement each other: NumPy does not allow categorical data to be mixed in with numerical values, while pandas is mostly limited to two-dimensional, database-like datasets. Combining these tools can help address many data processing needs, but when we are faced with big, multidimensional data that is also labeled, many performance-related problems arise.</p>
			<p>In the last section of this chapter, we will discuss xarray, a library that combines the best of both the NumPy and the pandas worlds and offers one of the best tools for working with labeled multidimensional data. We will explore some of its most prominent features while noting<a id="_idIndexMarker241"/> the improvements we achieve<a id="_idIndexMarker242"/> with xarray over other Python libraries.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor061"/>Analyzing <img src="img/Formula_3_B17499.png" alt="" width="103" height="52"/> concentration</h2>
			<p>To guide our discussion, we will be using the carbon dioxide concentration data, which was collected concerning<a id="_idIndexMarker243"/> the volcano Mauna Loa in Hawaii. The dataset is a time series of monthly measurements of the <img src="img/Formula_3_B174991.png" alt="" width="54" height="28"/> level, starting from 1958 to this day. We have prepared a cleaned version of this dataset for you, which is included in the code repository for this book in the <code>monthly_co2.csv</code> file. </p>
			<p>The data has three simple columns: </p>
			<ul>
				<li>The year of measurement</li>
				<li>The month of measurement</li>
				<li>The measurement itself</li>
			</ul>
			<p>Our goal is to analyze this dataset and visualize any time-related trends. Since we are already familiar with using pandas to work with a <code>.csv</code> file, let's proceed with the library to start:</p>
			<pre>import pandas as pd
df = pd.read_csv('monthly_co2.csv', index_col=[0, 1])
df.head()</pre>
			<p>Make sure that the data file is in the same directory as this code. You may remember that this will read in the file and store the data in a <code>DataFrame</code> object. Here, we are using the first two columns (by using the <code>index_col=[0, 1]</code> argument) as the index of this <code>DataFrame</code> object. Finally, we print out the first five rows of this dataset, which look as follows:</p>
			<pre>          co2
year    month    
1958    3    315.70
     4     317.45
     5     317.51
     6     317.24
     7     315.86</pre>
			<p>Here, we can see that in March <code>1958</code>, the <img src="img/Formula_3_B174992.png" alt="" width="57" height="29"/> level was <code>315.70</code>, and that the following month's measurement was <code>317.45</code>.</p>
			<p>The first thing we'd like to look at is a simple line graph corresponding to the <code>co2</code> column, which is simply<a id="_idIndexMarker244"/> the graph of the <img src="img/Formula_3_B174992.png" alt="" width="57" height="29"/> level as a function of time (in <code>month</code>). With the help of Matplotlib, the go-to plotting tool in Python, we can do this very easily:</p>
			<pre>import matplotlib.pyplot as plt
plt.plot(df.co2.values);</pre>
			<p>The preceding code will produce the following output:</p>
			<div><div><img src="img/Figure_3.4_B17499.jpg" alt="Figure 3.4 – Monthly co2 level " width="596" height="248"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – Monthly <img src="img/Formula_3_B174993.png" alt="" width="66" height="34"/> level</p>
			<p>We can notice two very distinct trends:</p>
			<ul>
				<li>The first is the <em class="italic">global</em> increasing trend, which roughly goes from 320 to 420 during our timeline.</li>
				<li>The second looks like a <em class="italic">seasonal</em> zigzag trend, which is present locally and repeats itself every year.</li>
			</ul>
			<p>To verify this intuition, we can inspect<a id="_idIndexMarker245"/> the average data across the years, which will tell us that the <img src="img/Formula_3_B174993.png" alt="" width="66" height="34"/> level has been rising as a global trend. We can also compute the average measurement for each month and consider how the data changed from January to December. To accomplish this, we will utilize the <code>groupby</code> function by computing the yearly averages and plotting them:</p>
			<pre>by_year = df.groupby('year').mean().co2
plt.plot(by_year);</pre>
			<p>This gives us the following output:</p>
			<div><div><img src="img/Figure_3.5_B17499.jpg" alt="Figure 3.5 – Yearly co2 level, averaged across months " width="529" height="252"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – Yearly <img src="img/Formula_3_B174992.png" alt="" width="57" height="29"/> level, averaged across months</p>
			<p>Just as we expected, we can see the global rising trend of the <img src="img/Formula_3_B174992.png" alt="" width="57" height="29"/> level:</p>
			<pre>by_month = df.groupby('month').mean().co2
plt.plot(by_month);</pre>
			<p>Similarly, the preceding code<a id="_idIndexMarker246"/> generates the following average-by-month data:</p>
			<div><div><img src="img/Figure_3.6_B17499.jpg" alt="Figure 3.6 – Monthly co2 level, averaged across years " width="565" height="248"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – Monthly <img src="img/Formula_3_B174992.png" alt="" width="57" height="29"/> level, averaged across years</p>
			<p>The seasonal trend we suspected<a id="_idIndexMarker247"/> now becomes clear: the <img src="img/Formula_3_B174992.png" alt="" width="57" height="29"/> level tends to rise during the summer and fall between fall and winter.</p>
			<p>So far, we have been using pandas to manipulate our data. Even with this minimal example, we can notice a few things:</p>
			<ul>
				<li>Loading the data, specifically the <code>co2</code> column, as a NumPy array would be inappropriate since we would lose information about the year and month each measurement was made. pandas is the better choice here.</li>
				<li>On the other hand, the <code>groupby</code> function can be unintuitive and costly to work with. Here, we simply want to compute the <em class="italic">average-by-month and average-by-year measurements</em>, but that requires us to group our data by the <code>month</code> column and then by <code>year</code>. Although pandas takes care of this grouping for us behind the scenes, it is an expensive operation, especially if we are working with a significantly large dataset.</li>
				<li>To bypass this inefficiency, we can think of representing the <code>co2</code> column as a two-dimensional NumPy array, where the rows represent years and the columns represent months, and each cell in the array holds the measurement. Now, to compute the averages we want, we could simply calculate the mean along each of the two axes, which we know NumPy can do efficiently. However, once again, we lose the expressiveness of the labeled <code>month</code> and <code>year</code> data we have under pandas.</li>
			</ul>
			<p>This dilemma we are currently<a id="_idIndexMarker248"/> facing is similar to the one that motivated the development of xarray, the premiere tool in Python for working with labeled, multidimensional data. The idea is to extend NumPy's support for fast, multidimensional array computations and allow dimensions (or axes) to have labels, which is one of the main selling points of pandas.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor062"/>The xarray library</h2>
			<p>xarray is developed <a id="_idIndexMarker249"/>and actively maintained by PyData and a part of the NumFOCUS project. To use the library, you must head to <a href="http://xarray.pydata.org/en/stable/installing.html">http://xarray.pydata.org/en/stable/installing.html</a> for more details on how to install it. To continue with our example of the <img src="img/Formula_3_B174992.png" alt="" width="57" height="29"/> concentration level, we will feed the data we have into a <code>Dataset</code> object in xarray using the following code:</p>
			<pre>import xarray as xr
ds = xr.Dataset.from_dataframe(df)</pre>
			<p>If we were to print this object out in a Jupyter notebook, the output would be formatted nicely:</p>
			<div><div><img src="img/Figure_3.7_B17499.jpg" alt="Figure 3.7 – A Dataset instance of xarray, printed in Jupyter " width="1650" height="749"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – A Dataset instance of xarray, printed in Jupyter</p>
			<p>xarray was able to infer that <code>month</code> and <code>year</code> (the index columns in <code>DataFrame</code>) should be the dimension, as indicated by the <code>co2</code> data we care about, which is now a two-dimensional array indexed by <code>year</code> and <code>month</code>.</p>
			<p>xarray makes interacting<a id="_idIndexMarker250"/> and inspecting its objects easy and interactive; you can inspect the values of the coordinates and data variables further by clicking on the icons highlighted in the preceding screenshot.</p>
			<p>We mentioned earlier that xarray combines the best features of NumPy and pandas; this is best illustrated via the slicing/indexing interface it provides. For example, let's say that we'd like to extract the measurements within the first 10 years of the dataset. For the first 5 months, we could apply NumPy-style slicing to the <code>'co2'</code> variable of the <code>ds</code> object:</p>
			<pre>ds['co2'][:10, :5]</pre>
			<p>This gives us a <code>DataArray</code> object containing the requested values:</p>
			<div><div><img src="img/Figure_3.8_B17499.jpg" alt="Figure 3.8 – A DataArray instance of xarray, printed in Jupyter " width="1314" height="768"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – A DataArray instance of xarray, printed in Jupyter</p>
			<p>While NumPy slicing can be flexible, it does not offer much expressiveness for labeled data: we would have to know that the first axis of the implied multidimensional array is <code>year</code>, that the second is <code>month</code>, and that <code>ds['co2'][:10, :5]</code> doesn't explicitly say which years we are selecting for.</p>
			<p>As such, we could use the <code>sel</code> function, which roughly offers the same functionality as pandas filtering. To select the values within the example year of <code>1960</code>, we can simply use the following code:</p>
			<pre>ds['co2'].sel(year=1960)</pre>
			<p>This explicitly tells <a id="_idIndexMarker251"/>us that we are selecting <code>1960</code> along the <code>year</code> axis. For more examples of the different APIs the library offers, you can check out the documentation at <a href="http://xarray.pydata.org/en/stable/api.html">http://xarray.pydata.org/en/stable/api.html</a>.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor063"/>Improved performance</h2>
			<p>Now, we will consider the performance<a id="_idIndexMarker252"/> improvements that xarray offers. Recall that our goal is to compute the average measurement, first for each year to visualize the global trend, and then for each month for the seasonal trend. To do this, we can simply call the <code>mean</code> function while specifying the appropriate (labeled!) dimension.</p>
			<p>First, to obtain the average-by-year, we must compute the mean across the <code>month</code> dimension:</p>
			<pre>ds.mean(dim='month')</pre>
			<p>This returns another <code>Dataset</code> object containing the computed values:</p>
			<div><div><img src="img/Figure_3.9_B17499.jpg" alt="Figure 3.9 – Taking the average across a dimension in xarray " width="1249" height="504"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – Taking the average across a dimension in xarray</p>
			<p>From here, we can simply access the <code>co2</code> variable and pass the array to the <code>plot</code> function of Matplotlib to replicate <em class="italic">Figure 3.5</em>. For <em class="italic">Figure 3.6</em>, we can follow the same procedure using <code>ds.mean(dim='year')</code>.</p>
			<p>The advantage we are gaining here is the expressiveness in our code. If we were using NumPy, we would need to specify the <code>'month'</code>, <code>'year'</code>). This might lead to hard-to-find bugs if you confuse<a id="_idIndexMarker253"/> which axis is which type of data in NumPy.</p>
			<p>Furthermore, the code is simpler and can take advantage of the optimized mean operation, which is managed under the hood by xarray, compared to the expensive <code>groupby</code> function from pandas. To see this, we can benchmark the two ways of computing the average-by-year that we have. First, we have the pandas way:</p>
			<pre>%timeit df.groupby('year').mean().co2
# Result:
# 534 µs ± 10.8 µs per loop (mean ± std. dev. of 7 runs, 
  1000 loops each)</pre>
			<p>Then, we have the xarray way:</p>
			<pre>%timeit ds.mean(dim='month').co2.values
# Result:
# 150 µs ± 1.27 µs per loop (mean ± std. dev. of 7 runs, 
  10000 loops each)</pre>
			<p>Here, we can see a clear performance<a id="_idIndexMarker254"/> improvement, achieved almost for free by passing our data to xarray!</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor064"/>Plotting with xarray</h2>
			<p>Labeled multidimensional arrays are ubiquitous in time series (where one of the dimensions is time), geospatial data (where some dimensions represent the coordinates on a map), or data that<a id="_idIndexMarker255"/> is both geospatial and time-dependent. In these data analysis tasks, data visualization is crucial; as such, xarray makes it easy to implement and call complex plotting functions on its data.</p>
			<p>Let's look at this through a quick example. First, we will read in an example dataset we have prepared for you, saved in a file named <code>2d_measurement.npy</code>, which can be read into a Python program using NumPy:</p>
			<pre>measures = np.load('2d_measure.npy')
measures.shape
# Result:
# (100, 100, 3)</pre>
			<p>As you can see, it is a 100x100x3 array. Let's say that this dataset contains a specific type of measurement, taken over a 100x100 grid of a two-dimensional space (corresponding to the first two axes) at three specific timestamps (corresponding to the third axis).</p>
			<p>We would like to visualize these measurements as three squares, where each square represents a specific time stamp, and each pixel in each square represents the intensity of the corresponding measurement.</p>
			<p>To do this in Matplotlib, we could use the <code>imshow</code> function, which takes in a two-dimensional array and plots it as an image. So, we would iterate through the three timestamps that we have and plot the corresponding grids one by one, as follows:</p>
			<pre>fig, ax = plt.subplots(1, 3, figsize=(10, 3))
for i in range(3):
    c = ax[i].imshow(measures[:, :, i], origin='lower')
    plt.colorbar(c, ax=ax[i])
    
plt.show()</pre>
			<p class="callout-heading">Note </p>
			<p class="callout">The index, <code>i</code>, iterates through the three indices of the third axis in the <code>measures</code> array. Again, we can see that this indexing scheme is not very expressive and readable. Here, we are also using the <code>colorbar</code> function to add a color bar to each of the plots. </p>
			<p>The preceding code produces the following output:</p>
			<div><div><img src="img/Figure_3.10_B17499.jpg" alt="Figure 3.10 – Regular imshow from Matplotlib " width="1232" height="358"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10 – Regular imshow from Matplotlib</p>
			<p>Although there is noise<a id="_idIndexMarker256"/> in the measurements, we can observe some global trends; specifically, we seem to have low-intensity measurements in the lower-left corner in the first plot, in the center in the second, and in the top-right corner in the third. As the final note, something we might want to change about this plot is making the three color bars have the same range, which may be hard to do with Matplotlib.</p>
			<p>Now, let's see how we can produce this plot using xarray. First, we must convert the NumPy array into a <code>DataArray</code> object:</p>
			<pre>da = xr.DataArray(measures, dims=['x', 'y', 'time'])</pre>
			<p>Here, we are also specifying<a id="_idIndexMarker257"/> the names for the three dimensions: <code>'x'</code>, <code>'y'</code>, and <code>'time'</code>. This will allow us to manipulate the data more expressively, as we saw previously. To plot out the 2D grids, we can use the similarly named <code>imshow</code> method:</p>
			<pre>da.plot.imshow(x='x', y='y', col='time', robust=True);</pre>
			<p>This results in the following output:</p>
			<div><div><img src="img/Figure_3.11_B17499.jpg" alt="Figure 3.11 – Specialized imshow from xarray " width="1286" height="416"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11 – Specialized imshow from xarray</p>
			<p>The plotting function is much simpler than the for loop we had earlier. Furthermore, with minimal code, xarray has taken care of many different aesthetics-related aspects of this plot: </p>
			<ul>
				<li>First, the plots have their titles and x- and y-axis labels automatically created. </li>
				<li>Second, by using a common color range, the fact that the measurements at the second timestamp are lower than those in the other two is now more obvious. This demonstrates that functions and methods in xarray are optimized to make working with labeled multidimensional data more efficient.</li>
			</ul>
			<p>On the topic of plotting, many data scientists work with map data. xarray nicely integrates with the popular Cartopy library for geospatial data processing and offers many plotting functionalities<a id="_idIndexMarker258"/> that incorporate world and country maps. More details can be found in their documentation: <a href="http://xarray.pydata.org/en/stable/plotting.html">http://xarray.pydata.org/en/stable/plotting.html</a>.</p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor065"/>Summary</h1>
			<p>In this chapter, we learned how to manipulate NumPy arrays and how to write fast mathematical expressions using array broadcasting. This knowledge will help you write more concise, expressive code and, at the same time, obtain substantial performance gains. We also introduced the <code>numexpr</code> library to further speed up NumPy calculations with minimal effort.</p>
			<p>pandas implements efficient data structures that are useful when analyzing large datasets. In particular, pandas shines when the data is indexed by non-integer keys and provides very fast hashing algorithms.</p>
			<p>NumPy and pandas work well when handling large, homogenous inputs, but they are not suitable when the expressions become complex and the operations cannot be expressed using the tools provided by these libraries. xarray comes in handy as an alternative option where we need to work with labeled, multidimensional data.</p>
			<p>In combination, the three libraries offer Python users powerful APIs and flexible functionalities to work with a wide range of data. By keeping them in your toolbox, you are well situated to tackle most data processing and engineering tasks using Python.</p>
			<p>In other cases, we can also leverage Python capabilities as a glue language by interfacing it with C using the Cython package, as we will see in the next chapter.</p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor066"/>Questions</h1>
			<ol>
				<li value="1">Name the advantages NumPy has over Python-native lists when working with multidimensional data.</li>
				<li>What are some of the database-style operations that pandas offers in its API?</li>
				<li>What problems does xarray address and why can they not be addressed by NumPy or pandas?</li>
			</ol>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor067"/>Further reading</h1>
			<ul>
				<li>An overview tutorial on NumPy and pandas: <a href="https://cloudxlab.com/blog/numpy-pandas-introduction/">https://cloudxlab.com/blog/numpy-pandas-introduction/</a></li>
				<li>Data structures in xarray: <a href="https://towardsdatascience.com/basic-data-structures-of-xarray-80bab8094efa">https://towardsdatascience.com/basic-data-structures-of-xarray-80bab8094efa</a></li>
			</ul>
		</div>
	</div>
</div>
</body></html>