<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer047">
			<h1 id="_idParaDest-48"><em class="italic"><a id="_idTextAnchor047"/>Chapter 3</em>: Fast Array Operations with NumPy, Pandas, and Xarray</h1>
			<p>NumPy is the <em class="italic">de facto</em> standard for scientific computing in Python. It offers flexible multidimensional arrays that allow you to perform fast and concise mathematical calculations.</p>
			<p>NumPy provides common data structures and algorithms designed to express complex mathematical operations using a concise syntax. The multidimensional array, <strong class="source-inline">numpy.ndarray</strong>, is internally based on C arrays. Apart from the performance benefits, this choice allows NumPy code to easily interface with the existing C and FORTRAN routines; NumPy helps bridge the gap between Python and the legacy code written using those languages.</p>
			<p>In this chapter, we will learn how to create and manipulate NumPy arrays. We will also explore the NumPy broadcasting feature, which is used to rewrite complex mathematical expressions efficiently and succinctly.</p>
			<p><strong class="bold">pandas</strong> is a tool that relies heavily on NumPy and provides additional data structures and algorithms targeted toward data analysis. We will introduce the main pandas features and their usage. We will also learn how to achieve high performance using pandas data structures and vectorized operations. </p>
			<p>Both NumPy and pandas are insufficient in many use cases concerning labeled, multidimensional data. The xarray library combines the best features from the other two tools and offers further optimized data processing functionalities. We will discuss the motivation for this tool and study the performance improvements it offers via explicit examples.</p>
			<p>In this chapter, we will be covering the following topics:</p>
			<ul>
				<li>Getting started with NumPy</li>
				<li>Rewriting the particle simulator in NumPy</li>
				<li>Reaching optimal performance with numexpr</li>
				<li>Working with database-style data with pandas</li>
				<li>High-performance labeled data with xarray</li>
			</ul>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>Technical requirement</h1>
			<p>The code files for this chapter can be accessed by going to this book's GitHub repository at <a href="https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/Chapter03">https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/Chapter03</a>.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>Getting started with NumPy</h1>
			<p>The NumPy library revolves around its multidimensional array object, <strong class="source-inline">numpy.ndarray</strong>. NumPy arrays are collections<a id="_idIndexMarker146"/> of elements of the same data type; this fundamental restriction allows NumPy to pack the data in a way that allows for high-performance mathematical operations.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Creating arrays</h2>
			<p>Let's explore NumPy's functionalities<a id="_idIndexMarker147"/> by following these steps:</p>
			<ol>
				<li>You can create NumPy arrays using the <strong class="source-inline">numpy.array</strong> function. It takes a list-like object (or another array) as input and, optionally, a string expressing its data type. You can interactively test array creation using an IPython shell, as follows:<p class="source-code">    import numpy as np </p><p class="source-code">    a = np.array([0, 1, 2]) </p></li>
				<li>Every NumPy array has an associated data type that can be accessed using the <strong class="source-inline">dtype</strong> attribute. If we inspect the <strong class="source-inline">a</strong> array, we will find that its <strong class="source-inline">dtype</strong> is <strong class="source-inline">int64</strong>, which stands for 64-bit integer:<p class="source-code">    a.dtype </p><p class="source-code">    # Result: </p><p class="source-code">    # dtype('int64') </p></li>
				<li>We may decide to convert those integer numbers into the <strong class="source-inline">float</strong> type. To do this, we can either pass the <strong class="source-inline">dtype</strong> argument at array initialization or cast the array to another data type using the <strong class="source-inline">astype</strong> method. These two ways to select a data type are shown in the following code:<p class="source-code">    a = np.array([1, 2, 3], dtype='float32') </p><p class="source-code">    a.astype('float32') </p><p class="source-code">    # Result:</p><p class="source-code">    # array([ 0.,  1.,  2.], dtype=float32) </p></li>
				<li>To create an array<a id="_idIndexMarker148"/> with two dimensions (an array of arrays), we can perform the required initialization using a nested sequence, as follows:<p class="source-code">    a = np.array([[0, 1, 2], [3, 4, 5]]) </p><p class="source-code">    print(a) </p><p class="source-code">    # Output:</p><p class="source-code">    # [[0 1 2]</p><p class="source-code">    #  [3 4 5]] </p></li>
				<li>An array that's created<a id="_idIndexMarker149"/> in this way has two dimensions, which are called <strong class="bold">axes</strong> in terms of NumPy's jargon. An array that's formed in this way is like a table that contains two rows and three columns. We can access the axes using the <strong class="source-inline">ndarray.shape</strong> attribute:<p class="source-code">    a.shape </p><p class="source-code">    # Result:</p><p class="source-code">    # (2, 3) </p></li>
				<li>Arrays can also be reshaped, so long as the product of the shape dimensions is equal to the total number of elements in the array (that is, the total number of elements is conserved). For example, we can reshape an array containing 16 elements in the following ways: <strong class="source-inline">(2, 8)</strong>, <strong class="source-inline">(4, 4)</strong>, or <strong class="source-inline">(2, 2, 4)</strong>. To reshape an array, we can either use the <strong class="source-inline">ndarray.reshape</strong> method or assign a new value to the <strong class="source-inline">ndarray.shape</strong> tuple. The following code<a id="_idIndexMarker150"/> illustrates the use of the <strong class="source-inline">ndarray.reshape</strong> method:<p class="source-code">    a = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, </p><p class="source-code">                  9, 10, 11, 12, 13, 14, 15]) </p><p class="source-code">    a.shape </p><p class="source-code">    # Output:</p><p class="source-code">    # (16,)</p><p class="source-code">    a.reshape(4, 4) # Equivalent: a.shape = (4, 4) </p><p class="source-code">    # Output: </p><p class="source-code">    # array([[ 0,  1,  2,  3],</p><p class="source-code">    #        [ 4,  5,  6,  7],</p><p class="source-code">    #        [ 8,  9, 10, 11],</p><p class="source-code">    #        [12, 13, 14, 15]]) </p></li>
			</ol>
			<p>Thanks to this property, you can freely add dimensions of size <strong class="source-inline">1</strong>. You can reshape an array with 16 elements to <strong class="source-inline">(16, 1)</strong>, <strong class="source-inline">(1, 16)</strong>, <strong class="source-inline">(16, 1, 1)</strong>, and so on. In the next section, we will extensively use this feature to implement complex operations through <em class="italic">broadcasting</em>. </p>
			<ol>
				<li value="7">NumPy provides convenience functions, as shown in the following code, to create arrays filled with zeros, ones, or with no initial value (in this case, their actual value is meaningless and depends on the memory state). Those functions take the array shape as a tuple and, optionally, its <strong class="source-inline">dtype</strong>:<p class="source-code">    np.zeros((3, 3)) </p><p class="source-code">    np.empty((3, 3)) </p><p class="source-code">    np.ones((3, 3), dtype='float32') </p></li>
			</ol>
			<p>In our examples, we will use the <strong class="source-inline">numpy.random</strong> module to generate random floating-point numbers in the <strong class="source-inline">(0, 1)</strong> interval. <strong class="source-inline">numpy.random.rand</strong> will take a shape and return an array of random numbers with that shape:</p>
			<p class="source-code">    np.random.rand(3, 3) </p>
			<ol>
				<li value="8">Sometimes, it is convenient to initialize arrays that have the same shape as that of some other array. For that purpose, NumPy provides some handy functions, such as <strong class="source-inline">zeros_like</strong>, <strong class="source-inline">empty_like</strong>, and <strong class="source-inline">ones_like</strong>. These functions can be used as follows:<p class="source-code">    np.zeros_like(a) </p><p class="source-code">    np.empty_like(a) </p><p class="source-code">    np.ones_like(a) </p></li>
			</ol>
			<p>These functions will return arrays<a id="_idIndexMarker151"/> with the specified values, whose shapes match exactly with that of array <strong class="source-inline">a</strong>.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor051"/>Accessing arrays</h2>
			<p>The NumPy array interface is, at a shallow<a id="_idIndexMarker152"/> level, similar to that of Python lists. NumPy arrays can be indexed using integers and iterated using a <strong class="source-inline">for</strong> loop:</p>
			<p class="source-code">    A = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8]) </p>
			<p class="source-code">    A[0] </p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # 0 </p>
			<p class="source-code">    [a for a in A] </p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # [0, 1, 2, 3, 4, 5, 6, 7, 8] </p>
			<p>However, explicitly<a id="_idIndexMarker153"/> looping over an array is, most of the time, not the most efficient way to access its elements. In this section, we will learn how to take advantage of NumPy's API to fully utilize its efficiency.</p>
			<h3 id="_idParaDest-53">Indexing and slicing</h3>
			<p>Indexing and slicing refer to the act of accessing elements within an array that are at certain locations or satisfy<a id="_idIndexMarker154"/> some condition that we are<a id="_idIndexMarker155"/> interested in. In NumPy, array elements and sub-arrays<a id="_idIndexMarker156"/> can be conveniently accessed by using multiple values separated<a id="_idIndexMarker157"/> by commas inside the subscript operator, <strong class="source-inline">[]</strong>. Let's get started:</p>
			<ol>
				<li value="1">If we take a <strong class="source-inline">(3,3)</strong> array (an array containing three triplets) and we access the element with an index of <strong class="source-inline">0</strong>, we can obtain the first row, as follows:<p class="source-code">    A = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) </p><p class="source-code">    A[0] </p><p class="source-code">    # Result:</p><p class="source-code">    # array([0, 1, 2]) </p></li>
				<li>We can index the row again by adding another index separated by a comma. To get the second element of the first row, we can use the <strong class="source-inline">(0, 1)</strong> index. An important observation is that the <strong class="source-inline">A[0, 1]</strong> notation is shorthand for <strong class="source-inline">A[(0, 1)]</strong>; that is, we are indexing using a <em class="italic">tuple</em>! Both versions are shown in the following snippet:<p class="source-code">    A[0, 1] </p><p class="source-code">    # Result:</p><p class="source-code">    # 1</p><p class="source-code">    # Equivalent version using tuple</p><p class="source-code">    A[(0, 1)]</p></li>
				<li>NumPy allows you to slice arrays into multiple dimensions. If we slice on the first dimension, we can obtain a collection of triplets, as follows:<p class="source-code">    A[0:2] </p><p class="source-code">    # Result:</p><p class="source-code">    # array([[0, 1, 2], </p><p class="source-code">    #        [3, 4, 5]]) </p></li>
				<li>If we slice the array<a id="_idIndexMarker158"/> again on the second dimension with <strong class="source-inline">0:2</strong>, we are extracting<a id="_idIndexMarker159"/> the first two elements<a id="_idIndexMarker160"/> from the collection of triplets<a id="_idIndexMarker161"/> shown earlier. This results in an array whose shape is <strong class="source-inline">(2, 2)</strong>, as shown in the following code:<p class="source-code">    A[0:2, 0:2] </p><p class="source-code">    # Result:</p><p class="source-code">    # array([[0, 1], </p><p class="source-code">    #        [3, 4]]) </p></li>
				<li>Intuitively, you can update the values in the array using both <em class="italic">numerical indexes</em> and <em class="italic">slices</em>. An example of this is illustrated in the following code snippet:<p class="source-code">    A[0, 1] = 8 </p><p class="source-code">    A[0:2, 0:2] = [[1, 1], [1, 1]]</p></li>
				<li>Indexing with the slicing syntax is very fast because, unlike lists, it doesn't produce a copy of the array. In NumPy's terminology, it returns a <em class="italic">view</em> of the same memory area. If we take a slice of the original array, and then we change one of its values, the original array will be updated as well. The following code illustrates an example of this feature:<p class="source-code">    a= np.array([1, 1, 1, 1]) </p><p class="source-code">    a_view = a[0:2] </p><p class="source-code">    a_view[0] = 2 </p><p class="source-code">    print(a) </p><p class="source-code">    # Output:</p><p class="source-code">    # [2 1 1 1] </p></li>
			</ol>
			<p>It is important to be extra careful when mutating NumPy arrays. Since views share data, changing the values of a view can result in hard-to-find bugs. To prevent side effects, you can set the <strong class="source-inline">a.flags.writeable = False</strong> flag, which will prevent the array or any of its views from being accidentally mutated.</p>
			<ol>
				<li value="7">Let's look at another example that shows how the slicing syntax can be used in a real-world setting. Let's define an <strong class="source-inline">r_i</strong> array, as shown in the following line of code, which contains a set of 10 coordinates (<em class="italic">x</em>, <em class="italic">y</em>). Its shape will be <strong class="source-inline">(10, 2)</strong>:<p class="source-code">    r_i = np.random.rand(10, 2)</p></li>
			</ol>
			<p>If you have a hard time<a id="_idIndexMarker162"/> distinguishing arrays that differ in the axes order, for<a id="_idIndexMarker163"/> example, between an array of shape <strong class="source-inline">(10, 2)</strong> and shape <strong class="source-inline">(2, 10)</strong>, it is useful to think that every time<a id="_idIndexMarker164"/> you say the word <em class="italic">of</em>, you should introduce a new dimension. An array<a id="_idIndexMarker165"/> with 10 elements <em class="italic">of</em> size two will be <strong class="source-inline">(10, 2)</strong>. Conversely, an array with two elements <em class="italic">of</em> size 10 will be <strong class="source-inline">(2, 10)</strong>.</p>
			<p>A typical operation we may be interested in is extracting the <em class="italic">x</em> component from each coordinate. In other words, you want to extract the <strong class="source-inline">(0, 0)</strong>, <strong class="source-inline">(1, 0)</strong>, and <strong class="source-inline">(2, 0)</strong>, items, resulting in an array with a shape of <strong class="source-inline">(10,)</strong>. It is helpful to think that the first index is <em class="italic">moving</em> while the second one is <em class="italic">fixed</em> (at <strong class="source-inline">0</strong>). With this in mind, we will slice every index on the first axis (the moving one) and take the first element (the fixed one) on the second axis, as shown in the following line of code:</p>
			<p class="source-code">    x_i = r_i[:, 0] </p>
			<p>On the other hand, the following expression will keep the first index fixed and the second index moving, returning the first (<em class="italic">x</em>, <em class="italic">y</em>) coordinate:</p>
			<p class="source-code">    r_0 = r_i[0, :] </p>
			<p>Slicing all the indexes over the last<a id="_idIndexMarker166"/> axis is optional; using <strong class="source-inline">r_i[0]</strong> has the same effect as using <strong class="source-inline">r_i[0, :]</strong>.</p>
			<h3 id="_idParaDest-54">Fancy indexing</h3>
			<p>NumPy allows you<a id="_idIndexMarker167"/> to index an array using another NumPy array<a id="_idIndexMarker168"/> made up of either integer or Boolean values. This is a feature called <strong class="bold">fancy indexing</strong>:</p>
			<ol>
				<li value="1">If you index an array (say, <strong class="source-inline">a</strong>) with another array of integers (say, <strong class="source-inline">idx</strong>), NumPy will interpret the integers as indexes and will return an array containing their corresponding values. If we index an array containing 10 elements with <strong class="source-inline">np.array([0, 2, 3])</strong>, we obtain an array of shape <strong class="source-inline">(3,)</strong> containing the elements at positions <strong class="source-inline">0</strong>, <strong class="source-inline">2</strong>, and <strong class="source-inline">3</strong>. The following code illustrates this concept:<p class="source-code">    a = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) </p><p class="source-code">    idx = np.array([0, 2, 3]) </p><p class="source-code">    a[idx] </p><p class="source-code">    # Result:</p><p class="source-code">    # array([9, 7, 6]) </p></li>
				<li>You can use fancy indexing on multiple dimensions by passing an array for each dimension. If we want to extract the <strong class="source-inline">(0, 2)</strong> and <strong class="source-inline">(1, 2)</strong> elements, we have to pack all the indexes acting on the first axis in one array, and the ones acting on the second axis in another. This can be seen in the following code:<p class="source-code">    a = np.array([[0, 1, 2], [3, 4, 5], \</p><p class="source-code">                  [6, 7, 8], [9, 10, 11]]) </p><p class="source-code">    idx1 = np.array([0, 1]) </p><p class="source-code">    idx2 = np.array([2, 2]) </p><p class="source-code">    a[idx1, idx2]</p></li>
				<li>You can also use normal lists as index arrays, but not tuples. For example, the following two statements are equivalent:<p class="source-code">    a[np.array([0, 1])] # is equivalent to</p><p class="source-code">    a[[0, 1]]</p></li>
			</ol>
			<p>However, if you use a tuple, NumPy will interpret the following statement as an index on multiple dimensions:</p>
			<p class="source-code">    a[(0, 1)] # is equivalent to</p>
			<p class="source-code">    a[0, 1] </p>
			<ol>
				<li value="4">The index arrays are not required to be one-dimensional; we can extract elements from<a id="_idIndexMarker169"/> the original array in any<a id="_idIndexMarker170"/> shape. For example, we can select elements from the original array to form a <strong class="source-inline">(2,2)</strong> array, as shown here:<p class="source-code">    idx1 = [[0, 1], [3, 2]] </p><p class="source-code">    idx2 = [[0, 2], [1, 1]] </p><p class="source-code">    a[idx1, idx2] </p><p class="source-code">    # Output: </p><p class="source-code">    # array([[ 0,  5],</p><p class="source-code">    #        [10,  7]]) </p></li>
				<li>The array slicing and fancy indexing features can be combined. This is useful, for instance, when we want to swap the <em class="italic">x</em> and <em class="italic">y</em> columns in a coordinate array. In the following code, the first index will be running over all the elements (a slice) and, for each of those, we extract the element in position <strong class="source-inline">1</strong> (the <em class="italic">y</em>) first and then the one in position <strong class="source-inline">0</strong> (the <em class="italic">x</em>):<p class="source-code">    r_i = np.random.rand(10, 2) </p><p class="source-code">    r_i[:, [0, 1]] = r_i[:, [1, 0]] </p></li>
				<li>When the index array is of the <strong class="source-inline">bool</strong> type, the rules are slightly different. The <strong class="source-inline">bool</strong> array will act as a <em class="italic">mask</em>; every element corresponding to <strong class="source-inline">True</strong> will be extracted and put in the output array. This is shown in the following code:<p class="source-code">    a = np.array([0, 1, 2, 3, 4, 5]) </p><p class="source-code">    mask = np.array([True, False, True, False, \</p><p class="source-code">      False, False]) </p><p class="source-code">    a[mask] </p><p class="source-code">    # Output:</p><p class="source-code">    # array([0, 2]) </p></li>
			</ol>
			<p>The same rules apply when dealing with multiple dimensions. Furthermore, if the index array has the same shape as the original array, the elements corresponding to <strong class="source-inline">True</strong> will be selected and put in the resulting array.</p>
			<ol>
				<li value="7">Indexing in NumPy is a reasonably fast operation. When speed is critical, you can use the slightly faster <strong class="source-inline">numpy.take</strong> and <strong class="source-inline">numpy.compress</strong> functions to squeeze out a little more performance. The first argument of <strong class="source-inline">numpy.take</strong> is the array we want<a id="_idIndexMarker171"/> to operate on, while the second<a id="_idIndexMarker172"/> is the list of indexes we want to extract. The last argument is <strong class="source-inline">axis</strong>; if this is not provided, the indexes will act on the flattened array; otherwise, they will act along the specified axis:<p class="source-code">    r_i = np.random.rand(100, 2) </p><p class="source-code">    idx = np.arange(50) # integers 0 to 50 </p><p class="source-code">    %timeit np.take(r_i, idx, axis=0) </p><p class="source-code">    1000000 loops, best of 3: 962 ns per loop </p><p class="source-code">    %timeit r_i[idx] </p><p class="source-code">    100000 loops, best of 3: 3.09 us per loop </p></li>
				<li>The similar but faster version for Boolean arrays is <strong class="source-inline">numpy.compress</strong>, which works in the same way. The use of <strong class="source-inline">numpy.compress</strong> is shown here:<p class="source-code">    In [51]: idx = np.ones(100, dtype='bool') # all </p><p class="source-code">      True values </p><p class="source-code">    In [52]: %timeit np.compress(idx, r_i, axis=0) </p><p class="source-code">    1000000 loops, best of 3: 1.65 us per loop </p><p class="source-code">    In [53]: %timeit r_i[idx] </p><p class="source-code">    100000 loops, best of 3: 5.47 us per loop </p></li>
			</ol>
			<p>As we can see, <strong class="source-inline">compress</strong> gives us a slight speed<a id="_idIndexMarker173"/> improvement, which will prove<a id="_idIndexMarker174"/> useful if you are dealing with large-sized arrays.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor052"/>Broadcasting</h2>
			<p>The true power of NumPy lies in its fast mathematical operations. The approach that's used by NumPy is to avoid stepping into the Python interpreter by performing element-wise calculations using optimized C code. <strong class="bold">Broadcasting</strong> is a clever set of rules<a id="_idIndexMarker175"/> that enables fast array calculations<a id="_idIndexMarker176"/> for arrays of similar (but not equal!) shapes. Let's see how that goes.</p>
			<p>Whenever you perform an arithmetic operation on two arrays (such as a product), if the two operands have the same shape, the operation will be applied in an element-wise fashion. For example, upon multiplying two <strong class="source-inline">(2,2)</strong> arrays, the operation will be done between pairs of corresponding elements, producing another <strong class="source-inline">(2, 2)</strong> array, as shown in the following code:</p>
			<p class="source-code">    A = np.array([[1, 2], [3, 4]]) </p>
			<p class="source-code">    B = np.array([[5, 6], [7, 8]]) </p>
			<p class="source-code">    A * B </p>
			<p class="source-code">    # Output:</p>
			<p class="source-code">    # array([[ 5, 12],           </p>
			<p class="source-code">    #        [21, 32]]) </p>
			<p>If the shapes of the operands don't match, NumPy will attempt to match them using broadcasting rules. If one of the operands is a <em class="italic">scalar</em> (for example, a number), it will be applied to every element of the array, as shown in the following code:</p>
			<p class="source-code">    A * 2 </p>
			<p class="source-code">    # Output: </p>
			<p class="source-code">    # array([[2, 4], </p>
			<p class="source-code">    #        [6, 8]]) </p>
			<p>If the operand is another array, NumPy will try to match the shapes starting from the last axis. For example, if we want to combine<a id="_idIndexMarker177"/> an array of shape <strong class="source-inline">(3, 2)</strong> with one of shape <strong class="source-inline">(2,)</strong>, the second array<a id="_idIndexMarker178"/> will be repeated three times to generate a <strong class="source-inline">(3, 2)</strong> array. In other words, the array is <em class="italic">broadcasted</em> along a dimension to match the shape of the other operand, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/Figure_3.1_B17499.jpg" alt="Figure 3.1 – Illustration of array broadcasting " width="894" height="161"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Illustration of array broadcasting</p>
			<p>If the shapes don't match – for example, when combining a <strong class="source-inline">(3, 2)</strong> array with a <strong class="source-inline">(2, 2)</strong> array – NumPy will throw an exception.</p>
			<p>If one of the axis's sizes is 1, the array will be repeated over this axis until the shapes match. To illustrate this point, let's consider that we have an array of the following shape:</p>
			<p class="source-code">    5, 10, 2 </p>
			<p>Now, let's consider that we want to broadcast it with an array of shape <strong class="source-inline">(5, 1, 2)</strong>; the array will be repeated on the second axis 10 times, as shown here:</p>
			<p class="source-code">    5, 10, 2 </p>
			<p class="source-code">    5,  1, 2 → repeated </p>
			<p class="source-code">    - - - - </p>
			<p class="source-code">    5, 10, 2 </p>
			<p>Earlier, we saw that it is possible to freely reshape arrays to add axes of size 1. Using the <strong class="source-inline">numpy.newaxis</strong> constant while indexing will<a id="_idIndexMarker179"/> introduce an extra dimension. For instance, if we have a <strong class="source-inline">(5, 2)</strong> array and we<a id="_idIndexMarker180"/> want to combine it with one of shape <strong class="source-inline">(5, 10, 2)</strong>, we can add an extra axis in the middle, as shown in the following code, to obtain a compatible <strong class="source-inline">(5, 1, 2)</strong> array:</p>
			<p class="source-code">    A = np.random.rand(5, 10, 2) </p>
			<p class="source-code">    B = np.random.rand(5, 2) </p>
			<p class="source-code">    A * B[:, np.newaxis, :] </p>
			<p>This feature can be used, for example, to operate on all possible combinations of the two arrays. One of these applications is the <em class="italic">outer product</em>. Let's consider that we have the following two arrays:</p>
			<p class="source-code">    a = [a1, a2, a3] </p>
			<p class="source-code">    b = [b1, b2, b3] </p>
			<p>The outer product is a matrix containing the product of all the possible combinations <em class="italic">(i, j)</em> of the two array elements, as shown in the following snippet:</p>
			<p class="source-code">    a x b = a1*b1, a1*b2, a1*b3 </p>
			<p class="source-code">            a2*b1, a2*b2, a2*b3 </p>
			<p class="source-code">            a3*b1, a3*b2, a3*b3 </p>
			<p>To calculate this using NumPy, we will repeat the <strong class="source-inline">[a1, a2, a3]</strong> elements in one dimension, the <strong class="source-inline">[b1, b2, b3]</strong> elements in another dimension, and then take their element-wise product, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/Figure_3.2_B17499.jpg" alt="Figure 3.2 – Illustration of an outer product " width="989" height="260"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Illustration of an outer product</p>
			<p>Using code, our strategy will be to transform the <strong class="source-inline">a</strong> array from shape <strong class="source-inline">(3,)</strong> into shape <strong class="source-inline">(3, 1)</strong>, and the <strong class="source-inline">b</strong> array from shape <strong class="source-inline">(3,)</strong> into shape <strong class="source-inline">(1, 3)</strong>. These two arrays are broadcasted in the two dimensions and get multiplied together using the following code:</p>
			<p class="source-code">    AB = a[:, np.newaxis] * b[np.newaxis, :] </p>
			<p>This operation is very fast<a id="_idIndexMarker181"/> and extremely effective as it avoids Python loops and can process<a id="_idIndexMarker182"/> a high number of elements at speeds comparable with pure C or FORTRAN code.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor053"/>Mathematical operations</h2>
			<p>NumPy includes the most<a id="_idIndexMarker183"/> common mathematical operations available for broadcasting by default, ranging from simple algebra to trigonometry, rounding, and logic.</p>
			<p>For instance, to take the square root of every element in the array, we can use <strong class="source-inline">numpy.sqrt</strong>, as shown in the following code:</p>
			<p class="source-code">    np.sqrt(np.array([4, 9, 16])) </p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # array([2., 3., 4.]) </p>
			<p>The comparison operators are useful when we're trying to filter certain elements based on a condition. Imagine that we have an array of random numbers from <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>, and we want to extract all the numbers greater than <strong class="source-inline">0.5</strong>. We can use the <strong class="source-inline">&gt;</strong> operator on the array to obtain a <strong class="source-inline">bool</strong> array, as follows:</p>
			<p class="source-code">    a = np.random.rand(5, 3) </p>
			<p class="source-code">    a &gt; 0.3 </p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # array([[ True, False,  True],</p>
			<p class="source-code">    #        [ True,  True,  True],</p>
			<p class="source-code">    #        [False,  True,  True],</p>
			<p class="source-code">    #        [ True,  True, False],</p>
			<p class="source-code">    #        [ True,  True, False]], dtype=bool) </p>
			<p>The resulting <strong class="source-inline">bool</strong> array can then be reused as an index to retrieve the elements that are greater than <strong class="source-inline">0.5</strong>:</p>
			<p class="source-code">    a[a &gt; 0.5] </p>
			<p class="source-code">    print(a[a&gt;0.5]) </p>
			<p class="source-code">    # Output:</p>
			<p class="source-code">    # [ 0.9755  0.5977  0.8287  0.6214  0.5669  0.9553  </p>
			<p class="source-code">        0.5894  0.7196  0.9200  0.5781  0.8281 ] </p>
			<p>NumPy also implements methods such as <strong class="source-inline">ndarray.sum</strong>, which takes the sum of all the elements on an axis. If we have an array of shape <strong class="source-inline">(5, 3)</strong>, we can use the <strong class="source-inline">ndarray.sum</strong> method to sum the elements<a id="_idIndexMarker184"/> on the first axis, the second axis, or over all the elements of the array, as illustrated in the following snippet:</p>
			<p class="source-code">    a = np.random.rand(5, 3) </p>
			<p class="source-code">    a.sum(axis=0) </p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # array([ 2.7454,  2.5517,  2.0303]) </p>
			<p class="source-code">    a.sum(axis=1) </p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # array([ 1.7498,  1.2491,  1.8151,  1.9320,  0.5814]) </p>
			<p class="source-code">    a.sum() # With no argument operates on flattened array </p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # 7.3275 </p>
			<p>Note that by summing the elements<a id="_idIndexMarker185"/> over an axis, we eliminate that axis. From the preceding example, the sum on axis <strong class="source-inline">0</strong> produces an array of shape <strong class="source-inline">(3,)</strong>, while the sum on axis <strong class="source-inline">1</strong> produces an array of shape <strong class="source-inline">(5,)</strong>.</p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor054"/>Calculating the norm</h2>
			<p>We can review the basic concepts illustrated in this section by calculating the <em class="italic">norm</em> of a set of coordinates. The norm<a id="_idIndexMarker186"/> of a pair of coordinates is an important concept in linear algebra and is often interpreted as the magnitude of the corresponding line segment. For a two-dimensional vector, the norm is defined as follows:</p>
			<p class="source-code">    norm = sqrt(x**2 + y**2) </p>
			<p>Given an array of 10 coordinates (<em class="italic">x</em>, <em class="italic">y</em>), we want to find the norm of each coordinate. We can calculate the norm by performing these steps:</p>
			<ol>
				<li value="1">Square the coordinates, obtaining an array that contains <strong class="source-inline">(x**2, y**2)</strong> elements.</li>
				<li>Sum those with <strong class="source-inline">numpy.sum</strong> over the last axis.</li>
				<li>Take the square root, element-wise, with <strong class="source-inline">numpy.sqrt</strong>.</li>
			</ol>
			<p>The final expression can be compressed into a single line:</p>
			<p class="source-code">    r_i = np.random.rand(10, 2) </p>
			<p class="source-code">    norm = np.sqrt((r_i ** 2).sum(axis=1)) </p>
			<p class="source-code">    print(norm)</p>
			<p class="source-code">    # Output:</p>
			<p class="source-code">    # [ 0.7314  0.9050  0.5063  0.2553  0.0778   0.9143   </p>
			<p class="source-code">        1.3245  0.9486  1.010   1.0212] </p>
			<p>We can verify that this method<a id="_idIndexMarker187"/> of calculating <strong class="source-inline">norm</strong> gives us the correct answer while having compact code.</p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor055"/>Rewriting the particle simulator in NumPy</h1>
			<p>In this section, we will<a id="_idIndexMarker188"/> optimize our particle simulator by rewriting some parts<a id="_idIndexMarker189"/> of it in NumPy. From the profiling we did in <a href="B17499_01_Final_SS_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Benchmarking and Profiling</em>, we found that the slowest part of our program is the following loop, which is contained in the <strong class="source-inline">ParticleSimulator.evolve</strong> method:</p>
			<p class="source-code">    for i in range(nsteps): </p>
			<p class="source-code">      for p in self.particles: </p>
			<p class="source-code">        norm = (p.x**2 + p.y**2)**0.5 </p>
			<p class="source-code">        v_x = (-p.y)/norm </p>
			<p class="source-code">        v_y = p.x/norm </p>
			<p class="source-code">        d_x = timestep * p.ang_vel * v_x </p>
			<p class="source-code">        d_y = timestep * p.ang_vel * v_y </p>
			<p class="source-code">        p.x += d_x </p>
			<p class="source-code">        p.y += d_y </p>
			<p>You may have noticed that the body of the loop acts solely on the current particle. If we had an array containing the particle positions and angular speed, we could rewrite the loop using a broadcasted operation. In contrast, the loop's steps depend on the previous step and cannot be parallelized in this way.</p>
			<p>So, it is natural to store<a id="_idIndexMarker190"/> all the array coordinates in an array<a id="_idIndexMarker191"/> of shape <strong class="source-inline">(nparticles, 2)</strong> and the angular speed in an array of shape <strong class="source-inline">(nparticles,)</strong>, where <strong class="source-inline">nparticles</strong> is the number of particles. We'll call those arrays <strong class="source-inline">r_i</strong> and <strong class="source-inline">ang_vel_i</strong>:</p>
			<p class="source-code">    r_i = np.array([[p.x, p.y] for p in self.particles]) </p>
			<p class="source-code">    ang_vel_i = np.array([p.ang_vel for p in \</p>
			<p class="source-code">      self.particles]) </p>
			<p>The velocity direction, which is perpendicular to the vector (<em class="italic">x</em>, <em class="italic">y</em>), was defined as follows:</p>
			<p class="source-code">    v_x = -y / norm </p>
			<p class="source-code">    v_y = x / norm </p>
			<p>The norm can be calculated using the strategy illustrated in the <em class="italic">Calculating the norm</em> section under the <em class="italic">Getting started with NumPy</em> heading:</p>
			<p class="source-code">    norm_i = ((r_i ** 2).sum(axis=1))**0.5 </p>
			<p>For the (<em class="italic">-y</em>, <em class="italic">x</em>) components, we need to swap the <em class="italic">x</em> and <em class="italic">y</em> columns in <strong class="source-inline">r_i</strong> and then multiply the first column by <strong class="source-inline">-1</strong>, as shown in the following code:</p>
			<p class="source-code">    v_i = r_i[:, [1, 0]] / norm_i </p>
			<p class="source-code">    v_i[:, 0] *= -1 </p>
			<p>To calculate the displacement, we need to compute the product of <strong class="source-inline">v_i</strong>, <strong class="source-inline">ang_vel_i</strong>, and <strong class="source-inline">timestep</strong>. Since <strong class="source-inline">ang_vel_i</strong> is of shape <strong class="source-inline">(nparticles,)</strong>, it needs a new axis to operate with <strong class="source-inline">v_i</strong> of shape <strong class="source-inline">(nparticles, 2)</strong>. We will do that using <strong class="source-inline">numpy.newaxis</strong>, as follows:</p>
			<p class="source-code">    d_i = timestep * ang_vel_i[:, np.newaxis] * v_i </p>
			<p class="source-code">    r_i += d_i </p>
			<p>Outside the loop, we have to update the particle instances with the new coordinates, <em class="italic">x</em> and <em class="italic">y</em>, as follows:</p>
			<p class="source-code">    for i, p in enumerate(self.particles): </p>
			<p class="source-code">      p.x, p.y = r_i[i] </p>
			<p>To summarize, we will implement<a id="_idIndexMarker192"/> a method called <strong class="source-inline">ParticleSimulator.evolve_numpy</strong> and benchmark it against the pure Python<a id="_idIndexMarker193"/> version, renamed as <strong class="source-inline">ParticleSimulator.evolve_python</strong>:</p>
			<p class="source-code">    def evolve_numpy(self, dt): </p>
			<p class="source-code">      timestep = 0.00001 </p>
			<p class="source-code">      nsteps = int(dt/timestep) </p>
			<p class="source-code">      r_i = np.array([[p.x, p.y] for p in self.particles]) </p>
			<p class="source-code">      ang_vel_i = np.array([p.ang_vel for p in \</p>
			<p class="source-code">        self.particles]) </p>
			<p class="source-code">      for i in range(nsteps): </p>
			<p class="source-code">        norm_i = np.sqrt((r_i ** 2).sum(axis=1)) </p>
			<p class="source-code">        v_i = r_i[:, [1, 0]] </p>
			<p class="source-code">        v_i[:, 0] *= -1 </p>
			<p class="source-code">        v_i /= norm_i[:, np.newaxis] </p>
			<p class="source-code">        d_i = timestep * ang_vel_i[:, np.newaxis] * v_i </p>
			<p class="source-code">        r_i += d_i </p>
			<p class="source-code">        for i, p in enumerate(self.particles): </p>
			<p class="source-code">          p.x, p.y = r_i[i] </p>
			<p>We will also update the benchmark<a id="_idIndexMarker194"/> to conveniently change<a id="_idIndexMarker195"/> the number of particles and the simulation method, as follows:</p>
			<p class="source-code">    def benchmark(npart=100, method='python'): </p>
			<p class="source-code">      particles = [Particle(uniform(-1.0, 1.0),     </p>
			<p class="source-code">                            uniform(-1.0, 1.0),</p>
			<p class="source-code">                            uniform(-1.0, 1.0))  </p>
			<p class="source-code">                            for i in range(npart)] </p>
			<p class="source-code">      simulator = ParticleSimulator(particles) </p>
			<p class="source-code">      if method=='python': </p>
			<p class="source-code">        simulator.evolve_python(0.1) </p>
			<p class="source-code">      elif method == 'numpy': </p>
			<p class="source-code">        simulator.evolve_numpy(0.1) </p>
			<p>Let's run the benchmark in an IPython session:</p>
			<p class="source-code">    from simul import benchmark </p>
			<p class="source-code">    %timeit benchmark(100, 'python') </p>
			<p class="source-code">    1 loops, best of 3: 614 ms per loop </p>
			<p class="source-code">    %timeit benchmark(100, 'numpy') </p>
			<p class="source-code">    1 loops, best of 3: 415 ms per loop </p>
			<p>We have made some improvements, but it doesn't look like a huge speed boost. The power of NumPy is revealed when handling big arrays. If we increase the number of particles, we will note a more significant performance boost:</p>
			<p class="source-code">    %timeit benchmark(1000, 'python') </p>
			<p class="source-code">    1 loops, best of 3: 6.13 s per loop </p>
			<p class="source-code">    %timeit benchmark(1000, 'numpy') </p>
			<p class="source-code">    1 loops, best of 3: 852 ms per loop </p>
			<p>The plot in the following diagram was produced by running the benchmark with different particle numbers:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/Figure_3.3_B17499.jpg" alt="Figure 3.3 – The running time growth of pure Python versus NumPy " width="797" height="391"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – The running time growth of pure Python versus NumPy</p>
			<p>The plot shows that both implementations scale linearly with particle size, but the runtime in the pure<a id="_idIndexMarker196"/> Python version grows much faster than the NumPy<a id="_idIndexMarker197"/> version; at greater sizes, we have a greater NumPy advantage. In general, when using NumPy, you should try to pack things into large arrays and group the calculations using the broadcasting feature.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor056"/>Reaching optimal performance with numexpr</h1>
			<p>When handling complex expressions, NumPy stores<a id="_idIndexMarker198"/> intermediate results in memory. David M. Cooke wrote a package called <strong class="source-inline">numexpr</strong>, which optimizes and compiles array expressions on the fly. It works by optimizing the usage of the CPU cache and by taking advantage of multiple processors.</p>
			<p>Its usage is generally straightforward and is based on a single function: <strong class="source-inline">numexpr.evaluate</strong>. The function takes a string<a id="_idIndexMarker199"/> containing an array expression as its first argument. The syntax is basically identical to that of NumPy. For example, we can calculate a simple <strong class="source-inline">a + b * c</strong> expression in the following way:</p>
			<p class="source-code">    a = np.random.rand(10000) </p>
			<p class="source-code">    b = np.random.rand(10000) </p>
			<p class="source-code">    c = np.random.rand(10000) </p>
			<p class="source-code">    d = ne.evaluate('a + b * c') </p>
			<p>The <strong class="source-inline">numexpr</strong> package increases performance in almost all cases, but to get a substantial advantage, you should use it with large arrays. An application that involves a large array is the calculation of a <em class="italic">distance matrix</em>. In a particle system, a distance matrix contains all the possible distances between the particles. To calculate it, we should calculate all the vectors connecting any two particles, <strong class="source-inline">(i,j)</strong>, as follows:</p>
			<p class="source-code">    x_ij = x_j - x_i </p>
			<p class="source-code">    y_ij = y_j - y_i. </p>
			<p>Then, we must calculate the length of this vector by taking its norm, as shown in the following code:</p>
			<p class="source-code">    d_ij = sqrt(x_ij**2 + y_ij**2) </p>
			<p>We can write this in NumPy by employing the usual broadcasting rules (the operation is similar to the outer product):</p>
			<p class="source-code">    r = np.random.rand(10000, 2) </p>
			<p class="source-code">    r_i = r[:, np.newaxis] </p>
			<p class="source-code">    r_j = r[np.newaxis, :] </p>
			<p class="source-code">    d_ij = r_j - r_i </p>
			<p>Finally, we must calculate the norm over the last axis using the following line of code:</p>
			<p class="source-code">    d_ij = np.sqrt((d_ij ** 2).sum(axis=2)) </p>
			<p>Rewriting the same expression using the <strong class="source-inline">numexpr</strong> syntax is extremely easy. The <strong class="source-inline">numexpr</strong> package (aliased <strong class="source-inline">ne</strong> in our following code) doesn't support slicing in its array expression; therefore, we first need to prepare the operands for broadcasting by adding an extra dimension, as follows:</p>
			<p class="source-code">    r = np.random.rand(10000, 2) </p>
			<p class="source-code">    r_i = r[:, np.newaxis] </p>
			<p class="source-code">    r_j = r[np.newaxis, :] </p>
			<p>At this point, we should try to pack as many operations as possible into a single expression to allow significant optimization.</p>
			<p>Most of the NumPy mathematical functions<a id="_idIndexMarker200"/> are also available in <strong class="source-inline">numexpr</strong>. However, there is a limitation – the reduction operations (the ones that reduce an axis, such as <strong class="source-inline">sum</strong>) have to happen last. Therefore, we have to calculate the sum first, then step out of <strong class="source-inline">numexpr</strong>, and finally calculate the square root in another expression:</p>
			<p class="source-code">    import numexpr as ne</p>
			<p class="source-code">    d_ij = ne.evaluate('sum((r_j - r_i)**2, 2)') </p>
			<p class="source-code">    d_ij = ne.evaluate('sqrt(d_ij)') </p>
			<p>The <strong class="source-inline">numexpr</strong> compiler will avoid redundant memory allocation by not storing intermediate results. When possible, it will also distribute the operations over multiple processors. In the <strong class="source-inline">distance_matrix.py</strong> file, you will find two functions that implement the two versions – <strong class="source-inline">distance_matrix_numpy</strong> and <strong class="source-inline">distance_matrix_numexpr</strong>:</p>
			<p class="source-code">    from distance_matrix import (distance_matrix_numpy, \</p>
			<p class="source-code">       distance_matrix_numexpr) </p>
			<p class="source-code">    %timeit distance_matrix_numpy(10000) </p>
			<p class="source-code">    1 loops, best of 3: 3.56 s per loop </p>
			<p class="source-code">    %timeit distance_matrix_numexpr(10000) </p>
			<p class="source-code">    1 loops, best of 3: 858 ms per loop </p>
			<p>By simply converting the expressions to use <strong class="source-inline">numexpr</strong>, we were able to obtain a 4.5x increase in performance over standard NumPy. The <strong class="source-inline">numexpr</strong> package can be used every time you need to optimize a NumPy expression that involves large arrays and complex operations, and you can do so while making minimal changes to the code.</p>
			<p>Overall, we have seen that NumPy, in combination with numexpr, offers powerful APIs when it comes to working with multidimensional data. However, in many use cases, data is only two-dimensional but is <em class="italic">labeled</em> in the sense that the data axes include explicit information about<a id="_idIndexMarker201"/> the type of data they contain. This is the case for data extracted from database tables. In such situations, pandas is the most popular and one of the best libraries in Python for this, as we will see next.</p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor057"/>Working with database-style data with pandas</h1>
			<p>pandas is a library that was originally<a id="_idIndexMarker202"/> developed by Wes McKinney. It was designed to analyze datasets in a seamless and performant way. In recent years, this powerful library has seen<a id="_idIndexMarker203"/> incredible growth and a huge<a id="_idIndexMarker204"/> adoption by the Python community. In this section, we will introduce the main concepts and tools provided in this library, and we will use them to increase the performance of various use cases that can't otherwise be addressed with NumPy's vectorized operations and broadcasting.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor058"/>pandas fundamentals</h2>
			<p>While NumPy deals mostly with arrays, pandas's main data structures are <strong class="source-inline">pandas.Series</strong>, <strong class="source-inline">pandas.DataFrame</strong>, and <strong class="source-inline">pandas.Panel</strong>. In the rest of this chapter, we will abbreviate <strong class="source-inline">pandas</strong> to <strong class="source-inline">pd</strong>.</p>
			<p>The main difference between a <strong class="source-inline">pd.Series</strong> object and an <strong class="source-inline">np.array</strong> is that a <strong class="source-inline">pd.Series</strong> object associates<a id="_idIndexMarker205"/> a specific <em class="italic">key</em> with each element of an array. Let's see how this works in practice with an example.</p>
			<p>Let's assume that we are trying to test a new blood pressure drug and we want to store, for each patient, whether the patient's blood pressure improved after administering the drug. We can encode this information by associating each subject ID (represented by an integer) with <strong class="source-inline">True</strong> if the drug was effective and <strong class="source-inline">False</strong> otherwise:</p>
			<ol>
				<li value="1">We can create a <strong class="source-inline">pd.Series</strong> object by associating an array of keys – the patients – to the array of values that represent the drug's effectiveness. This array of keys can<a id="_idIndexMarker206"/> be passed to the <strong class="source-inline">Series</strong> constructor using the <strong class="source-inline">index</strong> argument, as shown in the following snippet:<p class="source-code">    import pandas as pd</p><p class="source-code">    patients = [0, 1, 2, 3]</p><p class="source-code">    effective = [True, True, False, False]</p><p class="source-code">    effective_series = pd.Series(effective, \</p><p class="source-code">      index=patients)</p></li>
				<li>Associating a set of integers from <em class="italic">0</em> to <em class="italic">N</em> with a set of values can technically be implemented with <strong class="source-inline">np.array</strong> since, in this case, the key will simply be the position of the element in the array. In pandas, keys are not limited to integers; they can also be strings, floating-point numbers, and generic (hashable) Python objects. For example, we can easily turn our IDs into strings with little effort, as shown in the following code:<p class="source-code">    patients = ["a", "b", "c", "d"]</p><p class="source-code">    effective = [True, True, False, False]</p><p class="source-code">    effective_series = pd.Series(effective, \</p><p class="source-code">      index=patients)</p></li>
			</ol>
			<p>An interesting observation is that, while NumPy arrays can be thought of as a contiguous collection of values similar to Python lists, the pandas <strong class="source-inline">pd.Series</strong> object can be thought of as a structure that maps keys to values, similar to Python dictionaries.</p>
			<ol>
				<li value="3">What if you want to store the initial and final blood pressure for each patient? In pandas, you can use a <strong class="source-inline">pd.DataFrame</strong> object to associate multiple data with each key.</li>
			</ol>
			<p><strong class="source-inline">pd.DataFrame</strong> can be initialized, similarly to a <strong class="source-inline">pd.Series</strong> object, by passing a dictionary of columns and an index. In the following example, we will see how<a id="_idIndexMarker207"/> to create <strong class="source-inline">pd.DataFrame</strong> containing four columns that represent the initial and final measurements of systolic and diastolic blood pressure for our patients:</p>
			<p class="source-code">    patients = ["a", "b", "c", "d"]</p>
			<p class="source-code">    columns = {</p>
			<p class="source-code">      "sys_initial": [120, 126, 130, 115],</p>
			<p class="source-code">      "dia_initial": [75, 85, 90, 87],</p>
			<p class="source-code">      "sys_final": [115, 123, 130, 118],</p>
			<p class="source-code">      "dia_final": [70, 82, 92, 87]</p>
			<p class="source-code">    }</p>
			<p class="source-code">    </p>
			<p class="source-code">    df = pd.DataFrame(columns, index=patients)</p>
			<ol>
				<li value="4">Equivalently, you can think of <strong class="source-inline">pd.DataFrame</strong> as a collection of <strong class="source-inline">pd.Series</strong>. It is possible to directly initialize <strong class="source-inline">pd.DataFrame</strong> using a dictionary of <strong class="source-inline">pd.Series</strong> instances:<p class="source-code">    columns = {</p><p class="source-code">      "sys_initial": pd.Series([120, 126, 130, 115], \</p><p class="source-code">        index=patients),</p><p class="source-code">      "dia_initial": pd.Series([75, 85, 90, 87], \</p><p class="source-code">        index=patients),</p><p class="source-code">      "sys_final": pd.Series([115, 123, 130, 118], \</p><p class="source-code">        index=patients),</p><p class="source-code">      "dia_final": pd.Series([70, 82, 92, 87], \</p><p class="source-code">        index=patients)</p><p class="source-code">    }</p><p class="source-code">    df = pd.DataFrame(columns)</p></li>
				<li>To inspect the content of a <strong class="source-inline">pd.DataFrame</strong> or <strong class="source-inline">pd.Series</strong> object, you can use the <strong class="source-inline">pd.Series.head</strong> and <strong class="source-inline">pd.DataFrame.head</strong> methods, which<a id="_idIndexMarker208"/> print the first few rows of the dataset:<p class="source-code">    effective_series.head()</p><p class="source-code">    # Output:</p><p class="source-code">    # a True</p><p class="source-code">    # b True</p><p class="source-code">    # c False</p><p class="source-code">    # d False</p><p class="source-code">    # dtype: bool</p><p class="source-code">    df.head()</p><p class="source-code">    # Output:</p><p class="source-code">    #    dia_final  dia_initial  sys_final sys_initial</p><p class="source-code">    # a         70           75        115          </p><p class="source-code">    120</p><p class="source-code">    # b         82           85        123          </p><p class="source-code">    126</p><p class="source-code">    # c         92           90        130          </p><p class="source-code">    130</p><p class="source-code">    # d         87           87        118          </p><p class="source-code">    115</p></li>
			</ol>
			<p>Just like a <strong class="source-inline">pd.DataFrame</strong> can be used to store a collection of <strong class="source-inline">pd.Series</strong>, you can use a <strong class="source-inline">pd.Panel</strong> to store a collection of <strong class="source-inline">pd.DataFrames</strong>. We will not cover the usage of <strong class="source-inline">pd.Panel</strong> as it is not used<a id="_idIndexMarker209"/> as often as <strong class="source-inline">pd.Series</strong> and <strong class="source-inline">pd.DataFrame</strong>. To learn more about <strong class="source-inline">pd.Panel</strong>, make sure that you refer to the excellent documentation at <a href="http://pandas.pydata.org/pandas-docs/stable/dsintro.html#panel">http://pandas.pydata.org/pandas-docs/stable/dsintro.html#panel</a>.</p>
			<h3 id="_idParaDest-62">Indexing Series and DataFrame objects</h3>
			<p>In many instances, we might want to access<a id="_idIndexMarker210"/> certain elements stored inside a <strong class="source-inline">pd.Series</strong> or a <strong class="source-inline">pd.DataFrame</strong> object. In the following steps, we will see<a id="_idIndexMarker211"/> how we can index these objects:</p>
			<ol>
				<li value="1">Retrieving data from a <strong class="source-inline">pd.Series</strong>, given its <em class="italic">key</em>, can be done intuitively by indexing the <strong class="source-inline">pd.Series.loc</strong> attribute:<p class="source-code">    effective_series.loc["a"]</p><p class="source-code">    # Result:</p><p class="source-code">    # True</p></li>
				<li>It is also possible to access the elements, given their <em class="italic">position</em> in the underlying array, using the <strong class="source-inline">pd.Series.iloc</strong> attribute:<p class="source-code">    effective_series.iloc[0]</p><p class="source-code">    # Result:</p><p class="source-code">    # True</p></li>
				<li>Indexing <strong class="source-inline">pd.DataFrame</strong> works similarly. For example, you can use <strong class="source-inline">pd.DataFrame.loc</strong> to extract a row by key, and you can use <strong class="source-inline">pd.DataFrame.iloc</strong> to extract a row by position:<p class="source-code">    df.loc["a"]</p><p class="source-code">    df.iloc[0]</p><p class="source-code">    # Result:</p><p class="source-code">    # dia_final 70</p><p class="source-code">    # dia_initial 75</p><p class="source-code">    # sys_final 115</p><p class="source-code">    # sys_initial 120</p><p class="source-code">    # Name: a, dtype: int64</p></li>
				<li>An important aspect is that the return type in this case is a <strong class="source-inline">pd.Series</strong>, where each column is a new key. To retrieve a specific row and column, you can use the following code. The <strong class="source-inline">loc</strong> attribute will index both<a id="_idIndexMarker212"/> the row and the column by<a id="_idIndexMarker213"/> key, while the <strong class="source-inline">iloc</strong> version will index the row and the column by an integer:<p class="source-code">    df.loc["a", "sys_initial"] # is equivalent to</p><p class="source-code">    df.loc["a"].loc["sys_initial"]</p><p class="source-code">    df.iloc[0, 1] # is equivalent to</p><p class="source-code">    df.iloc[0].iloc[1]</p></li>
				<li>Retrieving a column from a <strong class="source-inline">pd.DataFrame</strong> by name can be achieved by regular indexing or attribute access. To retrieve a column by position, you can either use <strong class="source-inline">iloc</strong> or use the <strong class="source-inline">pd.DataFrame.column</strong> attribute to retrieve the name of the column:<p class="source-code">    # Retrieve column by name</p><p class="source-code">    df["sys_initial"] # Equivalent to</p><p class="source-code">    df.sys_initial</p><p class="source-code">    # Retrieve column by position</p><p class="source-code">    df[df.columns[2]] # Equivalent to</p><p class="source-code">    df.iloc[:, 2]</p></li>
			</ol>
			<p>These methods also support more advanced indexing, similar to those of NumPy, such as <strong class="source-inline">bool</strong>, lists, and <strong class="source-inline">int</strong> arrays.</p>
			<p>Now, it's time for some performance considerations. There are some differences between an index in pandas and a dictionary. For example, while the keys of a dictionary cannot contain duplicates, pandas indexes can contain repeated elements. This flexibility, however, comes at a cost – if we try to access an element in a non-unique index, we may incur substantial performance loss – the access will be <em class="italic">O</em>(<em class="italic">N</em>), like a linear search, rather than <em class="italic">O</em>(1), like a dictionary.</p>
			<p>A way to mitigate<a id="_idIndexMarker214"/> this effect is to sort the index; this will allow pandas to use<a id="_idIndexMarker215"/> a binary search algorithm with a computational complexity of <em class="italic">O</em>(<em class="italic">log</em>(<em class="italic">N</em>)), which is much better. This can be accomplished using the <strong class="source-inline">pd.Series.sort_index</strong> function, as shown in the following code (the same applies for <strong class="source-inline">pd.DataFrame</strong>):</p>
			<p class="source-code">    # Create a series with duplicate index</p>
			<p class="source-code">    index = list(range(1000)) + list(range(1000))</p>
			<p class="source-code">    # Accessing a normal series is a O(N) operation</p>
			<p class="source-code">    series = pd.Series(range(2000), index=index)</p>
			<p class="source-code">    # Sorting the will improve look-up scaling to O(log(N))</p>
			<p class="source-code">    series.sort_index(inplace=True)</p>
			<p>The timings for the different versions are summarized in the following table. If you'd like to rerun this benchmarking yourself, please refer to <strong class="source-inline">Chapter03/Pandas.ipynb</strong>:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/Table__3.1_B17499.jpg" alt="Table 3.1 – Performance analysis for pandas indexing " width="1650" height="378"/>
				</div>
			</div>
			<p class="figure-caption">Table 3.1 – Performance analysis for pandas indexing</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor059"/>Database-style operations with pandas</h2>
			<p>You may have noted that the <em class="italic">tabular</em> data is similar to what is usually stored in a database. A database is usually indexed using a primary key, and the various columns can have different data types, just like in a <strong class="source-inline">pd.DataFrame</strong>. </p>
			<p>The efficiency of the index operations<a id="_idIndexMarker216"/> in pandas makes it suitable for<a id="_idIndexMarker217"/> database-style manipulations, such as counting, joining, grouping, and aggregations.</p>
			<h3 id="_idParaDest-64">Mapping</h3>
			<p>pandas supports<a id="_idIndexMarker218"/> element-wise operations, just like NumPy (after all, <strong class="source-inline">pd.Series</strong> stores their data using <strong class="source-inline">np.array</strong>).</p>
			<p>For example, it is possible to apply transformation very easily to both <strong class="source-inline">pd.Series</strong> and <strong class="source-inline">pd.DataFrame</strong>:</p>
			<p class="source-code">    np.log(df.sys_initial) # Logarithm of a series</p>
			<p class="source-code">    df.sys_initial ** 2    # Square a series</p>
			<p class="source-code">    np.log(df)             # Logarithm of a dataframe</p>
			<p class="source-code">    df ** 2                # Square of a dataframe</p>
			<p>You can also perform element-wise operations between two <strong class="source-inline">pd.Series</strong> objects in a way similar to NumPy. An important difference is that the operands will be matched by key, rather than by position; if there is a mismatch in the index, the resulting value will be set to <strong class="source-inline">NaN</strong>. Both scenarios are exemplified in the following example:</p>
			<p class="source-code">    # Matching index</p>
			<p class="source-code">    a = pd.Series([1, 2, 3], index=["a", "b", "c"])</p>
			<p class="source-code">    b = pd.Series([4, 5, 6], index=["a", "b", "c"])</p>
			<p class="source-code">    a + b</p>
			<p class="source-code">    # Result: </p>
			<p class="source-code">    # a 5</p>
			<p class="source-code">    # b 7</p>
			<p class="source-code">    # c 9</p>
			<p class="source-code">    # dtype: int64</p>
			<p class="source-code">    # Mismatching index</p>
			<p class="source-code">    b = pd.Series([4, 5, 6], index=["a", "b", "d"])</p>
			<p class="source-code">    a + b</p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # a 5.0</p>
			<p class="source-code">    # b 7.0</p>
			<p class="source-code">    # c NaN</p>
			<p class="source-code">    # d NaN</p>
			<p class="source-code">    # dtype: float64</p>
			<p>For added flexibility, pandas offers the <strong class="source-inline">map</strong>, <strong class="source-inline">apply</strong>, and <strong class="source-inline">applymap</strong> methods, which can be used to apply specific transformations.</p>
			<p>The <strong class="source-inline">pd.Series.map</strong> method can be used to execute a function for each value and return a <strong class="source-inline">pd.Series</strong> containing each<a id="_idIndexMarker219"/> result. In the following example, we can see how to apply the <strong class="source-inline">superstar</strong> function to each element of a <strong class="source-inline">pd.Series</strong>:</p>
			<p class="source-code">    a = pd.Series([1, 2, 3], index=["a", "b", "c"])</p>
			<p class="source-code">    def superstar(x):</p>
			<p class="source-code">        return '*' + str(x) + '*'</p>
			<p class="source-code">    a.map(superstar)</p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # a *1*</p>
			<p class="source-code">    # b *2*</p>
			<p class="source-code">    # c *3*</p>
			<p class="source-code">    # dtype: object</p>
			<p>The <strong class="source-inline">pd.DataFrame.applymap</strong> function is the equivalent of <strong class="source-inline">pd.Series.map</strong>, but for <strong class="source-inline">DataFrames</strong>:</p>
			<p class="source-code">    df.applymap(superstar)</p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    #    dia_final  dia_initial  sys_final  sys_initial</p>
			<p class="source-code">    # a       *70*         *75*      *115*        *120*</p>
			<p class="source-code">    # b       *82*         *85*      *123*        *126*</p>
			<p class="source-code">    # c       *92*         *90*      *130*        *130*</p>
			<p class="source-code">    # d       *87*         *87*      *118*        *115*</p>
			<p>Finally, the <strong class="source-inline">pd.DataFrame.apply</strong> function can apply the passed function to each column or each row, rather<a id="_idIndexMarker220"/> than element-wise. This selection can be performed with the argument axis, where a value of <strong class="source-inline">0</strong> (the default) corresponds to columns and <strong class="source-inline">1</strong> corresponds to rows. Also, note that the return value of <strong class="source-inline">apply</strong> is a <strong class="source-inline">pd.Series</strong>:</p>
			<p class="source-code">    df.apply(superstar, axis=0)</p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # dia_final *a 70nb 82nc 92nd 87nName: dia...</p>
			<p class="source-code">    # dia_initial *a 75nb 85nc 90nd 87nName: dia...</p>
			<p class="source-code">    # sys_final *a 115nb 123nc 130nd 118nName:...</p>
			<p class="source-code">    # sys_initial *a 120nb 126nc 130nd 115nName:...</p>
			<p class="source-code">    # dtype: object</p>
			<p class="source-code">    df.apply(superstar, axis=1)</p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # a *dia_final 70ndia_initial 75nsys_f...</p>
			<p class="source-code">    # b *dia_final 82ndia_initial 85nsys_f...</p>
			<p class="source-code">    # c *dia_final 92ndia_initial 90nsys_f...</p>
			<p class="source-code">    # d *dia_final 87ndia_initial 87nsys_f...</p>
			<p class="source-code">    # dtype: object</p>
			<p>pandas also supports efficient <strong class="source-inline">numexpr</strong>-style expressions with the convenient <strong class="source-inline">eval</strong> method. For example, if we want to calculate<a id="_idIndexMarker221"/> the difference between the final and initial blood pressure, we can write the expression as a string, as shown in the following code:</p>
			<p class="source-code">    df.eval("sys_final - sys_initial")</p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # a -5</p>
			<p class="source-code">    # b -3</p>
			<p class="source-code">    # c 0</p>
			<p class="source-code">    # d 3</p>
			<p class="source-code">    # dtype: int64</p>
			<p>It is also possible to create new columns using the assignment operator in the <strong class="source-inline">pd.DataFrame.eval</strong> expression. Note that if the <strong class="source-inline">inplace=True</strong> argument is used, the operation will be applied directly to the original <strong class="source-inline">pd.DataFrame</strong>; otherwise, the function will return a new DataFrame. In the following example, we are computing the difference between <strong class="source-inline">sys_final</strong> and <strong class="source-inline">sys_initial</strong>, and we store it in the <strong class="source-inline">sys_delta</strong> column:</p>
			<p class="source-code">df.eval("sys_delta = sys_final - sys_initial", \</p>
			<p class="source-code">  inplace=False)</p>
			<p class="source-code"># Result:</p>
			<p class="source-code">#     dia_final   dia_initial   sys_final   sys_initial   </p>
			<p class="source-code">   sys_delta</p>
			<p class="source-code"># a          70            75         115           120     </p>
			<p class="source-code">-5</p>
			<p class="source-code"># b          82            85         123           126     </p>
			<p class="source-code">  -3</p>
			<p class="source-code"># c          92            90         130           130     </p>
			<p class="source-code">  0</p>
			<p class="source-code"># d          87            87         118           115     </p>
			<p class="source-code">  3</p>
			<h3 id="_idParaDest-65">Grouping, aggregations, and transforms</h3>
			<p>One of the most appreciated features of pandas is its simple and concise method of grouping, transforming, and aggregating data. To demonstrate this concept, let's extend our dataset<a id="_idIndexMarker222"/> by adding two new patients<a id="_idIndexMarker223"/> that we didn't administer<a id="_idIndexMarker224"/> the treatment to (this is usually called a <em class="italic">control group</em>). We will also include<a id="_idIndexMarker225"/> a column, <strong class="source-inline">drug_admst</strong>, which records whether the patient was administered the treatment:</p>
			<p class="source-code">    patients = ["a", "b", "c", "d", "e", "f"]</p>
			<p class="source-code">    columns = {</p>
			<p class="source-code">      "sys_initial": [120, 126, 130, 115, 150, 117],</p>
			<p class="source-code">      "dia_initial": [75, 85, 90, 87, 90, 74],</p>
			<p class="source-code">      "sys_final": [115, 123, 130, 118, 130, 121],</p>
			<p class="source-code">      "dia_final": [70, 82, 92, 87, 85, 74],</p>
			<p class="source-code">      "drug_admst": [True, True, True, True, False, False]</p>
			<p class="source-code">    }</p>
			<p class="source-code">    df = pd.DataFrame(columns, index=patients)</p>
			<p>At this point, we may be interested<a id="_idIndexMarker226"/> to know how the blood pressure<a id="_idIndexMarker227"/> changed between the two groups. You can group the patients according to <strong class="source-inline">drug_amst</strong> using the <strong class="source-inline">pd.DataFrame.groupby</strong> function. The return value will be the <strong class="source-inline">DataFrameGroupBy</strong> object, which can be iterated to obtain a new <strong class="source-inline">pd.DataFrame</strong> for each value of the <strong class="source-inline">drug_admst</strong> column:</p>
			<p class="source-code">    df.groupby('drug_admst')</p>
			<p class="source-code">    for value, group in df.groupby('drug_admst'):</p>
			<p class="source-code">        print("Value: {}".format(value))</p>
			<p class="source-code">        print("Group DataFrame:")</p>
			<p class="source-code">        print(group)</p>
			<p class="source-code"># Output:</p>
			<p class="source-code"># Value: False</p>
			<p class="source-code"># Group DataFrame:</p>
			<p class="source-code">#    dia_final   dia_initial   drug_admst   sys_final   </p>
			<p class="source-code">   sys_initial</p>
			<p class="source-code"># e         85            90        False         130       </p>
			<p class="source-code">  150</p>
			<p class="source-code"># f         74            74        False         121       </p>
			<p class="source-code">  117</p>
			<p class="source-code"># Value: True</p>
			<p class="source-code"># Group DataFrame:</p>
			<p class="source-code">#    dia_final   dia_initial   drug_admst   sys_final   </p>
			<p class="source-code">   sys_initial</p>
			<p class="source-code"># a         70            75         True         115       </p>
			<p class="source-code">  120</p>
			<p class="source-code"># b         82            85         True         123       </p>
			<p class="source-code">  126</p>
			<p class="source-code"># c         92            90         True         130       </p>
			<p class="source-code">  130</p>
			<p class="source-code"># d         87            87         True         118       </p>
			<p class="source-code">  115</p>
			<p>Iterating the <strong class="source-inline">DataFrameGroupBy</strong> object is rarely necessary because, thanks to method chaining, it is possible to calculate group-related properties directly. For example, we may want to<a id="_idIndexMarker228"/> calculate the mean, max, or standard deviation for each group. All those<a id="_idIndexMarker229"/> operations that summarize the data<a id="_idIndexMarker230"/> in some way are called aggregations and can be performed using the <strong class="source-inline">agg</strong> method. The result of <strong class="source-inline">agg</strong> is another <strong class="source-inline">pd.DataFrame</strong> that relates the grouping variables and the result of the aggregation, as illustrated in the following code:</p>
			<p class="source-code">df.groupby('drug_admst').agg(np.mean)</p>
			<p class="source-code">#              dia_final   dia_initial   sys_final   sys_in</p>
			<p class="source-code">  itial</p>
			<p class="source-code"># drug_admst </p>
			<p class="source-code"># False            79.50         82.00       125.5        </p>
			<p class="source-code">  133.50</p>
			<p class="source-code"># True             82.75         84.25       121.5        </p>
			<p class="source-code">  122.75</p>
			<p class="callout-heading">Note</p>
			<p class="callout">It is also possible to perform processing on the DataFrame groups that do not represent a summarization. One common<a id="_idIndexMarker231"/> example of such an operation is filling in missing values. Those intermediate steps are called <strong class="bold">transforms</strong>.</p>
			<p>We can illustrate<a id="_idIndexMarker232"/> this concept with an example. Let's assume<a id="_idIndexMarker233"/> that we have a few missing<a id="_idIndexMarker234"/> values in our dataset, and we want to replace those values with the average of the other values in the same group. This can be accomplished using a transform, as follows:</p>
			<p class="source-code">df.loc['a','sys_initial'] = None</p>
			<p class="source-code">df.groupby('drug_admst').transform(lambda df: \</p>
			<p class="source-code">  df.fillna(df.mean())) </p>
			<p class="source-code">#     dia_final    dia_initial   sys_final   sys_initial</p>
			<p class="source-code"># a          70             75         115    123.666667</p>
			<p class="source-code"># b          82             85         123    126.000000</p>
			<p class="source-code"># c          92             90         130    130.000000</p>
			<p class="source-code"># d          87             87         118    115.000000</p>
			<p class="source-code"># e          85             90         130    150.000000</p>
			<p class="source-code"># f          74             74         121    117.000000</p>
			<h3 id="_idParaDest-66">Joining</h3>
			<p><strong class="bold">Joins</strong> are useful for aggregating data that is scattered among different tables. Let's say that we want to include the location<a id="_idIndexMarker235"/> of the hospital in which patient measurements<a id="_idIndexMarker236"/> were taken in our dataset. We can reference the location for each patient using the <strong class="source-inline">H1</strong>, <strong class="source-inline">H2</strong>, and <strong class="source-inline">H3</strong> labels, and we can store the address and identifier of the hospital in a <strong class="source-inline">hospital</strong> table:</p>
			<p class="source-code">    hospitals = pd.DataFrame(</p>
			<p class="source-code">      { "name" : ["City 1", "City 2", "City 3"],</p>
			<p class="source-code">        "address" : ["Address 1", "Address 2", "Address \</p>
			<p class="source-code">          3"],</p>
			<p class="source-code">        "city": ["City 1", "City 2", "City 3"] },</p>
			<p class="source-code">      index=["H1", "H2", "H3"])</p>
			<p class="source-code">    hospital_id = ["H1", "H2", "H2", "H3", "H3", "H3"]</p>
			<p class="source-code">    df['hospital_id'] = hospital_id</p>
			<p>Now, we want to find the city where the measure was taken for each patient. We need to <em class="italic">map</em> the keys from the <strong class="source-inline">hospital_id</strong> column to the city stored in the <strong class="source-inline">hospitals</strong> table.</p>
			<p>This can be implemented in Python using dictionaries:</p>
			<p class="source-code">    hospital_dict = {</p>
			<p class="source-code">     "H1": ("City 1", "Name 1", "Address 1"),</p>
			<p class="source-code">     "H2": ("City 2", "Name 2", "Address 2"),</p>
			<p class="source-code">     "H3": ("City 3", "Name 3", "Address 3")</p>
			<p class="source-code">    }</p>
			<p class="source-code">    cities = [hospital_dict[key][0] </p>
			<p class="source-code">             for key in hospital_id]</p>
			<p>This algorithm runs efficiently with an <em class="italic">O</em>(<em class="italic">N</em>) time complexity, where <em class="italic">N</em> is the size of <strong class="source-inline">hospital_id</strong>. pandas allows you to encode the same operation using simple indexing; the advantage is that the<a id="_idIndexMarker237"/> join will be performed in heavily optimized Cython and with efficient hashing algorithms. The preceding simple Python expression can easily be converted into pandas in this way:</p>
			<p class="source-code">    cities = hospitals.loc[hospital_id, "city"]</p>
			<p>More advanced joins can also be performed with the <strong class="source-inline">pd.DataFrame.join</strong> method, which will produce a new <strong class="source-inline">pd.DataFrame</strong> that will attach the hospital information for each patient:</p>
			<p class="source-code">    result = df.join(hospitals, on='hospital_id')</p>
			<p class="source-code">    result.columns</p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # Index(['dia_final', 'dia_initial', 'drug_admst', </p>
			<p class="source-code">    # 'sys_final', 'sys_initial',</p>
			<p class="source-code">    # 'hospital_id', 'address', 'city', 'name'],</p>
			<p class="source-code">    # dtype='object')</p>
			<p>This concludes our discussion<a id="_idIndexMarker238"/> on pandas. In the next section, we will talk about xarray, the state-of-the-art tool for working with multidimensional labeled data in Python.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor060"/>High-performance labeled data with xarray</h1>
			<p>With NumPy, we can manipulate multidimensional numerical data and perform mathematical computations<a id="_idIndexMarker239"/> that are highly optimized by low-level C and FORTRAN<a id="_idIndexMarker240"/> code. On the other hand, we have seen that pandas allows us to work with labeled, categorical data that resembles data tables using database-like operations.</p>
			<p>These two tools complement each other: NumPy does not allow categorical data to be mixed in with numerical values, while pandas is mostly limited to two-dimensional, database-like datasets. Combining these tools can help address many data processing needs, but when we are faced with big, multidimensional data that is also labeled, many performance-related problems arise.</p>
			<p>In the last section of this chapter, we will discuss xarray, a library that combines the best of both the NumPy and the pandas worlds and offers one of the best tools for working with labeled multidimensional data. We will explore some of its most prominent features while noting<a id="_idIndexMarker241"/> the improvements we achieve<a id="_idIndexMarker242"/> with xarray over other Python libraries.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor061"/>Analyzing <img src="image/Formula_3_B17499.png" alt="" width="103" height="52"/> concentration</h2>
			<p>To guide our discussion, we will be using the carbon dioxide concentration data, which was collected concerning<a id="_idIndexMarker243"/> the volcano Mauna Loa in Hawaii. The dataset is a time series of monthly measurements of the <img src="image/Formula_3_B174991.png" alt="" width="54" height="28"/> level, starting from 1958 to this day. We have prepared a cleaned version of this dataset for you, which is included in the code repository for this book in the <strong class="source-inline">monthly_co2.csv</strong> file. </p>
			<p>The data has three simple columns: </p>
			<ul>
				<li>The year of measurement</li>
				<li>The month of measurement</li>
				<li>The measurement itself</li>
			</ul>
			<p>Our goal is to analyze this dataset and visualize any time-related trends. Since we are already familiar with using pandas to work with a <strong class="source-inline">.csv</strong> file, let's proceed with the library to start:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">df = pd.read_csv('monthly_co2.csv', index_col=[0, 1])</p>
			<p class="source-code">df.head()</p>
			<p>Make sure that the data file is in the same directory as this code. You may remember that this will read in the file and store the data in a <strong class="source-inline">DataFrame</strong> object. Here, we are using the first two columns (by using the <strong class="source-inline">index_col=[0, 1]</strong> argument) as the index of this <strong class="source-inline">DataFrame</strong> object. Finally, we print out the first five rows of this dataset, which look as follows:</p>
			<p class="source-code">          co2</p>
			<p class="source-code">year    month    </p>
			<p class="source-code">1958    3    315.70</p>
			<p class="source-code">     4     317.45</p>
			<p class="source-code">     5     317.51</p>
			<p class="source-code">     6     317.24</p>
			<p class="source-code">     7     315.86</p>
			<p>Here, we can see that in March <strong class="source-inline">1958</strong>, the <img src="image/Formula_3_B174992.png" alt="" width="57" height="29"/> level was <strong class="source-inline">315.70</strong>, and that the following month's measurement was <strong class="source-inline">317.45</strong>.</p>
			<p>The first thing we'd like to look at is a simple line graph corresponding to the <strong class="source-inline">co2</strong> column, which is simply<a id="_idIndexMarker244"/> the graph of the <img src="image/Formula_3_B174992.png" alt="" width="57" height="29"/> level as a function of time (in <strong class="source-inline">month</strong>). With the help of Matplotlib, the go-to plotting tool in Python, we can do this very easily:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">plt.plot(df.co2.values);</p>
			<p>The preceding code will produce the following output:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/Figure_3.4_B17499.jpg" alt="Figure 3.4 – Monthly co2 level " width="596" height="248"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – Monthly <img src="image/Formula_3_B174993.png" alt="" width="66" height="34"/> level</p>
			<p>We can notice two very distinct trends:</p>
			<ul>
				<li>The first is the <em class="italic">global</em> increasing trend, which roughly goes from 320 to 420 during our timeline.</li>
				<li>The second looks like a <em class="italic">seasonal</em> zigzag trend, which is present locally and repeats itself every year.</li>
			</ul>
			<p>To verify this intuition, we can inspect<a id="_idIndexMarker245"/> the average data across the years, which will tell us that the <img src="image/Formula_3_B174993.png" alt="" width="66" height="34"/> level has been rising as a global trend. We can also compute the average measurement for each month and consider how the data changed from January to December. To accomplish this, we will utilize the <strong class="source-inline">groupby</strong> function by computing the yearly averages and plotting them:</p>
			<p class="source-code">by_year = df.groupby('year').mean().co2</p>
			<p class="source-code">plt.plot(by_year);</p>
			<p>This gives us the following output:</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/Figure_3.5_B17499.jpg" alt="Figure 3.5 – Yearly co2 level, averaged across months " width="529" height="252"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – Yearly <img src="image/Formula_3_B174992.png" alt="" width="57" height="29"/> level, averaged across months</p>
			<p>Just as we expected, we can see the global rising trend of the <img src="image/Formula_3_B174992.png" alt="" width="57" height="29"/> level:</p>
			<p class="source-code">by_month = df.groupby('month').mean().co2</p>
			<p class="source-code">plt.plot(by_month);</p>
			<p>Similarly, the preceding code<a id="_idIndexMarker246"/> generates the following average-by-month data:</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/Figure_3.6_B17499.jpg" alt="Figure 3.6 – Monthly co2 level, averaged across years " width="565" height="248"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – Monthly <img src="image/Formula_3_B174992.png" alt="" width="57" height="29"/> level, averaged across years</p>
			<p>The seasonal trend we suspected<a id="_idIndexMarker247"/> now becomes clear: the <img src="image/Formula_3_B174992.png" alt="" width="57" height="29"/> level tends to rise during the summer and fall between fall and winter.</p>
			<p>So far, we have been using pandas to manipulate our data. Even with this minimal example, we can notice a few things:</p>
			<ul>
				<li>Loading the data, specifically the <strong class="source-inline">co2</strong> column, as a NumPy array would be inappropriate since we would lose information about the year and month each measurement was made. pandas is the better choice here.</li>
				<li>On the other hand, the <strong class="source-inline">groupby</strong> function can be unintuitive and costly to work with. Here, we simply want to compute the <em class="italic">average-by-month and average-by-year measurements</em>, but that requires us to group our data by the <strong class="source-inline">month</strong> column and then by <strong class="source-inline">year</strong>. Although pandas takes care of this grouping for us behind the scenes, it is an expensive operation, especially if we are working with a significantly large dataset.</li>
				<li>To bypass this inefficiency, we can think of representing the <strong class="source-inline">co2</strong> column as a two-dimensional NumPy array, where the rows represent years and the columns represent months, and each cell in the array holds the measurement. Now, to compute the averages we want, we could simply calculate the mean along each of the two axes, which we know NumPy can do efficiently. However, once again, we lose the expressiveness of the labeled <strong class="source-inline">month</strong> and <strong class="source-inline">year</strong> data we have under pandas.</li>
			</ul>
			<p>This dilemma we are currently<a id="_idIndexMarker248"/> facing is similar to the one that motivated the development of xarray, the premiere tool in Python for working with labeled, multidimensional data. The idea is to extend NumPy's support for fast, multidimensional array computations and allow dimensions (or axes) to have labels, which is one of the main selling points of pandas.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor062"/>The xarray library</h2>
			<p>xarray is developed <a id="_idIndexMarker249"/>and actively maintained by PyData and a part of the NumFOCUS project. To use the library, you must head to <a href="http://xarray.pydata.org/en/stable/installing.html">http://xarray.pydata.org/en/stable/installing.html</a> for more details on how to install it. To continue with our example of the <img src="image/Formula_3_B174992.png" alt="" width="57" height="29"/> concentration level, we will feed the data we have into a <strong class="source-inline">Dataset</strong> object in xarray using the following code:</p>
			<p class="source-code">import xarray as xr</p>
			<p class="source-code">ds = xr.Dataset.from_dataframe(df)</p>
			<p>If we were to print this object out in a Jupyter notebook, the output would be formatted nicely:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/Figure_3.7_B17499.jpg" alt="Figure 3.7 – A Dataset instance of xarray, printed in Jupyter " width="1650" height="749"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – A Dataset instance of xarray, printed in Jupyter</p>
			<p>xarray was able to infer that <strong class="source-inline">month</strong> and <strong class="source-inline">year</strong> (the index columns in <strong class="source-inline">DataFrame</strong>) should be the dimension, as indicated by the <strong class="bold">Dimensions</strong> section of the output. The <strong class="bold">Coordinates</strong> section shows the values that each of these dimensions can take on; in our case, the year could be anything between <strong class="bold">1958</strong> and <strong class="bold">2021</strong>, while the month could be anything between <strong class="bold">1</strong> and <strong class="bold">12</strong>. The <strong class="bold">Data variables</strong> section shows the actual <strong class="source-inline">co2</strong> data we care about, which is now a two-dimensional array indexed by <strong class="source-inline">year</strong> and <strong class="source-inline">month</strong>.</p>
			<p>xarray makes interacting<a id="_idIndexMarker250"/> and inspecting its objects easy and interactive; you can inspect the values of the coordinates and data variables further by clicking on the icons highlighted in the preceding screenshot.</p>
			<p>We mentioned earlier that xarray combines the best features of NumPy and pandas; this is best illustrated via the slicing/indexing interface it provides. For example, let's say that we'd like to extract the measurements within the first 10 years of the dataset. For the first 5 months, we could apply NumPy-style slicing to the <strong class="source-inline">'co2'</strong> variable of the <strong class="source-inline">ds</strong> object:</p>
			<p class="source-code">ds['co2'][:10, :5]</p>
			<p>This gives us a <strong class="source-inline">DataArray</strong> object containing the requested values:</p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/Figure_3.8_B17499.jpg" alt="Figure 3.8 – A DataArray instance of xarray, printed in Jupyter " width="1314" height="768"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – A DataArray instance of xarray, printed in Jupyter</p>
			<p>While NumPy slicing can be flexible, it does not offer much expressiveness for labeled data: we would have to know that the first axis of the implied multidimensional array is <strong class="source-inline">year</strong>, that the second is <strong class="source-inline">month</strong>, and that <strong class="source-inline">ds['co2'][:10, :5]</strong> doesn't explicitly say which years we are selecting for.</p>
			<p>As such, we could use the <strong class="source-inline">sel</strong> function, which roughly offers the same functionality as pandas filtering. To select the values within the example year of <strong class="source-inline">1960</strong>, we can simply use the following code:</p>
			<p class="source-code">ds['co2'].sel(year=1960)</p>
			<p>This explicitly tells <a id="_idIndexMarker251"/>us that we are selecting <strong class="source-inline">1960</strong> along the <strong class="source-inline">year</strong> axis. For more examples of the different APIs the library offers, you can check out the documentation at <a href="http://xarray.pydata.org/en/stable/api.html">http://xarray.pydata.org/en/stable/api.html</a>.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor063"/>Improved performance</h2>
			<p>Now, we will consider the performance<a id="_idIndexMarker252"/> improvements that xarray offers. Recall that our goal is to compute the average measurement, first for each year to visualize the global trend, and then for each month for the seasonal trend. To do this, we can simply call the <strong class="source-inline">mean</strong> function while specifying the appropriate (labeled!) dimension.</p>
			<p>First, to obtain the average-by-year, we must compute the mean across the <strong class="source-inline">month</strong> dimension:</p>
			<p class="source-code">ds.mean(dim='month')</p>
			<p>This returns another <strong class="source-inline">Dataset</strong> object containing the computed values:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/Figure_3.9_B17499.jpg" alt="Figure 3.9 – Taking the average across a dimension in xarray " width="1249" height="504"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – Taking the average across a dimension in xarray</p>
			<p>From here, we can simply access the <strong class="source-inline">co2</strong> variable and pass the array to the <strong class="source-inline">plot</strong> function of Matplotlib to replicate <em class="italic">Figure 3.5</em>. For <em class="italic">Figure 3.6</em>, we can follow the same procedure using <strong class="source-inline">ds.mean(dim='year')</strong>.</p>
			<p>The advantage we are gaining here is the expressiveness in our code. If we were using NumPy, we would need to specify the <strong class="bold">axis number</strong> (for example, 0, 1, 2, and so on), not the <strong class="bold">dimension name</strong>, (for example, <strong class="source-inline">'month'</strong>, <strong class="source-inline">'year'</strong>). This might lead to hard-to-find bugs if you confuse<a id="_idIndexMarker253"/> which axis is which type of data in NumPy.</p>
			<p>Furthermore, the code is simpler and can take advantage of the optimized mean operation, which is managed under the hood by xarray, compared to the expensive <strong class="source-inline">groupby</strong> function from pandas. To see this, we can benchmark the two ways of computing the average-by-year that we have. First, we have the pandas way:</p>
			<p class="source-code">%timeit df.groupby('year').mean().co2</p>
			<p class="source-code"># Result:</p>
			<p class="source-code"># 534 µs ± 10.8 µs per loop (mean ± std. dev. of 7 runs, </p>
			<p class="source-code">  1000 loops each)</p>
			<p>Then, we have the xarray way:</p>
			<p class="source-code">%timeit ds.mean(dim='month').co2.values</p>
			<p class="source-code"># Result:</p>
			<p class="source-code"># 150 µs ± 1.27 µs per loop (mean ± std. dev. of 7 runs, </p>
			<p class="source-code">  10000 loops each)</p>
			<p>Here, we can see a clear performance<a id="_idIndexMarker254"/> improvement, achieved almost for free by passing our data to xarray!</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor064"/>Plotting with xarray</h2>
			<p>Labeled multidimensional arrays are ubiquitous in time series (where one of the dimensions is time), geospatial data (where some dimensions represent the coordinates on a map), or data that<a id="_idIndexMarker255"/> is both geospatial and time-dependent. In these data analysis tasks, data visualization is crucial; as such, xarray makes it easy to implement and call complex plotting functions on its data.</p>
			<p>Let's look at this through a quick example. First, we will read in an example dataset we have prepared for you, saved in a file named <strong class="source-inline">2d_measurement.npy</strong>, which can be read into a Python program using NumPy:</p>
			<p class="source-code">measures = np.load('2d_measure.npy')</p>
			<p class="source-code">measures.shape</p>
			<p class="source-code"># Result:</p>
			<p class="source-code"># (100, 100, 3)</p>
			<p>As you can see, it is a 100x100x3 array. Let's say that this dataset contains a specific type of measurement, taken over a 100x100 grid of a two-dimensional space (corresponding to the first two axes) at three specific timestamps (corresponding to the third axis).</p>
			<p>We would like to visualize these measurements as three squares, where each square represents a specific time stamp, and each pixel in each square represents the intensity of the corresponding measurement.</p>
			<p>To do this in Matplotlib, we could use the <strong class="source-inline">imshow</strong> function, which takes in a two-dimensional array and plots it as an image. So, we would iterate through the three timestamps that we have and plot the corresponding grids one by one, as follows:</p>
			<p class="source-code">fig, ax = plt.subplots(1, 3, figsize=(10, 3))</p>
			<p class="source-code">for i in range(3):</p>
			<p class="source-code">    c = ax[i].imshow(measures[:, :, i], origin='lower')</p>
			<p class="source-code">    plt.colorbar(c, ax=ax[i])</p>
			<p class="source-code">    </p>
			<p class="source-code">plt.show()</p>
			<p class="callout-heading">Note </p>
			<p class="callout">The index, <strong class="source-inline">i</strong>, iterates through the three indices of the third axis in the <strong class="source-inline">measures</strong> array. Again, we can see that this indexing scheme is not very expressive and readable. Here, we are also using the <strong class="source-inline">colorbar</strong> function to add a color bar to each of the plots. </p>
			<p>The preceding code produces the following output:</p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/Figure_3.10_B17499.jpg" alt="Figure 3.10 – Regular imshow from Matplotlib " width="1232" height="358"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10 – Regular imshow from Matplotlib</p>
			<p>Although there is noise<a id="_idIndexMarker256"/> in the measurements, we can observe some global trends; specifically, we seem to have low-intensity measurements in the lower-left corner in the first plot, in the center in the second, and in the top-right corner in the third. As the final note, something we might want to change about this plot is making the three color bars have the same range, which may be hard to do with Matplotlib.</p>
			<p>Now, let's see how we can produce this plot using xarray. First, we must convert the NumPy array into a <strong class="source-inline">DataArray</strong> object:</p>
			<p class="source-code">da = xr.DataArray(measures, dims=['x', 'y', 'time'])</p>
			<p>Here, we are also specifying<a id="_idIndexMarker257"/> the names for the three dimensions: <strong class="source-inline">'x'</strong>, <strong class="source-inline">'y'</strong>, and <strong class="source-inline">'time'</strong>. This will allow us to manipulate the data more expressively, as we saw previously. To plot out the 2D grids, we can use the similarly named <strong class="source-inline">imshow</strong> method:</p>
			<p class="source-code">da.plot.imshow(x='x', y='y', col='time', robust=True);</p>
			<p>This results in the following output:</p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/Figure_3.11_B17499.jpg" alt="Figure 3.11 – Specialized imshow from xarray " width="1286" height="416"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11 – Specialized imshow from xarray</p>
			<p>The plotting function is much simpler than the for loop we had earlier. Furthermore, with minimal code, xarray has taken care of many different aesthetics-related aspects of this plot: </p>
			<ul>
				<li>First, the plots have their titles and x- and y-axis labels automatically created. </li>
				<li>Second, by using a common color range, the fact that the measurements at the second timestamp are lower than those in the other two is now more obvious. This demonstrates that functions and methods in xarray are optimized to make working with labeled multidimensional data more efficient.</li>
			</ul>
			<p>On the topic of plotting, many data scientists work with map data. xarray nicely integrates with the popular Cartopy library for geospatial data processing and offers many plotting functionalities<a id="_idIndexMarker258"/> that incorporate world and country maps. More details can be found in their documentation: <a href="http://xarray.pydata.org/en/stable/plotting.html">http://xarray.pydata.org/en/stable/plotting.html</a>.</p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor065"/>Summary</h1>
			<p>In this chapter, we learned how to manipulate NumPy arrays and how to write fast mathematical expressions using array broadcasting. This knowledge will help you write more concise, expressive code and, at the same time, obtain substantial performance gains. We also introduced the <strong class="source-inline">numexpr</strong> library to further speed up NumPy calculations with minimal effort.</p>
			<p>pandas implements efficient data structures that are useful when analyzing large datasets. In particular, pandas shines when the data is indexed by non-integer keys and provides very fast hashing algorithms.</p>
			<p>NumPy and pandas work well when handling large, homogenous inputs, but they are not suitable when the expressions become complex and the operations cannot be expressed using the tools provided by these libraries. xarray comes in handy as an alternative option where we need to work with labeled, multidimensional data.</p>
			<p>In combination, the three libraries offer Python users powerful APIs and flexible functionalities to work with a wide range of data. By keeping them in your toolbox, you are well situated to tackle most data processing and engineering tasks using Python.</p>
			<p>In other cases, we can also leverage Python capabilities as a glue language by interfacing it with C using the Cython package, as we will see in the next chapter.</p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor066"/>Questions</h1>
			<ol>
				<li value="1">Name the advantages NumPy has over Python-native lists when working with multidimensional data.</li>
				<li>What are some of the database-style operations that pandas offers in its API?</li>
				<li>What problems does xarray address and why can they not be addressed by NumPy or pandas?</li>
			</ol>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor067"/>Further reading</h1>
			<ul>
				<li>An overview tutorial on NumPy and pandas: <a href="https://cloudxlab.com/blog/numpy-pandas-introduction/">https://cloudxlab.com/blog/numpy-pandas-introduction/</a></li>
				<li>Data structures in xarray: <a href="https://towardsdatascience.com/basic-data-structures-of-xarray-80bab8094efa">https://towardsdatascience.com/basic-data-structures-of-xarray-80bab8094efa</a></li>
			</ul>
		</div>
	</div>
</div>
</body></html>