- en: '*Chapter 8*: Parallel Processing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With parallel processing using multiple cores, you can increase the number of
    calculations your program can do in a given time frame without needing a faster
    processor. The main idea is to divide a problem into independent subunits and
    use multiple cores to solve those subunits in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel processing is necessary to tackle large-scale problems. Every day,
    companies produce massive quantities of data that needs to be stored in multiple
    computers and analyzed. Scientists and engineers run parallel code on supercomputers
    to simulate massive systems.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel processing allows you to take advantage of multicore **central processing
    units** (**CPUs**) as well as **graphics processing units** (**GPUs**) that work
    extremely well with highly parallel problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to parallel programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multiple processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel Cython with **Open Multi-Processing** (**OpenMP**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code files for this chapter can be accessed through this link: [https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter08).'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to parallel programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To parallelize a program, it is necessary to divide the problem into subunits
    that can run independently (or almost independently) from each other.
  prefs: []
  type: TYPE_NORMAL
- en: A problem where the subunits are totally independent of each other is called
    *embarrassingly parallel*. An element-wise operation on an array is a typical
    example—the operation needs to only know the element it is handling now. Another
    example is our particle simulator. Since there are no interactions, each particle
    can evolve independently from the others. Embarrassingly parallel problems are
    very easy to implement and perform very well on parallel architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Other problems may be divided into subunits but must share some data to perform
    their calculations. In those cases, the implementation is less straightforward
    and can lead to performance issues because of the communication costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will illustrate the concept with an example. Imagine that you have a particle
    simulator, but this time, the particles attract other particles within a certain
    distance (as shown in the following figure):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Illustration of a neighboring region ](img/B17499_Figure_8.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Illustration of a neighboring region
  prefs: []
  type: TYPE_NORMAL
- en: To parallelize this problem, we divide the simulation box into regions and assign
    each region to a different processor. If we evolve the system one step at a time,
    some particles will interact with particles in a neighboring region. To perform
    the next iteration, communication with the new particle positions of the neighboring
    region is required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Communication between processes is costly and can seriously hinder the performance
    of parallel programs. There exist two main ways to handle data communication in
    parallel programs: shared memory and distributed memory.'
  prefs: []
  type: TYPE_NORMAL
- en: In **shared memory**, the subunits have access to the same memory space. An
    advantage of this approach is that you don't have to explicitly handle the communication
    as it is sufficient to write or read from the shared memory. However, problems
    arise when multiple processes try to access and change the same memory location
    at the same time. Care should be taken to avoid such conflicts using synchronization
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the **distributed memory** model, each process is completely separated from
    the others and possesses its own memory space. In this case, communication is
    handled explicitly between the processes. The communication overhead is typically
    costlier compared to shared memory as data can potentially travel through a network
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'One common way to achieve parallelism with the shared memory model is through
    **threads**. Threads are independent subtasks that originate from a process and
    share resources, such as memory. This concept is further illustrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Illustration of the difference between threads and processes
    ](img/B17499_Figure_8.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Illustration of the difference between threads and processes
  prefs: []
  type: TYPE_NORMAL
- en: Threads produce multiple execution contexts and share the same memory space,
    while processes provide multiple execution contexts that possess their own memory
    space, and communication must be handled explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Python can spawn and handle threads, but they can't be used to increase performance;
    due to the Python interpreter design, only one Python instruction is allowed to
    run at a time—this mechanism is called the **Global Interpreter Lock** (**GIL**).
    What happens is that each time a thread executes a Python statement, the thread
    acquires a lock and, when the execution is completed, the same lock is released.
    Since the lock can be acquired only by one thread at a time, other threads are
    prevented from executing Python statements while some other thread holds the lock.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the GIL prevents parallel execution of Python instructions, threads
    can still be used to provide concurrency in situations where the lock can be released,
    such as in time-consuming **input/output** (**I/O**) operations or in C extensions.
  prefs: []
  type: TYPE_NORMAL
- en: Why Not Remove the GIL?
  prefs: []
  type: TYPE_NORMAL
- en: In past years, many attempts have been made, including the most recent *Gilectomy*
    experiment. First, removing the GIL is not an easy task and requires modification
    of most of the Python data structures. Additionally, such fine-grained locking
    can be costly and may introduce substantial performance loss in single-threaded
    programs. Despite this, some Python implementations (notable examples are Jython
    and IronPython) do not use the GIL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GIL can be completely sidestepped using processes instead of threads. Processes
    don''t share the same memory area and are independent of each other—each process
    has its own interpreter. Processes have a few disadvantages: starting up a new
    process is generally slower than starting a new thread, they consume more memory,
    and **inter-process communication** (**IPC**) can be slow. On the other hand,
    processes are still very flexible, and they scale better as they can be distributed
    on multiple machines.'
  prefs: []
  type: TYPE_NORMAL
- en: GPUs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPUs are special processors designed for computer graphics applications. Those
    applications usually require processing the geometry of a **three-dimensional**
    (**3D**) scene and output an array of pixels to the screen. The operations performed
    by GPUs involve array and matrix operations on floating-point numbers.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs are designed to run this graphics-related operation very efficiently, and
    they achieve this by adopting a highly parallel architecture. Compared to a CPU,
    a GPU has many more (thousands) of small processing units. GPUs are intended to
    produce data at about 60 **frames per second** (**FPS**), which is much slower
    than the typical response time of a CPU, which possesses higher clock speeds.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs possess a very different architecture from a standard CPU and are specialized
    for computing floating-point operations. Therefore, to compile programs for GPUs,
    it is necessary to utilize special programming platforms, such as **Compute Unified
    Device Architecture** (**CUDA**) and **Open Computing Language** (**OpenCL**).
  prefs: []
  type: TYPE_NORMAL
- en: CUDA is a proprietary NVIDIA technology. It provides an **application programming
    interface** (**API**) that can be accessed from other languages. CUDA provides
    the **NVIDIA CUDA Compiler** (**NVCC**) tool that can be used to compile GPU programs
    written in a language such as C (CUDA C), as well as numerous libraries that implement
    highly optimized mathematical routines.
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenCL** is an open technology with an ability to write parallel programs
    that can be compiled for a variety of target devices (CPUs and GPUs of several
    vendors) and is a good option for non-NVIDIA devices.'
  prefs: []
  type: TYPE_NORMAL
- en: GPU programming sounds wonderful on paper. However, don't throw away your CPU
    yet. GPU programming is tricky, and only specific use cases benefit from the GPU
    architecture. Programmers need to be aware of the costs incurred in memory transfers
    to and from the main memory and how to implement algorithms to take advantage
    of the GPU architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, GPUs are great at increasing the number of operations you can perform
    per unit of time (also called **throughput**); however, they require more time
    to prepare the data for processing. In contrast, CPUs are much faster at producing
    an individual result from scratch (also called **latency**).
  prefs: []
  type: TYPE_NORMAL
- en: For the right problem, GPUs provide extreme (10 to 100 times) speedup. For this
    reason, they often constitute a very inexpensive solution (the same speedup will
    require hundreds of CPUs) to improve the performance of numerically intensive
    applications. We will illustrate how to execute some algorithms on a GPU in the
    *Automatic parallelism* section.
  prefs: []
  type: TYPE_NORMAL
- en: Having said that, we will begin our discussion on multiprocessing using standard
    processes in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The standard `multiprocessing` module can be used to quickly parallelize simple
    tasks by spawning several processes while avoiding the GIL problem. Its interface
    is easy to use and includes several utilities to handle task submission and synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: The Process and Pool classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can create a process that runs independently by subclassing `multiprocessing.Process`.
    You can extend the `__init__` method to initialize resources, and you can write
    a portion of the code that will be executed in a subprocess by implementing the
    `Process.run` method. In the following code snippet, we define a `Process` class
    that will wait for 1 second and print its assigned `id` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To spawn the process, we must instantiate the `Process` class and call the
    `Process.start` method. Note that you don''t directly call `Process.run`; the
    call to `Process.start` will create a new process that, in turn, will call the
    `Process.run` method. We can add the following lines at the end of the preceding
    snippet to create and start the new process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The special __name__ Variable
  prefs: []
  type: TYPE_NORMAL
- en: Note that we need to place any code that manages processes inside the `if __name__
    == '__main__'` condition, as shown in the previous code snippet, to avoid many
    undesirable behaviors. All the code shown in this chapter will be assumed to follow
    this practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The instructions after `Process.start` will be executed immediately without
    waiting for the `p` process to finish. To wait for the task completion, you can
    use the `Process.join` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can launch four different processes that will run in parallel in the same
    way. In a serial program, the total required time will be 4 seconds. Since the
    execution is concurrent, the resulting wall clock time will be of 1 second. In
    the following code snippet, we create four processes that will execute concurrently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that the order of the execution for parallel processes is unpredictable
    and ultimately depends on how the **operating system** (**OS**) schedules this.
    You can verify this behavior by executing the program multiple times; the order
    will likely be different between runs.
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing` module exposes a convenient interface that makes it easy
    to assign and distribute tasks to a set of processes that reside in the `multiprocessing.Pool`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing.Pool` class spawns a set of processes—called *workers*—and
    lets us submit tasks through the `apply`/`apply_async` and `map`/`map_async` methods.
  prefs: []
  type: TYPE_NORMAL
- en: The `pool.map` method applies a function to each element of a list and returns
    a list of results. Its usage is equivalent to the built-in (serial) `map`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a parallel map, you should first initialize a `multiprocessing.Pool`
    object that takes the number of workers as its first argument; if not provided,
    that number will be equal to the number of cores in the system. You can initialize
    a `multiprocessing.Pool` object in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see `pool.map` in action. If you have a function that computes the square
    of a number, you can map the function to the list by calling `pool.map` and passing
    the function and the list of inputs as arguments, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pool.map_async` function is just like `pool.map` but returns an `AsyncResult`
    object instead of the actual result. When we call `pool.map`, the execution of
    the main program is stopped until all the workers are finished processing the
    result. With `map_async`, the `AsyncResult` object is returned immediately without
    blocking the main program and the calculations are done in the background. We
    can then retrieve the result using the `AsyncResult.get` method at any time, as
    shown in the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`pool.apply_async` assigns a task consisting of a single function to one of
    the workers. It takes the function and its arguments and returns an `AsyncResult`
    object. We can obtain an effect similar to `map` using `apply_async`, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To use the results computed and returned by these processes, we can simply access
    the data stored in `results`.
  prefs: []
  type: TYPE_NORMAL
- en: The Executor interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From version 3.2 onward, it is possible to execute Python code in parallel using
    the `Executor` interface provided in the `concurrent.futures` module. We already
    saw the `Executor` interface in action in the previous chapter, when we used `ThreadPoolExecutor`
    to perform multiple tasks concurrently. In this subsection, we'll demonstrate
    the usage of the `ProcessPoolExecutor` class.
  prefs: []
  type: TYPE_NORMAL
- en: '`ProcessPoolExecutor` exposes a very lean interface, at least when compared
    to the more featureful `multiprocessing.Pool`. A `ProcessPoolExecutor` class can
    be instantiated, similar to `ThreadPoolExecutor`, by passing a number of worker
    threads using the `max_workers` argument (by default, `max_workers` will be the
    number of CPU cores available). The main methods available to the `ProcessPoolExecutor`
    class are `submit` and `map`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `submit` method will take a function and return a `Future` instance that
    will keep track of the execution of the submitted function. The `map` method works
    similarly to the `pool.map` function, except that it returns an iterator rather
    than a list. The code is illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To extract the result from one or more `Future` instances, you can use the
    `concurrent.futures.wait` and `concurrent.futures.as_completed` functions. The
    `wait` function accepts a list of `future` instances and will block the execution
    of the programs until all the futures have completed their execution. The result
    can then be extracted using the `Future.result` method. The `as_completed` function
    also accepts a function but will, instead, return an iterator over the results.
    The code is illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can generate futures using the `asyncio.run_in_executor`
    function and manipulate the results using all the tools and syntax provided by
    the `asyncio` libraries so that you can achieve concurrency and parallelism at
    the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo approximation of pi
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As an example, we will implement a canonical, embarrassingly parallel program—**the
    Monte Carlo approximation of pi**. Imagine that we have a square of size 2 units;
    its area will be 4 units. Now, we inscribe a circle of 1 unit radius in this square;
    the area of the circle will be ![](img/Formula_8.1_B17499.png). By substituting
    the value of *r* in the previous equation, we get that the numerical value for
    the area of the circle is ![](img/Formula_8.2_B17499.png) *= pi*. You can refer
    to the following screenshot for a graphical representation of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Illustration of our strategy of approximation of pi ](img/B17499_Figure_8.3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Illustration of our strategy of approximation of pi
  prefs: []
  type: TYPE_NORMAL
- en: 'If we shoot a lot of random points on this, some points will fall into the
    circle, which we''ll call *hits*, while the remaining points, *misses*, will be
    outside the circle. The area of the circle will be proportional to the number
    of hits, while the area of the square will be proportional to the total number
    of shots. To get the value of pi, it is sufficient to divide the area of the circle
    (equal to *pi*) by the area of the square (equal to 4), as illustrated in the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The strategy we will employ in our program will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate a lot of uniformly random (*x*, *y*) numbers in the range (-1, 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test whether those numbers lie inside the circle by checking whether *x**2 +
    y**2 <= 1*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first step when writing a parallel program is to write a serial version
    and verify that it works. In a real-world scenario, you also want to leave parallelization
    as the last step of your optimization process—first, because we need to identify
    the slow parts, and second, parallelization is time-consuming and *gives you at
    most a speedup equal to the number of processors*. The implementation of the serial
    program is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy of our approximation will improve as we increase the number of
    samples. Note that each loop iteration is independent of the other—this problem
    is embarrassingly parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'To parallelize this code, we can write a function, called `sample`, that corresponds
    to a single hit-miss check. If the sample hits the circle, the function will return
    `1`; otherwise, it will return `0`. By running `sample` multiple times and summing
    the results, we''ll get the total number of hits. We can run `sample` over multiple
    processors with `apply_async` and get the results in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can wrap the two versions in the `pi_serial` and `pi_apply_async` functions
    (you can find their implementation in the `pi.py` file) and benchmark the execution
    speed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the earlier benchmark, our first parallel version literally cripples
    our code, the reason being that the time spent doing the actual calculation is
    small compared to the overhead required to send and distribute the tasks to the
    workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve the issue, we have to make the overhead negligible compared to the
    calculation time. For example, we can ask each worker to handle more than one
    sample at a time, thus reducing the task communication overhead. We can write
    a `sample_multiple` function that processes more than one hit and modifies our
    parallel version by dividing our problem by 10; more intensive tasks are shown
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can wrap this in a function called `pi_apply_async_chunked` and run it as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The results are much better; we more than doubled the speed of our program.
    You can also notice that the `user` metric is larger than `real`; the total CPU
    time is larger than the total time because more than one CPU worked at the same
    time. If you increase the number of samples, you will note that the ratio of communication
    to calculation decreases, giving even better speedups.
  prefs: []
  type: TYPE_NORMAL
- en: Everything is nice and simple when dealing with embarrassingly parallel problems.
    However, you sometimes have to share data between processes.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization and locks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even if `multiprocessing` uses processes (with their own independent memory),
    it lets you define certain variables and arrays as shared memory. You can define
    a shared variable using `multiprocessing.Value`, passing its data type as a string
    (`i` for integer, `d` for double, `f` for float, and so on). You can update the
    content of the variable through the `value` attribute, as shown in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When using shared memory, you should be aware of concurrent access. Imagine
    that you have a shared integer variable, and each process increments its value
    multiple times. You will define a `Process` class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You can initialize the shared variable in the main program and pass it to `4`
    processes, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If you run this program (`shared.py` in the code directory), you will note that
    the final value of `counter` is not `4000`, but it has random values (on my machine,
    they are between `2000` and `2500`). If we assume that the arithmetic is correct,
    we can conclude that there's a problem with the parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: What happens is that multiple processes are trying to access the same shared
    variable at the same time. The situation is best explained by looking at the following
    diagram. In a serial execution, the first process reads the number (`0`), increments
    it, and writes the new value (`1`); the second process reads the new value (`1`),
    increments it, and writes it again (`2`).
  prefs: []
  type: TYPE_NORMAL
- en: 'In parallel execution, the two processes read the number (`0`), increment it,
    and write the value (`1`) at the same time, leading to a wrong answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Multiple processes accessing the same variable, leading to incorrect
    behavior ](img/B17499_Figure_8.4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Multiple processes accessing the same variable, leading to incorrect
    behavior
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, we need to synchronize the access to this variable so
    that only one process at a time can access, increment, and write the value on
    the shared variable. This feature is provided by the `multiprocessing.Lock` class.
    A lock can be acquired and released through the `acquire` and `release` methods
    respectively or by using the lock as a context manager. Since the lock can be
    acquired by only one process at a time, this method prevents multiple processes
    from executing the protected section of code at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a global lock and use it as a context manager to restrict access
    to the counter, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Synchronization primitives, such as locks, are essential to solving many problems,
    but they should be kept to a minimum to improve the performance of your program.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing` module includes other communication and synchronization
    tools; you can refer to the official documentation at [http://docs.python.org/3/library/multiprocessing.html](http://docs.python.org/3/library/multiprocessing.html)
    for a complete reference.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B17499_04_Final_SS_ePub.xhtml#_idTextAnchor068), *C Performance
    with Cython*, we discussed Cython as a method of speeding up our programs. Cython
    itself also allows parallel processing via OpenMP, which we will examine next.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Cython with OpenMP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cython provides a convenient interface to perform shared-memory parallel processing
    through *OpenMP*. This lets you write extremely efficient parallel code directly
    in Cython without having to create a C wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: OpenMP is a specification and an API designed to write multithreaded, parallel
    programs. The OpenMP specification includes a series of C preprocessor directives
    to manage threads and provides communication patterns, load balancing, and other
    synchronization features. Several C/C++ and Fortran compilers (including the **GNU
    Compiler Collection** (**GCC**)) implement the OpenMP API.
  prefs: []
  type: TYPE_NORMAL
- en: We can introduce the Cython parallel features with a small example. Cython provides
    a simple API based on OpenMP in the `cython.parallel` module. The simplest way
    to achieve parallelism is through `prange`, which is a construct that automatically
    distributes loop operations in multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we can write the serial version of a program that computes the
    square of each element of a NumPy array in the `hello_parallel.pyx` file. We define
    a function, `square_serial`, that takes a buffer as input and populates an output
    array with the squares of the input array elements; `square_serial` is shown in
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Implementing a parallel version of the loop over the array elements involves
    substituting the `range` call with `prange`. There's a caveat—to use `prange`,
    the body of the loop must be interpreter-free. As already explained, we need to
    release the GIL and, since interpreter calls generally acquire the GIL, they need
    to be avoided to make use of threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Cython, you can release the GIL using the `nogil` context, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can use the `nogil=True` option of `prange` that will automatically
    wrap the loop body in a `nogil` block, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Attempts to call Python code in a `prange` block will produce an error. Prohibited
    operations include function calls, object initialization, and so on. To enable
    such operations in a `prange` block (you may want to do so for debugging purposes),
    you have to re-enable the GIL using a `with gil` statement, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now test our code by compiling it as a Python extension module. To enable
    OpenMP support, it is necessary to change the `setup.py` file so that it includes
    the `-fopenmp` compilation option. This can be achieved by using the `distutils.extension.Extension`
    class in `distutils` and passing it to `cythonize`. The complete `setup.py` file
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `prange`, we can easily parallelize the Cython version of our `ParticleSimulator`
    class. The following code snippet contains the `c_evolve` function of the `cevolve.pyx`
    Cython module that was written in [*Chapter 4*](B17499_04_Final_SS_ePub.xhtml#_idTextAnchor068),
    *C Performance with Cython*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we will invert the order of the loops so that the outermost loop will
    be executed in parallel (each iteration is independent of the other). Since the
    particles don''t interact with each other, we can change the order of iteration
    safely, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will replace the `range` call of the outer loop with `prange` and
    remove calls that acquire the GIL. Since our code was already enhanced with static
    types, the `nogil` option can be applied safely, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now compare the functions by wrapping them in the `benchmark` function
    to assess any performance improvement, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, we achieved a two-times speedup by writing a parallel version
    using `prange`.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, normal Python programs have trouble achieving thread
    parallelism because of the GIL. So far, we worked around this problem using separate
    processes; starting a process, however, takes significantly more time and memory
    than starting a thread.
  prefs: []
  type: TYPE_NORMAL
- en: We also saw that sidestepping the Python environment allowed us to achieve a
    two-times speedup on already fast Cython code. This strategy allowed us to achieve
    lightweight parallelism but required a separate compilation step. In the next
    section, we will further explore this strategy using special libraries that are
    capable of automatically translating our code into a parallel version for efficient
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Examples of packages that implement automatic parallelism are the (by now) familiar
    `numexpr` and Numba. Other packages have been developed to automatically optimize
    and parallelize array and matrix-intensive expressions, which are crucial in specific
    numerical and **machine learning** (**ML**) applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Theano** is a project that allows you to define a mathematical expression
    on arrays (more generally, *tensors*), and compile them to a fast language, such
    as C or C++. Many of the operations that Theano implements are parallelizable
    and can run on both the CPU and GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow** is another library that, similar to Theano, is targeted toward
    array-intensive mathematical expressions but, rather than translating the expressions
    to specialized C code, executes the operations on an efficient C++ engine.'
  prefs: []
  type: TYPE_NORMAL
- en: Both Theano and TensorFlow are ideal when the problem at hand can be expressed
    in a chain of matrix and element-wise operations (such as *neural networks*).
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Theano
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Theano is somewhat similar to a compiler but with the added bonus of being able
    to express, manipulate, and optimize mathematical expressions as well as run code
    on the CPU and GPU. Since 2010, Theano has improved release after release and
    has been adopted by several other Python projects as a way to automatically generate
    efficient computational models on the fly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The package may be installed using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In Theano, you first define the function you want to run by specifying variables
    and transformation using a pure Python API. This specification will then be compiled
    into machine code for execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first example, let''s examine how to implement a function that computes
    the square of a number. The input will be represented by a scalar variable, `a`,
    and then we will transform it to obtain its square, indicated by `a_sq`. In the
    following code snippet, we will use the `T.scalar` function to define a variable
    and use the normal `**` operator to obtain a new variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, no specific value is computed, and the transformation we apply
    is purely symbolic. In order to use this transformation, we need to generate a
    function. To compile a function, you can use the `th.function` utility that takes
    a list of the input variables as its first argument and the output transformation
    (in our case, `a_sq`) as its second argument, as illustrated in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Theano will take some time and translate the expression to efficient C code
    and compile it, all in the background! The return value of `th.function` will
    be a ready-to-use Python function, and its usage is demonstrated in the next line
    of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Unsurprisingly, `compute_square` correctly returns the input value squared.
    Note, however, that the return type is not an integer (like the input type) but
    a floating-point number. This is because the Theano default variable type is `float64`.
    You can verify that by inspecting the `dtype` attribute of the `a` variable, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The Theano behavior is very different compared to what we saw with Numba. Theano
    doesn't compile generic Python code and, also, doesn't do any type of inference;
    defining Theano functions requires a more precise specification of the types involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'The real power of Theano comes from its support for array expressions. Defining
    a `T.vector` function; the returned variable supports broadcasting operations
    with the same semantics of NumPy arrays. For instance, we can take two vectors
    and compute the element-wise sum of their squares, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The idea is, again, to use the Theano API as a mini-language to combine various
    NumPy array expressions that will be compiled as efficient machine code.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: One of the selling points of Theano is its ability to perform arithmetic simplifications
    and automatic gradient calculations. For more information, refer to the official
    documentation ([https://theano-pymc.readthedocs.io/en/latest/](https://theano-pymc.readthedocs.io/en/latest/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate Theano functionality on a familiar use case, we can implement
    our parallel calculation of pi again. Our function will take a collection of two
    random coordinates as input and return the `pi` estimate. The input random numbers
    will be defined as vectors named `x` and `y`, and we can test their position inside
    the circle using a standard element-wise operation that we will store in the `hit_test`
    variable, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we need to count the number of `True` elements in `hit_test`,
    which can be done by taking its sum (it will be implicitly cast to integer). To
    obtain the `pi` estimate, we finally need to calculate the ratio of hits versus
    the total number of trials. The calculation is illustrated in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can benchmark the execution of the Theano implementation using `th.function`
    and the `timeit` module. In our test, we will pass two arrays of size `30000`
    and use the `timeit.timeit` utility to execute the `calculate_pi` function multiple
    times, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The serial execution of this function takes about 10 seconds. Theano is capable
    of automatically parallelizing the code by implementing element-wise and matrix
    operations using specialized packages, such as OpenMP and the **Basic Linear Algebra
    Subprograms** (**BLAS**) linear algebra routines. Parallel execution can be enabled
    using configuration options.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Theano, you can set up configuration options by modifying variables in the
    `theano.config` object at import time. For example, you can issue the following
    commands to enable OpenMP support:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters relevant to OpenMP are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`openmp_elemwise_minsize`: This is an integer number that represents the minimum
    size of the arrays where element-wise parallelization should be enabled (the overhead
    of the parallelization can harm performance for small arrays).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openmp`: This is a Boolean flag that controls the activation of OpenMP compilation
    (it should be activated by default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling the number of threads assigned for OpenMP execution can be done
    by setting the `OMP_NUM_THREADS` environmental variable before executing the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now write a simple benchmark to demonstrate OpenMP usage in practice.
    In a `test_theano.py` file, we will put the complete code for the `pi` estimation
    example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can run the code from the command line and assess the scaling
    with an increasing number of threads by setting the `OMP_NUM_THREADS` environment
    variable, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, there is a small speedup when using two threads, but the performance
    degrades quickly as we increase their number. This means that for this input size,
    it is not advantageous to use more than two threads as the price you pay to start
    new threads and synchronize their shared data is higher than the speedup that
    you can obtain from the parallel execution.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving good parallel performance can be tricky as this will depend on the
    specific operations and how they access the underlying data. As a general rule,
    measuring the performance of a parallel program is crucial, and obtaining substantial
    speedups is a work of trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, we can see that the parallel performance quickly degrades using
    slightly different code. In our hit test, we used the `sum` method directly and
    relied on the explicit casting of the `hit_tests` Boolean array. If we make the
    cast explicit, Theano will generate slightly different code that benefits less
    from multiple threads. We can modify the `test_theano.py` file to verify this
    effect, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'If we rerun our benchmark, we see that the number of threads does not affect
    the running time significantly, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Despite that, the timings improved considerably compared to the original version.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling Theano
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given the importance of measuring and analyzing performance, Theano provides
    powerful and informative profiling tools. To generate profiling data, the only
    modification needed is the addition of the `profile=True` option to `th.function`,
    as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The profiler will collect data as the function is being run (for example, through
    `timeit` or direct invocation). The profiling summary can be printed to the output
    by issuing the `summary` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: To generate profiling data, we can rerun our script after adding the `profile=True`
    option (for this experiment, we will set the `OMP_NUM_THREADS` environmental variable
    to `1`). Also, we will revert our script to the version that performed the casting
    of `hit_tests` implicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can also set up profiling globally using the `config.profile` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output printed by `calculate_pi.profile.summary()` is quite long and informative.
    A part of it is reported in the next block of code. The output is comprised of
    three sections that refer to timings sorted by `Class`, `Ops`, and `Apply`. In
    our example, we are concerned with `Ops`, which roughly maps to the functions
    used in the Theano compiled code. As you can see here, roughly 80% of the time
    is spent in taking the element-wise square and sum of the two numbers, while the
    rest of the time is spent calculating the sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This information is consistent with what was found in our first benchmark. The
    code went from about 11 seconds to roughly 8 seconds when two threads were used.
    From these numbers, we can analyze how the time was spent.
  prefs: []
  type: TYPE_NORMAL
- en: Out of these 11 seconds, 80% of the time (about 8.8 seconds) was spent doing
    element-wise operations. This means that, in perfectly parallel conditions, the
    increase in speed by adding two threads will be 4.4 seconds. In this scenario,
    the theoretical execution time would be 6.6 seconds. Considering that we obtained
    a timing of about 8 seconds, it looks like there is some extra overhead (1.4 seconds)
    for the thread usage.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow is another library designed for fast numerical calculations and automatic
    parallelism. It was released as an open source project by Google in 2015\. TensorFlow
    works by building mathematical expressions similar to Theano, except that the
    computation is not compiled as machine code but is executed on an external engine
    written in C++. TensorFlow supports the execution and deployment of parallel code
    on one or more CPUs and GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can install TensorFlow using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow Version Compatibility
  prefs: []
  type: TYPE_NORMAL
- en: Note that as the default option, TensorFlow 2.x will be installed without further
    specifications. However, since the number of users of TensorFlow 1.x is still
    considerable, the code we use next will follow the syntax of TensorFlow 1.x. You
    can either install version 1 by specifying `pip install tensorflow==1.15` or disable
    version 2's behavior using `import tensorflow.compat.v1 as tf; tf.disable_v2_behavior()`
    when importing the library, as shown next.
  prefs: []
  type: TYPE_NORMAL
- en: 'The usage of TensorFlow is quite similar to that of Theano. To create a variable
    in TensorFlow, you can use the `tf.placeholder` function that takes a data type
    as input, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow mathematical expressions can be expressed quite similarly to Theano,
    except for a few different naming conventions as well as more restricted support
    for the NumPy semantics.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow doesn't compile functions to C and then machine code like Theano
    does, but serializes the defined mathematical functions (the data structure containing
    variables and transformations is called a `tf.Session` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the desired expression is defined, a `tf.Session` object needs to be initialized
    and can be used to execute computation graphs using the `Session.run` method.
    In the following example, we demonstrate the usage of the TensorFlow API to implement
    a simple element-wise sum of squares:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Parallelism in TensorFlow is achieved automatically by its smart execution engine,
    and it generally works well without much fiddling. However, note that it is mostly
    suited for **deep learning** (**DL**) workloads that involve the definition of
    complex functions that use a lot of matrix multiplications and calculate their
    gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now replicate the estimation of pi example using TensorFlow capabilities
    and benchmark its execution speed and parallelism against the Theano implementation.
    What we will do is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Define our `x` and `y` variables and perform a hit test using broadcasted operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the sum of `hit_tests` using the `tf.reduce_sum` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize a `Session` object with the `inter_op_parallelism_threads` and `intra_op_parallelism_threads`
    configuration options. These options control the number of threads used for different
    classes of parallel operations. Note that the first `Session` instance created
    with such options sets the number of threads for the whole script (even future
    `Session` instances).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can now write a script name, `test_tensorflow.py`, containing the following
    code. Note that the number of threads is passed as the first argument of the script
    (`sys.argv[1]`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the script multiple times with different values of `NUM_THREADS`,
    we see that the performance is quite similar to Theano and that the speedup increased
    by parallelization is quite modest, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The main advantage of using software packages such as TensorFlow and Theano
    is the support for parallel matrix operations that are commonly used in ML algorithms.
    This is very effective because those operations can achieve impressive performance
    gains on GPU hardware that is designed to perform these operations with high throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Running code on a GPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we will demonstrate the usage of a GPU with Theano and TensorFlow.
    As an example, we will benchmark the execution of very simple matrix multiplication
    on the GPU and compare it to its running time on a CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The code in this subsection requires the possession of a GPU. For learning purposes,
    it is possible to use the Amazon **Elastic Compute Cloud** (**EC2**) service ([https://aws.amazon.com/ec2](https://aws.amazon.com/ec2))
    to request a GPU-enabled instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code performs a simple matrix multiplication using Theano. We
    use the `T.matrix` function to initialize a `T.dot` method to perform the matrix
    multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to ask Theano to execute this code on a GPU by setting the `config.device=gpu`
    option. For added convenience, we can set up the configuration value from the
    command line using the `THEANO_FLAGS` environmental variable, shown as follows.
    After copying the previous code into the `test_theano_matmul.py` file, we can
    benchmark the execution time by issuing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We can analogously run the same code on the CPU using the `device=cpu` configuration
    option, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the *GPU is 7.2 times faster than the CPU* version for this
    example!
  prefs: []
  type: TYPE_NORMAL
- en: 'For comparison, we may benchmark equivalent code using TensorFlow. The implementation
    of a TensorFlow version is shown in the next code snippet. The main differences
    with the Theano version are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: The usage of the `tf.device` config manager that serves to specify the target
    device (`/cpu:0` or `/gpu:0`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The matrix multiplication is performed using the `tf.matmul` operator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the `test_tensorflow_matmul.py` script with the appropriate `tf.device`
    option, we obtain the following timings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the performance gain is substantial (but not as good as the
    Theano version) in this simple case.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to achieve automatic GPU computation is the now-familiar Numba.
    With Numba, it is possible to compile Python code to programs that can be run
    on a GPU. This flexibility allows for advanced GPU programming as well as more
    simplified interfaces. In particular, Numba makes extremely easy-to-write, GPU-ready,
    generalized universal functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next example, we will demonstrate how to write a universal function
    that applies an exponential function on two numbers and sums the results. As we
    already saw in [*Chapter 5*](B17499_05_Final_SS_ePub.xhtml#_idTextAnchor085),
    *Exploring Compilers*, this can be accomplished using the `nb.vectorize` function
    (we''ll also specify the `cpu` target explicitly). The code is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The `expon_cpu` universal function can be compiled for the GPU device using
    the `target=''cuda''` option. Also, note that it is necessary to specify the input
    types for CUDA universal functions. The implementation of `expon_gpu` is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now benchmark the execution of the two functions by applying the functions
    on two arrays of size `1000000`. Also, note in the following code snippet that
    we execute the function before measuring the timings to trigger the Numba JIT
    compilation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Thanks to the GPU execution, we were able to achieve a three-times speedup over
    the CPU version. Note that transferring data on the GPU is quite expensive; therefore,
    GPU execution becomes advantageous only for very large arrays.
  prefs: []
  type: TYPE_NORMAL
- en: When To Use Which Package
  prefs: []
  type: TYPE_NORMAL
- en: To close out this chapter, we will include a brief discussion regarding the
    parallel processing tools that we have examined thus far. First, we have seen
    how to use `multiprocessing` to manage multiple processes natively in Python.
    If you are using Cython, you may appeal to OpenMP to implement parallelism while
    being able to avoid working with C wrappers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we study Theano and TensorFlow as two packages that automatically compile
    array-centric code and parallelize the execution. While these two packages offer
    similar advantages when it comes to automatic parallelism, at the time of this
    writing, TensorFlow has gained significant popularity, especially within the DL
    community, where the parallelism of matrix multiplications is the norm.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the active development of Theano stopped in 2018\. While
    the package may still be utilized for automatic parallelism and DL uses, no new
    versions will be released. For this reason, TensorFlow is often preferred by Python
    programmers nowadays.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel processing is an effective way to improve performance on large datasets.
    Embarrassingly parallel problems are excellent candidates for parallel execution
    that can be easily implemented to achieve good performance scaling.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we illustrated the basics of parallel programming in Python.
    We learned how to circumvent Python threading limitations by spawning processes
    using the tools available in the Python standard library. We also explored how
    to implement a multithreaded program using Cython and OpenMP.
  prefs: []
  type: TYPE_NORMAL
- en: For more complex problems, we learned how to use the Theano, TensorFlow, and
    Numba packages to automatically compile array-intensive expressions for parallel
    execution on CPU and GPU devices.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to apply parallel programming techniques
    to build a hands-on application that makes and handles web requests concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why doesn't running Python code across multiple threads offer any speedup? What
    is the alternative approach that we have discussed in this chapter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `multiprocessing` module, what is the difference between the `Process`
    and the `Pool` interface in terms of implementing multiprocessing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On a high level, how do libraries such as Theano and TensorFlow help in parallelizing
    Python code?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
