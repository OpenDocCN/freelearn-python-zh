<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer322">&#13;&#13;
    <h1 class="chapterNumber"><span class="python">7</span></h1>&#13;&#13;
    <h1 id="_idParaDest-139" class="chapterTitle"><span class="python">Event-Driven Structures</span></h1>&#13;&#13;
    <p class="normal"><span class="python">Request-response is not the only software architecture that can be used in a system. There can also be requests that don't require an immediate response. Perhaps there's no interest in a response, as the task can be done without the caller being required to wait, or perhaps it takes a long time and the caller doesn't want to be waiting for it. In any case, there's the option to, from the point of view of the caller, just send a message and proceed.</span></p>&#13;&#13;
    <p class="normal"><span class="python">This message is called an </span><em class="italic">event</em>, and there are multiple uses for this kind of system. In this chapter, we will introduce the concept, and we will describe in detail one of the most popular uses of it: creating asynchronous tasks that are executed in the background while the caller of the task continues uninterrupted.</p>&#13;&#13;
    <p class="normal">In the chapter, we will describe the basics of asynchronous tasks, including the details of queueing systems and how to generate automatically scheduled tasks.</p>&#13;&#13;
    <p class="normal">We will use Celery as an example of a popular task manager in Python that has multiple capabilities. We will show specific examples of how to perform common tasks. We will also explore Celery Flower, a tool that creates a web interface to monitor and control Celery and has an HTTP API that allows you to control that interface, including sending new tasks to execute.</p>&#13;&#13;
    <p class="normal">In this chapter, we'll cover the following topics:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Sending events</li>&#13;&#13;
      <li class="bullet">Asynchronous tasks</li>&#13;&#13;
      <li class="bullet">Subdividing tasks</li>&#13;&#13;
      <li class="bullet">Scheduled tasks</li>&#13;&#13;
      <li class="bullet">Queue effects</li>&#13;&#13;
      <li class="bullet">Celery</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Let's start by describing the basics of event-driven systems.</p>&#13;&#13;
    <h1 id="_idParaDest-140" class="title">Sending events</h1>&#13;&#13;
    <p class="normal">Event-driven structures are based on the fire-and-forget principle. Instead of sending data and waiting until the other part returns a response, it just sends data and continues executing.</p>&#13;&#13;
    <p class="normal">This makes it different <a id="_idIndexMarker468"/>from the request-response architecture that we saw in the previous chapter. A request-response process will wait until an appropriate response is generated. Meanwhile, the execution of more code will stop, as the new data produced by the external system is required to continue.</p>&#13;&#13;
    <p class="normal">In an event-driven system, there's no response data, at least not in the same sense. Instead, an event containing the request will be sent, and the task will just continue. Some minimal information could be returned to ensure that the event can be tracked later.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">Event-driven systems <a id="_idIndexMarker469"/>can be implemented with request-response servers. This doesn't make them a pure request-response system. For example, a RESTful API that creates an event and returns an event ID. Any work is not done yet, and the only detail returned is an identifier to be able to check the status of any follow-up tasks.</p>&#13;&#13;
      <p class="Information-Box--PACKT-">This is not the only option, as this event ID may be produced locally, or even not be produced at all.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">The difference is that the task itself won't be done in the same moment, so getting back from generating the event will be very fast. The event, once generated, will travel to a different system that will transmit it towards its destination.</p>&#13;&#13;
    <p class="normal">This system is called a <em class="italic">bus</em> and works <a id="_idIndexMarker470"/>to make messages flow through the system. An architecture can use a single bus that acts as a central place to send messages across systems, or it can use multiple ones.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">In general, it's advisable to use a single bus to communicate all the systems. There are multiple tools that allow us to implement multiple logical partitions, so the messages are routed to and from the right destinations.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">Each of the events will <a id="_idIndexMarker471"/>be inserted into a <em class="italic">queue</em>. A queue is a logical FIFO system that will transmit the events from the entry point to the defined next stage. At that point, another module will receive the event and process it.</p>&#13;&#13;
    <p class="normal">This new system is listening to the queue and extracts all the received events to process them. This worker can't communicate directly with the sender of the event through the same channel, but it can interact with other elements, like shared databases or exposed endpoints, and can even send more events into queues to further process the results.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">The systems at <a id="_idIndexMarker472"/>each end of the queue are called the <em class="italic">publisher</em> and the <em class="italic">subscriber</em>.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">Multiple subscribers <a id="_idIndexMarker473"/>can tend the same queue, and they'll be extracting events in parallel. Multiple publishers can also produce events into the same queue. The capacity <a id="_idIndexMarker474"/>of the queue will be described by the number of events that can be processed, and enough subscribers should be provided so the queue can be processed quickly enough.</p>&#13;&#13;
    <p class="normal">Typical tools that <a id="_idIndexMarker475"/>can work as a bus are RabbitMQ, Redis, and Apache Kafka. While it <a id="_idIndexMarker476"/>is possible to <a id="_idIndexMarker477"/>use a tool "as is," there are multiple libraries that will help you work with these tools to create your own way of handling sending messages.</p>&#13;&#13;
    <h1 id="_idParaDest-141" class="title">Asynchronous tasks</h1>&#13;&#13;
    <p class="normal">A simple event-driven system is one that allows you to execute asynchronous tasks. </p>&#13;&#13;
    <p class="normal">The events produced <a id="_idIndexMarker478"/>by an event-driven system describe a particular task to execute. Normally, each task will require some time to execute, which makes it impractical to be executed directly as part of the publisher code flow.</p>&#13;&#13;
    <p class="normal">The typical example is a web server that needs to respond to the user in a reasonable time. Some HTTP timeouts can produce errors if an HTTP request takes too long, and generally it is not a great experience to respond in more than a second or two. </p>&#13;&#13;
    <div class="packt_tip">&#13;&#13;
      <p class="Tip--PACKT-">These operations that take a long time may involve tasks like encoding video into a different resolution, analyzing images with a complex algorithm, sending 1,000 emails to customers, deleting a million registers in bulk, copying data from an external database into a local one, generating reports, or pulling data from multiple sources. </p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">The solution is to send an event to handle this task, generate a task ID, and return the task ID immediately. The event will be sent to a message queue that will deliver it to a back-end system. The back-end system will then execute the task, which can take as long as it needs to execute.</p>&#13;&#13;
    <p class="normal">Meanwhile, the task ID can be used to monitor the progress of the execution. The back-end task will update the status of the execution in shared storage, like a database, so when it's <a id="_idIndexMarker479"/>completed, the web front-end can inform the user. This shared storage can also store any produced results that may be interesting.</p>&#13;&#13;
    <figure class="mediaobject"><img src="Images/B17580_07_01.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="200"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 7.1: The flow of an event</p>&#13;&#13;
    <p class="normal">Because the status of the task is stored in a database that's accessible by the front-end web server, the user can ask for the status of the task at any point by identifying it through the task ID.</p>&#13;&#13;
    <figure class="mediaobject"><img src="Images/B17580_07_02.png" alt="Diagram, schematic&#13;&#10;&#13;&#10;Description automatically generated" width="829" height="1001"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 7.2: Checking the progress of an async task with shared storage</p>&#13;&#13;
    <p class="normal">The back-end system can produce intermediate updates if necessary, showing when 25% or 50% of the task has been completed. This will need to be stored in the same shared storage.</p>&#13;&#13;
    <p class="normal">This process is a simplification, though. The queue is usually capable of returning whether a task has been finished or not. The shared storage/database will be required only if the task is required to return some data. A database works fine for small results, but if big elements like documents are produced as part of the task, this may not be a valid option and a different kind of storage may be required.</p>&#13;&#13;
    <div class="packt_tip">&#13;&#13;
      <p class="Tip--PACKT-">For example, if a task is to generate a report, the back-end will store it in document storage like AWS S3 so it's available to be downloaded by the user later.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">A shared database is not <a id="_idIndexMarker480"/>the only way to be sure that the web server front-end is capable of receiving information. The web server can expose an internal API that allows the back-end to <a id="_idIndexMarker481"/>send back information. This is, to all effects, the same as sending the data to a different external service. The back-end will need to access the API, configure it, and perhaps be authenticated. The API can be created exclusively for the back-end or can be an API for general usage that also accepts the specific data that the back-end system will produce.</p>&#13;&#13;
    <div class="packt_tip">&#13;&#13;
      <p class="Tip--PACKT-">Sharing access to a database between two different systems can be difficult, as the database will need to be in sync for both systems. We need to detach the systems so they can be deployed independently and without breaking backward compatibility. Any change in the schema will require extra care to ensure that the system can perform at any point, without interruption. Exposing an API and keeping the database under the full control of the front-end service is a good solution, but keep in mind that requests originating from the back-end will compete with external requests, so we need enough capacity for both.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">In this case, all the information, task IDs, statuses, and results can remain inside the web server's internal storage.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">Remember that the queue is likely to store the task ID and the status of the task. This may be replicated for convenience in the internal storage.</p>&#13;&#13;
    </div>&#13;&#13;
    <figure class="mediaobject"><img src="Images/B17580_07_03.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="829" height="1001"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 7.3: Sending back information to the source service</p>&#13;&#13;
    <p class="normal">Remember that this API doesn't have to be directed to the same front-end. It can also call any other service, internal or external, generating a complex flow between elements. It even <a id="_idIndexMarker482"/>creates its own events that will be reintroduced into the queue to produce other tasks.</p>&#13;&#13;
    <h1 id="_idParaDest-142" class="title">Subdividing tasks</h1>&#13;&#13;
    <p class="normal">It's entirely possible to generate more tasks from an initial one. This is done by creating the right <a id="_idIndexMarker483"/>event inside a task and sending it to the right queue. </p>&#13;&#13;
    <p class="normal">This allows a single task to distribute its load and parallelize its action. For example, if a task generates a report and sends it by email to a group of recipients, the task can first generate the report and then send the emails in parallel by creating new tasks that will focus only on creating the emails and attaching the report.</p>&#13;&#13;
    <p class="normal">This spreads the load over multiple workers, speeding up the process. Another advantage is that individual tasks will be shorter, which makes them easier to control, monitor, and operate.</p>&#13;&#13;
    <div class="packt_tip">&#13;&#13;
      <p class="Tip--PACKT-">Some task managers may permit the creation of workflows where tasks are distributed, and their results are returned and combined. This can be used in some cases, but in practice it is less useful than it initially appears, as it introduces extra waiting and we can end up with the task taking a longer time.</p>&#13;&#13;
      <p class="Tip--PACKT-">But easy wins are bulk tasks performing similar actions on multiple elements without the need to combine the results, which are quite commonly encountered.</p>&#13;&#13;
    </div>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">Keep in mind, though, that this will make the initial task finish quickly, making the initial task's ID status a bad way to check whether the whole operation has been completed. The initial task may return the IDs of the new tasks if they need to be monitored.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">The process can be repeated, if necessary, with subtasks creating their own subtasks. Some tasks may require creating huge amounts of information in the background, so subdividing them may make sense, but it will also increase the complexity of following the flow of the code, so use this technique sparingly and only when it creates a clear advantage.</p>&#13;&#13;
    <h1 id="_idParaDest-143" class="title">Scheduled tasks</h1>&#13;&#13;
    <p class="normal">Asynchronous tasks don't need to be generated directly by a frontend and direct action by a user, but can also be set to run at specific times, through a schedule.</p>&#13;&#13;
    <p class="normal">Some examples <a id="_idIndexMarker484"/>of scheduled tasks include generating daily reports during night hours, updating information hourly via an external API, precaching values so they are quickly available later, generating a schedule for next week at the start of the week, and sending reminder emails every hour.</p>&#13;&#13;
    <p class="normal">Most task queues will allow the generation of scheduled tasks, indicating it clearly in their definition, so they will be triggered automatically.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">We will see later in the chapter how to generate a scheduled task for Celery.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">Some scheduled tasks can be quite big, such as each night sending emails to thousands of recipients. It's very useful to divide a scheduled task, so a small scheduled task is triggered just to add all the individual tasks to the queue that will be processed later. This distributes the load and allows the task to finish earlier, making full use of the system. </p>&#13;&#13;
    <p class="normal">In the example of sending emails, a single task triggers every night, reading the configuration and creating a new task for each email found. Then the new tasks will receive the email, compose the body by pulling from external information, and send it.</p>&#13;&#13;
    <h1 id="_idParaDest-144" class="title">Queue effects</h1>&#13;&#13;
    <p class="normal">An important element of asynchronous tasks is the effect that introducing a queue may have. As we've seen, the <a id="_idIndexMarker485"/>background tasks are slow, meaning that any worker running them will be busy for some time. </p>&#13;&#13;
    <p class="normal">Meanwhile, more tasks can be introduced, which may mean that the queue starts building up. </p>&#13;&#13;
    <figure class="mediaobject"><img src="Images/B17580_07_04.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated with low confidence" width="826" height="246"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 7.4: Single queue</p>&#13;&#13;
    <p class="normal">On the one hand, this can be a capacity problem. If the number of workers is not sufficient to handle the average number of tasks introduced in the queue, the queue will build up until it reaches its limit, and new tasks will be rejected.</p>&#13;&#13;
    <p class="normal">But typically, the load doesn't work like a constant influx of tasks. Instead, there are times when there are no tasks to execute, and other times when there's a sudden spike in the number <a id="_idIndexMarker486"/>of tasks to be executed, filling the queue. Also, there's a need to calculate the right number of workers to keep running to be sure that the waiting period for those spikes, where a task gets delayed because all the workers are busy, is not causing problems.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">Calculating the "right" amount of workers can be difficult, but with a bit of trial and error a "good enough" number can be obtained. There's a mathematical tool to deal with it, queueing theory, which calculates it based on several parameters.</p>&#13;&#13;
      <p class="Information-Box--PACKT-">In any case, these days resources for each worker are cheap and it's not imperative to generate the exact number of workers, as long as it's close enough so that any possible spike can be processed in a reasonable amount of time. </p>&#13;&#13;
      <p class="Information-Box--PACKT-">You can learn <a id="_idIndexMarker487"/>more about queueing theory at <a href="http://people.brunel.ac.uk/~mastjjb/jeb/or/queue.html"><span class="url">http://people.brunel.ac.uk/~mastjjb/jeb/or/queue.html</span></a>.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">An extra difficulty, as we saw with scheduled tasks, is that at a specific time, a considerable number of tasks can be triggered at the same time. This can saturate the queue at a particular time, requiring perhaps an hour to digest all the tasks, for example, creating daily reports, ingesting new updates in an external API every 4 hours, or aggregating data for the week. </p>&#13;&#13;
    <p class="normal">This means that, for example, if 100 tasks to create background reports are added, they will block a task to generate a report sent by a user, which will produce a bad experience. The user will have to wait for far too long if they ask for the report a few minutes after the scheduled tasks were fired.</p>&#13;&#13;
    <p class="normal">A possible <a id="_idIndexMarker488"/>solution is to use multiple queues, with different workers pulling from them. </p>&#13;&#13;
    <figure class="mediaobject"><img src="Images/B17580_07_05.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="829" height="410"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 7.5: Priority and background queue</p>&#13;&#13;
    <p class="normal">This makes those different tasks go to different workers, making it possible to reserve capacity for certain tasks to run uninterrupted. In our example, the background reports can go to their own dedicated workers, and the user reports have their own workers as well. This, though, wastes capacity. If the background reports run only once a day, once the 100 tasks are processed, the workers will be idle for the rest of the day, even if there's a long queue in the worker serving the user reports.</p>&#13;&#13;
    <p class="normal">Instead of that, a mixed approach can be used.</p>&#13;&#13;
    <figure class="mediaobject"><img src="Images/B17580_07_06.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="426"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 7.6: Regular worker pulling from multiple queues</p>&#13;&#13;
    <p class="normal">In this case, the user report worker will continue with the same approach, but the background <a id="_idIndexMarker489"/>report worker will pull tasks from both queues. In this case, we limit the capacity for background reports, but at the same time, we increase it for the user report tasks when there's available capacity.</p>&#13;&#13;
    <p class="normal">We reserve capacity for the user report tasks, which are priority, and make the rest of the workers pull from all available tasks, including priority and non-priority tasks.</p>&#13;&#13;
    <p class="normal">To be able to divide work into these two queues, the tasks need to be divided carefully:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet"><em class="italic">Priority tasks</em>. They are <a id="_idIndexMarker490"/>started on behalf of the user. They are time sensitive. They are fast to execute, so latency is important.</li>&#13;&#13;
      <li class="bullet"><em class="italic">Background tasks</em>. Normally <a id="_idIndexMarker491"/>started by automated systems and scheduled tasks. They are less time sensitive. They can run for long periods, so higher latency is easier to accept.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">The balance between them should be maintained. If too many tasks are labeled as priority, the queue will be quickly filled, rendering it pointless.</p>&#13;&#13;
    <div class="packt_tip">&#13;&#13;
      <p class="Tip--PACKT-">There's always the temptation to generate multiple queues to set up different priorities and reserve capacity for each of them. This is normally not a good idea, as they will waste capacity. The most efficient system is one with a single queue, as all capacity will be always used. There is a problem of priority, though, as it makes some tasks take too long. More than two queues overcomplicates and risks wasting capacity where many workers are idle most of the time, while other queues are filled. The simplicity of two queues helps develop the discipline of deciding between only two options and makes it easy to understand why we want multiple queues.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">The number of priority workers can be tweaked based on the number and frequency of spikes and expected turnaround time. Only enough priority workers to cover regular traffic at the <a id="_idIndexMarker492"/>times where there are big spikes in background tasks are required, as long as those spikes are predictable.</p>&#13;&#13;
    <div class="packt_tip">&#13;&#13;
      <p class="Tip--PACKT-">Good metrics are critical for monitoring and understanding the behavior of the queue. We will talk more about metrics in <em class="chapterRef">Chapter 13</em>,<em class="chapterRef"> </em><em class="italic">Metrics</em>.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">An alternative is to generate a priority system based on specific priorities, like numbers. That way, a task with priority 3 will be executed before a task with priority 2, and that before a task with priority 1, and so on. The great advantage of having priorities is that the workers can be working all the time, without wasting any capacity. </p>&#13;&#13;
    <p class="normal">But this approach has some problems:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">A lot of queue backends don't support it efficiently. To keep a queue sorted by priority costs more than just assigning tasks to a plain queue. In practice, it may not produce as good results as you expect, requiring many tweaks and adjustments.</li>&#13;&#13;
      <li class="bullet">It means you need to deal with priority inflation. It's very easy for teams to start increasing the priority of tasks over time, especially if multiple teams are involved. The decision on what task should return first could get complicated and pressure can grow the priority numbers over time. </li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">While it can appear that a sorted queue is ideal, the simplicity of two levels (priority and background) makes it very easy to understand the system and generates easy expectations when developing and creating new tasks. It's way easier to tweak and understand and will generate better results with less work.</p>&#13;&#13;
    <h2 id="_idParaDest-145" class="title">Single code for all workers</h2>&#13;&#13;
    <p class="normal">When having different workers pulling from different queues, the worker could have different <a id="_idIndexMarker493"/>codebases, making one with priority tasks and another with background tasks.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">Note that for this to work, it will require strict separation of tasks. More about this a bit later.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">This is generally <a id="_idIndexMarker494"/>not advisable, as it will differentiate the codebase and require maintaining two code bases in parallel, with some problems:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">It's likely that some tasks or task parts will be either priority or background, depending on what system or user triggers them. For example, reports that can be either produced on the fly for a user, or daily as part of a batch process to finally send them by mail. The report generation should remain common, so any change is applied to both.</li>&#13;&#13;
      <li class="bullet">Handling two codebases instead of one is more inconvenient. A big part of the general code is shared, so updates will need to be run independently.</li>&#13;&#13;
      <li class="bullet">A unique codebase can handle all kinds of tasks. That makes it possible to have a worker that handles both priority and background tasks. Two codebases will require strict task separation, not using the extra capacity available in the background workers to help with priority tasks.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">It is better to use a single worker when building, and through the configuration decide to receive messages from one queue or both. This simplifies the architecture for local development and testing.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">This may not be adequate when the nature of the tasks may create conflicts. For example, if some of the tasks require big dependencies or specialized hardware (as could be the case with some AI-related tasks) this may require that specific tasks run in dedicated workers, making it impractical for them to share the same codebase. These cases are rare, and unless they are encountered, it's better to try to consolidate and use the same worker for all tasks.</p>&#13;&#13;
    </div>&#13;&#13;
    <h2 id="_idParaDest-146" class="title">Cloud queues and workers</h2>&#13;&#13;
    <p class="normal">The main <a id="_idIndexMarker495"/>characteristic of cloud computing is that services <a id="_idIndexMarker496"/>can be started and stopped dynamically, allowing us to use only the resources required at a particular moment. This allows the system to increase and decrease capacity quickly.</p>&#13;&#13;
    <p class="normal">In cloud environments, it's possible that the number of workers extracting events from a queue can be modified. That alleviates the problems with resourcing that we discussed above. Do we have a full queue? Increase the number of workers on demand! Ideally, we could even spawn a single worker for each event that spawns a task, making the system infinitely scalable.</p>&#13;&#13;
    <p class="normal">This, obviously, is easier <a id="_idIndexMarker497"/>said than done, as there are some issues <a id="_idIndexMarker498"/>with trying to dynamically create workers on the spot:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">The start-up time can add significant time to the execution of the task, even to the point of being longer than the execution time of the task itself. Depending on how heavy the creation of a worker is, starting it can take a significant amount of time.<p class="bullet-para">In the traditional cloud setting, the lowest granularity required to start a new virtual server, which is relatively heavy, takes at least a couple of minutes. With newer tools, such as containers, this can be sped up sensibly, but the underlying principle will remain, as at some point in time a new virtual server will need to be spawned.</p>&#13;&#13;
      </li>&#13;&#13;
      <li class="bullet">A single new virtual worker may be too big for a single worker, making it inefficient to spawn one for each task. Again, containerized solutions can help by making it easier to separate between creating a new container and requiring spinning up a new virtual server in the cloud service.</li>&#13;&#13;
      <li class="bullet">Any cloud service should have limits. Each new worker created costs money and cloud services can get very expensive if scaled up without control. Without certain control on the cost side of things, this can grow to be a problem due to high, unexpected costs. Normally this can happen by accident, with some explosion of workers due to some problem on the system, but there's also a security attack, called Cash Overflow, aimed at making a service run as expensively as possible to force the owner of the service to stop it or even bankrupt them.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Because of these problems, normally a solution will need to work in sort of a batched way, allowing extra space to grow and generating extra virtual servers only when they are required to reduce the queue. In the same way, when the extra capacity is not required any more, it will be removed.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">Extra care should be taken to be sure that all the workers located in the same virtual server are idle before stopping it. This is done automatically by stopping the servers gracefully, so they'll finish any remaining tasks, start no new ones, and finish when everything is done.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">The <a id="_idIndexMarker499"/>process <a id="_idIndexMarker500"/>should be similar to this:</p>&#13;&#13;
    <figure class="mediaobject"><img src="Images/B17580_07_07.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="832"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 7.7: Starting up a new server </p>&#13;&#13;
    <p class="normal">Knowing exactly when a new server should be spawned depends greatly on the requirements <a id="_idIndexMarker501"/>for latency, traffic, and the speed of creating a <a id="_idIndexMarker502"/>new server (if the server starts quickly, perhaps it can be less aggressive in scaling up).</p>&#13;&#13;
    <div class="packt_tip">&#13;&#13;
      <p class="Tip--PACKT-">A good starting point is to create a new server each time the queue has a number of tasks equal to or greater than the number of workers in a single server. That triggers a new server that will be able to handle those tasks. If the creation is triggered with fewer tasks than that, it will create a server that is not quite filled. If the start-up time is very long, this can be reduced to ensure that the new server is up before there's a significant queue building up. But this will require experimentation and testing for a specific system.</p>&#13;&#13;
    </div>&#13;&#13;
    <h1 id="_idParaDest-147" class="title">Celery</h1>&#13;&#13;
    <p class="normal">Celery is the <a id="_idIndexMarker503"/>most popular task queue created in Python. It allows us to create new tasks easily and can handle the creation of the events that trigger new tasks.</p>&#13;&#13;
    <p class="normal">Celery requires to work to set up a <em class="italic">broker</em>, which will be used as a queue to handle the messages.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">In Celery parlance, the broker is the message queue, while the <em class="italic">backend</em> is reserved for interacting with a storage system to return information.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">The code that creates the message will add it to the broker, and the broker will pass it to one of the connected workers. When everything happens with Python code, where the <code class="Code-In-Text--PACKT-">celery</code> package can be installed, it's simple to operate. We'll see later how to operate it in other cases.</p>&#13;&#13;
    <p class="normal">Celery can use multiple systems as brokers. The most popular are Redis and RabbitMQ. </p>&#13;&#13;
    <div class="packt_tip">&#13;&#13;
      <p class="Tip--PACKT-">In our examples, we will use Redis as it can be used for the broker and the backend, and it's widely available in cloud systems. It's also quite scalable and handles big loads easily.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">Using a backend is optional, as tasks don't need to define a return value, and it's very common that asynchronous tasks don't directly return response data other than the status of the task. The key word here is "directly"; sometimes, a task will generate an external result that can be accessible, but not through the Celery system.</p>&#13;&#13;
    <p class="normal">Some examples of these values are reports that can be stored in other storage facilities, emails sent during task processing, and pre-caching of values, where there is not a direct result, but there's new data generated and stored in other places.</p>&#13;&#13;
    <p class="normal">The returning value needs also to be small enough that it can be stored in the system working as <a id="_idIndexMarker504"/>the backend. Also, if strong persistence is used, it's recommended that a database is used as the backend.</p>&#13;&#13;
    <p class="normal">We will use the example present on GitHub: <a href="https://github.com/PacktPublishing/Python-Architecture-Patterns/tree/main/chapter_07_event_driven/celery_example"><span class="url">https://github.com/PacktPublishing/Python-Architecture-Patterns/tree/main/chapter_07_event_driven/celery_example</span></a>. We will use the example to create a task to retrieve, from an external API, pending <code class="Code-In-Text--PACKT-">TO DO</code> actions by some users, and generate an email to send as a reminder.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">Remember to install the required dependencies by running <code class="Code-In-Text--PACKT-">pip install -r requirements.txt</code>.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">Let's take a look at the code.</p>&#13;&#13;
    <h2 id="_idParaDest-148" class="title">Configuring Celery</h2>&#13;&#13;
    <p class="normal">The code is divided into two files: <code class="Code-In-Text--PACKT-">celery_tasks.py</code>, which describes the tasks, and <code class="Code-In-Text--PACKT-">start_task.py</code>, which connects with the queue and enqueues a task.</p>&#13;&#13;
    <p class="normal">At the <a id="_idIndexMarker505"/>start of each, we need to configure the broker to use. In this case, we will use a Redis server running in the <code class="Code-In-Text--PACKT-">localhost</code>:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> celery <span class="hljs-keyword">import</span> Celery&#13;&#13;
app = Celery('tasks', broker='redis://localhost')&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">As a prerequisite, we need to set up a Redis server running in our expected <code class="Code-In-Text--PACKT-">localhost</code> address. An easy way of doing so, if you have Docker installed, is to start a container:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="language-bash">docker run -d -p 6379:6379 redis</span>&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This starts the standard Redis container that will expose the service over the standard port, 6379. That will connect automatically with the previous broker URL of <code class="Code-In-Text--PACKT-">redis://localhost</code>.</p>&#13;&#13;
    <p class="normal">This is all the configuration that's required, and it will allow both sides, the publisher and the subscriber, to connect to the queue.</p>&#13;&#13;
    <h2 id="_idParaDest-149" class="title">Celery worker</h2>&#13;&#13;
    <p class="normal">We will use <a href="https://jsonplaceholder.typicode.com/"><span class="url">https://jsonplaceholder.typicode.com/</span></a> to simulate calling an external API. This testing site exposes an accessible REST endpoint to retrieve some mock information. You can <a id="_idIndexMarker506"/>see their definition, but basically, we will access the <code class="Code-In-Text--PACKT-">/todos</code> and <code class="Code-In-Text--PACKT-">/users</code> endpoints. The <code class="Code-In-Text--PACKT-">/todos</code> endpoint exposes actions stored by the users, so we will query them to retrieve pending actions, and combine this with the information in the <code class="Code-In-Text--PACKT-">/users</code> endpoint.</p>&#13;&#13;
    <p class="normal">The <code class="Code-In-Text--PACKT-">celery_tasks.py</code> worker defines a main task, <code class="Code-In-Text--PACKT-">obtain_info</code>, and a secondary task, <code class="Code-In-Text--PACKT-">send_email</code>. The first one pulls the information from the API and decides what emails need to be sent. The second then sends the email. </p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">The sending of the email is just mocked to avoid complicating the system and needing to handle mocked email addresses. It's left as an exercise for the reader.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">The file starts with the configuration of the queue and imports:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> celery <span class="hljs-keyword">import</span> Celery&#13;&#13;
<span class="hljs-keyword">import</span> requests&#13;&#13;
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict&#13;&#13;
app = Celery('tasks', broker='redis://localhost')&#13;&#13;
logger = app.log.get_default_logger()&#13;&#13;
BASE_URL = 'https://jsonplaceholder.typicode.com'&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The <code class="Code-In-Text--PACKT-">logger</code> definition permits the use of native Celery logs that will be streamed into the Celery configuration for logs. By default, this is the standard output.</p>&#13;&#13;
    <p class="normal">Let's take a look at the <code class="Code-In-Text--PACKT-">obtain_info</code> task. Note the <code class="Code-In-Text--PACKT-">@app.task</code> that defines the function as a Celery task:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@app.task</span>&#13;&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">obtain_info</span>():&#13;&#13;
    logger.info('Stating task')&#13;&#13;
    users = {}&#13;&#13;
    task_reminders = defaultdict(<span class="hljs-built_in">list</span>)&#13;&#13;
    <span class="hljs-comment"># Call the /todos endpoint to retrieve all the tasks</span>&#13;&#13;
    response = requests.get(f'{BASE_URL}/todos')&#13;&#13;
    <span class="hljs-keyword">for</span> task <span class="hljs-keyword">in</span> response.json():&#13;&#13;
        <span class="hljs-comment"># Skip completed tasks</span>&#13;&#13;
        <span class="hljs-keyword">if</span> task['completed'] <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span>:&#13;&#13;
            <span class="hljs-keyword">continue</span>&#13;&#13;
        <span class="hljs-comment"># Retrieve user info. The info is cached to only ask</span>&#13;&#13;
        <span class="hljs-comment"># once per user</span>&#13;&#13;
        user_id = task['userId']&#13;&#13;
        <span class="hljs-keyword">if</span> user_id <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> users:&#13;&#13;
            users[user_id] = obtain_user_info(user_id)&#13;&#13;
        info = users[user_id]&#13;&#13;
        <span class="hljs-comment"># Append the task information to task_reminders, that</span>&#13;&#13;
        <span class="hljs-comment"># aggregates them per user</span>&#13;&#13;
        task_data = (info, task)&#13;&#13;
        task_reminders[user_id].append(task_data)&#13;&#13;
    <span class="hljs-comment"># The data is ready to process, create an email per</span>&#13;&#13;
    <span class="hljs-comment"># each user</span>&#13;&#13;
    <span class="hljs-keyword">for</span> user_id, reminders <span class="hljs-keyword">in</span> task_reminders.items():&#13;&#13;
        compose_email(reminders)&#13;&#13;
    logger.info('End task')&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">We wrap <a id="_idIndexMarker507"/>the function with <code class="Code-In-Text--PACKT-">INFO</code> logs to provide context to the task execution. First, it calls the <code class="Code-In-Text--PACKT-">/todos</code> endpoint on this line, which then goes through each task independently, skipping any completed task.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">    response = requests.get(f'{BASE_URL}/todos')&#13;&#13;
    <span class="hljs-keyword">for</span> task <span class="hljs-keyword">in</span> response.json():&#13;&#13;
        <span class="hljs-keyword">if</span> task['completed'] <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span>:&#13;&#13;
            <span class="hljs-keyword">continue</span>&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Then, it checks the information for the user and puts it into the <code class="Code-In-Text--PACKT-">info</code> variable. Because this information can be used multiple times in the same loop, it is cached in the <code class="Code-In-Text--PACKT-">users</code> dictionary. Once the info is cached, it's not asked for again:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">        user_id = task['userId']&#13;&#13;
        <span class="hljs-keyword">if</span> user_id <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> users:&#13;&#13;
            users[user_id] = obtain_user_info(user_id)&#13;&#13;
        info = users[user_id]&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The individual task data is added to a list created to store all the tasks for a user. The <code class="Code-In-Text--PACKT-">task_reminders</code> dictionary is created as a <code class="Code-In-Text--PACKT-">defaultdict(list)</code>, meaning that the first time a particular <code class="Code-In-Text--PACKT-">user_id</code> is accessed, if it's not present, it will be initialized as an empty list, allowing a new element to be appended.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">        task_data = (info, task)&#13;&#13;
        task_reminders[user_id].append(task_data)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Finally, the stored elements in <code class="Code-In-Text--PACKT-">task_reminders</code> are iterated to compose the resulting email:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> user_id, reminders <span class="hljs-keyword">in</span> task_reminders.items():&#13;&#13;
        compose_email(reminders)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Two follow-up <a id="_idIndexMarker508"/>functions are called: <code class="Code-In-Text--PACKT-">obtain_user_info</code> and <code class="Code-In-Text--PACKT-">compose_email</code>.</p>&#13;&#13;
    <p class="normal"><code class="Code-In-Text--PACKT-">obtain_user_info</code> retrieves the information directly from the <code class="Code-In-Text--PACKT-">/users/{user_id} </code>endpoint and returns it:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">obtain_user_info</span>(<span class="hljs-params">user_id</span>):&#13;&#13;
    logger.info(f'Retrieving info <span class="hljs-keyword">for</span> user {user_id}')&#13;&#13;
    response = requests.get(f'{BASE_URL}/users/{user_id}')&#13;&#13;
    data = response.json()&#13;&#13;
    logger.info(f'Info <span class="hljs-keyword">for</span> user {user_id} retrieved')&#13;&#13;
    <span class="hljs-keyword">return</span> data&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal"><code class="Code-In-Text--PACKT-">compose_email</code> takes the information in the task list, which includes a group of <code class="Code-In-Text--PACKT-">user_info, task_info</code>, extracts the title information for each <code class="Code-In-Text--PACKT-">task_info</code>, then the email from the matched <code class="Code-In-Text--PACKT-">user_info</code>, and then calls the <code class="Code-In-Text--PACKT-">send_email</code> task:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">compose_email</span>(<span class="hljs-params">remainders</span>):&#13;&#13;
    <span class="hljs-comment"># remainders is a list of (user_info, task_info)</span>&#13;&#13;
    <span class="hljs-comment"># Retrieve all the titles from each task_info</span>&#13;&#13;
    titles = [task['title'] <span class="hljs-keyword">for</span> _, task <span class="hljs-keyword">in</span> remainders]&#13;&#13;
    <span class="hljs-comment"># Obtain the user_info from the first element</span>&#13;&#13;
    <span class="hljs-comment"># The user_info is repeated and the same on each element</span>&#13;&#13;
    user_info, _ = remainders[<span class="hljs-number">0</span>]&#13;&#13;
    email = user_info['email']&#13;&#13;
    <span class="hljs-comment"># Start the task send_email with the proper info</span>&#13;&#13;
    send_email.delay(email, titles)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">As you can see, the <code class="Code-In-Text--PACKT-">send_email</code> task includes a <code class="Code-In-Text--PACKT-">.delay</code> call, which enqueues this task with the <a id="_idIndexMarker509"/>appropriate parameters. <code class="Code-In-Text--PACKT-">send_email</code> is another Celery task. It is very simple as we are just mocking the email delivery. It just logs its parameters:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@app.task</span>&#13;&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">send_email</span>(<span class="hljs-params">email, remainders</span>):&#13;&#13;
    logger.info(f'Send an email to {email}')&#13;&#13;
    logger.info(f'Reminders {remainders}')&#13;&#13;
</code></pre>&#13;&#13;
    <h2 id="_idParaDest-150" class="title">Triggering tasks</h2>&#13;&#13;
    <p class="normal">The <code class="Code-In-Text--PACKT-">start_task.py</code> script contains all the code to trigger the task. This is a simple script that imports <a id="_idIndexMarker510"/>the task from the other file.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> celery_tasks <span class="hljs-keyword">import</span> obtain_info&#13;&#13;
obtain_info.delay()&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Note that it inherits all the configuration from <code class="Code-In-Text--PACKT-">celery_tasks.py</code> when doing the import.</p>&#13;&#13;
    <p class="normal">Importantly, it calls the task with <code class="Code-In-Text--PACKT-">.delay()</code>. This sends the task to the queue so the worker can pull it out and execute it.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">Note that if you call the task directly with <code class="Code-In-Text--PACKT-">obtain_info()</code>, you'll execute the code directly, instead of submitting the task to the queue.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">Let's see now how both files interact.</p>&#13;&#13;
    <h2 id="_idParaDest-151" class="title">Connecting the dots</h2>&#13;&#13;
    <p class="normal">To be able <a id="_idIndexMarker511"/>to set both parts, the publisher and the consumer, first start the worker calling style:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="language-bash">celery -A celery_tasks worker --loglevel=INFO -c 3</span>&#13;&#13;
</code></pre>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-"><strong class="keyword">Note</strong>: Some of the modules used, such as Celery, might not be compatible with Windows systems. More information can be found at <a href="https://docs.celeryproject.org/en/stable/faq.html#does-celery-support-windows"><span class="url">https://docs.celeryproject.org/en/stable/faq.html#does-celery-support-windows</span></a>.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">This starts the <code class="Code-In-Text--PACKT-">celery_tasks</code> module (the <code class="Code-In-Text--PACKT-">celery_tasks.py</code> file) with the <code class="Code-In-Text--PACKT-">-A</code> parameter. It sets the log level to <code class="Code-In-Text--PACKT-">INFO</code> and starts three workers with the <code class="Code-In-Text--PACKT-">-c 3</code> parameter. It will display a starting log similar to this one:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="language-bash">celery -A celery_tasks worker --loglevel=INFO -c 3</span>&#13;&#13;
   v5.1.1 (sun-harmonics)&#13;&#13;
macOS-10.15.7-x86_64-i386-64bit 2021-06-22 20:14:09&#13;&#13;
[config]&#13;&#13;
.&gt; app:         tasks:0x110b45760&#13;&#13;
.&gt; transport:   redis://localhost:6379//&#13;&#13;
.&gt; results:     disabled://&#13;&#13;
.&gt; concurrency: 3 (prefork)&#13;&#13;
.&gt; task events: OFF (enable -E to monitor tasks in this worker)&#13;&#13;
[queues]&#13;&#13;
.&gt; celery           exchange=celery(direct) key=celery&#13;&#13;
[tasks]&#13;&#13;
  . celery_tasks.obtain_info&#13;&#13;
  . celery_tasks.send_email&#13;&#13;
[2021-06-22 20:14:09,613: INFO/MainProcess] Connected to redis://localhost:6379//&#13;&#13;
[2021-06-22 20:14:09,628: INFO/MainProcess] mingle: searching for neighbors&#13;&#13;
[2021-06-22 20:14:10,666: INFO/MainProcess] mingle: all alone&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Note that it <a id="_idIndexMarker512"/>displays the two available tasks, <code class="Code-In-Text--PACKT-">obtain_info</code> and <code class="Code-In-Text--PACKT-">send_email</code>. In another window, we can send tasks calling the <code class="Code-In-Text--PACKT-">start_task.py</code> script:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="language-bash">python3 start_task.py</span>&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This will trigger the task in the Celery worker, producing logs (edited for clarity and brevity). We will explain the logs in the next paragraphs.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">[2021-06-22 20:30:52,627: INFO/MainProcess] Task celery_tasks.obtain_info[5f6c9441-9dda-40df-b456-91100a92d42c] received&#13;&#13;
[2021-06-22 20:30:52,632: INFO/ForkPoolWorker-2] Stating task&#13;&#13;
[2021-06-22 20:30:52,899: INFO/ForkPoolWorker-2] Retrieving info for user 1&#13;&#13;
...&#13;&#13;
[2021-06-22 20:30:54,128: INFO/MainProcess] Task celery_tasks.send_email[08b9ed75-0f33-48f8-8b55-1f917cfdeae8] received&#13;&#13;
[2021-06-22 20:30:54,133: INFO/MainProcess] Task celery_tasks.send_email[d1f6c6a0-a416-4565-b085-6b0a180cad37] received&#13;&#13;
[2021-06-22 20:30:54,132: INFO/ForkPoolWorker-1] Send an email to Sincere@april.biz&#13;&#13;
[2021-06-22 20:30:54,134: INFO/ForkPoolWorker-1] Reminders ['delectus aut autem', 'quis ut nam facilis et officia qui', 'fugiat veniam minus', 'laboriosam mollitia et enim quasi adipisci quia provident illum', 'qui ullam ratione quibusdam voluptatem quia omnis', 'illo expedita consequatur quia in', 'molestiae perspiciatis ipsa', 'et doloremque nulla', 'dolorum est consequatur ea mollitia in culpa']&#13;&#13;
[2021-06-22 20:30:54,135: INFO/ForkPoolWorker-1] Task celery_tasks.send_email[08b9ed75-0f33-48f8-8b55-1f917cfdeae8] succeeded in 0.004046451000021989s: None&#13;&#13;
[2021-06-22 20:30:54,137: INFO/ForkPoolWorker-3] Send an email to Shanna@melissa.tv&#13;&#13;
[2021-06-22 20:30:54,181: INFO/ForkPoolWorker-2] Task celery_tasks.obtain_info[5f6c9441-9dda-40df-b456-91100a92d42c] succeeded in 1.5507660419999638s: None&#13;&#13;
...&#13;&#13;
[2021-06-22 20:30:54,141: INFO/ForkPoolWorker-3] Task celery_tasks.send_email[d1f6c6a0-a416-4565-b085-6b0a180cad37] succeeded in 0.004405897999959052s: None&#13;&#13;
[2021-06-22 20:30:54,192: INFO/ForkPoolWorker-2] Task celery_tasks.send_email[aff6dfc9-3e9d-4c2d-9aa0-9f91f2b35f87] succeeded in 0.0012900159999844618s: None&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Because we <a id="_idIndexMarker513"/>started three different workers, the logs are intertwined. Pay attention to the first task, which corresponds to <code class="Code-In-Text--PACKT-">obtain_info</code>. This task has been executed in the worker <code class="Code-In-Text--PACKT-">ForkPoolWorker-2</code> in our execution.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">[2021-06-22 20:30:52,627: INFO/MainProcess] Task celery_tasks.obtain_info[5f6c9441-9dda-40df-b456-91100a92d42c] received&#13;&#13;
[2021-06-22 20:30:52,632: INFO/ForkPoolWorker-2] Stating task&#13;&#13;
[2021-06-22 20:30:52,899: INFO/ForkPoolWorker-2] Retrieving info for user 1&#13;&#13;
...&#13;&#13;
[2021-06-22 20:30:54,181: INFO/ForkPoolWorker-2] Task celery_tasks.obtain_info[5f6c9441-9dda-40df-b456-91100a92d42c] succeeded in 1.5507660419999638s: None&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">While this <a id="_idIndexMarker514"/>task is being executed, the <code class="Code-In-Text--PACKT-">send_email</code> tasks are also being enqueued and executed by the other workers. </p>&#13;&#13;
    <p class="normal">For example:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">[2021-06-22 20:30:54,133: INFO/MainProcess] Task celery_tasks.send_email[d1f6c6a0-a416-4565-b085-6b0a180cad37] received&#13;&#13;
[2021-06-22 20:30:54,132: INFO/ForkPoolWorker-1] Send an email to Sincere@april.biz&#13;&#13;
[2021-06-22 20:30:54,134: INFO/ForkPoolWorker-1] Reminders ['delectus aut autem', 'quis ut nam facilis et officia qui', 'fugiat veniam minus', 'laboriosam mollitia et enim quasi adipisci quia provident illum', 'qui ullam ratione quibusdam voluptatem quia omnis', 'illo expedita consequatur quia in', 'molestiae perspiciatis ipsa', 'et doloremque nulla', 'dolorum est consequatur ea mollitia in culpa']&#13;&#13;
[2021-06-22 20:30:54,135: INFO/ForkPoolWorker-1] Task celery_tasks.send_email[08b9ed75-0f33-48f8-8b55-1f917cfdeae8] succeeded in 0.004046451000021989s: None&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">At the end of the execution, there's a log showing the time it has taken, in seconds.</p>&#13;&#13;
    <div class="packt_tip">&#13;&#13;
      <p class="Tip--PACKT-">If only one worker is involved, the tasks will be run consecutively, making it easier to differentiate between tasks.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">We can see how the <code class="Code-In-Text--PACKT-">send_email</code> tasks start before the end of the <code class="Code-In-Text--PACKT-">obtain_info</code> task, and that there are still <code class="Code-In-Text--PACKT-">send_email</code> tasks running after the end of the <code class="Code-In-Text--PACKT-">obtain_info</code> task, showing how the tasks are running independently.</p>&#13;&#13;
    <h2 id="_idParaDest-152" class="title">Scheduled tasks</h2>&#13;&#13;
    <p class="normal">Inside Celery, we can also generate tasks with a certain schedule, so they can be triggered <a id="_idIndexMarker515"/>automatically at the proper time.</p>&#13;&#13;
    <p class="normal">To do so, we need to define a task and a schedule. We defined them in the <code class="Code-In-Text--PACKT-">celery_scheduled_tasks.py</code> file. Let's take a look:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> celery <span class="hljs-keyword">import</span> Celery&#13;&#13;
<span class="hljs-keyword">from</span> celery.schedules <span class="hljs-keyword">import</span> crontab&#13;&#13;
app = Celery('tasks', broker='redis://localhost')&#13;&#13;
logger = app.log.get_default_logger()&#13;&#13;
<span class="hljs-meta">@app.task</span>&#13;&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">scheduled_task</span>(<span class="hljs-params">timing</span>):&#13;&#13;
    logger.info(f'Scheduled task executed {timing}')&#13;&#13;
app.conf.beat_schedule = {&#13;&#13;
    <span class="hljs-comment"># Executes every 15 seconds</span>&#13;&#13;
    'every-<span class="hljs-number">15</span>-seconds': {&#13;&#13;
        'task': 'celery_scheduled_tasks.scheduled_task',&#13;&#13;
        'schedule': <span class="hljs-number">15</span>,&#13;&#13;
        'args': ('every <span class="hljs-number">15</span> seconds',),&#13;&#13;
    },&#13;&#13;
    <span class="hljs-comment"># Executes following crontab</span>&#13;&#13;
    'every-<span class="hljs-number">2</span>-minutes': {&#13;&#13;
        'task': 'celery_scheduled_tasks.scheduled_task',&#13;&#13;
        'schedule': crontab(minute='*/<span class="hljs-number">2</span>'),&#13;&#13;
        'args': ('crontab every <span class="hljs-number">2</span> minutes',),&#13;&#13;
    },&#13;&#13;
}&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This file <a id="_idIndexMarker516"/>starts with the same configuration as the previous example, and we define a small, simple task that just displays when it is executed.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@app.task</span>&#13;&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">scheduled_task</span>(<span class="hljs-params">timing</span>):&#13;&#13;
    logger.info(f'Scheduled task executed {timing}')&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The interesting bit comes later, as the schedule is configured in the <code class="Code-In-Text--PACKT-">app.conf.beat_schedule</code> parameter. We created two entries.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">app.conf.beat_schedule = {&#13;&#13;
    <span class="hljs-comment"># Executes every 15 seconds</span>&#13;&#13;
    'every-<span class="hljs-number">15</span>-seconds': {&#13;&#13;
        'task': 'celery_scheduled_tasks.scheduled_task',&#13;&#13;
        'schedule': <span class="hljs-number">15</span>,&#13;&#13;
        'args': ('every <span class="hljs-number">15</span> seconds',),&#13;&#13;
    },&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The first one defines an execution of the proper task every 15 seconds. The task needs to include the module name (<code class="Code-In-Text--PACKT-">celery_scheduled_tasks</code>). The <code class="Code-In-Text--PACKT-">schedule</code> parameter is defined in seconds. The <code class="Code-In-Text--PACKT-">args</code> parameter contains any parameter to pass for the execution. Note that it's defined as a list of parameters. In this case, we create a tuple with a single entry, as there's only one argument.</p>&#13;&#13;
    <p class="normal">The second <a id="_idIndexMarker517"/>entry defines the schedule instead as a crontab entry.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-comment"># Executes following crontab</span>&#13;&#13;
    'every-<span class="hljs-number">2</span>-minutes': {&#13;&#13;
        'task': 'celery_scheduled_tasks.scheduled_task',&#13;&#13;
        'schedule': crontab(minute='*/<span class="hljs-number">2</span>'),&#13;&#13;
        'args': ('crontab every <span class="hljs-number">2</span> minutes',),&#13;&#13;
    },&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This <code class="Code-In-Text--PACKT-">crontab</code> object, which is passed as the <code class="Code-In-Text--PACKT-">schedule</code> parameter, executes the task once every two minutes. Crontab entries are very flexible and allow for a wide range of possible actions.</p>&#13;&#13;
    <p class="normal">Some examples are as follows:</p>&#13;&#13;
    <table id="table001-4" class="No-Table-Style _idGenTablePara-1">&#13;&#13;
      <colgroup>&#13;&#13;
        <col/>&#13;&#13;
        <col/>&#13;&#13;
      </colgroup>&#13;&#13;
      <tbody>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Heading--PACKT-"><span class="python">Crontab entry</span></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Heading--PACKT-"><span class="python">Description</span></p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">crontab()</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-"><span class="python">Execute every minute, the lowest possible resolution</span></p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">crontab(minute=0)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-"><span class="python">Execute every hour, at minute 0</span></p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">crontab(minute=15)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-"><span class="python">Execute hourly, at minute 15</span></p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">crontab(hour=0, minute=0)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-"><span class="python">Execute daily, at midnight (in your time zone)</span></p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">crontab(hour=6, minute=30, day_of_week='monday')</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-"><span class="python">Execute every Monday, at 6:30</span></p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">crontab(hour='*/8', minute=0)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-"><span class="python">Execute every hour divisible by 8 (0, 8, 16). Three times a day, at minute 0 in each case</span></p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">crontab(day_of_month=1, hour=0, minute=0)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-"><span class="python">Execute on the first of each month, at midnight</span></p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">crontab(minute='*/2')</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-"><span class="python">Execute every minute divisible by 2. Once every two minutes</span></p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
      </tbody>&#13;&#13;
    </table>&#13;&#13;
    <p class="normal">There are more options, including relating the time to solar times, like dawn and dusk, or custom schedulers, but most use cases will be perfectly fine either once every X seconds or with a crontab definition.</p>&#13;&#13;
    <div class="packt_tip">&#13;&#13;
      <p class="Tip--PACKT-">You can check the full documentation here: <a href="https://docs.celeryproject.org/en/stable/userguide/periodic-tasks.html#starting-the-scheduler"><span class="url">https://docs.celeryproject.org/en/stable/userguide/periodic-tasks.html#starting-the-scheduler</span></a>.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">To start <a id="_idIndexMarker518"/>the scheduler, we need to start a specific worker, the <code class="Code-In-Text--PACKT-">beat</code> worker:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="language-bash">celery -A celery_scheduled_tasks beat</span>&#13;&#13;
celery beat v4.4.7 (cliffs) is starting.&#13;&#13;
__    -    ... __   -        _&#13;&#13;
LocalTime -&gt; 2021-06-28 13:53:23&#13;&#13;
Configuration -&gt;&#13;&#13;
    . broker -&gt; redis://localhost:6379//&#13;&#13;
    . loader -&gt; celery.loaders.app.AppLoader&#13;&#13;
    . scheduler -&gt; celery.beat.PersistentScheduler&#13;&#13;
    . db -&gt; celerybeat-schedule&#13;&#13;
    . logfile -&gt; [stderr]@%WARNING&#13;&#13;
    . maxinterval -&gt; 5.00 minutes (300s) &#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">We start the <code class="Code-In-Text--PACKT-">celery_scheduled_tasks</code> worker in the usual way.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="language-bash">celery -A celery_scheduled_tasks worker --loglevel=INFO -c 3</span>&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">But you can see that there's still no incoming tasks. We need to start <code class="Code-In-Text--PACKT-">celery beat</code>, which is a specific worker that inserts the tasks in the queue:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="language-bash">celery -A celery_scheduled_tasks beat</span>&#13;&#13;
celery beat v4.4.7 (cliffs) is starting.&#13;&#13;
__    -    ... __   -        _&#13;&#13;
LocalTime -&gt; 2021-06-28 15:13:06&#13;&#13;
Configuration -&gt;&#13;&#13;
    . broker -&gt; redis://localhost:6379//&#13;&#13;
    . loader -&gt; celery.loaders.app.AppLoader&#13;&#13;
    . scheduler -&gt; celery.beat.PersistentScheduler&#13;&#13;
    . db -&gt; celerybeat-schedule&#13;&#13;
    . logfile -&gt; [stderr]@%WARNING&#13;&#13;
    . maxinterval -&gt; 5.00 minutes (300s)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Once <code class="Code-In-Text--PACKT-">celery beat</code> is <a id="_idIndexMarker519"/>started, you'll start seeing the tasks being scheduled and executed as expected:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">[2021-06-28 15:13:06,504: INFO/MainProcess] Received task: celery_scheduled_tasks.scheduled_task[42ed6155-4978-4c39-b307-852561fdafa8]&#13;&#13;
[2021-06-28 15:13:06,509: INFO/MainProcess] Received task: celery_scheduled_tasks.scheduled_task[517d38b0-f276-4c42-9738-80ca844b8e77]&#13;&#13;
[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-2] Scheduled task executed every 15 seconds&#13;&#13;
[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-1] Scheduled task executed crontab every 2 minutes&#13;&#13;
[2021-06-28 15:13:06,511: INFO/ForkPoolWorker-2] Task celery_scheduled_tasks.scheduled_task[42ed6155-4978-4c39-b307-852561fdafa8] succeeded in 0.0016690909999965697s: None&#13;&#13;
[2021-06-28 15:13:06,512: INFO/ForkPoolWorker-1] Task celery_scheduled_tasks.scheduled_task[517d38b0-f276-4c42-9738-80ca844b8e77] succeeded in 0.0014504210000154671s: None&#13;&#13;
[2021-06-28 15:13:21,486: INFO/MainProcess] Received task: celery_scheduled_tasks.scheduled_task[4d77b138-283c-44c8-a8ce-9183cf0480a7]&#13;&#13;
[2021-06-28 15:13:21,488: INFO/ForkPoolWorker-2] Scheduled task executed every 15 seconds&#13;&#13;
[2021-06-28 15:13:21,489: INFO/ForkPoolWorker-2] Task celery_scheduled_tasks.scheduled_task[4d77b138-283c-44c8-a8ce-9183cf0480a7] succeeded in 0.0005252540000242334s: None&#13;&#13;
[2021-06-28 15:13:36,486: INFO/MainProcess] Received task: celery_scheduled_tasks.scheduled_task[2eb2ee30-2bcd-45af-8ee2-437868be22e4]&#13;&#13;
[2021-06-28 15:13:36,489: INFO/ForkPoolWorker-2] Scheduled task executed every 15 seconds&#13;&#13;
[2021-06-28 15:13:36,489: INFO/ForkPoolWorker-2] Task celery_scheduled_tasks.scheduled_task[2eb2ee30-2bcd-45af-8ee2-437868be22e4] succeeded in 0.000493534999975509s: None&#13;&#13;
[2021-06-28 15:13:51,486: INFO/MainProcess] Received task: celery_scheduled_tasks.scheduled_task[c7c0616c-857a-4f7b-ae7a-dd967f9498fb]&#13;&#13;
[2021-06-28 15:13:51,488: INFO/ForkPoolWorker-2] Scheduled task executed every 15 seconds&#13;&#13;
[2021-06-28 15:13:51,489: INFO/ForkPoolWorker-2] Task celery_scheduled_tasks.scheduled_task[c7c0616c-857a-4f7b-ae7a-dd967f9498fb] succeeded in 0.0004461000000333115s: None&#13;&#13;
[2021-06-28 15:14:00,004: INFO/MainProcess] Received task: celery_scheduled_tasks.scheduled_task[59f6a323-4d9f-4ac4-b831-39ca6b342296]&#13;&#13;
[2021-06-28 15:14:00,006: INFO/ForkPoolWorker-2] Scheduled task executed crontab every 2 minutes&#13;&#13;
[2021-06-28 15:14:00,006: INFO/ForkPoolWorker-2] Task celery_scheduled_tasks.scheduled_task[59f6a323-4d9f-4ac4-b831-39ca6b342296] succeeded in 0.0004902660000425385s: None&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">You can see that both kinds of tasks are scheduled accordingly. In this log, check the times and <a id="_idIndexMarker520"/>see that they are 15 seconds apart:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-2] Scheduled task executed every 15 seconds&#13;&#13;
[2021-06-28 15:13:21,488: INFO/ForkPoolWorker-2] Scheduled task executed every 15 seconds&#13;&#13;
[2021-06-28 15:13:36,489: INFO/ForkPoolWorker-2] Scheduled task executed every 15 seconds&#13;&#13;
[2021-06-28 15:13:51,488: INFO/ForkPoolWorker-2] Scheduled task executed every 15 seconds&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The other task happens exactly every 2 minutes. Note that the first execution may not be totally precise. In this case, the schedule was triggered in the later seconds of 15:12 and still got executed later than that. In any case, it will be within the 1-minute resolution window of the crontab.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-1] Scheduled task executed crontab every 2 minutes&#13;&#13;
[2021-06-28 15:14:00,006: INFO/ForkPoolWorker-2] Scheduled task executed crontab every 2 minutes&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">When creating periodic tasks, keep in mind the different priorities, as we described previously in the chapter.</p>&#13;&#13;
    <div class="packt_tip">&#13;&#13;
      <p class="Tip--PACKT-">It is good practice to use a periodic task as a "heartbeat" to check that the system is working correctly. This task can be used to monitor that the tasks in the system are flowing as expected, with no big delays or problems.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">This leads <a id="_idIndexMarker521"/>to the way of monitoring how the different tasks are being executed, in a better way than just by checking the logs.</p>&#13;&#13;
    <h2 id="_idParaDest-153" class="title">Celery Flower</h2>&#13;&#13;
    <p class="normal">Obtaining good monitoring in Celery is important if you want to understand the executed tasks <a id="_idIndexMarker522"/>and find and fix problems. A good tool for that is Flower, which enhances Celery by adding a real-time monitoring web page that allows you to control Celery through the web page and through an HTTP API.</p>&#13;&#13;
    <div class="note">&#13;&#13;
      <p class="Information-Box--PACKT-">You can check the whole documentation at <a href="https://flower.readthedocs.io/en/latest/"><span class="url">https://flower.readthedocs.io/en/latest/</span></a>.</p>&#13;&#13;
    </div>&#13;&#13;
    <p class="normal">It's also <a id="_idIndexMarker523"/>very easy to set up and integrate with Celery. First, we need to be sure that the <code class="Code-In-Text--PACKT-">flower</code> package is installed. The package is included in the <code class="Code-In-Text--PACKT-">requirements.txt</code> after the previous step, but if it's not, you can install it independently using <code class="Code-In-Text--PACKT-">pip3</code>.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="language-bash">pip3 install flower</span>&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Once it is installed, you can start <code class="Code-In-Text--PACKT-">flower</code> with the following command:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="language-bash">celery --broker=redis://localhost flower -A celery_tasks  --port=5555</span>&#13;&#13;
[I 210624 19:23:01 command:135] Visit me at http://localhost:5555&#13;&#13;
[I 210624 19:23:01 command:142] Broker: redis://localhost:6379//&#13;&#13;
[I 210624 19:23:01 command:143] Registered tasks:&#13;&#13;
    ['celery.accumulate',&#13;&#13;
     'celery.backend_cleanup',&#13;&#13;
     'celery.chain',&#13;&#13;
     'celery.chord',&#13;&#13;
     'celery.chord_unlock',&#13;&#13;
     'celery.chunks',&#13;&#13;
     'celery.group',&#13;&#13;
     'celery.map',&#13;&#13;
     'celery.starmap',&#13;&#13;
     'celery_tasks.obtain_info',&#13;&#13;
     'celery_tasks.send_email']&#13;&#13;
[I 210624 19:23:01 mixins:229] Connected to redis://localhost:6379//&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The command is very similar to starting the Celery workers, but includes the definition of the broker using Redis, as we saw before, with <code class="Code-In-Text--PACKT-">--broker=redis://localhost</code>, and specifying <a id="_idIndexMarker524"/>the port to expose, <code class="Code-In-Text--PACKT-">--port=5555</code>.</p>&#13;&#13;
    <p class="normal">The interface is exposed in <code class="Code-In-Text--PACKT-">http://localhost:5555</code>.</p>&#13;&#13;
    <figure class="mediaobject"><img src="Images/B17580_07_08.png" alt="Graphical user interface, application&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="265"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 7.8: Celery Flower interface</p>&#13;&#13;
    <p class="normal">The front page shows the different workers in the system. Note that it shows the number of active tasks, as well as processed tasks. In this case, we have 11 tasks corresponding to a whole run of <code class="Code-In-Text--PACKT-">start_task.py</code>. You can go to the <strong class="screenText">Tasks</strong> tab to see the details of each of the tasks executed, which looks like this:</p>&#13;&#13;
    <figure class="mediaobject"><img src="Images/B17580_07_09.png" alt="Graphical user interface&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="481"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 7.9: Tasks page</p>&#13;&#13;
    <p class="normal">You can <a id="_idIndexMarker525"/>see information such as the input parameters, the state of the task, the name of the task, and how long it ran for.</p>&#13;&#13;
    <p class="normal">Each Celery process will appear independently, even if it's capable of running multiple workers. You can check its parameters on the <strong class="screenText">Worker</strong> page. See the <strong class="screenText">Max concurrency</strong> parameter.</p>&#13;&#13;
    <figure class="mediaobject"><img src="Images/B17580_07_10.png" alt="Graphical user interface, application&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="435"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 7.10: Worker page</p>&#13;&#13;
    <p class="normal">From here, you <a id="_idIndexMarker526"/>can also review and change the configuration of the number of workers per Celery process, set rate limits, and more.</p>&#13;&#13;
    <h2 id="_idParaDest-154" class="title">Flower HTTP API</h2>&#13;&#13;
    <p class="normal">A great addition from Flower <a id="_idIndexMarker527"/>is the HTTP API, which allows us to control Flower through HTTP calls. This <a id="_idIndexMarker528"/>enables the automatic control of the system and allows us to trigger the tasks directly with an HTTP request. This can be used to call the tasks in any programming language, and greatly increases the flexibility of Celery.</p>&#13;&#13;
    <p class="normal">The URL to call a task asynchronously is the following:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">POST /api/task/async-apply/{task}&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">It requires a POST, and the arguments of the call should be included in the body. For example, make a call with <code class="Code-In-Text--PACKT-">curl</code>:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="language-bash">curl -X POST -d '{"args":["example@email.com",["msg1", "msg2"]]}' http://localhost:5555/api/task/async-apply/celery_tasks.send_email</span>&#13;&#13;
{"task-id": "79258153-0bdf-4d67-882c-30405d9a36f0"}&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The task is executed in the worker:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">[2021-06-24 22:35:33,052: INFO/MainProcess] Received task: celery_tasks.send_email[79258153-0bdf-4d67-882c-30405d9a36f0]&#13;&#13;
[2021-06-24 22:35:33,054: INFO/ForkPoolWorker-2] Send an email to example@email.com&#13;&#13;
[2021-06-24 22:35:33,055: INFO/ForkPoolWorker-2] Reminders ['msg1', 'msg2']&#13;&#13;
[2021-06-24 22:35:33,056: INFO/ForkPoolWorker-2] Task celery_tasks.send_email[79258153-0bdf-4d67-882c-30405d9a36f0] succeeded in 0.0021811629999999305s: None&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Using the <a id="_idIndexMarker529"/>same API, the status of the task can be retrieved with a GET request:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">GET /api/task/info/{task_id}&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">For example:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="language-bash">curl  http://localhost:5555/api/task/info/79258153-0bdf-4d67-882c-30405d9a36f0</span>&#13;&#13;
{"uuid": "79258153-0bdf-4d67-882c-30405d9a36f0", "name": "celery_tasks.send_email", "state": "SUCCESS", "received": 1624571191.674537, "sent": null, "started": 1624571191.676534, "rejected": null, "succeeded": 1624571191.679662, "failed": null, "retried": null, "revoked": null, "args": "['example@email.com', ['msg1', 'msg2']]", "kwargs": "{}", "eta": null, "expires": null, "retries": 0, "worker": "celery@Jaimes-iMac-5K.local", "result": "None", "exception": null, "timestamp": 1624571191.679662, "runtime": 0.0007789200000161145, "traceback": null, "exchange": null, "routing_key": null, "clock": 807, "client": null, "root": "79258153-0bdf-4d67-882c-30405d9a36f0", "root_id": "79258153-0bdf-4d67-882c-30405d9a36f0", "parent": null, "parent_id": null, "children": []}&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Note the <code class="Code-In-Text--PACKT-">state</code> parameter, which here shows the task is finished successfully, but it will return <code class="Code-In-Text--PACKT-">PENDING</code> if it's not done yet.</p>&#13;&#13;
    <p class="normal">This can be used to poll the status of the task until it's completed or it shows an error, as we described earlier in the chapter.</p>&#13;&#13;
    <h1 id="_idParaDest-155" class="title">Summary</h1>&#13;&#13;
    <p class="normal">In this chapter, we have seen what event-driven structures are. We started with a general discussion about how events can be used to create different flows than the traditional request-response structure. We talked about how the events are introduced into queues to be transmitted to other systems. We introduced the idea of a publisher and a subscriber to introduce or extract events from that queue.</p>&#13;&#13;
    <p class="normal">We described how this structure could be used to act on asynchronous tasks: tasks that run in the background and allow other elements of the interface to respond quickly. We described how dividing asynchronous tasks into smaller ones can help increase throughput by taking advantage of having multiple subscribers that can execute these smaller tasks. We described how tasks can be added automatically at certain times to allow the execution of predetermined tasks periodically.</p>&#13;&#13;
    <p class="normal">As the introduction of tasks can happen with great variability, we discussed some important details of how queues work, the different problems that we can encounter, and strategies to deal with them. We talked about how a simple strategy for a background queue and a priority queue works in most scenarios and warned about overcomplicating it. We also explained that, in the same spirit, it's better to keep the code synchronized among all workers, even in cases when the queues may be different. We also briefly touched on the capabilities of cloud computing as applied to asynchronous workers.</p>&#13;&#13;
    <p class="normal">We explained how to use Celery, a popular task manager, to create asynchronous tasks. We covered setting up the different elements, including the back-end broker, how to define a proper worker, and how to generate tasks from a different service. We included a section on how to create scheduled tasks in Celery as well.</p>&#13;&#13;
    <p class="normal">We presented Celery Flower, a complement for Celery that includes a web interface with which we can monitor and control Celery. It also includes an HTTP API that allows us to create tasks by sending HTTP requests, allowing any programming language to interact with our Celery system.</p>&#13;&#13;
  </div>&#13;&#13;
</div></body></html>