<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dividing Text Data and Building Text Classifiers</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>This chapter presents the following recipes:</span></p>
<ul>
<li>Building a text classifier</li>
<li>Preprocessing data using tokenization</li>
<li>Stemming text data</li>
<li>Dividing text using chunking</li>
<li>Building a bag-of-words model</li>
<li>Applications of text classifiers</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>This chapter presents recipes to build text classifiers. This includes extracting vital features from the database, training, testing, and validating the text classifier. Initially, a text classifier is trained using commonly used words. Later, the trained text classifier is used for prediction. Building a text classifier includes preprocessing the data using tokenization, stemming text data, dividing text using chunking, and building a bag-of-words model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a text classifier</h1>
                </header>
            
            <article>
                
<p>Classifier units are normally considered to separate a database into various classes. The Naive Bayes classifier scheme is widely considered in literature to segregate the texts based on the trained model. This section of the chapter initially considers a text database with keywords; feature extraction extracts the key phrases from the text and trains the classifier system. Then, <strong>term frequency-inverse document frequency</strong> (<strong>tf-idf</strong>) transformation is implemented to specify the importance of the word. Finally, the output is predicted and printed using the classifier system.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Include the following lines in a new Python file to add datasets:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.datasets import fetch_20newsgroups 
category_mapping = {'misc.forsale': 'Sellings', 'rec.motorcycles': 'Motorbikes', 
        'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography', 
        'sci.space': 'OuterSpace'} 
 
training_content = fetch_20newsgroups(subset='train', 
categories=category_mapping.keys(), shuffle=True, random_state=7) </pre>
<ol start="2">
<li>Perform feature extraction to extract the main words from the text:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.feature_extraction.text import CountVectorizer 
 
vectorizing = CountVectorizer() 
train_counts = vectorizing.fit_transform(training_content.data) 
print "nDimensions of training data:", train_counts.shape </pre>
<ol start="3">
<li>Train the classifier:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.naive_bayes import MultinomialNB 
from sklearn.feature_extraction.text import TfidfTransformer 
 
input_content = [ 
    "The curveballs of right handed pitchers tend to curve to the left", 
    "Caesar cipher is an ancient form of encryption", 
    "This two-wheeler is really good on slippery roads" 
] 
 
tfidf_transformer = TfidfTransformer() 
train_tfidf = tfidf_transformer.fit_transform(train_counts) </pre>
<ol start="4">
<li>Implement the Multinomial Naive Bayes classifier:</li>
</ol>
<pre style="padding-left: 60px">classifier = MultinomialNB().fit(train_tfidf, training_content.target) 
input_counts = vectorizing.transform(input_content) 
input_tfidf = tfidf_transformer.transform(input_counts) </pre>
<ol start="5">
<li>Predict the output categories:</li>
</ol>
<pre style="padding-left: 60px">categories_prediction = classifier.predict(input_tfidf) </pre>
<ol start="6">
<li>Print the output:</li>
</ol>
<pre style="padding-left: 60px">for sentence, category in zip(input_content, categories_prediction): 
    print 'nInput:', sentence, 'nPredicted category:',  
            category_mapping[training_content.target_names[category]] </pre>
<p style="padding-left: 60px">The following screenshot provides examples of predicting the object based on the input from the database:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-657 image-border" src="Images/18727b52-014c-4371-b8b5-87afba4948fc.png" style="width:40.50em;height:13.92em;" width="651" height="224"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The previous section of this chapter provided insight regarding the implemented classifier section and some sample results. The classifier section works based on a comparison between the previous text in the trained Naive Bayes with the key test in the test sequence<em>.</em></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Please refer to the following articles:</p>
<ul>
<li><em>Sentiment analysis algorithms and applications: A survey </em>at <a href="https://www.sciencedirect.com/science/article/pii/S2090447914000550" target="_blank">https://www.sciencedirect.com/science/article/pii/S2090447914000550</a>.</li>
</ul>
<ul>
<li>S<em>entiment classification of online reviews: using sentence-based language model</em> to learn how sentiment prediction works at <a href="https://www.tandfonline.com/doi/abs/10.1080/0952813X.2013.782352?src=recsys&amp;journalCode=teta20" target="_blank">https://www.tandfonline.com/doi/abs/10.1080/0952813X.2013.782352?src=recsys&amp;journalCode=teta20</a>.</li>
</ul>
<ul>
<li><em>Sentiment analysis using product review data</em> and <span><em>Sentence-level sentiment analysis in the presence of modalities</em> to learn more about various metrics used in recommendation systems at </span><a href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-015-0015-2" target="_blank">https://journalofbigdata.springeropen.com/articles/10.1186/s40537-015-0015-2</a> and ;<a href="https://link.springer.com/chapter/10.1007/978-3-642-54903-8_1" target="_blank">https://link.springer.com/chapter/10.1007/978-3-642-54903-8_1</a>.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Pre-processing data using tokenization</h1>
                </header>
            
            <article>
                
<p>The pre-processing of data involves converting the existing text into acceptable information for the learning algorithm.</p>
<p>Tokenization is the process of dividing text into a set of meaningful pieces. These pieces are called tokens.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Introduce sentence tokenization:</li>
</ol>
<pre style="padding-left: 60px">from nltk.tokenize import sent_tokenize</pre>
<ol start="2">
<li>Form a new text tokenizer:</li>
</ol>
<pre style="padding-left: 60px">tokenize_list_sent = sent_tokenize(text)<br/>print "nSentence tokenizer:" 
print tokenize_list_sent </pre>
<ol start="3">
<li>Form a new word tokenizer:</li>
</ol>
<pre style="padding-left: 60px">from nltk.tokenize import word_tokenize 
print "nWord tokenizer:" 
print word_tokenize(text) </pre>
<ol start="4">
<li>Introduce a new WordPunct tokenizer:</li>
</ol>
<pre style="padding-left: 60px">from nltk.tokenize import WordPunctTokenizer 
word_punct_tokenizer = WordPunctTokenizer() 
print "nWord punct tokenizer:" 
print word_punct_tokenizer.tokenize(text) </pre>
<p class="mce-root CDPAlignLeft CDPAlign">The result obtained by the tokenizer is shown here. It divides a sentence into word groups:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="Images/edefd48e-c6ec-4a05-8c74-06f86de0fe71.png" width="571" height="215"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Stemming text data</h1>
                </header>
            
            <article>
                
<p>The stemming procedure involves creating a suitable word with reduced letters for the words of the tokenizer.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Initialize the stemming process with a new Python file:</li>
</ol>
<pre style="padding-left: 60px">from nltk.stem.porter import PorterStemmer 
from nltk.stem.lancaster import LancasterStemmer 
from nltk.stem.snowball import SnowballStemmer </pre>
<ol start="2">
<li>Let's describe some words to consider, as follows:</li>
</ol>
<pre style="padding-left: 60px">words = ['ability', 'baby', 'college', 'playing', 'is', 'dream', 'election', 'beaches', 'image', 'group', 'happy'] </pre>
<ol start="3">
<li>Identify a group of <kbd>stemmers</kbd> to be used:</li>
</ol>
<pre style="padding-left: 60px">stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL'] </pre>
<ol start="4">
<li>Initialize the necessary tasks for the chosen <kbd>stemmers</kbd>:</li>
</ol>
<pre style="padding-left: 60px">stem_porter = PorterStemmer() 
stem_lancaster = LancasterStemmer() 
stem_snowball = SnowballStemmer('english') </pre>
<ol start="5">
<li>Format a table to print the results:</li>
</ol>
<pre style="padding-left: 60px">formatted_row = '{:&gt;16}' * (len(stemmers) + 1) 
print 'n', formatted_row.format('WORD', *stemmers), 'n' </pre>
<ol start="6">
<li>Repeatedly check the list of words and arrange them using chosen <kbd>stemmers</kbd>:</li>
</ol>
<pre style="padding-left: 60px">for word in words:<br/>  stem_words = [stem_porter.stem(word), 
  stem_lancaster.stem(word), 
  stem_snowball.stem(word)] 
  print formatted_row.format(word, *stem_words) </pre>
<p class="mce-root CDPAlignLeft CDPAlign">The result obtained from the stemming process is shown in the following screenshot:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="Images/028b5d01-6a9b-47d5-a58c-1f20e599e27c.png" width="575" height="224"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dividing text using chunking</h1>
                </header>
            
            <article>
                
<p>The chunking procedure can be used to divide the large text into small, meaningful words.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Develop and import the following packages using Python:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
from nltk.corpus import brown </pre>
<ol start="2">
<li>Describe a function that divides text into chunks:</li>
</ol>
<pre style="padding-left: 60px"># Split a text into chunks 
def splitter(content, num_of_words): 
   words = content.split(' ') 
   result = [] </pre>
<ol start="3">
<li>Initialize the following programming lines to get the assigned variables:</li>
</ol>
<pre style="padding-left: 60px">   current_count = 0 
   current_words = []</pre>
<ol start="4">
<li>Start the iteration using words:</li>
</ol>
<pre style="padding-left: 60px">   for word in words: 
     current_words.append(word) 
     current_count += 1 </pre>
<ol start="5">
<li>After getting the essential amount of words, reorganize the variables:</li>
</ol>
<pre style="padding-left: 60px">     if current_count == num_of_words: 
       result.append(' '.join(current_words)) 
       current_words = [] 
       current_count = 0 </pre>
<ol start="6">
<li>Attach the chunks to the output variable:</li>
</ol>
<pre style="padding-left: 60px">       result.append(' '.join(current_words)) 
       return result </pre>
<ol start="7">
<li>Import the data of <kbd>Brown corpus</kbd> and consider the first <kbd>10000</kbd> words:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
  # Read the data from the Brown corpus 
  content = ' '.join(brown.words()[:10000]) </pre>
<ol start="8">
<li>Describe the word size in every chunk:</li>
</ol>
<pre style="padding-left: 60px">  # Number of words in each chunk 
  num_of_words = 1600 </pre>
<ol start="9">
<li>Initiate a pair of significant variables:</li>
</ol>
<pre style="padding-left: 60px">  chunks = [] 
  counter = 0 </pre>
<ol start="10">
<li>Print the result by calling the <kbd>splitter</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">  num_text_chunks = splitter(content, num_of_words) 
  print "Number of text chunks =", len(num_text_chunks) </pre>
<ol start="11">
<li>The result obtained after chunking is shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="Images/eb43cd8a-4d36-444a-bfcd-106fb68016f2.png" style="width:32.83em;height:3.67em;" width="441" height="49"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a bag-of-words model</h1>
                </header>
            
            <article>
                
<p>When working with text documents that include large words, we need to switch them to several types of arithmetic depictions. We need to formulate them to be suitable for machine learning algorithms. These algorithms require arithmetical information so that they can examine the data and provide significant details. The bag-of-words procedure helps us to achieve this. Bag-of-words creates a text model that discovers vocabulary using all the words in the document. Later, it creates the models for every text by constructing a histogram of all the words in the text.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Initialize a new Python file by importing the following file:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
from nltk.corpus import brown 
from chunking import splitter </pre>
<ol start="2">
<li>Define the <kbd>main</kbd> function and read the input data from <kbd>Brown corpus</kbd>:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
        content = ' '.join(brown.words()[:10000]) </pre>
<ol start="3">
<li>Split the text content into chunks:</li>
</ol>
<pre style="padding-left: 60px">    num_of_words = 2000 
    num_chunks = [] 
    count = 0 
    texts_chunk = splitter(content, num_of_words) </pre>
<ol start="4">
<li>Build a vocabulary based on these <kbd>text</kbd> chunks:</li>
</ol>
<pre style="padding-left: 60px">    for text in texts_chunk: 
      num_chunk = {'index': count, 'text': text} 
      num_chunks.append(num_chunk) 
      count += 1</pre>
<ol start="5">
<li>Extract a document word matrix, which effectively counts the amount of incidences of each word in the document:</li>
</ol>
<pre style="padding-left: 60px">  from sklearn.feature_extraction.text      <br/>  import CountVectorizer</pre>
<ol start="6">
<li>Extract the document term <kbd>matrix:</kbd></li>
</ol>
<pre style="padding-left: 60px">from sklearn.feature_extraction.text import CountVectorizer 
vectorizer = CountVectorizer(min_df=5, max_df=.95) 
matrix = vectorizer.fit_transform([num_chunk['text'] for num_chunk in num_chunks]) </pre>
<ol start="7">
<li>Extract the vocabulary and print it:</li>
</ol>
<pre style="padding-left: 60px">vocabulary = np.array(vectorizer.get_feature_names()) 
print "nVocabulary:" 
print vocabulary </pre>
<ol start="8">
<li>Print the document term <kbd>matrix</kbd>:</li>
</ol>
<pre style="padding-left: 60px">print "nDocument term matrix:" 
chunks_name = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4'] 
formatted_row = '{:&gt;12}' * (len(chunks_name) + 1) 
print 'n', formatted_row.format('Word', *chunks_name), 'n' </pre>
<ol start="9">
<li>Iterate throughout the words, and print the reappearance of every word in various chunks:</li>
</ol>
<pre style="padding-left: 60px">for word, item in zip(vocabulary, matrix.T): 
# 'item' is a 'csr_matrix' data structure 
 result = [str(x) for x in item.data] 
 print formatted_row.format(word, *result)</pre>
<ol start="10">
<li>The result obtained after executing the bag-of-words model is shown as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="Images/9d8a48dd-5943-4e49-8134-cf93269fd1df.png" style="width:35.33em;height:11.83em;" width="577" height="194"/></div>
<div class="CDPAlignCenter CDPAlign"><img src="Images/db6c5542-f530-4b93-aa57-327b956e6d99.png" style="width:29.42em;height:37.42em;" width="567" height="722"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft"><span>In order to understand how it works on a given sentence, refer to the following:</span></p>
<ul>
<li><em>Introduction to Sentiment Analysis</em>, explained here: <a href="https://blog.algorithmia.com/introduction-sentiment-analysis/">https://blog.algorithmia.com/introduction-sentiment-analysis/</a></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Applications of text classifiers</h1>
                </header>
            
            <article>
                
<p>Text classifiers are used to analyze customer sentiments, in product reviews, when searching queries on the internet, in social tags, to predict the novelty of research articles, and so on.</p>


            </article>

            
        </section>
    </div>



  </body></html>