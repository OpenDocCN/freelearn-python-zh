- en: Putting It All Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book has so far introduced scraping techniques using a custom website,
    which helped us focus on learning particular skills. In this chapter, we will
    analyze a variety of real-world websites to show how the techniques we've learned
    in the book can be applied. First, we'll use Google to show a real-world search
    form, then Facebook for a JavaScript-dependent website and API, Gap for a typical
    online store, and finally, BMW for a map interface. Since these are live websites,
    there is a risk they will change by the time you read this. However, this is fine
    because the purpose of this chapter's examples is to show you how the techniques
    learned so far can be applied, rather than to show you how to scrape any particular
    website. If you choose to run an example, first check whether the website structure
    has changed since these examples were made and whether their current terms and
    conditions prohibit scraping.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Scraping a Google search result web page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigating the Facebook API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multiple threads with the Gap website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reverse engineering the BMW dealer locator page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google search engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To investigate using our knowledge of CSS selectors, we will scrape Google search
    results. According to the Alexa data used in [Chapter 4](py-web-scrp-2e_ch04.html),
    *Concurrent Downloading*, google.com is the world's most popular website, and
    conveniently, its structure is simple and straightforward to scrape.
  prefs: []
  type: TYPE_NORMAL
- en: International Google may redirect to a country-specific version, depending on
    your location. In these examples, Google is set to the Romanian version, so your
    results may look slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the Google search homepage loaded with browser tools to inspect the
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/google_search-1.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see here that the search query is stored in an input with name `q`, and
    then the form is submitted to the path `/search` set by the `action` attribute.
    We can test this by doing a test search to submit the form, which would then be
    redirected to a URL, such as [https://www.google.ro/?gws_rd=cr,ssl&ei=TuXYWJXqBsGsswHO8YiQAQ#q=test&*](https://www.google.ro/?gws_rd=cr,ssl&ei=TuXYWJXqBsGsswHO8YiQAQ#q=test&*).
    The exact URL will depend on your browser and location. Also if you have Google
    Instant enabled, AJAX will be used to load the search results dynamically rather
    than submitting the form. This URL has many parameters, but the only one required
    is `q` for the query.
  prefs: []
  type: TYPE_NORMAL
- en: 'The URL [https://www.google.com/search?q=test](https://www.google.com/search?q=test) shows
    we can use this URL to produce a search result, as shown in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The structure of the search results can be examined with your browser tools,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/google_results.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we see that the search results are structured as links whose parent element
    is a `<h3>` tag with class "`r`".
  prefs: []
  type: TYPE_NORMAL
- en: 'To scrape the search results, we will use a CSS selector, which was introduced
    in [Chapter 2](py-web-scrp-2e_ch02.html), *Scraping the Data*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: So far, we downloaded the Google search results and used `lxml` to extract the
    links. In the preceding screenshot, the link includes a bunch of extra parameters
    alongside the actual website URL, which are used for tracking clicks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the first link we find on the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The content we want here is `http://www.speedtest.net/`, which can be parsed
    from the query string using the `urlparse` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This query string parsing can be applied to extract all links.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Success! The links from the first page of this Google search have been successfully
    scraped. The full source for this example is available at [https://github.com/kjam/wswp/blob/master/code/chp9/scrape_google.py](https://github.com/kjam/wswp/blob/master/code/chp9/scrape_google.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'One difficulty with Google is that a CAPTCHA image will be shown if your IP
    appears suspicious, for example, when downloading too fast:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4364OS_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: This CAPTCHA image could be solved using the techniques covered in Chapter 7,
    *Solving CAPTCHA*, though it would be preferable to avoid suspicion and download
    slowly, or use proxies if a faster download rate is required. Overloading Google
    can get your IP or even set of IPs banned from Google domains for a series of
    hours or day; so ensure you are courteous to others' (and your own) use of the
    site so your home or office doesn't get blacklisted.
  prefs: []
  type: TYPE_NORMAL
- en: Facebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate using a browser and API, we will investigate Facebook's site.
    Currently, Facebook is the world's largest social network in terms of monthly
    active users, and therefore, its user data is extremely valuable.
  prefs: []
  type: TYPE_NORMAL
- en: The website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is an example Facebook page for Packt Publishing at[https://www.facebook.com/PacktPub](https://www.facebook.com/PacktPub):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05679_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Viewing the source of this page, you would find that the first few posts are
    available, and that later posts are loaded with AJAX when the browser scrolls.
    Facebook also has a mobile interface, which, as mentioned in [Chapter 1](py-web-scrp-2e_ch01.html),
    *Introduction to Web Scraping*, is often easier to scrape. The same page using
    the mobile interface is available at [https://m.facebook.com/PacktPub](https://m.facebook.com/PacktPub):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we interacted with the mobile website and then checked our browser tools,
    we would find that this interface uses a similar structure for the AJAX events,
    so it isn't easier to scrape. These AJAX events can be reverse engineered; however,
    different types of Facebook pages use different AJAX calls, and from my past experience,
    Facebook often changes the structure of these calls; so, scraping them will require
    ongoing maintenance. Therefore, as discussed in Chapter 5, *Dynamic Content*,
    unless performance is crucial, it would be preferable to use a browser rendering
    engine to execute the JavaScript events for us and give us access to the resulting
    HTML.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example snippet using Selenium to automate logging in to Facebook
    and then redirecting to the given page URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This function can then be called to load the Facebook page of interest and scrape
    the resulting generated HTML, using a valid Facebook e-mail and password.
  prefs: []
  type: TYPE_NORMAL
- en: Facebook API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in [Chapter 1](py-web-scrp-2e_ch01.html), *Introduction to Web
    Scraping*, scraping a website is a last resort when the data is not available
    in a structured format. Facebook does offer APIs for a vast majority of the public
    or private (via your user account) data, so we should check whether these APIs
    provide access to what we are after before building an intensive browser scraper.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to do is determine what data is available via the API. To figure
    this out, we should first reference the API documentation. The developer documentation
    available at [https://developers.facebook.com/docs/](https://developers.facebook.com/docs/)
    shows all different types of APIs, including the Graph API, which is the one containing
    the information we desire. If you need to build other interactions with Facebook
    (via the API or SDK), the documentation is regularly updated and easy to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also available via the documentation links is the in-browser Graph API Explorer,
    located at [https://developers.facebook.com/tools/explorer/](https://developers.facebook.com/tools/explorer/).
    As shown in the following screenshot, the Explorer is a great place to test queries
    and their results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/graph_explorer.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, I can search the API to retrieve the PacktPub Facebook Page ID. This Graph
    Explorer can also be used to generate access tokens, which we will use to navigate
    the API.
  prefs: []
  type: TYPE_NORMAL
- en: 'To utilize the Graph API with Python, we need to use special access tokens
    with slightly more advanced requests. Luckily, there is already a well-maintained
    library for us, called `facebook-sdk` ([https://facebook-sdk.readthedocs.io](https://facebook-sdk.readthedocs.io)).
    We can easily install it using pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example of using Facebook''s Graph API to extract data from the
    Packt Publishing page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We see the same results as from the browser-based Graph Explorer. We can request
    more information about the page by passing some extra details we would like to
    extract. To determine which details, we can see all available fields for pages
    in the Graph documentation [https://developers.facebook.com/docs/graph-api/reference/page/](https://developers.facebook.com/docs/graph-api/reference/page/).
    Using the keyword argument `fields`, we can extract these extra available fields
    from the API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We can see that this response is a well-formatted Python dictionary, which we
    can easily parse.
  prefs: []
  type: TYPE_NORMAL
- en: The Graph API provides many other calls to access user data, which are documented
    on Facebook's developer page at [https://developers.facebook.com/docs/graph-api](https://developers.facebook.com/docs/graph-api).
    Depending on the data you need, you may also want to create a Facebook developer
    application, which can give you a longer usable access token.
  prefs: []
  type: TYPE_NORMAL
- en: Gap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate using a Sitemap to investigate content, we will use the Gap website.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gap has a well structured website with a `Sitemap` to help web crawlers locate
    their updated content. If we use the techniques from [Chapter 1](py-web-scrp-2e_ch01.html),
    *Introduction to Web Scraping*, to investigate a website, we would find their
    `robots.txt` file at [http://www.gap.com/robots.txt](http://www.gap.com/robots.txt),
    which contains a link to this Sitemap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the contents of the linked `Sitemap` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown here, this `Sitemap` link is just an index and contains links to other
    `Sitemap` files. These other `Sitemap` files then contain links to thousands of
    product categories, such as [http://www.gap.com/products/womens-jogger-pants.jsp](http://www.gap.com/products/womens-jogger-pants.jsp):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/gap_pants.png)'
  prefs: []
  type: TYPE_IMG
- en: There is a lot of content to crawl here, so we will use the threaded crawler
    developed in [Chapter 4](py-web-scrp-2e_ch04.html), *Concurrent Downloading*.
    You may recall that this crawler supports a URL pattern to match on the page.
    We can also define a `scraper_callback` keyword argument variable, which will
    allow us to parse more links.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example callback to crawl the Gap `Sitemap` link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This callback first checks the downloaded URL extension. If the extension is
    `.xml`, the downloaded URL is for a `Sitemap` file, and the `lxml``etree` module
    is used to parse the XML and extract the links from it. Otherwise, this is a category
    URL, although this example does not implement scraping the category. Now we can
    use this callback with the threaded crawler to crawl `gap.com`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, `lxml` expects to load content from bytes or XML fragments, and
    we have instead stored the Unicode response (so we could parse using regular expressions
    and easily save to disk in Chapter 3, *Caching Downloads* and Chapter 4, *Concurrent
    Downloading*). However, we do have access to the URL in this function. Although
    it is inefficient, we could load the page again; if we only do this for XML pages,
    it should keep the number of requests down and therefore not add too much load
    time. Of course, if we are using caching this also makes it more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try rewriting the callback function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we try running it again, we see success:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the `Sitemap` files were first downloaded and then the clothing
    categories. You'll find throughout your web scraping projects that you may need
    to modify and adapt your code and classes so they fit with new problems. This
    is just one of the many exciting challenges of scraping content from the Internet.
  prefs: []
  type: TYPE_NORMAL
- en: BMW
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To investigate how to reverse engineer a new website, we will take a look at
    the BMW site. The BMW website has a search tool to find local dealerships, available
    at [https://www.bmw.de/de/home.html?entryType=dlo](https://www.bmw.de/de/home.html?entryType=dlo):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This tool takes a location and then displays the points near it on a map, such
    as this search for `Berlin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using browser developer tools such as the Network tab, we find that the search
    triggers this AJAX request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `maxResults` parameter is set to `99`. However, we can increase this
    to download all locations in a single query, a technique covered in [Chapter 1](py-web-scrp-2e_ch01.html),
    *Introduction to Web Scraping*. Here is the result when `maxResults` is increased
    to `1000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This AJAX request provides the data in **JSONP** format, which stands for **JSON
    with padding**. The padding is usually a function to call, with the pure JSON
    data as an argument, in this case the `callback` function call. The padding is
    not easily understood by parsing libraries, so we need to remove it to properly
    parse the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To parse this data with Python''s `json` module, we need to first strip this
    padding, which we can do with slicing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have all the German BMW dealers loaded in a JSON object-currently, 715
    of them. Here is the data for the first dealer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now save the data of interest. Here is a snippet to write the name and
    latitude and longitude of these dealers to a spreadsheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this example, the contents of the `bmw.csv` spreadsheet will
    look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The full source code for scraping this data from BMW is available at [https://github.com/kjam/wswp/blob/master/code/chp9/bmw_scraper.py](https://github.com/kjam/wswp/blob/master/code/chp9/bmw_scraper.py).
  prefs: []
  type: TYPE_NORMAL
- en: Translating foreign content
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that the first screenshot for BMW was in German, but the
    second was in English. This is because the text for the second was translated
    using the Google Translate browser extension. This is a useful technique when
    trying to understand how to navigate a website in a foreign language. When the
    BMW website is translated, the website still works as usual. Be aware, though,
    as Google Translate will break some websites, for example, if the content of a
    select box is translated and a form depends on the original value.
  prefs: []
  type: TYPE_NORMAL
- en: Google Translate is available as the `Google Translate` extension for Chrome,
    the `Google Translator` add-on for Firefox, and can be installed as the`Google
    Toolbar` for Internet Explorer. Alternatively, [http://translate.google.com](http://translate.google.com)
    can be used for translations; however, this is only useful for raw text as the
    formatting is not preserved.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter analyzed a variety of prominent websites and demonstrated how the
    techniques covered in this book can be applied to them. We used CSS selectors
    to scrape Google results, tested a browser renderer and an API for Facebook pages,
    used a `Sitemap` to crawl Gap, and took advantage of an AJAX call to scrape all
    BMW dealers from a map.
  prefs: []
  type: TYPE_NORMAL
- en: You can now apply the techniques covered in this book to scrape websites that
    contain data of interest to you. As demonstrated by this chapter, the tools and
    methods you have learned throughout the book can help you scrape many different
    sites and content from the Internet. I hope this begins a long and fruitful career
    in extracting content from the Web and automating data extraction with Python!
  prefs: []
  type: TYPE_NORMAL
