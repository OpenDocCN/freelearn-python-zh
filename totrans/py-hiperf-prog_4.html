<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Parallel Processing</h1></div></div></div><p>With parallel processing you <a class="indexterm" id="id165"/>can increase the amount of calculations your program can do in a given time without needing a faster processor. <a class="indexterm" id="id166"/>The main idea is to divide a task into many sub-units and employ multiple processors to solve them independently.</p><p>CPUs containing several cores (2, 4, 6, 8, ...) have become a common trend in technology. Increasing the speed of a single processor is costly and problematic; while leveraging the parallel capabilities of cheaper multi-core processors is a feasible route to increase performance.</p><p>Parallel processing lets you <a class="indexterm" id="id167"/>tackle large scale problems. Scientists and engineers commonly run parallel code on supercomputers—huge networks of standard processors—to simulate massive systems. Parallel techniques can also take advantage of graphics chips (a hardware optimized for parallelization).</p><p>Python can be used in all of these domains, allowing us to apply parallel processing to all sorts of problems with simplicity and elegance, opening the door to infinite possibilities.</p><p>In this chapter, we will:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Briefly introduce the fundamentals of parallel processing</li><li class="listitem" style="list-style-type: disc">Illustrate how to parallelize simple problems with the multiprocessing Python library</li><li class="listitem" style="list-style-type: disc">Learn how to write programs with the <strong>IPython parallel</strong> framework</li><li class="listitem" style="list-style-type: disc">Further optimize our program using multithreading with Cython and OpenMP</li></ul></div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec32"/>Introduction to parallel programming</h1></div></div></div><p>In order to parallelize a program, we <a class="indexterm" id="id168"/>need to divide the problem into sub-units that can run independently (or almost independently) from each other.</p><p>A problem where the sub-units are totally independent from each other is called <strong>embarrassingly parallel</strong>. An <a class="indexterm" id="id169"/>element-wise operation on an array is a typical example—the operation needs only to know the element it is handling at the moment. Another example, is our particle simulator—since there are no interactions, each particle can evolve in time independently from the others. Embarrassingly parallel problems are very easy to implement and they perform optimally on parallel architectures.</p><p>Other problems may be divided into sub-units but have to share some data to perform their calculations. In those cases, the implementation is less straightforward and can lead to performance issues because of the communication costs.</p><p>We will illustrate the concept with an example. Imagine you have a particle simulator, but this time the particles attract other particles within a certain distance (as shown in the following figure). To parallelize this problem we divide the simulation box in regions and assign each region to a different processor. If we evolve the system for one step, some particles will interact with particles in a neighboring region. To perform the next iteration, the new particle positions of the neighboring region are required:</p><div><img alt="Introduction to parallel programming" src="img/8458OS_04_1.jpg"/></div><p>Communication between processes is costly and can seriously hinder the performance of parallel programs. There exists <a class="indexterm" id="id170"/>two main ways to handle data communication in parallel programs:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Shared memory</strong></li><li class="listitem" style="list-style-type: disc"><strong>Distributed memory</strong></li></ul></div><p>In shared memory, the <a class="indexterm" id="id171"/>sub-units have access to the same memory space. The advantage of <a class="indexterm" id="id172"/>this approach, is that you don't have to explicitly handle the communication as it is sufficient to write or read from the shared memory. However, problems arise when multiple processes try to access and change the same memory location at the same time. Care should be taken to avoid such conflict using synchronization techniques.</p><p>In the distributed memory model <a class="indexterm" id="id173"/>each process is completely separated from the <a class="indexterm" id="id174"/>others and possesses its own memory space. In this case, communication is handled explicitly between the processes. The communication overhead is typically costlier compared to shared memory, as data can potentially <a class="indexterm" id="id175"/>travel through a network interface.</p><p>One common way to achieve parallelism with the shared memory model is <strong>threads</strong>. Threads are independent sub-tasks that originate from a process and share resources such as memory.</p><p>Python can spawn and handle threads, but they can't be used to increase performance due to the Python interpreter design—only one Python instruction is allowed to run at a time. This mechanism is called <strong>Global Interpreter Lock</strong> (<strong>GIL</strong>). What <a class="indexterm" id="id176"/>happens is that, each time a thread executes a Python statement, a lock is acquired which prevents other threads to run until it is released. The GIL avoids conflicts between threads, simplifying the implementation of the <strong>CPython</strong> interpreter. <a class="indexterm" id="id177"/>Despite this limitation, threads can still be used to provide concurrency in situations where the lock can be released, such as in time-consuming I/O operations or in C extensions.</p><p>The GIL can be completely avoided by using processes instead of threads. Processes don't share the same memory area and are independent from each other—each process has its own interpreter. By using processes, we'll have very few disadvantages: inter-process communication is less efficient than shared memory, but it is more flexible and explicit.</p><div><img alt="Introduction to parallel programming" src="img/8458OS_04_2.jpg"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec33"/>The multiprocessing module</h1></div></div></div><p>The standard <code class="literal">multiprocessing</code> <a class="indexterm" id="id178"/>module can be used to quickly parallelize simple tasks by spawning several processes. Its interface is easy-to-use and includes several utilities to handle task submission and synchronization.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec15"/>The Process and Pool classes</h2></div></div></div><p>You can create a process <a class="indexterm" id="id179"/>that runs independently  by subclassing <code class="literal">multiprocessing.Process</code>. You can extend the <code class="literal">__init__</code> method to initialize resources and you can write the portion of the code destined to the subprocess by implementing a <a class="indexterm" id="id180"/>
<code class="literal">Process.run</code> method. In the following code, we define a process that will wait for one second and print its assigned <code class="literal">id</code>:</p><div><pre class="programlisting">import multiprocessing
import time

class Process(multiprocessing.Process):
    def __init__(self, id):
        super(Process, self).__init__()
        self.id = id

    def run(self):
        time.sleep(1)
        print("I'm the process with id: {}".format(self.id))</pre></div><p>To spawn the process, we have to initialize our <code class="literal">Process</code> object and call the <code class="literal">Process.start</code> method. <a class="indexterm" id="id181"/>Notice that you don't directly call <code class="literal">Process.run</code>: the call to <code class="literal">Process.start</code> will create a new process and, in turn, call the <code class="literal">Process.run</code> method. We can add the <a class="indexterm" id="id182"/>following lines at the end of the script to initialize and start the new process:</p><div><pre class="programlisting">if __name__ == '__main__':
    p = Process(0)
    <strong>p.start()</strong>
</pre></div><p>The instructions after <code class="literal">Process.start</code> will be executed immediately without waiting for the process <code class="literal">p</code> to finish. To wait for the task completion you can use the method <code class="literal">Process.join</code>, as follows:</p><div><pre class="programlisting"> if __name__ == '__main__':
    p = Process(0)
    p.start()
    <strong>p.join()</strong>
</pre></div><p>We can launch in the same way four different processes that will run in parallel. In a serial program, the total required time would be four seconds. Since we run it parallelly, each process will run at the same time, <a class="indexterm" id="id183"/>resulting in a 1-second wallclock time. In the following code, we create four processes and start them parallelly:</p><div><pre class="programlisting">if __name__ == '__main__':
    processes = Process(1), Process(2), Process(3), Process(4)
    [p.start() for p in processes]</pre></div><p>Notice that the order of the execution of parallel processes is unpredictable, it ultimately depends on how the operating system schedules the process execution. You can verify this behavior by running the program multiple times—the order will be different at each run.</p><p>The <code class="literal">multiprocessing</code> <a class="indexterm" id="id184"/>module exposes a convenient interface that makes it easy to assign and distribute tasks to a set of <a class="indexterm" id="id185"/>processes, the <code class="literal">multiprocessing.Pool</code> class.</p><p>The <code class="literal">multiprocessing.Pool</code> class <a class="indexterm" id="id186"/>spawns a set of processes—called <strong>workers</strong>—<a class="indexterm" id="id187"/>and lets submit tasks through the methods <code class="literal">apply</code>/<code class="literal">apply_async</code> and <code class="literal">map</code>/<code class="literal">map_async</code>.</p><p>The <code class="literal">Pool.map</code> method applies a <a class="indexterm" id="id188"/>function to each element of a list and returns the list of results. Its usage is equivalent to the built-in (serial) <code class="literal">map</code>.</p><p>To use a parallel map, you should first initialize a <code class="literal">multiprocessing.Pool</code> object. It takes the number of workers as its first <a class="indexterm" id="id189"/>argument; if not provided, that number will be equal to the number of cores in the system. You can initialize a <code class="literal">multiprocessing.Pool</code> object in the following way:</p><div><pre class="programlisting">pool = multiprocessing.Pool()
pool = multiprocessing.Pool(processes=4)</pre></div><p>Let's see <code class="literal">Pool.map</code> in action. If you have a function that computes the square of a number, you can map the function to the list by calling <code class="literal">Pool.map</code> and passing the function and the list of inputs as arguments, as follows:</p><div><pre class="programlisting">def square(x):
    return x * x

inputs = [0, 1, 2, 3, 4]
<strong>outputs = pool.map(square, inputs)</strong>
</pre></div><p>The <code class="literal">Pool.map_async</code> method is <a class="indexterm" id="id190"/>just like <code class="literal">Pool.map</code> but returns an <code class="literal">AsyncResult</code> <a class="indexterm" id="id191"/>object instead of the actual result. When we call the normal <code class="literal">map</code>, the execution of the main program is stopped until all the workers are finished processing the result. With <code class="literal">map_async</code>, the <code class="literal">AsyncResult</code> object is <a class="indexterm" id="id192"/>returned immediately without blocking the main program and the calculations are done in the background. We can then retrieve the result by using the <code class="literal">AsyncResult.get</code> method at any time, as shown in the following lines:</p><div><pre class="programlisting">outputs_async = pool.map_async(square, inputs)
<strong>outputs = outputs_async.get()</strong>
</pre></div><p>
<code class="literal">Pool.apply_async</code> <a class="indexterm" id="id193"/>assigns a task consisting of a single function to one of the workers. It takes the function and its arguments and returns an <code class="literal">AsyncResult</code> object. <a class="indexterm" id="id194"/>We can obtain an effect similar to <code class="literal">map</code> by using <code class="literal">apply_async</code>, as shown in the following code:</p><div><pre class="programlisting">results_async = [<strong>pool.apply_async(square, i)</strong> for i in range(100))]
results = [<strong>r.get()</strong> for r in results_async]</pre></div><p>As an example, we will implement a canonical, embarassingly parallel program: the <strong>Monte Carlo approximation of pi</strong>.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec16"/>Monte Carlo approximation of pi</h2></div></div></div><p>Imagine we have a square <a class="indexterm" id="id195"/>with a side length of 2 units; its area will be 4 units. Now, <a class="indexterm" id="id196"/>we inscribe a circle with a radius 1 unit in this square, the area of the circle will be <code class="literal">pi * r^2</code>. By substituting the value of <code class="literal">r</code> in the previous equation we get that the numerical value for the area of the circle is <code class="literal">pi * (1)^2 = pi</code>. You can refer to the following figure for a graphical representation.</p><p>If we shoot a lot of random points on this figure, some points will fall into the circle—we'll call them <strong>hits</strong>—while the remaining points—<strong>misses</strong>—will be outside the circle. The idea of the Monte Carlo method is that the <a class="indexterm" id="id197"/>area of the circle will be proportional to the number of hits, while the area of the square will be proportional to the total number of shots. To get the value of <code class="literal">pi</code>, it is <a class="indexterm" id="id198"/>sufficient to divide the area of the circle (equal to <code class="literal">pi</code>) by the area of the square (equal to 4) and solve for <code class="literal">pi</code>:</p><div><pre class="programlisting">hits/total = area_circle/area_square = pi/4
pi = 4 * hits/total</pre></div><div><img alt="Monte Carlo approximation of pi" src="img/8458OS_04_3.jpg"/></div><p>The strategy we will employ in our program will be:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Generate a lot of sample (<em>x</em>, <em>y</em>) numbers in the range (-1, 1)</li><li class="listitem" style="list-style-type: disc">Test if those numbers lie inside the circle by checking if <code class="literal">x**2 + y**2 == 1</code></li></ul></div><p>We first write a serial version and check if it works. Then, we can write the parallel version. The implementation of the serial program is as follows:</p><div><pre class="programlisting">import random

samples = 1000000
hits = 0

for i in range(samples):
    x = random.uniform(-1.0, 1.0)
    y = random.uniform(-1.0, 1.0)
    
    if x**2 + y**2 &lt;= 1:
        hits += 1

pi = 4.0 * hits/samples</pre></div><p>The accuracy of our approximation will improve as we increase the number of samples. You can notice that each loop iteration is independent from the other—this problem is embarassingly parallel.</p><p>To parallelize this code, we can write a function called <code class="literal">sample</code> that corresponds to a single hit-miss check. If the sample hits the circle, the function will return <code class="literal">1</code>; otherwise it will return <code class="literal">0</code>. By running <code class="literal">sample</code> <a class="indexterm" id="id199"/>multiple times and summing the results, we'll get the total number of hits. We can run <code class="literal">sample</code> <a class="indexterm" id="id200"/>over multiple processors with <code class="literal">apply_async</code> and get the results in the following way:</p><div><pre class="programlisting">def sample():
    x = random.uniform(-1.0, 1.0)
    y = random.uniform(-1.0, 1.0)
    
    if x**2 + y**2 &lt;= 1:
        return 1
    else:
        return 0

pool = multiprocessing.Pool()
results_async = [pool.apply_async(sample) for i in range(samples)]
hits = sum(r.get() for r in results_async)</pre></div><p>We can wrap the two versions in the functions <code class="literal">pi_serial</code> and <code class="literal">pi_apply_async</code> (you can find their implementation in the <code class="literal">pi.py</code> file) and benchmark the execution speed as follows:</p><div><pre class="programlisting">
<strong>$ time python -c 'import pi; pi.pi_serial()'</strong>
<strong>real    0m0.734s</strong>
<strong>user    0m0.731s</strong>
<strong>sys    0m0.004s</strong>
<strong>$ time python -c 'import pi; pi.pi_apply_async()'</strong>
<strong>real    1m36.989s</strong>
<strong>user    1m55.984s</strong>
<strong>sys    0m50.386</strong>
</pre></div><p>As shown in the previous benchmark, our first parallel version literally cripples our code. The reason is that the time spent doing the actual calculation is small compared to the overhead required to send and distribute the tasks to the workers.</p><p>To solve the issue, we have to make the <a class="indexterm" id="id201"/>overhead negligible compared to the calculation time. For example, we can ask each worker to handle more than one sample at a time, thus reducing the task communication <a class="indexterm" id="id202"/>overhead. We can write a function <code class="literal">sample_multiple</code> that processes more than one hit and modifies our parallel version by splitting our problem in 10, more intensive tasks as shown in the following code:</p><div><pre class="programlisting">
<strong>def sample_multiple(samples_partial):</strong>
<strong>    return sum(sample() for i in range(samples_partial))</strong>

ntasks = 10
chunk_size = int(samples/ntasks)
pool = multiprocessing.Pool()
results_async = <strong>[pool.apply_async(sample_multiple, chunk_size)</strong>
<strong>                 for i in range(ntasks)]</strong>
hits = sum(r.get() for r in results_async)</pre></div><p>We can wrap this in a function called <code class="literal">pi_apply_async_chunked</code> and run it as follows:</p><div><pre class="programlisting">
<strong>$ time python -c 'import pi; pi.pi_apply_async_chunked()'</strong>
<strong>real    0m0.325s</strong>
<strong>user    0m0.816s</strong>
<strong>sys    0m0.008s</strong>
</pre></div><p>The results are much better; we more than doubled the speed of our program. You can also notice that the <code class="literal">user</code> metric is larger than <code class="literal">real</code>: the total CPU time is larger than the total time because more than one CPU worked at the same time. If you increase the number of samples, you will notice that the ratio of communication to calculation decreases, giving even better speedups.</p><p>Everything is nice and simple when <a class="indexterm" id="id203"/>dealing with embarassingly parallel <a class="indexterm" id="id204"/>problems. But sometimes, you have to share data between processes.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec17"/>Synchronization and locks</h2></div></div></div><p>Even if <code class="literal">multiprocessing</code> uses <a class="indexterm" id="id205"/>processes (with their own independent memory), it lets you define certain variables and arrays as shared memory. You can define a shared variable by using <code class="literal">multiprocessing.Value</code> passing its data type as a string (<code class="literal">i</code> integer, <code class="literal">d</code> double, <code class="literal">f</code> float, and so on). You can update the content of the variable through the <code class="literal">value</code> attribute, as shown in the following code snippet:</p><div><pre class="programlisting">shared_variable = multiprocessing.Value('f')
shared_variable.value = 0</pre></div><p>When using shared memory, you should be aware of concurrent accesses. Imagine you have a shared integer variable and each process increments its value multiple times. You would define a process class as follows:</p><div><pre class="programlisting">class Process(multiprocessing.Process):

    def __init__(self, <strong>counter</strong>):
        super(Process, self).__init__()
        <strong>self.counter = counter</strong>

    def run(self):
        for i in range(1000):
            <strong>self.counter.value += 1</strong>
</pre></div><p>You can initialize the shared variable in the main program and pass it to <code class="literal">4</code> processes, as shown in the following code:</p><div><pre class="programlisting">def main():
    counter = multiprocessing.Value('i', lock=True)
    counter.value = 0

    processes = [Process(counter) for i in range(4)]
    [p.start() for p in processes]
    [p.join() for p in processes] # processes are done
    print(counter.value)
main()</pre></div><p>If you run this program (<code class="literal">shared.py</code> in the code directory) you will notice that the final value of <code class="literal">counter</code> is not 4000, but it has random values (on my machine they are between 2000 and 2500). If we assume that the arithmetic is correct, we can conclude that there's a problem with the parallelization.</p><p>What happens is that multiple processes are trying to access the same shared variable at the same time. The situation is best explained by looking at the following figure. In a serial execution, the first process reads (the number <code class="literal">0</code>), increments it, and writes the new value (<code class="literal">1</code>); the second process reads the new value (<code class="literal">1</code>), increments it, and writes it again (<code class="literal">2</code>). In the parallel execution, the two <a class="indexterm" id="id206"/>processes read the value (<code class="literal">0</code>), increment it, and write it (<code class="literal">1</code>) at the same time, leading to a wrong answer.</p><div><img alt="Synchronization and locks" src="img/8458OS_04_4.jpg"/></div><p>To solve this problem, we need to synchronize the access to this variable so that only one process at a time can access, increment, and write the value on the shared variable. This feature is provided by the <a class="indexterm" id="id207"/>
<code class="literal">multiprocessing.Lock</code> class. A lock can be acquired and released through the <code class="literal">acquire</code> and <code class="literal">release</code> methods, or by using the lock as a context manager. When a process acquires a lock, other processes are prevented to acquire it until the lock is released.</p><p>We can define a global lock, and use it as a context manager to restrict the access to the counter, as shown in the following code snippet:</p><div><pre class="programlisting">
<strong>lock = multiprocessing.Lock()</strong>

class Process(multiprocessing.Process):

    def __init__(self, counter):
        super(Process, self).__init__()
        self.counter = counter

    def run(self):
        for i in range(1000):
            <strong>with lock: # acquire the lock</strong>
<strong>                self.counter.value += 1</strong>
<strong>            # release the lock</strong>
</pre></div><p>Synchronization primitives such as locks are essential to solve many problems but you should <a class="indexterm" id="id208"/>avoid overusing them because they can decrease the performance of your program.</p><div><div><h3 class="title"><a id="note11"/>Note</h3><p>
<code class="literal">multiprocessing</code> includes <a class="indexterm" id="id209"/>other communication and synchronization tools, you can refer to the official documentation for a complete reference:</p><p>
<a class="ulink" href="http://docs.python.org/3/library/multiprocessing.html">http://docs.python.org/3/library/multiprocessing.html</a>
</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec34"/>IPython parallel</h1></div></div></div><p>IPython's power is not limited <a class="indexterm" id="id210"/>to its advanced shell. Its <code class="literal">parallel</code> package includes a framework to setup and run calculations on single and multi-core machines, as well as on multiple nodes connected to a network. IPython is great because it gives an interactive twist to parallel computing and provides a common interface to different communication protocols.</p><p>To use <code class="literal">IPython.parallel</code>, you have to start a set of workers—<a class="indexterm" id="id211"/>
<strong>Engines</strong>—that are managed by a <a class="indexterm" id="id212"/>
<strong>Controller</strong> (an entity that mediates the communication between the client and the engines). The approach is totally different from multiprocessing; you start the worker processes separately, and they will wait indefinitely, listening for commands from the client.</p><p>To start the controller and a set of engines (by default, one engine per processing unit) you can use the <code class="literal">ipcluster</code> shell command, as follows:</p><div><pre class="programlisting">
<strong>$ ipcluster start</strong>
</pre></div><p>With <code class="literal">ipcluster</code> you can also set up multiple nodes to distribute your calculations over a network by writing a custom profile. You can refer to the official documentation for specific instructions at the following website:</p><p>
<a class="ulink" href="http://ipython.org/ipython-doc/dev/parallel/parallel_process.html">http://ipython.org/ipython-doc/dev/parallel/parallel_process.html</a>
</p><p>After starting the controller and the <a class="indexterm" id="id213"/>engines, we can use an IPython shell to perform calculations in parallel. IPython provides two basic interfaces (or views): <strong>direct</strong> and <strong>task-based</strong>.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec18"/>Direct interface</h2></div></div></div><p>The direct interface <a class="indexterm" id="id214"/>lets you issue commands explicitly to each of the computing units. The interface is intuitive, flexible, and easy-to-use, especially when used in an interactive session.</p><p>After starting the engines, you have to start an IPython session in a separate shell to interact with them. By creating a client, you can establish a connection to the controller. In the following code, we import the <code class="literal">Client</code> class and create an instance:</p><div><pre class="programlisting">In [1]: from IPython.parallel import Client
In [2]: rc = Client()</pre></div><p>The attribute <code class="literal">Client.ids</code> will give you a list of integers representing the available engines, as shown in the following code snippet:</p><div><pre class="programlisting">In [3]: rc.ids
Out[4]: [0, 1, 2, 3]</pre></div><p>We can issue commands to the engines by obtaining a <code class="literal">DirectView</code> instance. You can get a <code class="literal">DirectView</code> instance by either indexing the <code class="literal">Client</code> instance or by calling the <code class="literal">DirectView.direct_view</code> method. <a class="indexterm" id="id215"/>The following code shows different ways to obtain a <code class="literal">DirectView</code> instance from the <a class="indexterm" id="id216"/>previously created <code class="literal">Client</code>:</p><div><pre class="programlisting">In [5]: dview = rc[0] # Select the first engine
In [6]: dview = rc[::2] # Select every other engine
In [7]: dview = rc[:] # Selects all the engines
In [8]: dview = rc.direct_view('all') # Alternative</pre></div><p>You can treat the engines like fresh IPython sessions. At the finest level, you can execute commands remotely by using the <code class="literal">DirectView.execute</code> method:</p><div><pre class="programlisting">In [9]: dview.execute('a = 1')</pre></div><p>The command will be sent and executed individually by each engine. The return value will be an <code class="literal">AsyncResult </code>object and the actual return value can be retrieved using the <code class="literal">get</code> method.</p><p>As shown in the following code, you can retrieve the data contained in a remote variable by <a class="indexterm" id="id217"/>using the <a class="indexterm" id="id218"/>
<code class="literal">DirectView.pull</code> method and send the data to a remote variable with <a class="indexterm" id="id219"/>the <code class="literal">DirectView.push</code> method. The <code class="literal">DirectView</code> class also supports a convenient dictionary-like interface:</p><div><pre class="programlisting">In [10]: dview.pull('a').get() # equivalent: dview['a']
Out[10]: [1, 1, 1, 1]
In [11]: dview.push({'a': 2}) # equivalent: dview['a'] = 2</pre></div><p>It is possible to send and retrieve every object that can be serialized using the <code class="literal">pickle</code> module. On top of that, special handling is reserved for data structures such as <strong>NumPy</strong> arrays to increase the efficiency.</p><p>If you issue a statement that causes <a class="indexterm" id="id220"/>an exception, you will receive a summary of the exceptions in each engine:</p><div><pre class="programlisting">In [12]: res = dview.execute('a = *__*') # Invalid
In [13]: res.get()
<strong>[0:execute]</strong>:
  File "&lt;ipython-input-3-945a473d5cbb&gt;", line 1
    a = *__*
        ^
SyntaxError: invalid syntax
 
<strong>[1:execute]</strong>:
  File "&lt;ipython-input-3-945a473d5cbb&gt;", line 1
    a = *__*
        ^
SyntaxError: invalid syntax
<strong>[2: execute]:</strong>
...</pre></div><p>Engines should be treated as independent IPython sessions, and imports and custom-defined functions must <a class="indexterm" id="id221"/>be synchronized over the network. To import some libraries, both locally and in the engines, you <a class="indexterm" id="id222"/>can use the <code class="literal">DirectView.sync_imports</code> context manager:</p><div><pre class="programlisting">with dview.sync_imports():
    import numpy
<code class="literal">    </code># The syntax import _ as _ is not supported</pre></div><p>To submit calculations to the engines, <code class="literal">DirectView</code> provides some utilities for common use cases such as map and apply. The <a class="indexterm" id="id223"/>
<code class="literal">DirectView.map</code> method works similarly to <code class="literal">Pool.map_async</code>, as shown in the <a class="indexterm" id="id224"/>following code snippet. You map a function to a sequence, returning an <code class="literal">AsyncResult</code> object</p><div><pre class="programlisting">In [14]: a = range(100)
In [15]: def square(x): return x * x
In [16]: result_async = dview.map(square, a)
In [17]: result = result_async.get()</pre></div><p>IPython provides a more <a class="indexterm" id="id225"/>convenient map implementation through the <code class="literal">DirectView.parallel</code> decorator. <a class="indexterm" id="id226"/>If you apply the decorator on a function, the function will now have a <code class="literal">map</code> method that can be applied to a sequence. In the following code, we apply the parallel decorator to the <code class="literal">square</code> function and map it over a series of numbers:</p><div><pre class="programlisting">In [18]: @dview.parallel()
    ...: def square(x):
    ...:     return x * x
In [19]: square.map(range(100))</pre></div><div><div><h3 class="title"><a id="tip10"/>Tip</h3><p>To get the non-blocking version of <code class="literal">map</code>, you can either use the <code class="literal">DirectView.map_sync</code> method or pass the <code class="literal">block=True</code> option to the <code class="literal">DirectView.parallel</code> decorator.</p></div></div><p>The <code class="literal">DirectView.apply</code> method behaves <a class="indexterm" id="id227"/>in a different way <a class="indexterm" id="id228"/>than <code class="literal">Pool.apply_async</code>. The function gets executed on <em>every</em> engine. For <a class="indexterm" id="id229"/>example, if we have selected four engines and we apply the <code class="literal">square</code> function, the function gets executed once per engine and it returns four results, as shown in the following code snippet:</p><div><pre class="programlisting">In [20]: def square(x):
            return x * x
In [21]: result_async = dview.apply(square, 2)
In [22]: result_async.get()
Out[22]: [4, 4, 4, 4]</pre></div><p>The <code class="literal">DirectiView.remote</code> <a class="indexterm" id="id230"/>decorator lets you create a <a class="indexterm" id="id231"/>function that will run directly on each engine. Its usage is as follows:</p><div><pre class="programlisting">In [23]: @dview.remote()
    ...: def square(x):
    ...:     return x * x
    ...:
In [24]: square(2)
Out[24]: [4, 4, 4, 4]</pre></div><p>The <code class="literal">DirectView</code> also provides two other kinds of communication scheme: <strong>scatter</strong> and <strong>gather</strong>.</p><p>Scatter distributes a <a class="indexterm" id="id232"/>list of inputs to the engines. Imagine you have four <a class="indexterm" id="id233"/>inputs and four engines; you can distribute those inputs in a remote variable with <code class="literal">DirectView.scatter</code>, as follows:</p><div><pre class="programlisting">In [25]: dview.scatter('a', [0, 1, 2, 3])
In [26]: dview['a']
Out[26]: [[0], [1], [2], [3]]</pre></div><p>Scatter will try to distribute the inputs as equally as possible even when the number of inputs is not a multiple of the number of engines. The following code shows how a list of 11 computations gets processed in three batches of three items per batch and one batch of two items:</p><div><pre class="programlisting">In [13]: dview.scatter('a', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
In [14]: dview['a']
Out[14]: [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]</pre></div><p>The <code class="literal">gather</code> function <a class="indexterm" id="id234"/>simply retrieves the <a class="indexterm" id="id235"/>scattered values and merges them back. In the following snippet, we merge <a class="indexterm" id="id236"/>back the scattered results:</p><div><pre class="programlisting">In [17]: dview.gather('a').get()
Out[17]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</pre></div><p>We can use the <code class="literal">scatter</code> and <code class="literal">gather</code> functions to parallelize one of our simulations. In our system, each particle is independent from the other, therefore, we can use <code class="literal">scatter</code> and <code class="literal">gather</code> to divide the particles equally between the available engines, evolve them, and get the particles back from the engines.</p><p>At first, we have to <a class="indexterm" id="id237"/>set up the engines. <a class="indexterm" id="id238"/>The <code class="literal">ParticleSimulator</code> class should be made available to all the engines.</p><p>Remember that the engines have started in a separate process and the <code class="literal">simul</code> module should be importable by them. You can achieve this in two ways:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">By launching <code class="literal">ipcluster</code> in the directory, where <code class="literal">simul.py</code> is located</li><li class="listitem" style="list-style-type: disc">By adding that directory to <code class="literal">PYTHONPATH</code></li></ul></div><p>If you're using the code examples, don't forget to compile the Cython extensions using <code class="literal">setup.py</code>.</p><p>In the following code, we create the particles and obtain a <code class="literal">DirectView</code> instance:</p><div><pre class="programlisting">from random import uniform
from simul import Particle
from IPython.parallel import Client

particles = [Particle(uniform(-1.0, 1.0),
                      uniform(-1.0, 1.0),
                      uniform(-1.0, 1.0)) for i in range(10000)]
rc = Client()
dview = rc[:]</pre></div><p>Now, we can scatter the particles to a remote variable <code class="literal">particle_chunk</code>, perform the particle evolution using <a class="indexterm" id="id239"/>
<code class="literal">DirectView.execute</code> and retrieve the particles. We do this using <code class="literal">scatter</code>, <code class="literal">execute</code>, and <code class="literal">gather</code>, as shown in the following code:</p><div><pre class="programlisting">dview.scatter('particle_chunk', particles, block=True)

dview.execute('from simul import ParticleSimulator')
dview.execute('simulator = ParticleSimulator(particle_chunk)')
dview.execute('simulator.evolve_cython(0.1)')

particles = dview.gather('particle_chunk', block=True)</pre></div><p>We can now wrap the parallel version and <a class="indexterm" id="id240"/>benchmark it against the serial one (refer to the file <code class="literal">simul_parallel.py</code>) in the following way:</p><div><pre class="programlisting">In [1]: from simul import benchmark
In [2]: from simul_parallel import scatter_gather
In [5]: %timeit benchmark(10000, 'cython')
1 loops, best of 3: 1.34 s per loop
In [6]: %timeit scatter_gather(10000)
1 loops, best of 3: 720 ms per loop</pre></div><p>The code is extremely simple and gives us a 2x speedup, scalable on any number of engines.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec19"/>Task-based interface</h2></div></div></div><p>IPython has an interface that can <a class="indexterm" id="id241"/>handle computing tasks in a smart way. <a class="indexterm" id="id242"/>While this implies a less flexible interface from the user point of view, it can improve performance by balancing the load on the engines and by re-submitting failed jobs. In this section, we will introduce the <code class="literal">map</code> and <code class="literal">apply</code> functions in the task-based interface.</p><p>The task interface is provided by the <code class="literal">LoadBalancedView</code> class, which can be obtained from a client using <a class="indexterm" id="id243"/>the <a class="indexterm" id="id244"/>
<code class="literal">load_balanced_view</code> method, as follows:</p><div><pre class="programlisting">In [1]: from IPython.parallel import Client
In [2]: rc = Client()
In [3]: tview = rc.load_balanced_view()</pre></div><p>At this point we can run some <a class="indexterm" id="id245"/>tasks <a class="indexterm" id="id246"/>using <code class="literal">map</code> and <code class="literal">apply</code>. The <code class="literal">LoadBalancedView</code> class works similarly to <code class="literal">multiprocessing.Pool</code>, the tasks are submitted and handled by a scheduler; in the case of <code class="literal">LoadBalancedView</code>, the task assignment is based on how much load is present on an engine at a given time, ensuring that all the engines are working without downtimes.</p><p>It's helpful to explain an important difference between <code class="literal">apply</code> in <code class="literal">DirectView</code> and <code class="literal">LoadBalancedView</code>. A call to <a class="indexterm" id="id247"/>
<code class="literal">DirectView.apply</code> will run on <em>every</em> selected engine, while a call to <code class="literal">LoadBalancedView.apply</code> <a class="indexterm" id="id248"/>will schedule a <em>single</em> task to one of the engines. In the first case, the result <a class="indexterm" id="id249"/>will be a list, and in the latter, it will be a single value, as shown in the following code snippet:</p><div><pre class="programlisting">In [4]: dview = rc[:]
In [5]: tview = rc.load_balanced_view()
In [6]: def square(x):
   ...:     return x * x
   ...:
In [7]: dview.apply(square, 2).get()
Out[7]: [4, 4, 4, 4]
In [8]: tview.apply(square, 2).get()
Out[8]: 4</pre></div><p>
<code class="literal">LoadBalancedView</code> is also able to <a class="indexterm" id="id250"/>handle <a class="indexterm" id="id251"/>failures and run tasks on engines when certain conditions are met. This feature is provided through a dependency system. We will not cover this aspect in this book, but interested readers can refer to the official documentation at the following link:</p><p>
<a class="ulink" href="http://ipython.org/ipython-doc/rel-1.1.0/parallel/parallel_task.html">http://ipython.org/ipython-doc/rel-1.1.0/parallel/parallel_task.html</a>
</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec35"/>Parallel Cython with OpenMP</h1></div></div></div><p>Cython provides a convenient interface to perform shared-memory parallel processing through <strong>OpenMP</strong>. This lets <a class="indexterm" id="id252"/>you write extremely efficient parallel code directly in Cython <a class="indexterm" id="id253"/>without having to create a C wrapper.</p><p>OpenMP is a specification to write multithreaded programs, and includes series of C preprocessor directives to manage threads; these include communication patterns, load balancing, and synchronization features. Several C/C++ and Fortran compilers (including GCC) implement the OpenMP API.</p><p>Let's introduce Cython parallel features with a small example. Cython provides a simple API based on OpenMP in the <a class="indexterm" id="id254"/>
<code class="literal">cython.parallel</code> module. The simplest construct is <code class="literal">prange</code>: a construct that automatically distributes loop operations in multiple threads.</p><p>First of all, we can write a serial version <a class="indexterm" id="id255"/>of a program that computes the square of each element of a NumPy array in the <code class="literal">hello_parallel.pyx</code> file. We get a buffer as input and we create an output array by populating it with the squares of the input array elements. </p><p>The serial version, <code class="literal">square_serial</code>, is shown in the following code snippet:</p><div><pre class="programlisting">import numpy as np

def square_serial(double[:] inp):
    cdef int i, size
    cdef double[:] out
    size = inp.shape[0]
    out_np = np.empty(size, 'double')
    out = out_np
    
    for i in range(size):
        out[i] = inp[i]*inp[i]
    
    return out_np  </pre></div><p>Now, we can change the loop in a parallel version by substituting the range call with <code class="literal">prange</code>. There's a caveat, you need to make sure that the body of the loop is interpreter-free. As already explained, to make use of threads we need to release the GIL, since interpreter calls acquire and release the GIL, we should avoid them. Failure in doing so will result in compilation errors.</p><p>In Cython, you can release the GIL <a class="indexterm" id="id256"/>by using <code class="literal">nogil</code>, as follows:</p><div><pre class="programlisting">with nogil:
    for i in <strong>prange(size)</strong>:
        out[i] = inp[i]*inp[i]</pre></div><p>Alternatively, you can use the convenient option <code class="literal">nogil=True</code> of <code class="literal">prange</code> that will automatically wrap the loop in a <a class="indexterm" id="id257"/>
<code class="literal">nogil</code> block:</p><div><pre class="programlisting">for i in <strong>prange(size, nogil=True)</strong>:
    out[i] = inp[i]*inp[i]</pre></div><div><div><h3 class="title"><a id="note12"/>Note</h3><p>Attempts to call Python code in a <code class="literal">prange</code> block results in an error. This includes assignment operations, function calls, objects initialization, and so on. To include such operations in a <code class="literal">prange</code> block (you may want to do so for debugging purposes) you have to re-enable the GIL using the <code class="literal">with gil</code> statement:</p><div><pre class="programlisting">for i in prange(size, nogil=True):
    out[i] = inp[i]*inp[i]
    <strong>with gil:  </strong>
<strong>        x = 0 # Python assignment</strong>
</pre></div></div></div><p>At this point, we need to recompile our extension. We need to change <code class="literal">setup.py</code> to enable OpenMP support. You have to specify the <a class="indexterm" id="id258"/>GCC option <code class="literal">-fopenmp</code> using the <a class="indexterm" id="id259"/>
<code class="literal">Extension</code> class in <code class="literal">distutils</code> and pass it to the <code class="literal">cythonize</code> function. The following code shows the complete <code class="literal">setup.py</code> file:</p><div><pre class="programlisting">from distutils.core import setup
<strong>from distutils.extension import Extension</strong>
from Cython.Build import cythonize

<strong>hello_parallel = Extension('hello_parallel',</strong>
<strong>                           ['hello_parallel.pyx'],</strong>
<strong>                           extra_compile_args=['-fopenmp'],</strong>
<strong>                           extra_link_args=['-fopenmp'])</strong>

setup(
   name='Hello',
   ext_modules = cythonize(['cevolve.pyx', <strong>hello_parallel</strong>]),
)</pre></div><p>Now that we know how to use <code class="literal">prange</code>, we can quickly parallelize the Cython version of our <code class="literal">ParticleSimulator</code>.</p><p>In the following code, we can take a look at the <code class="literal">c_evolve</code> function contained in the Cython module <code class="literal">cevolve.pyx</code> <a class="indexterm" id="id260"/>that we wrote in <a class="link" href="ch02.html" title="Chapter 2. Fast Array Operations with NumPy">Chapter 2</a>, <em>Fast Array Operations with NumPy</em>:</p><div><pre class="programlisting">def c_evolve(double[:, :] r_i,double[:] ang_speed_i,
             double timestep,int nsteps):

    # cdef declarations

    for i in range(nsteps):
        for j in range(nparticles):
            # loop body</pre></div><p>The first thing we have to do is invert the order of the loops; we want the outermost loop to be the parallel one, where each iteration is independent from the other. Since the particles don't interact with each other, we can change the order of iteration safely, as shown in the following code snippet:</p><div><pre class="programlisting">  <strong>for j</strong> in range(nparticles):
        <strong>for i</strong> in range(nsteps):

            # loop body</pre></div><p>At that point we can parallelize the loop using <code class="literal">prange</code>, we already removed the interpreter-related calls when we added static <a class="indexterm" id="id261"/>typing, so the <code class="literal">nogil</code> block can be applied safely, as follows:</p><div><pre class="programlisting">for j in prange(nparticles, nogil=True)
</pre></div><p>We can now wrap the two different versions into separate functions and we can time them, as follows:</p><div><pre class="programlisting">In [3]: %timeit benchmark(10000, 'openmp')
1 loops, best of 3: 599 ms per loop
In [4]: %timeit benchmark(10000, 'cython')
1 loops, best of 3: 1.35 s per loop</pre></div><p>With OpenMP, we are able to obtain a <a class="indexterm" id="id262"/>significant speedup compared to the serial Cython version by changing a single line of code.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec36"/>Summary</h1></div></div></div><p>Parallel processing is an effective way to increase the speed of your programs or to handle large amounts of data. Embarassingly parallel problems are excellent candidates for parallelization and lead to a straightforward implementation and optimal scaling.</p><p>In this chapter, we illustrated the basics of parallel programming in Python. We learned how to use multiprocessing to easily parallelize programs with the tools already included in Python. Another more powerful tool for parallel processing is IPython parallel. This package allows you to interactively prototype parallel programs and manage a network of computing nodes effectively. Finally, we explored the easy-to-use multithreading capabilities of Cython and OpenMP.</p><p>During the course of this book, we learned the most effective techniques to design, benchmark, profile, and optimize Python applications. NumPy can be used to elegantly rewrite Python loops, and if it is not enough, you can use Cython to generate efficient C code. At the last stage, you can easily parallelize your program using the tools presented in this chapter.</p></div></body></html>