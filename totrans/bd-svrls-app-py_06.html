<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">Scaling Up Serverless Architectures</h1>
                
            
            
                
<p class="calibre2">So far, we have learned how to build, monitor, and log serverless functions. In this chapter, we will be learning concepts and engineering techniques that will help scale up serverless applications to be distributed, and that will also enable them to handle heavy workloads with high standards of security and throughput. In this chapter, we will also use some third-party tools, such as Ansible, to scale up our Lambda functions. We will be scaling up our Lambda functions to spawn a distributed serverless architecture, which will involve spawning multiple servers (or instances in the AWS environment). You therefore need to keep that in mind while following the examples mentioned in this chapter.</p>
<p class="calibre2">This chapter assumes a working knowledge of a provisioning tool, such as <strong class="calibre4">Ansible</strong>, <strong class="calibre4">Chef</strong>, and so on. You can quickly read up on or refresh your knowledge of these on their respective sites, where they have quick tutorials. If not, then you can safely skip this chapter and move on to the next.</p>
<p class="calibre2">This chapter consists of five sections, which cover all of the basics of scaling up serverless architectures and will set you up for building bigger, complex serverless architectures:</p>
<ul class="calibre9">
<li class="calibre10">Third-party orchestration tools</li>
<li class="calibre10">The creation and termination of servers</li>
<li class="calibre10">Security best practices</li>
<li class="calibre10">Difficulties of scaling up</li>
<li class="calibre10">Handling difficulties</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Third-party orchestration tools</h1>
                
            
            
                
<p class="calibre2">In this section, we will learn and become versed in the concept of infrastructure provisioning and orchestration. We will be exploring a couple of tools, namely Chef and Ansible. Let's get started by following these steps:</p>
<ol class="calibre13">
<li value="1" class="calibre10">We will begin with getting introduced to Chef. You can visit the official website of Chef at <a href="https://www.chef.io/chef/" class="calibre8">https://www.chef.io/chef/</a>:</li>
</ol>
<div><img src="img/00209.jpeg" class="calibre48"/></div>
<ol start="2" class="calibre13">
<li value="2" class="calibre10">Chef has a very good set of tutorials for getting your hands dirty. These are organized in the form of mini 10 to 15 minute tutorials for easy consumption. Head over to <a href="https://learn.chef.io/" class="calibre8">https://learn.chef.io/</a> to access them:</li>
</ol>
<div><img src="img/00210.jpeg" class="calibre48"/></div>
<ol start="3" class="calibre13">
<li value="3" class="calibre10">For getting started with infrastructure provisioning and orchestrating, you can refer to the Chef documentation here: <a href="https://docs.chef.io/" class="calibre8">https://docs.chef.io/</a>. The page looks like this:</li>
</ol>
<div><img src="img/00211.jpeg" class="calibre48"/></div>
<ol start="4" class="calibre13">
<li value="4" class="calibre10">You can refer to the AWS Driver Resources page in the documentation to understand how to interact with various AWS services via Chef at: <a href="https://docs.chef.io/provisioning_aws.html" class="calibre8">https://docs.chef.io/provisioning_aws.html</a>. The page looks like this:</li>
</ol>
<div><img src="img/00212.jpeg" class="calibre48"/></div>
<ol start="5" class="calibre13">
<li value="5" class="calibre10">You can also refer to the aws Cookbook for the same purpose, too. This resource has very good documentation and APIs for interacting with several AWS services. The URL of this documentation is <a href="https://supermarket.chef.io/cookbooks/aws" class="calibre8">https://supermarket.chef.io/cookbooks/aws</a>. The page looks like this:</li>
</ol>
<div><img src="img/00213.jpeg" class="calibre48"/></div>
<ol start="6" class="calibre13">
<li value="6" class="calibre10">A detailed description of the cookbook can be seen when you scroll down, directly after the title of the cookbook:</li>
</ol>
<div><img src="img/00214.jpeg" class="calibre48"/></div>
<ol start="7" class="calibre13">
<li value="7" class="calibre10">One other good tool for provisioning and orchestrating software resources is Ansible. This helps software engineers write code for automating several parts of their infrastructure via <em class="calibre28">yaml scripts</em>. Similar to the Chef environment, these scripts are called <strong class="calibre1">cookbooks</strong>.</li>
<li value="8" class="calibre10">We will be using this tool for learning how to provision our infrastructure in the subsequent sections. The documentation for Ansible can be found at <a href="http://docs.ansible.com/" class="calibre8">http://docs.ansible.com/</a>:</li>
</ol>
<div><img src="img/00215.jpeg" class="calibre157"/></div>
<ol start="9" class="calibre13">
<li value="9" class="calibre10">The product, ANSIBLE TOWER, is out of scope for this book. We will be learning and be working with ANSIBLE CORE, which is the flagship product of Ansible and its parent company, Red Hat.</li>
<li value="10" class="calibre10">Ansible has a very helpful video for helping you better understand and make sense of the tool. It can be accessed when you click on the Quick Start Video link in the documentation page: </li>
</ol>
<div><img src="img/00216.jpeg" class="calibre158"/></div>
<ol start="11" class="calibre13">
<li value="11" class="calibre10">After watching the video, you can proceed to understand the product from the documentation itself. The complete documentation of Ansible can be accessed at: <a href="http://docs.ansible.com/ansible/latest/index.html" class="calibre8">http://docs.ansible.com/ansible/latest/index.html</a>:</li>
</ol>
<div><img src="img/00217.jpeg" class="calibre48"/></div>
<ol start="12" class="calibre13">
<li value="12" class="calibre10">The EC2 module is the one we will be using for provisioning and orchestrating our AWS EC2 instances. This part of the documentation has a very clear explanation and demonstration of starting up and terminating EC2 instances, along with adding and mounting volumes; it also enables us to provision our EC2 instances into our own specific <strong class="calibre1">Virtual Private Cloud</strong> (<strong class="calibre1">VPC</strong>) and/or in our own <strong class="calibre1">Security Groups</strong> (<strong class="calibre1">SGs</strong>). The EC2 documentation screen looks like this:</li>
</ol>
<div><img src="img/00218.jpeg" class="calibre48"/></div>
<ol start="13" class="calibre13">
<li value="13" class="calibre10">You can find this at the following URL of Ansible Core's documentation: <a href="http://docs.ansible.com/ansible/latest/ec2_module.html" class="calibre8">http://docs.ansible.com/ansible/latest/ec2_module.html</a>. When you scroll down further, you can see several examples of how to use the EC2 module of Ansible for various tasks concerning AWS EC2 instances. Some of them can be seen as follows:</li>
</ol>
<div><img src="img/00219.jpeg" class="calibre48"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">The creation and termination of servers</h1>
                
            
            
                
<p class="calibre2">In this chapter, we will learn how to use some third-party tools that will help us in building the required architecture. Like all of the sections in this chapter, the information will be broken down into steps:</p>
<ol class="calibre13">
<li value="1" class="calibre10">The first tool we will be learning about is Ansible. It is a provisioning and orchestrating tool, that helps in automating several parts of an infrastructure. Depending on when you are reading this book, the Ansible project's homepage (<a href="https://www.ansible.com/" class="calibre8">https://www.ansible.com/</a>) will look something like this:</li>
</ol>
<div><img src="img/00220.jpeg" class="calibre48"/></div>
<ol start="2" class="calibre13">
<li value="2" class="calibre10">The installation process for Ansible is different for different operating systems. The instructions for some popular operating systems are as follows:</li>
</ol>
<ul class="calibre9">
<li class="front-matter">
<ul class="calibre20">
<li class="calibre10"><strong class="calibre1">For Ubuntu</strong>:</li>
</ul>
</li>
</ul>
<pre class="calibre159"><strong class="calibre1">sudo apt-get update</strong><br class="title-page-name"/><strong class="calibre1">sudo apt-get install software-properties-common</strong><br class="title-page-name"/><strong class="calibre1">sudo apt-add-repository ppa:ansible/ansible</strong><br class="title-page-name"/><strong class="calibre1">sudo apt-get update</strong><br class="title-page-name"/><strong class="calibre1">sudo apt-get install ansible</strong></pre>
<ul class="calibre9">
<li class="front-matter">
<ul class="calibre20">
<li class="calibre10"><strong class="calibre1">For Linux</strong>:</li>
</ul>
</li>
</ul>
<pre class="calibre159"><strong class="calibre1">git clone https://github.com/ansible/ansible.git</strong><br class="title-page-name"/><strong class="calibre1">cd ./ansible</strong><br class="title-page-name"/><strong class="calibre1">make rpm</strong><br class="title-page-name"/><strong class="calibre1">sudo rpm -Uvh ./rpm-build/ansible-*.noarch.rpm</strong></pre>
<ul class="calibre9">
<li class="front-matter">
<ul class="calibre20">
<li class="calibre10"><strong class="calibre1">For OS X</strong>:</li>
</ul>
</li>
</ul>
<pre class="calibre159"><strong class="calibre1">sudo pip install ansible</strong></pre>
<ol start="3" class="calibre13">
<li value="3" class="calibre10">Now, we will understand the concept of <strong class="calibre1">nohup</strong>. So, you don't need to have a persistent SSH connection to the server for making a <kbd class="calibre12">nohup</kbd> command run, therefore we will be using this technique for running our master–server architecture (to know more about nohup refer to: <a href="https://en.wikipedia.org/wiki/Nohup" class="calibre8">https://en.wikipedia.org/wiki/Nohup</a>).</li>
</ol>
<p>Let's look at its definition on Wikipedia (from the time of writing this book), <strong class="calibre161">nohup</strong> is a POSIX command to ignore the HUP (hangup) signal. The HUP signal is, by convention, the way a terminal warns dependent processes of logout.</p>
<ol start="4" class="calibre13">
<li value="4" class="calibre10">We will now learn how to provision servers from Ansible, SSH into them, run a simple <kbd class="calibre12">apt-get update</kbd> task in them, and terminate them. From this, you will learn how to write Ansible scripts, as well as understand how Ansible handles the provisioning of cloud resources. The following Ansible script will help you understand how to provision an EC2 instance:</li>
</ol>
<pre class="calibre17">- hosts: localhost
  connection: local
  remote_user: test
  gather_facts: no
<br class="title-page-name"/>  environment:
    AWS_ACCESS_KEY_ID: "{{ aws_id }}"
    AWS_SECRET_ACCESS_KEY: "{{ aws_key }}"

    AWS_DEFAULT_REGION: "{{ aws_region }}"<br class="title-page-name"/><br class="title-page-name"/>  tasks: <br class="title-page-name"/>- name: Provisioning EC2 instaces <br class="title-page-name"/>  ec2: <br class="title-page-name"/>    assign_public_ip: no<br class="title-page-name"/>    aws_access_key: "{{ access_key }}"<br class="title-page-name"/>    aws_secret_key: "{{ secret_key }}"<br class="title-page-name"/>    region: "{{ aws_region }}"<br class="title-page-name"/>    image: "{{ image_instance }}"<br class="title-page-name"/>    instance_type: "{{ instance_type }}"<br class="title-page-name"/>    key_name: "{{ ssh_keyname }}"<br class="title-page-name"/>    state: present<br class="title-page-name"/>    group_id: "{{ security_group }}"<br class="title-page-name"/>    vpc_subnet_id: "{{ subnet }}"<br class="title-page-name"/>    instance_profile_name: "{{ Profile_Name }}"<br class="title-page-name"/>    wait: true<br class="title-page-name"/>    instance_tags: <br class="title-page-name"/>      Name: "{{ Instance_Name }}" <br class="title-page-name"/>    delete_on_termination: yes<br class="title-page-name"/>    register: ec2 <br class="title-page-name"/>    ignore_errors: True</pre>
<p class="calibre34">The values in the <kbd class="calibre12">{{ }}</kbd> brackets need to be filled in as per your convenience and specifications. The preceding code will create an EC2 instance in your console and name it, as per the specification which is given in the <kbd class="calibre12">{{ Instance_Name }}</kbd> section.</p>
<ol start="5" class="calibre13">
<li value="5" class="calibre10">The <kbd class="calibre12">ansible.cfg</kbd> file should include all of the details which give instructions about the control path, the details regarding the forwarding agent, and also the path to the EC2 instance key. The <kbd class="calibre12">ansible.cfg</kbd> file should look like this:</li>
</ol>
<pre class="calibre17">[ssh_connection]
ssh_args=-o ControlMaster=auto -o ControlPersist=60s -o ControlPath=/tmp/ansible-ssh-%h-%p-%r -o ForwardAgent=yes

[defaults]
private_key_file=/path/to/key/key.pem</pre>
<ol start="6" class="calibre13">
<li value="6" class="calibre10">When you execute this code using <kbd class="calibre12">ansible-playbook -vvv &lt; name-of-playbook &gt;.yml</kbd>, you can see the EC2 instance being created in your EC2 console:</li>
</ol>
<div><img src="img/00221.jpeg" class="calibre48"/></div>
<ol start="7" class="calibre13">
<li value="7" class="calibre10">Now, we will terminate the instance which we have just created via Ansible. This will also be done in an Ansible script, similar to how we provisioned the instance. The following code does this:</li>
</ol>
<pre class="calibre154">  tasks:
    - name: Terminate instances that were previously launched
      connection: local
      become: false
      ec2:
        state: 'absent'
        instance_ids: '{{ ec2.instance_ids }}'
        region: '{{ aws_region }}'
      register: TerminateWorker
      ignore_errors: True</pre>
<ol start="8" class="calibre13">
<li value="8" class="calibre10">So, now you can see the instance being terminated in the console. Note that the code is the same up until the tasks, such as provisioning and terminating instances, so you can copy and paste from the provisioning task:</li>
</ol>
<div><img src="img/00222.jpeg" class="calibre48"/></div>
<p class="calibre34">So, we have successfully learned how to provision and terminate EC2 instances via an Ansible script. We will use this knowledge for provisioning and will be terminating EC2 instances at the same time.</p>
<ol start="9" class="calibre13">
<li value="9" class="calibre10">Making a small change to the provisioning code in the yaml script we used previously, we can provision multiple servers (EC2 instances) at the same time, by simply adding the <kbd class="calibre12">count</kbd> parameter. The following code will provision the number of instances mentioned in the <em class="calibre28">jinja template</em>, beside the <kbd class="calibre12">count</kbd> parameter. In our example, it is <kbd class="calibre12">ninstances</kbd>:</li>
</ol>
<pre class="calibre17">- hosts: localhost
  connection: local
  remote_user: test
  gather_facts: no


  environment:
    AWS_ACCESS_KEY_ID: "{{ aws_id }}"
    AWS_SECRET_ACCESS_KEY: "{{ aws_key }}"

    AWS_DEFAULT_REGION: "{{ aws_region }}"<br class="title-page-name"/><br class="title-page-name"/>  tasks: <br class="title-page-name"/>- name: Provisioning EC2 instaces <br class="title-page-name"/>  ec2: <br class="title-page-name"/>    assign_public_ip: no<br class="title-page-name"/>    aws_access_key: "{{ access_key }}"<br class="title-page-name"/>    aws_secret_key: "{{ secret_key }}"<br class="title-page-name"/>    region: "{{ aws_region }}"<br class="title-page-name"/>    image: "{{ image_instance }}"<br class="title-page-name"/>    instance_type: "{{ instance_type }}"<br class="title-page-name"/>    key_name: "{{ ssh_keyname }}"<br class="title-page-name"/>    count: "{{ ninstances }}"<br class="title-page-name"/>    state: present<br class="title-page-name"/>    group_id: "{{ security_group }}"<br class="title-page-name"/>    vpc_subnet_id: "{{ subnet }}"<br class="title-page-name"/>    instance_profile_name: "{{ Profile_Name }}"<br class="title-page-name"/>    wait: true<br class="title-page-name"/>    instance_tags: <br class="title-page-name"/>      Name: "{{ Instance_Name }}" <br class="title-page-name"/>    delete_on_termination: yes<br class="title-page-name"/>    register: ec2 </pre>
<ol start="10" class="calibre13">
<li value="10" class="calibre10">Now, as we have our Ansible script ready, we will now use it to start our infrastructure from the Lambda function. For that, we will make use of our knowledge of nohup.</li>
<li value="11" class="calibre10">In your Lambda function, all you need to do is to write the logic for creating a server, and then do some basic installations using the library, <kbd class="calibre12">paramiko</kbd>, and then run the Ansible script in a nohup mode, as shown here:</li>
</ol>
<pre class="calibre17">import paramiko<br class="title-page-name"/>import boto3<br class="title-page-name"/>import logging<br class="title-page-name"/><br class="title-page-name"/>logger = logging.getLogger(__name__)<br class="title-page-name"/>logger.setLevel(logging.CRITICAL)<br class="title-page-name"/>region = 'us-east-1'<br class="title-page-name"/>image = 'ami-&lt;&gt;'<br class="title-page-name"/>ubuntu_image = 'ami-&lt;&gt;'<br class="title-page-name"/>keyname = '&lt;&gt;'<br class="title-page-name"/><br class="title-page-name"/>def lambda_handler(event, context):<br class="title-page-name"/>    credentials = {&lt;&gt;}<br class="title-page-name"/>    k = paramiko.RSAKey.from_private_key_file("&lt;&gt;")<br class="title-page-name"/>        c = paramiko.SSHClient()<br class="title-page-name"/>    c.set_missing_host_key_policy(paramiko.AutoAddPolicy())<br class="title-page-name"/>    logging.critical("Creating Session")<br class="title-page-name"/>    session = boto3.Session(credentials['AccessKeyId'], <br class="title-page-name"/>    credentials['SecretAccessKey'],<br class="title-page-name"/>    aws_session_token=credentials['SessionToken'], region_name=region)<br class="title-page-name"/>    logging.critical("Created Session")<br class="title-page-name"/>    logging.critical("Create Resource")<br class="title-page-name"/>    ec2 = session.resource('ec2', region_name=region)<br class="title-page-name"/>    logging.critical("Created Resource")<br class="title-page-name"/>    logging.critical("Key Verification")<br class="title-page-name"/><br class="title-page-name"/>    key = '&lt;&gt;'<br class="title-page-name"/>    k = paramiko.RSAKey.from_private_key_file(key)<br class="title-page-name"/>    c = paramiko.SSHClient()<br class="title-page-name"/>    c.set_missing_host_key_policy(paramiko.AutoAddPolicy())<br class="title-page-name"/>    logging.critical("Key Verification done")<br class="title-page-name"/>    # Generate Presigned URL for downloading EC2 key from    an S3 bucket into master<br class="title-page-name"/>    s3client = session.client('s3')<br class="title-page-name"/><br class="title-page-name"/># Presigned url for downloading pem file of the server from an S3 bucket<br class="title-page-name"/>    url = s3client.generate_presigned_url('get_object',     Params={'Bucket': '&lt;bucket_name&gt;', 'Key': '&lt;file_name_of_key&gt;'},<br class="title-page-name"/>ExpiresIn=300)<br class="title-page-name"/>    command = 'wget ' + '-O &lt;&gt;.pem ' + "'" + url + "'"<br class="title-page-name"/>    logging.critical("Create Instance")<br class="title-page-name"/>    <br class="title-page-name"/>while True:<br class="title-page-name"/>    try:<br class="title-page-name"/>        logging.critical("Trying")<br class="title-page-name"/>        c.connect(hostname=dns_name, username="ubuntu", pkey=k)<br class="title-page-name"/>    except:<br class="title-page-name"/>        logging.critical("Failed")<br class="title-page-name"/>    continue<br class="title-page-name"/>        break<br class="title-page-name"/>    logging.critical("connected")<br class="title-page-name"/><br class="title-page-name"/>    <br class="title-page-name"/>    if size == 0:<br class="title-page-name"/>        s3client.upload_file('&lt;&gt;.pem', '&lt;bucket_name&gt;', '&lt;&gt;.pem')<br class="title-page-name"/>    else:<br class="title-page-name"/>        pass<br class="title-page-name"/>    set_key = credentials['AccessKeyId']<br class="title-page-name"/>    set_secret = credentials['SecretAccessKey']<br class="title-page-name"/>    set_token = credentials['SessionToken']<br class="title-page-name"/><br class="title-page-name"/># Commands to run inside the SSH session of the server<br class="title-page-name"/>    commands = [command,<br class="title-page-name"/>"sudo apt-get -y update",<br class="title-page-name"/>"sudo apt-add-repository -y ppa:ansible/ansible",<br class="title-page-name"/>"sudo apt-get -y update",<br class="title-page-name"/>"sudo apt-get install -y ansible python-pip git awscli",<br class="title-page-name"/>"sudo pip install boto markupsafe boto3 python-dateutil     futures",<br class="title-page-name"/>"ssh-keyscan -H github.com &gt;&gt; ~/.ssh/known_hosts",<br class="title-page-name"/>"git clone &lt;repository where your ansible script is&gt; /home/ubuntu/&lt;&gt;/",<br class="title-page-name"/>"chmod 400 &lt;&gt;.pem",<br class="title-page-name"/>"cd &lt;&gt;/&lt;&gt;/; pwd ; nohup ansible-playbook -vvv provision.yml &gt; ansible.out 2&gt; ansible.err &lt; /dev/null &amp;"]<br class="title-page-name"/><br class="title-page-name"/># Running the commands<br class="title-page-name"/>    for command in commands:<br class="title-page-name"/>        logging.critical("Executing %s", command)<br class="title-page-name"/>stdin, stdout, stderr = c.exec_command(command)<br class="title-page-name"/>    logging.critical(stdout.read())<br class="title-page-name"/>    logging.critical("Errors : %s", stderr.read())<br class="title-page-name"/>        c.close()<br class="title-page-name"/>    return dns_name</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Security best practices</h1>
                
            
            
                
<p class="calibre2">Ensuring high-level security has always been a major problem for microservices. There are multiple levels of software that you need to keep in mind while designing the security layers. The engineers need to define the security protocols for each of the services and then also define the protocols for the data interaction and transfer between each service.</p>
<p class="calibre2">You have to keep all these aspects in mind before architecting distributed serverless systems, where (almost) each Ansible task is a microservice. In this section, we will understand how to architect the security protocols, and also monitor them using some of AWS's built-in services.</p>
<p class="calibre2">We will go through a step-by-step understanding of how to write security protocols for our serverless architectures:</p>
<ol class="calibre13">
<li value="1" class="calibre10">Firstly, whenever you are creating a session inside your AWS Python scripts using <strong class="calibre1">Boto</strong>, try to create temporary credentials using the <strong class="calibre1">AWS Secure Token Service</strong> (<strong class="calibre1">STS</strong>), which creates temporary credentials for a specific period of time:</li>
</ol>
<div><img src="img/00223.jpeg" class="calibre48"/></div>
<p>You can look at the documentation of the STS at: <a href="https://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html" class="calibre162">https://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html</a>.</p>
<ol start="2" class="calibre13">
<li value="2" class="calibre10">The AssumeRole API of the STS service enables programmers to assumes IAM roles into their code:</li>
</ol>
<div><img src="img/00224.jpeg" class="calibre48"/></div>
<p>You can find its documentation on the following page: <a href="https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html" class="calibre162">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></p>
<ol start="3" class="calibre13">
<li value="3" class="calibre10">The Python version of this can be referred to, in the <kbd class="calibre12">boto3</kbd> documentation:</li>
</ol>
<div><img src="img/00225.jpeg" class="calibre48"/></div>
<p>This documentation can be found here: <a href="http://boto3.readthedocs.io/en/latest/reference/services/sts.html" class="calibre162">http://boto3.readthedocs.io/en/latest/reference/services/sts.html</a>.</p>
<ol start="4" class="calibre13">
<li value="4" class="calibre10">Scrolling down, you can find the usage of the AssumeRole API in Python:</li>
</ol>
<div><img src="img/00226.jpeg" class="calibre48"/></div>
<ol start="5" class="calibre13">
<li value="5" class="calibre10">Proper care should be taken so that the data exchange between microservices and/or between the microservices and other AWS resources happens securely with authentication. For example, the developer can configure S3 buckets to restrict actions such as unencrypted uploads, downloads, and insecure file transfers. The bucket policy can be written as follows to ensure all of these things are taken care of:</li>
</ol>
<pre class="calibre17">{<br class="title-page-name"/>    "Version": "2012-10-17",<br class="title-page-name"/>    "Id": "PutObjPolicy",<br class="title-page-name"/>    "Statement": [<br class="title-page-name"/>    {<br class="title-page-name"/>        "Sid": "DenyIncorrectEncryptionHeader",<br class="title-page-name"/>        "Effect": "Deny",<br class="title-page-name"/>        "Principal": "*",<br class="title-page-name"/>        "Action": "s3:PutObject",<br class="title-page-name"/>        "Resource": "arn:aws:s3:::&lt;bucket_name&gt;/*",<br class="title-page-name"/>        "Condition": {<br class="title-page-name"/>            "StringNotEquals": {<br class="title-page-name"/>                "s3:x-amz-server-side-encryption": "aws:kms"<br class="title-page-name"/>            }<br class="title-page-name"/>        }<br class="title-page-name"/>    },<br class="title-page-name"/>    {<br class="title-page-name"/>        "Sid": "DenyUnEncryptedObjectUploads",<br class="title-page-name"/>        "Effect": "Deny",<br class="title-page-name"/>        "Principal": "*",<br class="title-page-name"/>        "Action": "s3:PutObject",<br class="title-page-name"/>        "Resource": "arn:aws:s3:::&lt;bucket_name2&gt;/*",<br class="title-page-name"/>        "Condition": {<br class="title-page-name"/>            "Null": {<br class="title-page-name"/>                "s3:x-amz-server-side-encryption": "true"<br class="title-page-name"/>            }<br class="title-page-name"/>        }<br class="title-page-name"/>    },<br class="title-page-name"/>    {<br class="title-page-name"/>        "Sid": "DenyNonSecureTraffic",<br class="title-page-name"/>        "Effect": "Deny",<br class="title-page-name"/>        "Principal": "*",<br class="title-page-name"/>        "Action": "s3:*",<br class="title-page-name"/>        "Resource": "arn:aws:s3:::&lt;bucket_name&gt;/*",<br class="title-page-name"/>        "Condition": {<br class="title-page-name"/>            "Bool": {<br class="title-page-name"/>                "aws:SecureTransport": "false"<br class="title-page-name"/>            }<br class="title-page-name"/>        }<br class="title-page-name"/>    },<br class="title-page-name"/>    {<br class="title-page-name"/>        "Sid": "DenyNonSecureTraffic",<br class="title-page-name"/>        "Effect": "Deny",<br class="title-page-name"/>        "Principal": "*",<br class="title-page-name"/>        "Action": "s3:*",<br class="title-page-name"/>        "Resource": "arn:aws:s3:::&lt;bucket_name2&gt;/*",<br class="title-page-name"/>        "Condition": {<br class="title-page-name"/>            "Bool": {<br class="title-page-name"/>                "aws:SecureTransport": "false"<br class="title-page-name"/>            }<br class="title-page-name"/>        }<br class="title-page-name"/>    }<br class="title-page-name"/>]<br class="title-page-name"/>}</pre>
<ol start="6" class="calibre13">
<li value="6" class="calibre10">Once you have finished writing the bucket policy, you can update it in the Bucket Policy section of S3: </li>
</ol>
<div><img src="img/00227.jpeg" class="calibre163"/></div>
<ol start="7" class="calibre13">
<li value="7" class="calibre10">AWS Config provides a very useful interface for monitoring several security threats and helps in efficiently avoiding or catching them. The dashboard of AWS Config looks like this:</li>
</ol>
<div><img src="img/00228.jpeg" class="calibre164"/></div>
<ol start="8" class="calibre13">
<li value="8" class="calibre10">You can see that the dashboard shows 2 non-compliant resource(s) which means that two of my AWS resources are not complying with the rules that I have put into config. Let's have a look at these rules:</li>
</ol>
<div><img src="img/00229.jpeg" class="calibre48"/></div>
<p class="calibre2">This means that we have two AWS S3 buckets which do not have SSL requests turned on via the bucket policy. Once you click on the Rules link, you can see more details which include the bucket(s) names, and also the timestamps at which these configuration changes have been recorded:</p>
<div><img src="img/00230.jpeg" class="calibre48"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Identifying and handling difficulties in scaling</h1>
                
            
            
                
<p class="calibre2">Scaling up distributed serverless systems comes with its own set of engineering roadblocks and problems, and the fact that the concept of serverless systems is still in a very infantile stage, means that most of those problems are still unsolved. But, that shouldn't stop us from trying to solve and work around these roadblocks.</p>
<p class="calibre2">We will try and understand some of these roadblocks, and also learn how to solve or work around them, as discussed here:</p>
<ul class="calibre9">
<li class="calibre10">This is more of an architect's mistake rather than a roadblock. However, it is important to address this as one too many architects/software engineers fell and fall into the overestimation or the underestimation trap. The problem we will try to address is the exact number of instances you have to launch when scaling up. In most self-hosted MapReduce-style systems, it is taken care of out of the box.</li>
<li class="calibre10">This problem can be taken care of, by properly benchmarking the workloads beforehand on different types of instances, and scale accordingly. Let's understand this by taking an example of a machine learning pipeline. Thanks to our benchmarking efforts, we already know that an <em class="calibre28">m3.medium</em> instance can handle 100 files in 10 minutes. So, if my workload has 202 files and I want it to be completed in close to 10 minutes, I would like to have two such instances for handling this. Even if we don't know the workloads in advance, we can write a Python script for getting that number from wherever the data is, be it an SQS queue pointer, or S3, or some other database; and that number can be entered into the Ansible script and make the playbook run.</li>
<li class="calibre10">As we have already learned about handling security in huge serverless systems, we will keep this short. There are several complex data movements happening inside a large distributed serverless workload. Using proper security protocols and monitoring them, as mentioned in detail in the previous security section, will help in overcoming this problem.</li>
<li class="calibre10">Logging is a major problem in distributed serverless systems, which is also still unsolved completely. As the systems and containers are destroyed once the workload has been completed, logging has been a very difficult task to undertake. There are several ways you can log the workflow. The most popular ones are logging every Ansible task separately, and one where the last Ansible task is to zip up the logs and send the zipped file to a data store, such as S3 or Logstash. The last one is the most preferred way as it captures the execution flow better, as the entire log trace is in a single file.</li>
<li class="calibre10">Monitoring is similar to logging. Monitoring these systems is also mostly an unsolved problem. As the servers are all terminated once the workload is run, we can't poll for historic logs from the servers, and latency also will not be tolerated, or more precisely, will not be possible. Monitor every task of Ansible by having a task after each, that sends a custom metric to CloudWatch upon a condition that the previous task has executed successfully or not. This will look something like this:</li>
</ul>
<div><img src="img/00231.jpeg" class="calibre165"/></div>
<ul class="calibre9">
<li class="calibre10">Debugging trial runs can also become very frustrating, very fast. This is because, if you are not quick, the entire system can be terminated before you even get a chance to look at the logs. Also, Ansible emits very verbose logs while debugging, which might seem overwhelming when spawning several instances.</li>
<li class="calibre10">Some basic Unix hacks can help in handling these problems. The most important one is to monitor the tail of the log file, about 50 lines or so. This helps in not getting overwhelmed by the huge amount of logs, and it also keeps an eye on the execution of the Ansible notebook.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="calibre2">In this chapter, we have learned how to scale up our serverless architecture(s) to being massively distributed serverless infrastructure(s). We have learned how to build on our existing knowledge of building and deploying Lambda infrastructures to handle massive workloads.</p>
<p class="calibre2">We have learned to use the concept of nohup to use our Lambda function as a launch board for building a master-worker architecture that takes parallel computing into account. We have learned how to leverage configuration and orchestration tools, such as Ansible and Chef, to spawn and orchestrate multiple EC2 instances.</p>
<p class="calibre2">The knowledge gained from this chapter will open doors for building many complex infrastructures which can handle data and requests, both in terms of size and speed. This will allow you to operate multiple microservices closely intertwined together. This will also help you to build MapReduce-style systems and interact with other AWS services, seamlessly.</p>


            

            
        
    </body></html>