["```py\nfrom chp1.throttle import Throttle\nfrom random import choice\nimport requests\n\nclass Downloader:\n    def __init__(self, delay=5, user_agent='wswp', proxies=None, cache={}):\n        self.throttle = Throttle(delay)\n        self.user_agent = user_agent\n        self.proxies = proxies\n        self.num_retries = None  # we will set this per request\n        self.cache = cache\n\n    def __call__(self, url, num_retries=2):\n        self.num_retries = num_retries\n        try:\n             result = self.cache[url]\n             print('Loaded from cache:', url)\n        except KeyError:\n             result = None\n        if result and self.num_retries and 500 <= result['code'] < 600:\n            # server error so ignore result from cache\n            # and re-download\n            result = None\n        if result is None:\n             # result was not loaded from cache\n             # so still need to download\n             self.throttle.wait(url)\n             proxies = choice(self.proxies) if self.proxies else None\n             headers = {'User-Agent': self.user_agent}\n             result = self.download(url, headers, proxies)\n             if self.cache:\n                 # save result to cache\n                 self.cache[url] = result\n        return result['html']\n\n    def download(self, url, headers, proxies, num_retries): \n        ... \n        return {'html': html, 'code': resp.status_code } \n\n```", "```py\ndef link_crawler(..., num_retries=2, cache={}): \n    crawl_queue = [seed_url] \n    seen = {seed_url: 0}  \n    rp = get_robots(seed_url) \n    D = Downloader(delay=delay, user_agent=user_agent, proxies=proxies, cache=cache) \n\n    while crawl_queue: \n        url = crawl_queue.pop() \n        # check url passes robots.txt restrictions \n        if rp.can_fetch(user_agent, url): \n            depth = seen.get(url, 0)\n            if depth == max_depth: \n                continue\n            html = D(url, num_retries=num_retries)\n            if not html:\n                continue\n            ...\n\n```", "```py\n>>> import re \n>>> url = 'http://example.webscraping.com/default/view/Australia-1' \n>>> re.sub('[^/0-9a-zA-Z\\-.,;_ ]', '_', url) \n'http_//example.webscraping.com/default/view/Australia-1' \n\n```", "```py\n>>> filename = re.sub('[^/0-9a-zA-Z\\-.,;_ ]', '_', url)\n>>> filename = '/'.join(segment[:255] for segment in filename.split('/'))\n>>> print(filename)\n'http_//example.webscraping.com/default/view/Australia-1' \n\n```", "```py\n>>> from urllib.parse import urlsplit \n>>> components = urlsplit('http://example.webscraping.com/index/') \n>>> print(components) \nSplitResult(scheme='http', netloc='example.webscraping.com', path='/index/', query='', fragment='') \n>>> print(components.path) \n'/index/' \n\n```", "```py\n>>> path = components.path \n>>> if not path: \n>>>     path = '/index.html' \n>>> elif path.endswith('/'): \n>>>     path += 'index.html' \n>>> filename = components.netloc + path + components.query \n>>> filename \n'example.webscraping.com/index/index.html' \n\n```", "```py\nimport os \nimport re \nfrom urllib.parse import urlsplit \n\nclass DiskCache: \n    def __init__(self, cache_dir='cache', max_len=255): \n        self.cache_dir = cache_dir \n        self.max_len = max_len \n\n    def url_to_path(self, url): \n        \"\"\" Return file system path string for given URL\"\"\" \n        components = urlsplit(url) \n        # append index.html to empty paths \n        path = components.path \n        if not path: \n            path = '/index.html' \n        elif path.endswith('/'): \n            path += 'index.html' \n        filename = components.netloc + path + components.query \n        # replace invalid characters \n        filename = re.sub('[^/0-9a-zA-Z\\-.,;_ ]', '_', filename) \n        # restrict maximum number of characters \n        filename = '/'.join(seg[:self.max_len] for seg in filename.split('/')) \n        return os.path.join(self.cache_dir, filename) \n\n```", "```py\nimport json \nclass DiskCache: \n    ... \n    def __getitem__(self, url): \n        \"\"\"Load data from disk for given URL\"\"\" \n        path = self.url_to_path(url) \n        if os.path.exists(path): \n            return json.load(path)\n        else: \n            # URL has not yet been cached \n            raise KeyError(url + ' does not exist') \n\n    def __setitem__(self, url, result): \n        \"\"\"Save data to disk for given url\"\"\" \n        path = self.url_to_path(url) \n        folder = os.path.dirname(path) \n        if not os.path.exists(folder): \n            os.makedirs(folder) \n        json.dump(result, path) \n\n```", "```py\nIn [1]: from chp3.diskcache import DiskCache\n\nIn [2]: from chp3.advanced_link_crawler import link_crawler\n\nIn [3]: %time link_crawler('http://example.webscraping.com/', '/(index|view)', cache=DiskCache())\nDownloading: http://example.webscraping.com/\nDownloading: http://example.webscraping.com/index/1\nDownloading: http://example.webscraping.com/index/2\n...\nDownloading: http://example.webscraping.com/view/Afghanistan-1\nCPU times: user 300 ms, sys: 16 ms, total: 316 ms\nWall time: 1min 44s\n\n```", "```py\nIn [4]: %time link_crawler('http://example.webscraping.com/', '/(index|view)', cache=DiskCache())\nLoaded from cache: http://example.webscraping.com/\nLoaded from cache: http://example.webscraping.com/index/1\nLoaded from cache: http://example.webscraping.com/index/2\n...\nLoaded from cache: http://example.webscraping.com/view/Afghanistan-1\nCPU times: user 20 ms, sys: 0 ns, total: 20 ms\nWall time: 1.1 s\n\n```", "```py\nclass DiskCache:\n    def __init__(self, cache_dir='../data/cache', max_len=255, compress=True, \n                 encoding='utf-8'):\n        ...\n        self.compress = compress\n        self.encoding = encoding\n\n```", "```py\n# in __getitem__ method for DiskCache class\nmode = ('rb' if self.compress else 'r')\nwith open(path, mode) as fp:\n    if self.compress:\n        data = zlib.decompress(fp.read()).decode(self.encoding)\n        return json.loads(data)\n    return json.load(fp)\n\n# in __setitem__ method for DiskCache class\nmode = ('wb' if self.compress else 'w')\nwith open(path, mode) as fp:\n    if self.compress:\n        data = bytes(json.dumps(result), self.encoding)\n        fp.write(zlib.compress(data))\n else:\n json.dump(result, fp)\n\n```", "```py\nfrom datetime import datetime, timedelta \n\nclass DiskCache:\n     def __init__(..., expires=timedelta(days=30)):\n         ...\n         self.expires = expires\n\n## in __getitem___ for DiskCache class\nwith open(path, mode) as fp:\n    if self.compress:\n        data = zlib.decompress(fp.read()).decode(self.encoding)\n        data = json.loads(data)\n    else:\n        data = json.load(fp)\n    exp_date = data.get('expires')\n    if exp_date and datetime.strptime(exp_date,\n                                      '%Y-%m-%dT%H:%M:%S') <= datetime.utcnow():\n        print('Cache expired!', exp_date)\n        raise KeyError(url + ' has expired.')\n    return data\n\n## in __setitem___ for DiskCache class\nresult['expires'] = (datetime.utcnow() + self.expires).isoformat(timespec='seconds')\n\n```", "```py\n >>> cache = DiskCache(expires=timedelta(seconds=5)) \n >>> url = 'http://example.webscraping.com' \n >>> result = {'html': '...'} \n >>> cache[url] = result \n >>> cache[url] \n {'html': '...'} \n >>> import time; time.sleep(5) \n >>> cache[url] \n Traceback (most recent call last): \n ... \n KeyError: 'http://example.webscraping.com has expired' \n\n```", "```py\n    pip install redis\n\n```", "```py\n    $ redis-server\n\n```", "```py\n1212:M 18 Feb 20:24:44.590 * The server is now ready to accept connections on port 6379\n\n```", "```py\nIn [1]: import redis\n\nIn [2]: r = redis.StrictRedis(host='localhost', port=6379, db=0)\n\nIn [3]: r.set('test', 'answer')\nOut[3]: True\n\nIn [4]: r.get('test')\nOut[4]: b'answer'\n\n```", "```py\nIn [5]: url = 'http://example.webscraping.com/view/United-Kingdom-239' \n\nIn [6]: html = '...'\n\nIn [7]: results = {'html': html, 'code': 200}\n\nIn [8]: r.set(url, results)\nOut[8]: True\n\nIn [9]: r.get(url)\nOut[9]: b\"{'html': '...', 'code': 200}\"\n\n```", "```py\nIn [10]: r.set(url, {'html': 'new html!', 'code': 200})\nOut[10]: True\n\nIn [11]: r.get(url)\nOut[11]: b\"{'html': 'new html!', 'code': 200}\"\n\n```", "```py\nIn [12]: r.keys()\nOut[12]: [b'test', b'http://example.webscraping.com/view/United-Kingdom-239']\n\nIn [13]: r.delete('test')\nOut[13]: 1\n\nIn [14]: r.keys()\nOut[14]: [b'http://example.webscraping.com/view/United-Kingdom-239']\n\n```", "```py\nIn [15]: r.flushdb()\nOut[15]: True\n\nIn [16]: r.keys()\nOut[16]: []\n\n```", "```py\nimport json\nfrom datetime import timedelta \nfrom redis import StrictRedis\n\nclass RedisCache: \n    def __init__(self, client=None, expires=timedelta(days=30), encoding='utf-8'): \n        # if a client object is not passed then try \n        # connecting to redis at the default localhost port \n        self.client = StrictRedis(host='localhost', port=6379, db=0) \n            if client is None else client \n        self.expires = expires\n        self.encoding = encoding\n\n    def __getitem__(self, url): \n        \"\"\"Load value from Redis for the given URL\"\"\" \n        record = self.client.get(url) \n        if record: \n            return json.loads(record.decode(self.encoding))\n        else: \n            raise KeyError(url + ' does not exist') \n\n    def __setitem__(self, url, result): \n        \"\"\"Save value in Redis for the given URL\"\"\" \n        data = bytes(json.dumps(result), self.encoding)\n        self.client.setex(url, self.expires, data)\n\n```", "```py\nIn [1]: from chp3.rediscache import RedisCache\n\nIn [2]: from datetime import timedelta\n\nIn [3]: cache = RedisCache(expires=timedelta(seconds=20))\n\nIn [4]: cache['test'] = {'html': '...', 'code': 200}\n\nIn [5]: cache['test']\nOut[5]: {'code': 200, 'html': '...'}\n\nIn [6]: import time; time.sleep(20)\n\nIn [7]: cache['test']\n---------------------------------------------------------------------------\nKeyError Traceback (most recent call last)\n...\nKeyError: 'test does not exist'\n\n```", "```py\nimport zlib \nfrom bson.binary import Binary \n\nclass RedisCache:\n    def __init__(..., compress=True):\n        ...\n        self.compress = compress\n\n    def __getitem__(self, url): \n        record = self.client.get(url)\n        if record:\n            if self.compress:\n                record = zlib.decompress(record)\n            return json.loads(record.decode(self.encoding))\n        else: \n            raise KeyError(url + ' does not exist') \n\n    def __setitem__(self, url, result): \n        data = bytes(json.dumps(result), self.encoding)\n        if self.compress:\n            data = zlib.compress(data)\n        self.client.setex(url, self.expires, data)\n\n```", "```py\nIn [1]: from chp3.advanced_link_crawler import link_crawler\n\nIn [2]: from chp3.rediscache import RedisCache\n\nIn [3]: %time link_crawler('http://example.webscraping.com/', '/(index|view)', cache=RedisCache())\nDownloading: http://example.webscraping.com/\nDownloading: http://example.webscraping.com/index/1\nDownloading: http://example.webscraping.com/index/2\n...\nDownloading: http://example.webscraping.com/view/Afghanistan-1\nCPU times: user 352 ms, sys: 32 ms, total: 384 ms\nWall time: 1min 42s\n\nIn [4]: %time link_crawler('http://example.webscraping.com/', '/(index|view)', cache=RedisCache())\nLoaded from cache: http://example.webscraping.com/\nLoaded from cache: http://example.webscraping.com/index/1\nLoaded from cache: http://example.webscraping.com/index/2\n...\nLoaded from cache: http://example.webscraping.com/view/Afghanistan-1\nCPU times: user 24 ms, sys: 8 ms, total: 32 ms\nWall time: 282 ms\n\n```", "```py\npip install requests-cache\n\n```", "```py\nIn [1]: import requests_cache\n\nIn [2]: import requests\n\nIn [3]: requests_cache.install_cache(backend='redis')\n\nIn [4]: requests_cache.clear()\n\nIn [5]: url = 'http://example.webscraping.com/view/United-Kingdom-239'\n\nIn [6]: resp = requests.get(url)\n\nIn [7]: resp.from_cache\nOut[7]: False\n\nIn [8]: resp = requests.get(url)\n\nIn [9]: resp.from_cache\nOut[9]: True\n\n```", "```py\nfrom datetime import timedelta\nrequests_cache.install_cache(backend='redis', expire_after=timedelta(days=30))\n\n```", "```py\nIn [1]: from chp3.requests_cache_link_crawler import link_crawler\n...\nIn [3]: %time link_crawler('http://example.webscraping.com/', '/(index|view)')\nReturning from cache: http://example.webscraping.com/\nReturning from cache: http://example.webscraping.com/index/1\nReturning from cache: http://example.webscraping.com/index/2\n...\nReturning from cache: http://example.webscraping.com/view/Afghanistan-1\nCPU times: user 116 ms, sys: 12 ms, total: 128 ms\nWall time: 359 ms\n\n```"]