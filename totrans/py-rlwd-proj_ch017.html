<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="pandoc" name="generator"/>
<title>ch017.xhtml</title>

<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body epub:type="bodymatter">

<h1 data-number="17">Chapter 13<br/>
Project 4.1: Visual Analysis Techniques</h1>
<p>When doing <strong>exploratory data analysis </strong>(<strong>EDA</strong>), one common practice is to use graphical techniques to help understand the nature of data distribution. The US <strong>National Institute of Standards and Technology</strong> (<strong>NIST</strong>) has an <em>Engineering Statistics Handbook </em>that strongly emphasizes the need for graphic techniques. See <a class="url" href="https://doi.org/10.18434/M32189">https://doi.org/10.18434/M32189</a>.</p>
<p>This chapter will create some additional Jupyter notebooks to present a few techniques for displaying univariate and multivariate distributions.</p>
<p>In this chapter, we’ll focus on some important skills for creating diagrams for the cleaned data:</p>
<ul>
<li><p>Additional Jupyter Notebook techniques</p></li>
<li><p>Using <strong>PyPlot </strong>to present data</p></li>
<li><p>Unit testing for Jupyter Notebook functions</p></li>
</ul>
<p>This chapter has one project, to build the start of a more complete analysis notebook. A notebook can be saved and exported as a PDF file, allowing an analyst to share preliminary results for early conversations. In the next chapter, we’ll expand on the notebook to create a presentation that can be shared with colleagues.</p>
<p>Looking further down the road, a notebook can help to identify important aspects of the data that need ongoing monitoring. The computations created here will often become the basis for more fully automated reporting tools and notifications. This analysis activity is an important step toward understanding the data and designing a model for the data.</p>
<p>We’ll start with a description of an analysis notebook. </p>

<h2 data-number="17.1">13.1  Description</h2>
<p>In the previous chapters, the sequence of projects created a pipeline to acquire and then clean the raw data. The intent is to build automated data gathering as Python applications.</p>
<p>We noted that ad hoc data inspection is best done with a notebook, not an automated CLI tool. Similarly, creating command-line applications for analysis and presentation can be challenging. Analytical work seems to be essentially exploratory, making it helpful to have immediate feedback from looking at results.</p>
<p>Additionally, analytical work transforms raw data into information, and possibly even insight. Analytical results need to be shared to create significant value. A Jupyter notebook is an exploratory environment that can create readable, helpful presentations.</p>
<p>One of the first things to do with raw data is to create diagrams to illustrate the distribution of univariate data and the relationships among variables in multivariate data. We’ll emphasize the following common kinds of diagrams:</p>
<ul>
<li><p><strong>Histograms </strong>A histogram summarizes the distribution of values for a variable in a dataset. The histogram will have data values on one axis and frequency on the other axis.</p></li>
<li><p><strong>Scatter Plots </strong>A scatter plot summarizes the relationships between values for two variables in a dataset. The visual clustering can be apparent to the casual observer.</p></li>
</ul>
<p>For small datasets, each relationship in a scatter plot can be a single dot. For larger datasets, where a number of points have similar relationships, it can be helpful to create “bins” that reflect how many points have the same relationship.</p>
<p>There are a number of ways of showing the size of these bins. This includes using a color code for more popular combinations. For some datasets, the size of an enclosing circle can show the relative concentration of similarly-valued data. The reader is encouraged to look at alternatives to help emphasize the interesting relationships among the attributes of the various samples.</p>
<p>The use of <strong>Seaborn </strong>to provide colorful styles is also important when working with diagrams. You are encouraged to explore various color palettes to help emphasize interesting data. </p>


<h2 data-number="17.2">13.2  Overall approach</h2>
<p>We’ll take some guidance from the C4 model ( <a class="url" href="https://c4model.com">https://c4model.com</a>) when looking at our approach:</p>
<ul>
<li><p><strong>Context</strong>: For this project, the context diagram has two use cases: the acquire-to-clean process and this analysis notebook.</p></li>
<li><p><strong>Containers</strong>: There’s one container for analysis application: the user’s personal computer.</p></li>
<li><p><strong>Components</strong>: The software components include the existing analysis models that provide handy definitions for the Python objects.</p></li>
<li><p><strong>Code</strong>: The code is scattered in two places: supporting modules as well as the notebook itself.</p></li>
</ul>
<p>A context diagram for this application is shown in <a href="#13.1"><em>Figure 13.1</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 13.1: Context diagram " src="img/file55.jpg"/>
<figcaption class="IMG---Caption">Figure 13.1: Context diagram </figcaption>
</figure>
<p>The analyst will often need to share their analytical results with stakeholders. An initial notebook might provide confirmation that some data does not conform to the null hypothesis, suggesting an interesting relationship that deserves deeper exploration. This could be part of justifying a budget allocation to do more analysis based on preliminary results. Another possible scenario is sharing a notebook to confirm the null hypothesis is likely true, and that the variations in the data have a high probability of being some kind of measurement noise. This could be used to end one investigation and focus on alternatives.</p>
<p>In <a href="ch010.xhtml#x1-1460006"><em>Chapter</em><em> 6</em></a>, <a href="ch010.xhtml#x1-1460006"><em>Project 2.1: Data Inspection Notebook</em></a>, the data inspection notebook was described. The use of a single acquisition model module for the data was mentioned, but the details of the Python implementation weren’t emphasized. The raw data module often provides little useful structure to the data when doing inspections.</p>
<p>Moving forward into more complicated projects, we’ll see the relationship between the notebooks and the modules that define the data model become more important.</p>
<p>For this analysis notebook, the analysis data model, created in <a href="ch013.xhtml#x1-2080009"><em>Chapter</em><em> 9</em></a>, <a href="ch013.xhtml#x1-2080009"><em>Project 3.1: Data Cleaning Base Application</em></a>, will be a central part of the notebook for doing analysis. This will be imported and used in the analysis notebook. The analysis process may lead to changes to the analysis model to reflect lessons learned.</p>
<p>A technical complication arises from the directory structure. The data acquisition and cleaning applications are in the <code>src</code> directory, where the notebooks are kept in a separate <code>notebooks</code> directory.</p>
<p>When working with a notebook in the <code>notebooks</code> directory, it is difficult to make the Python <code>import</code> statement look into the adjacent <code>src</code> directory. The <code>import</code> statement scans a list of directories defined by the <code>sys.path</code> value. This value is seeded from some defined rules, the current working directory, and the value of the <code>PYTHONPATH</code> environment variable.</p>
<p>There are two ways to make the <code>import</code> statement load modules from the adjacent <code>src</code> directory:</p>
<ol>
<li><div><p>Put <code>../src</code> into the <code>PYTHONPATH</code> environment variable before starting JupyterLab.</p>
</div></li>
<li><div><p>Put the absolute path to <code>../src</code> into the <code>sys.path</code> list after starting JupyterLab.</p>
</div></li>
</ol>
<p>The two are equivalent. The first can be done by updating one’s <code>~/.zshrc</code> file to make sure the <code>PYTHONPATH</code> environment variable is set each time a terminal session starts. There are other files appropriate for other shells; for example, <code>~/.bashrc</code> or <code>./rc</code> for the classic <code>sh</code> shell. For Windows, there’s a dialog that allows one to edit the system’s environment variables.</p>
<p>The alternative is to update <code>sys.path</code> with a cell containing code like the following example:</p>
<div><div><pre class="source-code">import sys
from pathlib import Path

src_path = Path.cwd().parent / "src"
sys.path.append(str(src_path))</pre>
</div>
</div>
<p>This cell will add the peer <code>../src</code> directory to the system path. After this is done, the <code>import</code> statement will bring in modules from the <code>../src</code> directory as well as bringing in built-in standard library modules, and modules installed with <strong>conda </strong>or <strong>pip</strong>.</p>
<p>An initialization module can be defined as part of the IPython startup. This module can alter the <code>sys.path</code> value in a consistent way for a number of related notebooks in a project.</p>
<div><div><p>While some developers object to tinkering with <code>sys.path</code> in a notebook, it has the advantage of being explicit.</p>
<p>Setting <code>PYTHONPATH</code> in one’s <code>~/.zshrc</code> file is a very clean and reliable solution. It then becomes necessary to put a reminder in a <code>README</code> file so that new team members can also make this change to their personal home directory.</p>
<p>When sharing notebooks, it becomes imperative to make sure all stakeholders have access to the entire project the notebook depends on. This can lead to a need to create a Git repository that contains the notebook being shared along with reminders, test cases, and needed modules.</p>
</div>
</div>
<p>Once we have the path defined properly, the notebook can share classes and functions with the rest of the applications. We’ll move on to looking at one possible organization of an analysis notebook. </p>

<h3 data-number="17.2.1">13.2.1  General notebook organization</h3>
<p>An analytical notebook is often the primary method for presenting and sharing results. It differs from a “lab” notebook. It’s common for lab notebooks to contain a number of experiments, some of which are failures, and some of which have more useful results. Unlike a lab notebook, an analytical notebook needs to have a tidy organization with carefully-written Markdown cells interspersed with data processing cells. In effect, an analysis notebook needs to tell a kind of story. It needs to expose actors and actions and the consequences of those actions.</p>
<p>It’s essential that the notebook’s cells execute correctly from beginning to end. The author should be able to restart the kernel and run all of the cells at any time to redo the computations. While this may be undesirable for a particularly long-running computation, it still must be true.</p>
<p>A notebook may have preliminary operations that aren’t relevant to most readers of the report. A specific example is setting the <code>sys.path</code> to import modules from an adjacent <code>../src</code> directory. It can be helpful to make use of JupyterLab’s ability to collapse a cell as a way to set some of the computation details aside to help the reader focus on the key concepts.</p>
<p>It seems sensible to formalize this with Markdown cells to explain the preliminaries. The remaining cells in this section can be collapsed visually to minimize distractions.</p>
<p>The preliminaries can include technical and somewhat less technical aspects. For example, setting <code>sys.path</code> is purely technical, and few stakeholders will need to see this. On the other hand, reading the <code>SeriesSample</code> objects from an ND JSON format file is a preliminary step that’s somewhat more relevant to the stakeholder’s problems.</p>
<p>After the preliminaries, the bulk of the notebook can focus on two topics:</p>
<ul>
<li><p>Summary statistics</p></li>
<li><p>Visualizations</p></li>
</ul>
<p>We’ll look at the summary statistics in the next section. </p>


<h3 data-number="17.2.2">13.2.2  Python modules for summarizing</h3>
<p>For initial reports, Python’s <code>statistics</code> module offers a few handy statistical functions. This module offers <code>mean()</code>, <code>median()</code>, <code>mode()</code>, <code>stdev()</code>, <code>variance()</code>. There are numerous other functions here the reader is encouraged to explore.</p>
<p>These functions can be evaluated in a cell, and the results will be displayed below the cell. In many cases, this is all that’s required.</p>
<p>In a few cases, though, results need to be truncated to remove meaningless trailing digits. In other cases, it can help to use an f-string to provide a label for a result. A cell might look like the following:</p>
<pre class="source-code">f"mean = {statistics.mean(data_values): .2f}"</pre>
<p>This provides a label, and truncates the output to two places to the right of the decimal point.</p>
<p>In some cases, we might want to incorporate computed values into markdown cells. The <strong>Python Markdown </strong>extension provides a tidy way to incorporate computed values into markdown content.</p>
<p>See <a class="url" href="https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/python-markdown/readme.html">https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/python-markdown/readme.html</a>.</p>
<p>The most important part of the notebook is the graphic visualization of the data, we’ll turn to that in the next section. </p>


<h3 data-number="17.2.3">13.2.3  PyPlot graphics</h3>
<p>The <strong>matplotlib </strong>package works well for creating images and diagrams. Within this extensive, sophisticated graphic library is a smaller library, <code>pyplot</code>, that’s narrowly focused on data visualizations. Using the <code>pyplot</code> library permits a few lines of code to create a useful display of data.</p>
<p>The module is often renamed to make it easier to type. The common convention is shown in the following line of code:</p>
<pre class="source-code">import matplotlib.pyplot as plt</pre>
<p>This lets us use <code>plt</code> as a namespace to refer to functions defined in the <code>pyplot</code> module of the <code>matplotlib</code> package.</p>
<p>In some cases, JupyterLab may not have the <code>matplotlib</code> library prepared for interactive use. (This will be clear when the plotted images are not shown in the notebook.) In these cases, the interactive use of the <code>matplotlib</code> library needs to be enabled. Use the following magic commands in a cell of the notebook to enable interactive use:</p>
<div><div><pre class="source-code">%matplotlib inline
%config InlineBackend.figure_formats = {’png’, ’retina’}</pre>
</div>
</div>
<p>The first command enables the <code>matplotlib</code> library to create graphics immediately in the notebook. It won’t create separate files or pop-up windows. The second command produces readily-shared PNG output files. It also helps <strong>MAC OS X </strong>users to optimize the graphics for their high-resolution displays.</p>
<p>This isn’t always required. It’s needed only for those installations that didn’t configure the <code>matplotlib</code> library for interactive use in a Jupyter notebook.</p>
<p>In many cases, the <code>pyplot</code> library expects simple sequences of values for the various plotting functions. The <em>x </em>and <em>y </em>values of a scatter plot, for example, are expected to be two parallel lists of the same length. This will lead to a few extra cells to restructure data.</p>
<p>The data cleaning applications from previous chapters produced a single sequence of compound sample objects. Each object was a separate sample, with the related values for all of the variables. We’ll need to convert from this sample record organization to parallel sequences of values for the individual variables.</p>
<p>This can involve something like the following:</p>
<div><div><pre class="source-code">x = [s.x for s in series_1]
y = [s.y for s in series_1]</pre>
</div>
</div>
<p>An alternative is to use the <code>operator.attrgetter()</code> function. It looks like the following:</p>
<div><div><pre class="source-code">from operator import attrgetter

x = list(map(attrgetter(’x’), series_1))
y = list(map(attrgetter(’y’), series_1))</pre>
</div>
</div>
<p>We’ll start with a histogram to show the distribution of values for a single variable.</p>

<h4 class="likesubsubsectionHead" data-number="17.2.3.1">Data frequency histograms</h4>
<p>Often, a document — like a book — has individual figures. A figure may contain a single plot. Or, it may contain a number of “subplots” within a single figure. The <code>pyplot</code> library provides support for creating a single figure that contains many subplots. The idea of a figure with a single plot can be seen as a special case of this generalized approach.</p>
<p>A figure with a single plot can be prepared with a statement like the following:</p>
<div><div><pre class="source-code">fig, ax = plt.subplots()</pre>
</div>
</div>
<p>The <code>fig</code> object is the figure as a whole. The <code>ax</code> object is the set of axes for the subplot within the figure. While the explicit <code>ax</code> object can seem unnecessary, it’s part of a more general approach that allows figures with related plots to be built.</p>
<p>The more general case stacks multiple subplots within a figure. The <code>sublots()</code> function can return axes for each subplot. The call to create a two-plot figure might look like this:</p>
<div><div><pre class="source-code">fig, (ax_0, ax_1) = plt.subplots(1, 2)</pre>
</div>
</div>
<p>Here we’ve stated we’ll have 1 row with 2 subplots stacked next to each other. The <code>fig</code> object is the figure as a whole. The <code>ax_0</code> and <code>ax_1</code> objects are the sets of axes for each of these two subplots within the figure.</p>
<p>Here’s an example of creating a single histogram from the <code>y</code> values of one of the four Anscombe’s quartet series:</p>
<div><div><pre class="source-code">fig, ax = plt.subplots()

# Labels and Title
ax.set_xlabel(’Y’)
ax.set_ylabel(’Counts’)
ax.set_title(’Series I’)

# Draw Histogram
_ = ax.hist(y, fill=False)</pre>
</div>
</div>
<p>For this example, the <code>y</code> variable must be a list of <em>y </em>attribute values from Series I. The series data must be read from the cleaned source file.</p>
<p>The <code>_</code><code> =</code><code> ax.hist(...)</code> statement assigns the results of the <code>hist()</code> function to the variable <code>_</code> as a way to suppress displaying the result value in this cell. Without this assignment statement, the notebook will show the result of the <code>hist()</code> function, which isn’t very interesting and clutters up the output. Since each series has both an <em>x </em>and a <em>y </em>value, it helps to stack two histograms that shows the distribution of these values. The reader is encouraged to develop a figure with two stacked subplots.</p>
<p>The number of options for color and borders on the histogram bars are breathtaking in their complexity. It helps to try several variants in a single notebook and then delete the ones that aren’t helpful.</p>
<p>The comparison between values is often shown in a scatter plot that shows each (<em>x,y</em>) pair. We’ll look at this next.</p>


<h4 class="likesubsubsectionHead" data-number="17.2.3.2">X-Y scatter plot</h4>
<p>A scatter plot is one of many ways to show the relationship between two variables. Here’s an example of creating a single plot from the <code>x</code> and <code>y</code> values of one of the four Anscombe’s quartet series:</p>
<div><div><pre class="source-code">fig, ax = plt.subplots()

# Labels and Title
ax.set_xlabel(’X’)
ax.set_ylabel(’Y’)
ax.set_title(’Series I’)

# Draw Scatter
_ = ax.scatter(x, y)</pre>
</div>
</div>
<p>The <code>_</code><code> =</code><code> ax.scatter(...)</code> statement assigns the results of the <code>scatter()</code> function to the variable <code>_</code> as a way to suppress displaying the value. This keeps the output focused on the figure.</p>
<p>The above example presumes the data have been extracted from a sequence of <code>SeriesSample</code> instances using code like the following:</p>
<div><div><pre class="source-code">x = [s.x for s in series_1]
y = [s.y for s in series_1]</pre>
</div>
</div>
<p>This, in turn, presumes the value for <code>series_1</code> was read from the clean data created by the <strong>acquire </strong>and <strong>clean </strong>applications.</p>
<p>Now that we have an approach to building the notebook, we can address the way notebooks tend to evolve. </p>



<h3 data-number="17.2.4">13.2.4  Iteration and evolution</h3>
<p>A notebook is often built iteratively. Cells are added, removed, and modified as the data is understood.</p>
<p>On a purely technical level, the Python programming in the cells needs to evolve from a good idea to software that’s testable. It’s often easiest to do this by rewriting selected cells into functions. For example, define a function to acquire the ND JSON data. This can be supplemented by <code>doctest</code> comments to confirm the function works as expected.</p>
<p>A collection of functions can be refactored into a separate, testable module if needed. This can permit wider reuse of good ideas in multiple notebooks.</p>
<p>Equally important is to avoid “over-engineering” a notebook. It’s rarely worth the time and effort to carefully specify the contents of a notebook, and then write code that meets those specifications. It’s far easier to create — and refine — the notebook.</p>
<p>In some large organizations, a senior analyst may direct the efforts of junior analysts. In this kind of enterprise, it can be helpful to provide guidance to junior analysts. When formal methods are needed, the design guidance can take the form of a notebook with markdown cells to explain the desired goal.</p>
<p>Now that we have an approach to building the notebook, we can enumerate the deliverables for this project. </p>



<h2 data-number="17.3">13.3  Deliverables</h2>
<p>This project has the following deliverables:</p>
<ul>
<li><p>A <code>requirements-dev.txt</code> file that identifies the tools used, usually <code>jupyterlab==3.5.3</code> and <code>matplotlib==3.7.0</code>.</p></li>
<li><p>Documentation in the <code>docs</code> folder.</p></li>
<li><p>Unit tests for any new application modules in the <code>tests</code> folder.</p></li>
<li><p>Any new application modules in the <code>src</code> folder with code to be used by the inspection notebook.</p></li>
<li><p>A notebook to summarize the clean data. In the case of Anscombe’s quartet, it’s essential to show the means and variances are nearly identical, but the scatter plots are dramatically different.</p></li>
</ul>
<p>We’ll look at a few of these deliverables in a little more detail. </p>

<h3 data-number="17.3.1">13.3.1  Unit test</h3>
<p>There are two distinct kinds of modules that can require testing:</p>
<ul>
<li><p>The notebook with any function or class definitions. All of these definitions require unit tests.</p></li>
<li><p>If functions are factored from the notebook into a supporting module, this module will need unit tests. Many previous projects have emphasized these tests.</p></li>
</ul>
<p>A notebook cell with a computation cell is notoriously difficult to test. The visual output from the <code>hist()</code> or <code>scatter()</code> functions seems almost impossible to test in a meaningful way.</p>
<p>In addition, there are numerous usability tests that can’t be automated. Poor choice of colors, for example, can obscure an important relationship. Consider the following questions:</p>
<ol>
<li><div><p>Is it informative?</p>
</div></li>
<li><div><p>Is it relevant?</p>
</div></li>
<li><div><p>Is it misleading in any way?</p>
</div></li>
</ol>
<p>In many cases, these questions are difficult to quantify and difficult to test. As a consequence, it’s best to focus automated testing on the Python programming.</p>
<p>It’s imperative to avoid testing the internals of <code>matplotlib.pyplot</code>.</p>
<p>What’s left to test?</p>
<ul>
<li><p>The data loading.</p></li>
<li><p>Any ad hoc transformations that are part of the notebook.</p></li>
</ul>
<p>The data loading should be reduced to a single function that creates a sequence of <code>SeriesSample</code> instances from the lines in an ND JSON file of clean data. This loading function can include a test case.</p>
<p>We might define the function as follows:</p>
<div><div><pre class="source-code">def load(source_file):
    """
    &gt;&gt;&gt; from io import StringIO
    &gt;&gt;&gt; file = StringIO(’’’{"x": 2.0, "y": 3.0}\\n{"x": 5.0, "y": 7.0}’’’)
    &gt;&gt;&gt; d = load(file)
    &gt;&gt;&gt; len(d)
    2
    &gt;&gt;&gt; d[0]
    SeriesSample(x=2.0, y=3.0)
    """
    data = [
        SeriesSample(**json.loads(line)) for line in source_file if line
    ]
    return data</pre>
</div>
</div>
<p>This permits testing by adding a cell to the notebook that includes the following:</p>
<div><div><pre class="source-code">import doctest
doctest.testmod()</pre>
</div>
</div>
<p>This cell will find the functions defined by the notebook, extract any doctest cases from the docstrings in the function definitions, and confirm the doctest cases pass.</p>
<p>For more complicated numerical processing, the <strong>hypothesis </strong>library is helpful. See <a href="ch014.xhtml#x1-2600001"><em>Hypothesis testing</em></a> in <a href="ch014.xhtml#x1-22900010"><em>Chapter</em><em> 10</em></a>, <a href="ch014.xhtml#x1-22900010"><em>Data Cleaning Features</em></a> for more information. </p>


<h3 data-number="17.3.2">13.3.2  Acceptance test</h3>
<p>An automated acceptance test is difficult to define for a notebook. It’s hard to specify ways in which a notebook is helpful, meaningful, or insightful in the simple language of Gherkin scenarios.</p>
<p>The <code>jupyter</code><code> execute</code><code> &lt;filename&gt;</code> command will execute an <code>.ipynb</code> notebook file. This execution is entirely automated, allowing a kind of sanity check to be sure the notebook runs to completion. If there is a problem, the command will exit with a return code of 1, and the cell with the error will be displayed in detail. This can be handy for confirming the notebook isn’t trivially broken.</p>
<p>The <code>.ipynb</code> file is a JSON document. An application (or a step definition for <strong>Behave</strong>) can read the file to confirm some of its properties. An acceptance test case might look for error messages, for example, to see if the notebook failed to work properly.</p>
<p>Cells with <code>"type":</code><code> "code"</code> will also have <code>"outputs"</code>. If one of the outputs has <code>"output_type":</code><code> "error"</code>; this cell indicates a problem in the notebook. The notebook did not run to completion, and the acceptance test should be counted as a failure.</p>
<p>We can use projects like <strong>Papermill </strong>to automate notebook refresh with new data. This project can execute a template notebook and save the results as a finalized output notebook with values available and computations performed.</p>
<p>For more information, see <a class="url" href="https://papermill.readthedocs.io">https://papermill.readthedocs.io</a>. </p>



<h2 data-number="17.4">13.4  Summary</h2>
<p>This project begins the deeper analysis work on clean data. It emphasizes several key skills, including:</p>
<ul>
<li><p>More advanced Jupyter Notebook techniques. This includes setting the <code>PYTHONPATH</code> to import modules and creating figures with plots to visualize data.</p></li>
<li><p>Using <strong>PyPlot </strong>to present data. The project uses popular types of visualizations: histograms and scatter plots.</p></li>
<li><p>Unit testing for Jupyter Notebook functions.</p></li>
</ul>
<p>In the next chapter, we’ll formalize the notebook into a presentation “slide deck” that can be shown to a group of stakeholders. </p>


<h2 data-number="17.5">13.5  Extras</h2>
<p>Here are some ideas for the reader to add to these projects. </p>

<h3 data-number="17.5.1">13.5.1  Use Seaborn for plotting</h3>
<p>An alternative to the <strong>pyplot </strong>package is the <strong>Seaborn </strong>package. This package also provides statistical plotting functions. It provides a wider variety of styling options, permitting more colorful (and perhaps more informative) plots.</p>
<p>See <a class="url" href="https://seaborn.pydata.org">https://seaborn.pydata.org</a> for more information.</p>
<p>This module is based on <code>matplotlib</code>, making it compatible with JupyterLab.</p>
<p>Note that the <strong>Seaborn </strong>package can work directly with a list-of-dictionary structure. This matches the ND JSON format used for acquiring and cleaning the data.</p>
<p>Using a list-of-dictionary type suggests it might be better to avoid the analysis model structure, and stick with dictionaries created by the <strong>clean </strong>application. Doing this might sacrifice some model-specific processing and validation functionality.</p>
<p>On the other hand, the <code>pydantic</code> package offers a built-in <code>dict()</code> method that covers a sophisticated analysis model object into a single dictionary, amenable to use with the <strong>Seaborn </strong>package. This seems to be an excellent way to combine these packages. We encourage the reader to explore this technology stack. </p>


<h3 data-number="17.5.2">13.5.2  Adjust color palettes to emphasize key points about the data</h3>
<p>Both the <strong>pyplot </strong>package and the <strong>Seaborn </strong>package have extensive capabilities for applying color to the plot. A choice of colors can sometimes help make a distinction visible, or it can obscure important details.</p>
<p>You can consider alternative styles to see which seems more useful.</p>
<p>In some enterprise contexts, there are enterprise communications standards, with colors and fonts that are widely used. An important technique when using Seaborn is to create a style that matches enterprise communication standards.</p>
<p>A number of websites provide website color schemes and design help. Sites like <a class="url" href="https://paletton.com">https://paletton.com</a> or <a class="url" href="https://colordesigner.io">colordesigner.io</a> provide complementary color palettes. With some effort, we can take these kinds of designs for color palettes and create Seaborn styles that permit a consistent and unique presentation style. </p>



</body>
</html>
