<html><head></head><body><div><p>&#13;&#13;
    <h1 class="chapterNumber">10</h1>&#13;&#13;
    <h1 id="_idParaDest-194" class="chapterTitle">Testing and TDD</h1>&#13;&#13;
    <p class="normal">No matter how good a developer is, they'll write code that doesn't always perform correctly. This is unavoidable, as no developer is perfect. But it's also because the expected results are sometimes not the ones that one would think of while immersed in coding.</p>&#13;&#13;
    <p class="normal">Designs rarely go as expected and there's always a discussion going back and forth while they are being implemented, until refining them and getting them correct.</p>&#13;&#13;
    <blockquote class="packt_quote">Everyone has a plan until they get punched in the mouth. – Mike Tyson</blockquote>&#13;&#13;
    <p class="normal">Writing software is notoriously difficult because of its extreme plasticity, but at the same time, we can use software to double-check that the code is doing what it is supposed to do.</p>&#13;&#13;
    <div>&#13;&#13;
      <p class="Tip--PACKT-">Be aware that, as with any other code, tests can have bugs as well.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Writing tests allows you to detect problems while the code is fresh and with some sane skepticism to verify that the expected results are the actual results. We will see during the chapter how to write tests easily, as well as different strategies to write different tests for capturing different kinds of problems.</p>&#13;&#13;
    <p class="normal">We will describe how to work under TDD, a methodology that works by defining the tests first, to ensure that the validation is as independent of the actual code implementation as possible.</p>&#13;&#13;
    <p class="normal">We will also show how to create tests in Python using common unit test frameworks, the standard <code class="Code-In-Text--PACKT-">unittest</code> module, and the more advanced and powerful <code class="Code-In-Text--PACKT-">pytest</code>.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Note this chapter is a bit longer than others, mostly due to the need to present example code.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">In this chapter, we'll cover the following topics:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Testing the code</li>&#13;&#13;
      <li class="bullet">Different levels of testing</li>&#13;&#13;
      <li class="bullet">Testing philosophy</li>&#13;&#13;
      <li class="bullet">Test-Driven Development</li>&#13;&#13;
      <li class="bullet">Introduction to unit testing in Python</li>&#13;&#13;
      <li class="bullet">Testing external dependencies</li>&#13;&#13;
      <li class="bullet">Advanced pytest</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Let's start with some basic concepts about testing.</p>&#13;&#13;
    <h1 id="_idParaDest-195" class="title">Testing the code</h1>&#13;&#13;
    <p class="normal">The first question when discussing testing the code is a simple one: What exactly do we mean by testing the code?</p>&#13;&#13;
    <p class="normal">While there are multiple answers to that, in the broadest sense, the answer could be "<em class="italic">any procedure that probes the application to check that it works correctly before it reaches the final customers.</em>" In this sense, any formal or informal testing procedure will fulfil the definition.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">The most relaxed approach, which is sometimes seen in small applications with one or two developers, is to not create specific tests but to do informal "full application runs" checking that a newly implemented feature works as expected.</p>&#13;&#13;
      <p class="Tip--PACKT-">This approach may work for small, simple applications, but the main problem is ensuring that older features remain stable.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">But, for high-quality software that is big and complex enough, we need to be a bit more careful about the testing. So, let's try to come up with a more precise definition of testing: <em class="italic">Testing is any documented procedure, preferably automated, that, from a known setup, checks the different elements of the application work correctly before it reaches the final customers.</em></p>&#13;&#13;
    <p class="normal">If we check the differences with the previous definition, there are several key words. Let's check each of them to see the different details:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet"><strong class="keyword">Documented</strong>: Compared with the previous version, the aim should be that the tests are documented. This allows you to reproduce them precisely if necessary and allows you to compare them to discover blind spots.<p class="bullet">There are multiple ways that a test can be documented, either by specifying a list of steps to run and expected results or by creating code that runs the test. The main idea is that a test can be analyzed, be run several times by different people, be changed if necessary, and have a clear design and result.</p>&#13;&#13;
      </li>&#13;&#13;
      <li class="bullet"><strong class="keyword">Preferably automated</strong>:<strong class="keyword"> </strong>Tests<strong class="keyword"> </strong>should be able to be run automatically, with as little human intervention as possible. This allows you to trigger Continuous Integration techniques to run many tests over and over, creating a "safety net" that is able to catch unexpected errors as early as possible. We say "preferably" because perhaps some tests are impossible or very costly to totally automate. In any case, the objective should be to have the vast majority of tests automated, to allow computers to do the heavy lifting and save precious human time. There are also multiple software tools that allow you to run tests, which can help.</li>&#13;&#13;
      <li class="bullet"><strong class="keyword">From a known setup</strong>: To be able to run tests in isolation, we need to know what the status of the system should be before running the test. That ensures that the result of a test will not create a certain state that could interfere with the next test. Before and after a test, certain cleanup may be required.<p class="bullet">This can make running tests in batches slower, compared with not worrying about the initial or end status, but it will create a solid foundation to avoid problems.</p>&#13;&#13;
      </li>&#13;&#13;
    </ul>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">As a general rule, and especially in automated tests, the order in which the tests are executed should be irrelevant, to avoid cross-contamination. This is easier said than done, and in some cases, the order of tests can create problems. For example, test A creates an entry that test B reads. If test B is run in isolation, it will fail as it expects the entry created by A. These cases should be fixed, as they can greatly complicate debugging. Also, being able to run tests independently allows them to be parallelized.</p>&#13;&#13;
    </p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet"><strong class="keyword">Different elements of the application</strong>: Most tests should not address the whole application, but smaller parts of it. We will talk more later about the different levels of testing, but tests should be specific about what are they testing and cover different elements, as tests covering more ground will be costlier.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">A key element of testing is to have a good return on investment. Designing and running tests takes time, and that time needs to be well spent. Any test needs to be maintained, which should be worth it. Over the whole chapter, we will be commenting on this important aspect of testing.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">There's an important kind of testing that we are not covering with this definition, which is called <em class="italic">exploratory testing</em>. These tests are typically run by QA engineers, who use the final application without a clear preconceived idea but try to pre-emptively find problems. If the application has a customer-facing UI, this style of testing can be invaluable in detecting inconsistencies and problems that are not detected in the design phase.</p>&#13;&#13;
      <p class="Tip--PACKT-">For example, a good QA engineer will be able to say that the color of a button on page X is not the same as the button on page Y, or that the button is not evident enough to perform an action, or that to perform a certain action there's a prerequisite that's not evident or possible with the new interface. Any <strong class="keyword">user experience</strong> (<strong class="keyword">UX</strong>) check will probably fall into this category.</p>&#13;&#13;
      <p class="Tip--PACKT-">By its nature, this kind of testing cannot be "designed" or "documented," as it ultimately comes down to interpretation and a good eye to understand whether the application <em class="italic">feels correct</em>. Once a problem is detected, then it can be documented to be avoided.</p>&#13;&#13;
      <p class="Tip--PACKT-">While this is certainly useful and recommended, this style of testing is more an art than an engineering practice and we won't be discussing it in detail.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">This general definition helps to start the discussion, but we can be more concrete about the different tests defined by how much of the system is under test, during each test.</p>&#13;&#13;
    <h1 id="_idParaDest-196" class="title">Different levels of testing</h1>&#13;&#13;
    <p class="normal">As we described <a id="_idIndexMarker649"/>before, tests should cover different elements of the system. This means that a test can address a small or big part of the system (or the whole system), trying to reduce its range of action.</p>&#13;&#13;
    <p class="normal">When testing a small part of the system, we reduce the complexity of the test and scope. We need to call only that small part of the system, and the setup is easier to start with. In general, the smaller the element to test, the faster and easier it is to test it.</p>&#13;&#13;
    <p class="normal">We will define three different levels or kinds of tests, from small to big scopes:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet"><strong class="keyword">Unit tests</strong>, for tests that check only part of a service</li>&#13;&#13;
      <li class="bullet"><strong class="keyword">Integration tests</strong>, for tests that check a single service as a whole</li>&#13;&#13;
      <li class="bullet"><strong class="keyword">System tests</strong>, for tests that check multiple services working together</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Names can actually vary quite a lot. In this book, we won't be very strict with definitions, instead defining soft limits and suggesting finding a balance that works for your specific project. Don't be shy to take decisions on the proper level for each test and define your own nomenclature, and always keep in mind how much effort it takes to create tests to be sure that they are always worth it.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">The definition of the levels can be a little blurred. For example, integration and unit tests can be defined side by side, and the difference between them could be more academic in that case.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Let's start <a id="_idIndexMarker650"/>describing each of the levels in more detail.</p>&#13;&#13;
    <h2 id="_idParaDest-197" class="title">Unit tests</h2>&#13;&#13;
    <p class="normal">The smallest <a id="_idIndexMarker651"/>kind of <a id="_idIndexMarker652"/>test is also the one where most effort is typically invested, the <em class="italic">unit test</em>. This kind of test checks the behavior of a small unit of code, not the whole system. This unit of code could be as small as a single function or test a single API endpoint, and so on.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">As we said above, there's a lot of debate on how big a unit test should actually be, based on what the "unit" is and whether it is actually a unit. For example, in some cases, people will only call a test a unit test if it involves a single function or class.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Because a unit test checks a small part of the functionality, it can be very easy to set up and quick to run. Therefore, making new unit tests is quick and can thoroughly test the system, checking that the small individual pieces that make the whole system work as expected.</p>&#13;&#13;
    <p class="normal">The objective of unit tests is to check in depth the behavior of a defined feature of a service. Any external requests or elements should be simulated, meaning that they are defined as part of the test. We will cover unit tests in more detail later in the chapter, as they are the key elements of the TDD approach.</p>&#13;&#13;
    <h2 id="_idParaDest-198" class="title">Integration tests</h2>&#13;&#13;
    <p class="normal">The next <a id="_idIndexMarker653"/>level<a id="_idIndexMarker654"/> is the integration test. This is checking the whole behavior of a service or a couple of services.</p>&#13;&#13;
    <p class="normal">The main goal of integration testing is to be sure that the different services or different modules inside the same service can work with each other. While in unit tests, external requests are simulated, integration tests use the real service.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">The simulation of external APIs may still be required. For example, simulating an external payment provider for the tests. But, in general, as many real services should be used for integration tests as possible, as the point of the test is to test that the different services work together.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">It's important to note that, commonly, different services will be developed by different developers or even different teams, and they can diverge in their understanding of how a particular API is implemented, even in the event of a well-defined spec.</p>&#13;&#13;
    <p class="normal">The setup in integration tests is more complex than in unit tests, as more elements need to be properly set up. This makes integration tests slower and more expensive than unit tests.</p>&#13;&#13;
    <p class="normal">Integration tests are great to check that different services work in unison, but there are some limitations.</p>&#13;&#13;
    <p class="normal">Integration tests are normally not as thorough as unit tests, focusing on checking basic functionality and following a <em class="italic">happy path</em>. A happy path is a concept in testing meaning that the test case should produce no errors or exceptions. </p>&#13;&#13;
    <p class="normal">Expected errors and exceptions are normally tested in unit tests, since they are also elements that can fail. That doesn't <a id="_idIndexMarker655"/>mean that every single integration test should follow a <a id="_idIndexMarker656"/>happy path; some integration errors may be worth checking, but in general, a happy path tests the expected general behavior of the feature. They will compose the bulk of the integration tests.</p>&#13;&#13;
    <h2 id="_idParaDest-199" class="title">System tests</h2>&#13;&#13;
    <p class="normal">The final level<a id="_idIndexMarker657"/> is the<a id="_idIndexMarker658"/> system level. System tests check that all the different services work correctly together.</p>&#13;&#13;
    <p class="normal">A requirement for this kind of test is that there are actually multiple services in the system. If not, they are not different from tests at the lower levels. The main objective of these tests is to check that the different services can cooperate, and the configuration is correct.</p>&#13;&#13;
    <p class="normal">System tests are slow and difficult to implement. They require the whole system to be set up, with all the different services properly configured. Creating that environment can be complicated. Sometimes, it's so difficult that the only way of actually performing any system tests is to run them in the live environment.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">The environment configuration is an important part of what these tests check. That may make them important to run on each environment that is under test, including the live environment.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">While this is not ideal, sometimes it is unavoidable and can help to improve confidence after deployments, to ensure that the new code is working correctly. In that case, given the constraints, only a minimum amount of tests should be run, as the live environment is critical. The tests to run should also exercise the maximum amount of common functionality and services to detect any critical problem as fast as possible. This set of tests is sometimes called <em class="italic">acceptance tests</em> or <em class="italic">smoke tests</em>. They may be run manually, as a way of ensuring that everything looks correct.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Of course, smoke tests can be run not only on the live environment and can work as a way to ensure that other environments are working correctly.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Smoke tests should be very clear, well documented, and designed carefully to cover the most critical <a id="_idIndexMarker659"/>parts of the whole system. Ideally, they should also be<a id="_idIndexMarker660"/> read-only, so they don't leave useless data after their execution.</p>&#13;&#13;
    <h1 id="_idParaDest-200" class="title">Testing philosophy</h1>&#13;&#13;
    <p class="normal">A key element<a id="_idIndexMarker661"/> of everything involved with testing is another question: <em class="italic">Why test?</em> What are we trying to achieve with it?</p>&#13;&#13;
    <p class="normal">As we've seen, testing is a way of ensuring that the behavior of the code is the expected one. The objective of testing is to detect possible problems (sometimes called <em class="italic">defects</em>) before the code is published and used by real users.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">There's a subtle difference between <em class="italic">defects</em> and <em class="italic">bugs</em>. Bugs are a kind of defect where the software behaves in a way that it's not expected to. For example, certain input produces an unexpected error. Defects are more general. A defect could be that a button is not visible enough, or that the logo on a page is not the correct one. In general, tests are way better at detecting bugs than other defects, but remember what we said about exploratory testing.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">A defect that goes undetected and gets deployed into a live system is pretty expensive to repair. First of all, it needs to be detected. In a live application with a lot of activity, detecting a problem can be difficult (though we will talk about it in <em class="chapterRef">Chapter 16</em>, <em class="italic">Ongoing Architecture</em>), but even worse, it will normally be detected by a user of the system using the application. It's possible that the user won't properly communicate the problem back, so the problem is still present, creating problems or limiting activity. The detecting user might abandon the system, or at the very least their confidence in the system will decrease.</p>&#13;&#13;
    <p class="normal">Any reputational cost will be bad, but it can also be difficult to extract enough information from the user to know exactly what happened and how to fix it. This makes the cycle between detecting the problem and fixing it long.</p>&#13;&#13;
    <p class="normal">Any testing system will improve the ability to fix defects earlier. Not only can we create a specific test that simulates exactly the same problem, but we can also create a framework that executes tests regularly to have a clear approach to how to detect and fix problems.</p>&#13;&#13;
    <p class="normal">Different testing levels have different effects on this cost. In general, any problem that can be detected at the unit test level is going to be cheaper to fix there, and the cost increases from there. Designing and running a unit test is easier and faster than doing the same with an<a id="_idIndexMarker662"/> integration test, and an integration test is cheaper than a system test.</p>&#13;&#13;
    <p class="normal">The different test levels could be understood as different layers capturing possible problems. Each layer will capture different problems if they appear. The closer to the start of the process (design and unit tests while coding), the cheaper it is to create a dense net that will detect and alert for problems. The cost of fixing a problem increases the farther away it is from the controlled environment at the start of the process.</p>&#13;&#13;
    <figure class="mediaobject"><img src="img/B17580_10_01.png" alt="Diagram&#13;&#10;&#13;&#10;Description automatically generated" width="826" height="223"/></figure>&#13;&#13;
    <p class="packt_figref">Figure 10.1: The cost of fixing defects increases the later they get detected</p>&#13;&#13;
    <p class="normal">Some defects are impossible to detect at the unit test level, like the integration of different parts. That's where the next level comes into play. As we've seen, the worst scenario is not detecting a problem and it affecting real users on the live system.</p>&#13;&#13;
    <p class="normal">But having tests is not only a good way of capturing problems once. Because a test can still remain, and be run on new code changes, it also creates a safety net while developing to be sure that creating new code or modifying the code does not affect the old functionality.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">This is one of the best arguments for running tests automatically and constantly, as per Continuous Integration practices. The developer can focus on the feature being developed, while the Continuous Integration tool will run every test, alerting early if there's a problem with some test. A problem with previously introduced functionality that is failing is called a <em class="italic">regression</em>.</p>&#13;&#13;
      <p class="Information-Box--PACKT-">Regression problems are quite common, so having good test coverage is great to prevent them going undetected. Specific tests covering previous functionality to ensure that it keeps running as expected can be introduced. These are regression tests, and sometimes they are added after we have detected a regression problem.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Another benefit of <a id="_idIndexMarker663"/>having good tests that check the behavior of the system is that the code itself can be changed heavily, knowing that the behavior will remain the same. These changes can be made to restructure the code, clean it, and in general improve it. These changes are called <em class="italic">refactoring</em> the code, changing how the code is written without changing the expected behavior of it.</p>&#13;&#13;
    <p class="normal">Now, we should answer the question "what is a good test?" As we discussed, writing a test is not free, there's an effort involved, and we need to be sure that it's worth it. How can we create good ones?</p>&#13;&#13;
    <h2 id="_idParaDest-201" class="title">How to design a great test</h2>&#13;&#13;
    <p class="normal">Designing good<a id="_idIndexMarker664"/> tests requires a certain mindset. The objective while designing the code that covers certain functionality is to make the code fulfill that functionality while at the same time being efficient, writing clear code that could even be described as elegant.</p>&#13;&#13;
    <p class="normal">The objective of the test is to be sure that the functionality sticks to the expected behavior, and that all the different problems that can arise produce results that make sense.</p>&#13;&#13;
    <p class="normal">Now, to be able to really put the functionality to the test, the mindset should be to stress the code as much as possible. For example, let's imagine a function <code class="Code-In-Text--PACKT-">divide(A, B)</code>, that divides two integers between -100 and 100: <code class="Code-In-Text--PACKT-">A</code> between <code class="Code-In-Text--PACKT-">B</code>.</p>&#13;&#13;
    <p class="normal">While approaching the test, we need to check what the limits are of this, trying to check that the function is performing properly with the expected behavior. For example, the following tests could<a id="_idIndexMarker665"/> be created:</p>&#13;&#13;
    <table id="table001-5" class="No-Table-Style _idGenTablePara-1">&#13;&#13;
      <colgroup>&#13;&#13;
        <col/>&#13;&#13;
        <col/>&#13;&#13;
        <col/>&#13;&#13;
      </colgroup>&#13;&#13;
      <tbody>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Heading--PACKT-">Action</p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Heading--PACKT-">Expected behavior</p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Heading--PACKT-">Comments</p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">divide(10, 2)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">return 5</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-">Basic case</p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">divide(-20, 4)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">return -5</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-">Divide one negative and one positive integer</p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">divide(-10, -5)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">return 2</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-">Divide two negative integers</p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">divide(12, 2)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">return 5 </code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-">Not exact division</p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">divide(100, 50)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">return 2 </code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-">Maximum value of A</p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">divide(101, 50)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">Produce an input error</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-">Value of A exceeding the maximum</p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">divide(50, 100)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">return 0 </code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-">Maximum value of B</p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">divide(50, 101)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">Produce an input error</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-">Value of B exceeding the maximum</p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">divide(10, 0)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">Produce an exception</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-">Divide by zero</p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">divide('10', 2)</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">Produce an input error</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-">Invalid format for parameter A</p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
        <tr class="No-Table-Style">&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">divide(10, '2')</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="normal"><code class="Code-In-Text--PACKT-">Produce an input error</code></p>&#13;&#13;
          </td>&#13;&#13;
          <td class="No-Table-Style">&#13;&#13;
            <p class="Table-Column-Content--PACKT-">Invalid format for parameter B</p>&#13;&#13;
          </td>&#13;&#13;
        </tr>&#13;&#13;
      </tbody>&#13;&#13;
    </table>&#13;&#13;
    <p class="normal">Note how we are testing different possibilities:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">The usual behavior of all the parameters is correct, and the division works correctly. This includes both positive and negative numbers, exact division, and inexact division.</li>&#13;&#13;
      <li class="bullet">Values within the maximum and minimum values: We check that the maximum values are hit and correct, and the next value is properly detected.</li>&#13;&#13;
      <li class="bullet">Division by zero: A known limitation on functionality that should produce a predetermined response (exception).</li>&#13;&#13;
      <li class="bullet">Wrong input format.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">We can really create a lot of test cases for simple functionality! Note that all these cases can be expanded. For example, we can add <code class="Code-In-Text--PACKT-">divide(-100, 50)</code> and <code class="Code-In-Text--PACKT-">divide(100, -50) </code>cases. In those cases, the question is the same: are those tests adding better detection of problems?</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">The best test is the test that really stresses the code and ensures that it's working as expected, trying very hard to cover the most difficult use cases. Making the tests ask difficult questions of the code under test is the best way of preparing your code for the real action. A system under load will see all kinds of combinations, so the best preparation for that is to create tests that try as hard as possible to find problems, to be able to solve them before moving to the next phase.</p>&#13;&#13;
      <p class="Information-Box--PACKT-">This is analogous to football training, where a series of very demanding exercises are presented to be sure that the trainee will be able to perform later, during the match. Be sure that your training regime is hard enough to properly prepare for demanding matches!</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">The proper <a id="_idIndexMarker666"/>balance between the number of tests and not having tests that cover functionality already checked by an existing test (for example, creating a big table dividing numbers with a lot of divisions) may depend greatly on the code under test and practices in your organization. Some critical areas may require more thorough testing as a failure there could be more important.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">For example, any external API should test any input with care and be really defensive about that, as external users may abuse external APIs. For example, testing what happens when strings are input in integer fields, infinity or <code class="Code-In-Text--PACKT-">NaN</code> (Not a Number) values are added, payload limits are exceeded, the maximum size of a list or page is exceeded, etc.</p>&#13;&#13;
      <p class="Tip--PACKT-">By comparison, interfaces that are mostly internal will require less testing, as the internal code is less likely to abuse the API. For example, if the <code class="Code-In-Text--PACKT-">divide</code> function is only internal, it might not be required to test that the input format is incorrect, just to check that the limits are respected.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Note that tests are done independently from the implementation of the code. A test definition is done purely from an external view of the function to test, without requiring knowing what's inside. This is called <em class="italic">black-box testing</em>. A heathy test suite always starts with this approach.</p>&#13;&#13;
    <p class="normal">A critical ability to develop as a developer writing tests is to detach from the knowledge of the code itself and approach tests independently.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Testing can be so detached that it may use independent people just to create the tests, like a QA team performing tests. Unfortunately, this is not a possible approach for unit tests, which will likely be created by the same developers that write the code itself.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">In some cases, this external approach won't be enough. If the developer knows that there's some specific area where there could be problems, it may be good to complement it with tests that check functionality that is not apparent from an external point of view.</p>&#13;&#13;
    <p class="normal">For example, a function that calculates a result based on some input may have an internal point where the algorithm changes to calculate it using different models. This information doesn't need to be known by the external user, but it will be good to add a couple of checks that the transition works correctly.</p>&#13;&#13;
    <p class="normal">This kind of testing is called <em class="italic">white-box testing</em>, in comparison to the black-box approach discussed early.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">It's important to remember that, in a test suite, white-box tests should always be secondary to black-box tests. The main objective is to test the functionality from an external perspective. White-box testing may be a good addition, especially in some aspects, but it should have a lower priority.</p>&#13;&#13;
      <p class="Tip--PACKT-">Developing the ability to be able to create good black-box tests is important and should be transmitted to the team.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Black-box testing tries to avoid a common problem where the same developer writes both the code and the test and then checks that the interpretation of the feature implemented <a id="_idIndexMarker667"/>in the code works as expected, instead of checking that it works as it should when looking from an external endpoint. We will take a look later at TDD, which tries to ensure tests are created without the implementation in mind by writing the tests before writing the code.</p>&#13;&#13;
    <h2 id="_idParaDest-202" class="title">Structuring tests</h2>&#13;&#13;
    <p class="normal">In terms of<a id="_idIndexMarker668"/> structure, especially for unit tests, a nice way to structure tests<a id="_idIndexMarker669"/> is using the <strong class="keyword">Arrange Act Assert</strong> (<strong class="keyword">AAA</strong>) pattern.</p>&#13;&#13;
    <p class="normal">This pattern means <a id="_idIndexMarker670"/>the test is in three different phases:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet"><strong class="keyword">Arrange</strong>: Prepare the environment for the tests. This includes all the setup to get the system right at the point before performing the next step, at a stable moment. </li>&#13;&#13;
      <li class="bullet"><strong class="keyword">Act</strong>: Perform the action that is the objective of the test.</li>&#13;&#13;
      <li class="bullet"><strong class="keyword">Assert</strong>: Check that the result of the action is the expected one.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">The test gets structured as a sentence like this:</p>&#13;&#13;
    <p class="normal"><strong class="keyword">GIVEN</strong><em class="italic"> </em>(Arrange)<em class="italic"> </em>an environment known, the<em class="italic"> </em><strong class="keyword">ACTION</strong><em class="italic"> </em>(Act)<em class="italic"> </em>produces the specified <strong class="keyword">RESULT</strong> (Assert)</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">This pattern is also sometimes called <em class="italic">GIVEN</em>, <em class="italic">WHEN</em>, <em class="italic">THEN</em> as each step can be described in those terms.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Note that this structure aims for all the tests to be independent, and for each to test a single thing.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">A common different pattern is to group act steps in tests, testing multiple functionalities in a single test. For example, test that writing a value is correct and then <a id="_idIndexMarker671"/>check that the search for the value returns the proper value. This won't follow the AAA pattern. Instead, to follow the AAA pattern, two tests should be created, the first one to validate that the write works correctly and the second where the value is created as part of the setup in the Arrange step before doing the search.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Note that this structure can be used whether the tests are executed through code or run manually, though they'll be used more for automated tests. When running them manually, the Arrange stage can take a long time to produce for each test, leading to a lot of time spent on that. Instead, manual tests are normally grouped together in the pattern that we describe above, executing a series of Act and Assert and using the input in the previous stage as setup for the next. This creates a dependency in requiring to run tests in a specific sequence, which is not great for unit test suites, but it can be better for smoke tests or other environments where the Arrange step is very expensive.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">In the same way, if the code to test is purely functional (meaning that only the input parameters are the ones that determine its state, like the <code class="Code-In-Text--PACKT-">divide</code> example above), the Arrange step is not required.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Let's see an example of code created with this structure. Imagine that we have a method that we want to test, called <code class="Code-In-Text--PACKT-">method_to_test</code>. The method is part of a class called <code class="Code-In-Text--PACKT-">ClassToTest</code>.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code"> def test_example():&#13;&#13;
    # Arrange step&#13;&#13;
    # Create the instance of the class to test&#13;&#13;
    object_to_test = ClassToTest(paramA='some init param', &#13;&#13;
                                 paramB='another init param')&#13;&#13;
    # Act step&#13;&#13;
    response = object_to_test.method_to_test(param='execution_param')&#13;&#13;
    # Assert step&#13;&#13;
    assert response == 'expected result'&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Each of the<a id="_idIndexMarker672"/> steps is very clearly defined. The first one prepares, in this case, an object in the class that we want to test. Note that we may need to add some parameters or some preparation so the object is in a known starting point so the next steps work as expected.</p>&#13;&#13;
    <p class="normal">The Act step just generates the action that is under test. In this case, call the <code class="Code-In-Text--PACKT-">method_to_test</code> method for the prepared object with the proper parameter.</p>&#13;&#13;
    <p class="normal">Finally, the Assert step is very straightforward and just checks the response is the expected one.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">In general, both the Act and Assert steps are simple to define and write. The Arrange step is where most of the effort of the test will normally be.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Another common pattern that appears using the AAA pattern for tests is to create common functions for testing in Arrange steps. For example, creating a basic environment, which could require a complex setup, and then having multiple copies where the Act and Assert steps are different. This reduces the repetition of code.</p>&#13;&#13;
    <p class="normal">For example:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def create_basic_environment():&#13;&#13;
    object_to_test = ClassToTest(paramA='some init param', &#13;&#13;
                                 paramB='another init param')&#13;&#13;
    # This code may be much more complex and perhaps have&#13;&#13;
    # 100 more lines of code, because the basic environment&#13;&#13;
    # to test requires a lot of things to set up&#13;&#13;
    return object_to_test&#13;&#13;
def test_exampleA():&#13;&#13;
    # Arrange&#13;&#13;
    object_to_test = create_basic_environment()&#13;&#13;
    # Act&#13;&#13;
    response = object_to_test.method_to_test(param='execution_param')&#13;&#13;
    # Assert&#13;&#13;
    assert response == 'expected result B'&#13;&#13;
def test_exampleB():&#13;&#13;
    # Arrange&#13;&#13;
    object_to_test = create_basic_environment()&#13;&#13;
    # Act&#13;&#13;
    response = object_to_test.method_to_test(param='execution_param')&#13;&#13;
    # Assert&#13;&#13;
    assert response == 'expected result B'&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">We will see <a id="_idIndexMarker673"/>later how we can structure multiple tests that are very similar to avoid repetition, which is a problem when having big test suites. Having big test suites is important to create good test coverage, as we saw above.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Repetition in tests is, up to a certain point, unavoidable and even healthy to a certain degree. When changing the behavior of some part of the code because there are changes, the tests need to be changed accordingly to accommodate the changes. This change helps to weigh the size of the changes and avoid making big changes lightly, as the tests will work as a reminder of the affected functionality.</p>&#13;&#13;
      <p class="Information-Box--PACKT-">Nonetheless, mindless repetition is not great, and we will see later some options to reduce the amount of code to be repeated.</p>&#13;&#13;
    </p>&#13;&#13;
    <h1 id="_idParaDest-203" class="title">Test-Driven Development</h1>&#13;&#13;
    <p class="normal">A very popular <a id="_idIndexMarker674"/>technique to approach programming is <strong class="keyword">Test-Driven Development</strong> or <strong class="keyword">TDD</strong>. TDD consists of putting tests at the center of the developing experience.</p>&#13;&#13;
    <p class="normal">This builds on some of the ideas that we exposed earlier in the chapter, though working on them with a more consistent view.</p>&#13;&#13;
    <p class="normal">The TDD flow to develop software works as follows:</p>&#13;&#13;
    <ol>&#13;&#13;
      <li class="numbered">New functionality is decided on to be added to the code.</li>&#13;&#13;
      <li class="numbered">A new test is written to define the new functionality. Note that this is done <em class="italic">before</em> the code.</li>&#13;&#13;
      <li class="numbered">The test suite is run to show that it's failing.</li>&#13;&#13;
      <li class="numbered">The new functionality is then added to the main code, focusing on simplicity. Only the required feature, without extra details, should be added.</li>&#13;&#13;
      <li class="numbered">The test suite is run to show that the new test is working. This may need to be done several times until the code is ready.</li>&#13;&#13;
      <li class="numbered">The new functionality is ready! Now the code can be refactored to improve it, avoiding duplication, rearranging elements, grouping it with previously existing code, etc.</li>&#13;&#13;
    </ol>&#13;&#13;
    <p class="normal">The cycle can start again for any new functionality.</p>&#13;&#13;
    <p class="normal">As you can see, TDD <a id="_idIndexMarker675"/>is based on three main ideas:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet"><strong class="keyword">Write the tests before writing the code</strong>: This prevents the problem of creating a test that is too tightly coupled with the current implementation, forcing the developer to think about the test and the feature before jumping into writing it. It also forces the developer to check that the test actually fails before the feature is written, being sure that a problem later on will be detected. This is similar to the black box testing approach that we described earlier in the <em class="italic">How to design a great test</em> section.</li>&#13;&#13;
      <li class="bullet"><strong class="keyword">Run the tests constantly</strong>: A critical part of the process is running the whole test suite to check that all the functionality in the system is correct. This is done over and over, every time that a new test is created, but also while the functionality is being written. Running the tests is an essential part of developing in TDD. This ensures that all functionality is always checked and that the code works as expected at all times so any bug or discrepancy can be solved quickly.</li>&#13;&#13;
      <li class="bullet"><strong class="keyword">Work in very small increments</strong>: Focus on the task at hand, so each step builds and grows a test <a id="_idIndexMarker676"/>suite that is big and covers the whole functionality of the code in depth.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">This big test suite creates a safety net that allows you to perform refactors of the code often, big and small, therefore improving the code constantly. Small increments mean small tests that are specific and need to be thought about before adding the code.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">An extension of this idea is a focus on writing only the code that's required for the task at hand and not more. This is sometimes referred to as the <strong class="keyword">YAGNI</strong> principle (<strong class="keyword">You Ain't Gonna Need It</strong>). The intention of this principle is to prevent overdesigning <a id="_idIndexMarker677"/>or creating code for "foreseeable requests in the future," which, in practice, have a high probability of never materializing and, even worse, makes the code more difficult to change in other directions. Given that software development is notoriously difficult to plan in advance, the emphasis should be on keeping things small and not getting too far ahead of yourself.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">These three ideas<a id="_idIndexMarker678"/> interact constantly during the development cycle, and it keeps the tests at the center of the development process, hence the name of the practice.</p>&#13;&#13;
    <p class="normal">Another important advantage of TDD is that putting the focus so heavily on the tests means that how the code is going to be tested is thought about from the start, which helps in designing code that's easily testable. Also, reducing the amount of code to write, focusing on it being strictly required to pass the test reduces the probability of overdesign. The requirement to create small tests and work in increments also tends to generate modular code, in small units that are combined together but are able to be tested independently.</p>&#13;&#13;
    <p class="normal">The general flow is to be constantly working with new failing tests, making them pass and then refactoring, sometimes called the "<em class="italic">red/green/refactor</em>" pattern: red when the test is failing and green when all tests are passing.</p>&#13;&#13;
    <p class="normal">Refactoring is a critical aspect of the TDD process. It is strongly encouraged, to constantly improve the quality of the existing code. One of the best outcomes of this way of working is the generation of very extensive test suites that cover each detail of the code functionality, meaning that refactoring code can be done knowing that there's a solid ground that is going to capture any problems introduced by changing the code and adding bugs.</p>&#13;&#13;
    <p class="normal">Improving the code's readability, usability, and so on, by refactoring is known to have a good impact in terms of improving the morale of developers and increasing the pace at which changes can be introduced, as the code is kept in good shape.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">In general, and not only in TDD, allowing time to clean up old code and improve it is critical to maintain a good pace for changes. Old code that is stale tends to be more and more difficult to work with, and over time it will require way more effort to change it to make more changes. Encouraging healthy habits to care about the current state of the code and allowing time to perform maintenance improvements is critical for the long-term sustainability of any software system.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Another important aspect of TDD is the requirement of speedy tests. As tests are always running following TDD practices, the total execution time is quite important. The time that it takes for each test should be considered carefully, as the growing size of the test suite will make it take longer to run.</p>&#13;&#13;
    <p class="normal">There's a general<a id="_idIndexMarker679"/> threshold where focus gets lost, so running tests taking longer than around 10 seconds will make them not "part of the same operation," risking the developer thinking about other stuff.</p>&#13;&#13;
    <p class="normal">Obviously, running the whole test suite in under 10 seconds will be extremely difficult, especially as the number of tests grows. A full unit test suite for a complex application can consist of 10,000 tests or more! In real life, there are multiple strategies that can help alleviate this fact.</p>&#13;&#13;
    <p class="normal">The whole test suite doesn't need to be run all the time. Instead, any test runner should allow you to select a range of tests to run, allowing you to reduce the number of tests to run on each run while the feature is in development. This means running only the tests that are relevant for the same module, for example. It can even mean running a single test, in certain cases, to speed up the result.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Of course, at some point, the whole test suite should be run. TDD is actually aligned with Continuous Integration, as it is also based on running tests, this time automatically once the code is checked out into a repo. The combination of being able to run a few tests locally to ensure that things are working correctly while developing with the whole test suite running in the background once the code is committed to the repo is great.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Anyway, as the time taken to run tests is important in TDD, observing the duration of tests is important, and generating tests that can run quickly is key to being able to work in the TDD way. This is mainly achieved by creating tests that cover small portions of the code, and therefore the time to set up can be kept under control.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">TDD practices work best with unit tests. Integration and system tests may require a big setup that is not compatible with the speed and tight feedback loop required for TDD to work.</p>&#13;&#13;
      <p class="Tip--PACKT-">Fortunately, as <a id="_idIndexMarker680"/>we saw before, unit testing is where the bulk of testing is typically focused on most projects.</p>&#13;&#13;
    </p>&#13;&#13;
    <h2 id="_idParaDest-204" class="title">Introducing TDD into new teams</h2>&#13;&#13;
    <p class="normal">Introducing<a id="_idIndexMarker681"/> TDD practices in an organization can be tricky, as they change the way to perform actions that are quite basic, and go a bit against the usual way of working (writing tests after writing the code).</p>&#13;&#13;
    <p class="normal">When considering introducing TDD into a team, it's good to have an advocate that can act as a point of contact for the rest of the team and solve the questions and problems that may arise through creating tests.</p>&#13;&#13;
    <p class="normal">TDD is very popular in environments where pair programming is also common, so it's another possibility to have someone drive a session while training the other developers and introducing the practice.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Remember, the key element of TDD is the mindset of forcing the developer to think first about how a particular feature is going to be tested before starting to think about the implementation. This mindset doesn't come naturally and needs to be trained and practiced.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">It may be challenging to apply TDD techniques with already existing code, as pre-existing code can be difficult to test in this configuration, especially if the developers are new to the practice. TDD works great for new projects, though, as a test suite for new code will be created at the same time as the code. A mixed approach of starting a new module inside an existing project, so most code is new and can be designed using TDD techniques, reduces the problem of dealing with legacy code.</p>&#13;&#13;
    <p class="normal">If you want to see if TDD can be effective for new code, try to start small, using some small project with a small team to be sure that it's not too disruptive and that the principles can be properly digested and applied. There are some developers that really love to use TDD principles, as it fits their personality and how they approach the process of developing. Remember that this is not necessarily how everyone will feel and that starting with these practices requires time, and perhaps it won't be possible to apply them 100% as <a id="_idIndexMarker682"/>the previous code might limit it.</p>&#13;&#13;
    <h2 id="_idParaDest-205" class="title">Problems and limitations</h2>&#13;&#13;
    <p class="normal">TDD practices <a id="_idIndexMarker683"/>are very popular and widely followed in the industry, though they have their limits. One is the problem of big tests that take too long to run. These tests may be unavoidable in certain situations.</p>&#13;&#13;
    <p class="normal">Another is the difficulty of fully taking this approach if it is not done from the beginning, as parts of the code will already be written, and perhaps new tests should be added, violating the rule of creating the tests before the code.</p>&#13;&#13;
    <p class="normal">Another problem is designing new code while the features to be implemented are fluid and not fully defined. This requires experimentation, for example, to design a function to return a color that contrasts with an input color, for example, to present a contrast color based on a theme selectable by the user. This function may require inspection to see if it "looks right," which can require tweaking that's difficult to achieve with a preconfigured unit test.</p>&#13;&#13;
    <p class="normal">Not a problem specifically with TDD, but something to be careful about is to remember to avoid dependencies between tests. This can happen with any test suite, but given the focus on creating new tests, it's a likely problem if the team is starting with TDD practices. Dependencies can be introduced by requiring tests to run in a particular order, as the tests can contaminate the environment. This is normally not done on purpose, but it's done inadvertently while writing multiple tests.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">A typical effect on that will be that some tests fail if run independently, as their dependencies are not run in that case.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">In any case, remember that TDD is not necessarily something that it's all or nothing, but a set of ideas and practices that can help you design code that's well tested and high quality. Not every single test in the system needs to be designed using TDD, but a lot of them can be.</p>&#13;&#13;
    <h2 id="_idParaDest-206" class="title">Example of the TDD process</h2>&#13;&#13;
    <p class="normal">Let's imagine<a id="_idIndexMarker684"/> that we need to create a function that:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">For values lower than 0, returns zero</li>&#13;&#13;
      <li class="bullet">For values greater than 10, returns 100</li>&#13;&#13;
      <li class="bullet">For values between, it returns the power of two of the value. Note that for the edges, it returns the power of two of the input (0 for 0 and 100 for 10)</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">To write the code in full TDD fashion, we start with the smallest possible test. Let's create the smallest skeleton and the first test.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def parameter_tdd(value):&#13;&#13;
    pass&#13;&#13;
assert parameter_tdd(5) == 25&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">We run the test, and get an error with the test failing. Right now, we will use pure Python code, but<a id="_idIndexMarker685"/> later in the chapter, we'll see how to run tests more efficiently.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ python3 tdd_example.py&#13;&#13;
Traceback (most recent call last):&#13;&#13;
  File ".../tdd_example.py", line 6, in &lt;module&gt;&#13;&#13;
    assert parameter_tdd(5) == 25&#13;&#13;
AssertionError&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The implementation of the use case is quite straightforward.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def parameter_tdd(value):&#13;&#13;
    return 25&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Yes, we are actually returning a hardcoded value, but that's really all that is required to pass the first tests. Let's run the tests now and you'll see no errors.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ python3 tdd_example.py&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">But now we add tests for the lower edge. While these are two lines, they can be considered the same test, as they're checking that the edge is correct.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">assert parameter_tdd(-1) == 0&#13;&#13;
assert parameter_tdd(0) == 0&#13;&#13;
assert parameter_tdd(5) == 25&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Let's run the tests again.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ python3 tdd_example.py&#13;&#13;
Traceback (most recent call last):&#13;&#13;
  File ".../tdd_example.py", line 6, in &lt;module&gt;&#13;&#13;
    assert parameter_tdd(-1) == 0&#13;&#13;
AssertionError&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">We need to add code to handle the lower edge.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def parameter_tdd(value):&#13;&#13;
    if value &lt;= 0:&#13;&#13;
        return 0&#13;&#13;
    return 25&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">When running the test, we see that it's running the tests correctly. Let's add parameters now to <a id="_idIndexMarker686"/>handle the upper edge.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">assert parameter_tdd(-1) == 0&#13;&#13;
assert parameter_tdd(0) == 0&#13;&#13;
assert parameter_tdd(5) == 25&#13;&#13;
assert parameter_tdd(10) == 100&#13;&#13;
assert parameter_tdd(11) == 100&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This triggers the corresponding error.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ python3 tdd_example.py&#13;&#13;
Traceback (most recent call last):&#13;&#13;
  File "…/tdd_example.py", line 12, in &lt;module&gt;&#13;&#13;
    assert parameter_tdd(10) == 100&#13;&#13;
AssertionError&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Let's add the higher edge.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def parameter_tdd(value):&#13;&#13;
    if value &lt;= 0:&#13;&#13;
        return 0&#13;&#13;
    if value &gt;= 10:&#13;&#13;
        return 100&#13;&#13;
    return 25&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This runs correctly. We are not confident that all the code is fine, and we really want to be sure that the intermediate section is correct, so we add another test.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">assert parameter_tdd(-1) == 0&#13;&#13;
assert parameter_tdd(0) == 0&#13;&#13;
assert parameter_tdd(5) == 25&#13;&#13;
assert parameter_tdd(7) == 49&#13;&#13;
assert parameter_tdd(10) == 100&#13;&#13;
assert parameter_tdd(11) == 100&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Aha! Now it <a id="_idIndexMarker687"/>shows an error, due to the initial hardcoding.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ python3 tdd_example.py&#13;&#13;
Traceback (most recent call last):&#13;&#13;
  File "/…/tdd_example.py", line 15, in &lt;module&gt;&#13;&#13;
    assert parameter_tdd(7) == 49&#13;&#13;
AssertionError&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">So let's fix it.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def parameter_tdd(value):&#13;&#13;
    if value &lt;= 0:&#13;&#13;
        return 0&#13;&#13;
    if value &gt;= 10:&#13;&#13;
        return 100&#13;&#13;
    return value ** 2&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This runs all the tests correctly. Now, with the safety net of the tests, we think we can refactor the code a little bit to clean it up.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def parameter_tdd(value):&#13;&#13;
    if value &lt; 0:&#13;&#13;
        return 0&#13;&#13;
    if value &lt; 10:&#13;&#13;
        return value ** 2&#13;&#13;
    return 100&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">We can run the tests all through the process and be sure that the code is correct. The final result may be different based on what the team considers good code or what is more explicit, but we have our test suite that will ensure that the tests are consistent, and the behavior is correct.</p>&#13;&#13;
    <p class="normal">The function <a id="_idIndexMarker688"/>here is quite small, but this shows what the flow is when writing code in the TDD style.</p>&#13;&#13;
    <h1 id="_idParaDest-207" class="title">Introduction to unit testing in Python</h1>&#13;&#13;
    <p class="normal">There are<a id="_idIndexMarker689"/> multiple <a id="_idIndexMarker690"/>ways to run tests in Python. One, as we have seen above, a bit crude, is to execute code with multiple asserts. A common one is the standard library <code class="Code-In-Text--PACKT-">unittest</code>.</p>&#13;&#13;
    <h2 id="_idParaDest-208" class="title">Python unittest</h2>&#13;&#13;
    <p class="normal"><code class="Code-In-Text--PACKT-">unittest</code> is a module <a id="_idIndexMarker691"/>included in the Python standard library. It is based on the concept of creating a testing class that groups several testing methods. Let's write a new file with the tests written in the proper format, called <code class="Code-In-Text--PACKT-">test_unittest_example.py</code>.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">import unittest&#13;&#13;
from tdd_example import parameter_tdd&#13;&#13;
class TestTDDExample(unittest.TestCase):&#13;&#13;
    def test_negative(self):&#13;&#13;
        self.assertEqual(parameter_tdd(-1), 0)&#13;&#13;
    def test_zero(self):&#13;&#13;
        self.assertEqual(parameter_tdd(0), 0)&#13;&#13;
    def test_five(self):&#13;&#13;
        self.assertEqual(parameter_tdd(5), 25)&#13;&#13;
    def test_seven(self):&#13;&#13;
        # Note this test is incorrect&#13;&#13;
        self.assertEqual(parameter_tdd(7), 0)&#13;&#13;
    def test_ten(self):&#13;&#13;
        self.assertEqual(parameter_tdd(10), 100)&#13;&#13;
    def test_eleven(self):&#13;&#13;
        self.assertEqual(parameter_tdd(11), 100)&#13;&#13;
if __name__ == '__main__':&#13;&#13;
    unittest.main()&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Let's analyze the different elements. The first ones are the imports on top.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">import unittest&#13;&#13;
from tdd_example import parameter_tdd&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">We import the <code class="Code-In-Text--PACKT-">unittest</code> module and the function to test. The most important part comes next, which<a id="_idIndexMarker692"/> defines the tests.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">class TestTDDExample(unittest.TestCase):&#13;&#13;
    def test_negative(self):&#13;&#13;
        self.assertEqual(parameter_tdd(-1), 0)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The class <code class="Code-In-Text--PACKT-">TestTDDExample</code> groups the different tests. Notice that it's inheriting from <code class="Code-In-Text--PACKT-">unittest.TestCase</code>. Then, methods that start with <code class="Code-In-Text--PACKT-">test_</code> will produce the independent tests. Here, we will show one. Internally, it calls the function and compares the result with 0, using the <code class="Code-In-Text--PACKT-">self.assertEqual</code> function.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Notice that <code class="Code-In-Text--PACKT-">test_seven</code> is defined incorrectly. We do this to produce an error when running it.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Finally, we add this code.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">if __name__ == '__main__':&#13;&#13;
    unittest.main()&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This runs the tests automatically if we run the file. So, let's run the file:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ python3 test_unittest_example.py&#13;&#13;
...F..&#13;&#13;
======================================================================&#13;&#13;
FAIL: test_seven (__main__.TestTDDExample)&#13;&#13;
----------------------------------------------------------------------&#13;&#13;
Traceback (most recent call last):&#13;&#13;
  File ".../unittest_example.py", line 17, in test_seven&#13;&#13;
    self.assertEqual(parameter_tdd(7), 0)&#13;&#13;
AssertionError: 49 != 0&#13;&#13;
----------------------------------------------------------------------&#13;&#13;
Ran 6 tests in 0.001s&#13;&#13;
FAILED (failures=1)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">As you can see, it has run all six tests, and shows any errors. Here, we can clearly see the problem. If we need more detail, we can run with <code class="Code-In-Text--PACKT-">-v showing</code> showing each of the tests that are <a id="_idIndexMarker693"/>being run:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ python3 test_unittest_example.py -v&#13;&#13;
test_eleven (__main__.TestTDDExample) ... ok&#13;&#13;
test_five (__main__.TestTDDExample) ... ok&#13;&#13;
test_negative (__main__.TestTDDExample) ... ok&#13;&#13;
test_seven (__main__.TestTDDExample) ... FAIL&#13;&#13;
test_ten (__main__.TestTDDExample) ... ok&#13;&#13;
test_zero (__main__.TestTDDExample) ... ok&#13;&#13;
======================================================================&#13;&#13;
FAIL: test_seven (__main__.TestTDDExample)&#13;&#13;
----------------------------------------------------------------------&#13;&#13;
Traceback (most recent call last):&#13;&#13;
  File ".../unittest_example.py", line 17, in test_seven&#13;&#13;
    self.assertEqual(parameter_tdd(7), 0)&#13;&#13;
AssertionError: 49 != 0&#13;&#13;
----------------------------------------------------------------------&#13;&#13;
Ran 6 tests in 0.001s&#13;&#13;
FAILED (failures=1)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">You can also run a single test or combination of them using the <code class="Code-In-Text--PACKT-">-k</code> option, which searches for matching tests.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ python3 test_unittest_example.py -v -k test_ten&#13;&#13;
test_ten (__main__.TestTDDExample) ... ok&#13;&#13;
----------------------------------------------------------------------&#13;&#13;
Ran 1 test in 0.000s&#13;&#13;
OK&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal"><code class="Code-In-Text--PACKT-">unittest</code> is extremely popular and can accept a lot of options, and it's compatible with virtually every framework in Python. It's also very flexible in terms of ways of testing. For example, there are multiple methods to compare values, like <code class="Code-In-Text--PACKT-">assertNotEqual</code> and <code class="Code-In-Text--PACKT-">assertGreater</code>.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">There's a specific assert function that works differently, which is <code class="Code-In-Text--PACKT-">assertRaises</code>, used to detect when the code generates an exception. We will take a look at it later when testing mocking external calls.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">It also has <code class="Code-In-Text--PACKT-">setUp</code> and <code class="Code-In-Text--PACKT-">tearDown</code> methods to execute code before and after the execution of each test<a id="_idIndexMarker694"/> in the class.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Be sure to take a look at the official documentation: <a href="https://docs.python.org/3/library/unittest.html">https://docs.python.org/3/library/unittest.html</a>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">While <code class="Code-In-Text--PACKT-">unittest</code> is probably the most popular test framework, it's not the most powerful one. Let's take a look at it.</p>&#13;&#13;
    <h2 id="_idParaDest-209" class="title">Pytest</h2>&#13;&#13;
    <p class="normal">Pytest simplifies<a id="_idIndexMarker695"/> writing tests even further. One common complaint about <code class="Code-In-Text--PACKT-">unittest</code> is that it forces you to set a lot of <code class="Code-In-Text--PACKT-">assertCompare</code> calls that are not obvious. It also needs to structure the tests, adding a bit of boilerplate code, like the <code class="Code-In-Text--PACKT-">test</code> class. Other problems are not as obvious, but when creating big test suites, the setup of different tests can start to get complicated.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">A common pattern is to create classes that inherit from other test classes. Over time, that can grow legs of its own.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Pytest instead simplifies the running and defining of tests, and captures all the relevant information using standard <code class="Code-In-Text--PACKT-">assert</code> statements that are easier to read and recognize.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">In this section, we will use <code class="Code-In-Text--PACKT-">pytest</code> in the simplest way. Later in the chapter, we will cover more interesting cases.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Be sure to install <code class="Code-In-Text--PACKT-">pytest</code> through pip in your environment.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ pip3 install pytest&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Let's see how<a id="_idIndexMarker696"/> to run the tests defined in the <code class="Code-In-Text--PACKT-">unittest</code>, in the file <code class="Code-In-Text--PACKT-">test_pytest_example.py</code>.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">from tdd_example import parameter_tdd&#13;&#13;
def test_negative():&#13;&#13;
    assert parameter_tdd(-1) == 0&#13;&#13;
def test_zero():&#13;&#13;
    assert parameter_tdd(0) == 0&#13;&#13;
def test_five():&#13;&#13;
    assert parameter_tdd(5) == 25&#13;&#13;
def test_seven():&#13;&#13;
    # Note this test is deliberatly set to fail&#13;&#13;
    assert parameter_tdd(7) == 0&#13;&#13;
def test_ten():&#13;&#13;
    assert parameter_tdd(10) == 100&#13;&#13;
def test_eleven():&#13;&#13;
    assert parameter_tdd(11) == 100&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">If you compare it with the equivalent code in <code class="Code-In-Text--PACKT-">test_unittest_example.py</code>, the code is significantly leaner. When<a id="_idIndexMarker697"/> running it with <code class="Code-In-Text--PACKT-">pytest</code>, it also shows more detailed, colored information.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ pytest test_unittest_example.py&#13;&#13;
================= test session starts =================&#13;&#13;
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1&#13;&#13;
collected 6 items&#13;&#13;
test_unittest_example.py ...F..                 [100%]&#13;&#13;
====================== FAILURES =======================&#13;&#13;
______________ TestTDDExample.test_seven ______________&#13;&#13;
self = &lt;test_unittest_example.TestTDDExample testMethod=test_seven&gt;&#13;&#13;
    def test_seven(self):&#13;&#13;
&gt;       self.assertEqual(parameter_tdd(7), 0)&#13;&#13;
E       AssertionError: 49 != 0&#13;&#13;
test_unittest_example.py:17: AssertionError&#13;&#13;
=============== short test summary info ===============&#13;&#13;
FAILED test_unittest_example.py::TestTDDExample::test_seven&#13;&#13;
============= 1 failed, 5 passed in 0.10s =============&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">As with <code class="Code-In-Text--PACKT-">unittest</code>, we can see more information with <code class="Code-In-Text--PACKT-">-v</code> and run a selection of tests with <code class="Code-In-Text--PACKT-">-k</code>.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ pytest -v test_unittest_example.py&#13;&#13;
========================= test session starts =========================&#13;&#13;
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/local/opt/python@3.9/bin/python3.9&#13;&#13;
cachedir: .pytest_cache&#13;&#13;
collected 6 items&#13;&#13;
test_unittest_example.py::TestTDDExample::test_eleven PASSED      [16%]&#13;&#13;
test_unittest_example.py::TestTDDExample::test_five PASSED        [33%]&#13;&#13;
test_unittest_example.py::TestTDDExample::test_negative PASSED    [50%]&#13;&#13;
test_unittest_example.py::TestTDDExample::test_seven FAILED       [66%]&#13;&#13;
test_unittest_example.py::TestTDDExample::test_ten PASSED         [83%]&#13;&#13;
test_unittest_example.py::TestTDDExample::test_zero PASSED        [100%]&#13;&#13;
============================== FAILURES ===============================&#13;&#13;
______________________ TestTDDExample.test_seven ______________________&#13;&#13;
self = &lt;test_unittest_example.TestTDDExample testMethod=test_seven&gt;&#13;&#13;
    def test_seven(self):&#13;&#13;
&gt;       self.assertEqual(parameter_tdd(7), 0)&#13;&#13;
E       AssertionError: 49 != 0&#13;&#13;
test_unittest_example.py:17: AssertionError&#13;&#13;
======================= short test summary info =======================&#13;&#13;
FAILED test_unittest_example.py::TestTDDExample::test_seven - AssertionErr...&#13;&#13;
===================== 1 failed, 5 passed in 0.08s =====================&#13;&#13;
$ pytest test_pytest_example.py -v -k test_ten&#13;&#13;
========================= test session starts =========================&#13;&#13;
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/local/opt/python@3.9/bin/python3.9&#13;&#13;
cachedir: .pytest_cache&#13;&#13;
collected 6 items / 5 deselected / 1 selected&#13;&#13;
test_pytest_example.py::test_ten PASSED                           [100%]&#13;&#13;
=================== 1 passed, 5 deselected in 0.02s ===================&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">And it's totally compatible with <code class="Code-In-Text--PACKT-">unittest</code> defined tests, which allows you to combine both styles or migrate them.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ pytest test_unittest_example.py&#13;&#13;
========================= test session starts =========================&#13;&#13;
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1&#13;&#13;
collected 6 items&#13;&#13;
test_unittest_example.py ...F..                                   [100%]&#13;&#13;
============================== FAILURES ===============================&#13;&#13;
______________________ TestTDDExample.test_seven ______________________&#13;&#13;
self = &lt;test_unittest_example.TestTDDExample testMethod=test_seven&gt;&#13;&#13;
    def test_seven(self):&#13;&#13;
&gt;       self.assertEqual(parameter_tdd(7), 0)&#13;&#13;
E       AssertionError: 49 != 0&#13;&#13;
test_unittest_example.py:17: AssertionError&#13;&#13;
======================= short test summary info =======================&#13;&#13;
FAILED test_unittest_example.py::TestTDDExample::test_seven - AssertionErr...&#13;&#13;
===================== 1 failed, 5 passed in 0.08s =====================&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Another great <a id="_idIndexMarker698"/>feature of <code class="Code-In-Text--PACKT-">pytest</code> is easy autodiscovery to find files that start with <code class="Code-In-Text--PACKT-">test_</code> and run inside all the tests. If we try it, pointing at the current directory, we can see it runs both <code class="Code-In-Text--PACKT-">test_unittest_example.py</code> and <code class="Code-In-Text--PACKT-">test_pytest_example.py</code>.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ pytest .&#13;&#13;
========================= test session starts =========================&#13;&#13;
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1&#13;&#13;
collected 12 items&#13;&#13;
test_pytest_example.py ...F..                                    [50%]&#13;&#13;
test_unittest_example.py ...F..                                  [100%]&#13;&#13;
============================== FAILURES ===============================&#13;&#13;
_____________________________ test_seven ______________________________&#13;&#13;
    def test_seven():&#13;&#13;
        # Note this test is deliberatly set to fail&#13;&#13;
&gt;       assert parameter_tdd(7) == 0&#13;&#13;
E       assert 49 == 0&#13;&#13;
E        +  where 49 = parameter_tdd(7)&#13;&#13;
test_pytest_example.py:18: AssertionError&#13;&#13;
______________________ TestTDDExample.test_seven ______________________&#13;&#13;
self = &lt;test_unittest_example.TestTDDExample testMethod=test_seven&gt;&#13;&#13;
    def test_seven(self):&#13;&#13;
&gt;       self.assertEqual(parameter_tdd(7), 0)&#13;&#13;
E       AssertionError: 49 != 0&#13;&#13;
test_unittest_example.py:17: AssertionError&#13;&#13;
======================= short test summary info =======================&#13;&#13;
FAILED test_pytest_example.py::test_seven - assert 49 == 0&#13;&#13;
FAILED test_unittest_example.py::TestTDDExample::test_seven - AssertionErr...&#13;&#13;
==================== 2 failed, 10 passed in 0.23s =====================&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">We will continue<a id="_idIndexMarker699"/> talking about more features of <code class="Code-In-Text--PACKT-">pytest</code> during the chapter, but first, we need to go back to how to define tests when the code has dependencies.</p>&#13;&#13;
    <h1 id="_idParaDest-210" class="title">Testing external dependencies</h1>&#13;&#13;
    <p class="normal">When building<a id="_idIndexMarker700"/> unit tests, we talked about how it's based around the concept of isolating a unit in the code to test it independently.</p>&#13;&#13;
    <p class="normal">This isolation concept is key, as we want to focus on small sections of the code to create small, clear tests. Creating small tests also helps in keeping the tests fast.</p>&#13;&#13;
    <p class="normal">In our example above, we tested a purely functional function, <code class="Code-In-Text--PACKT-">parameter_tdd</code>, that had no dependencies. It was not using any external library or any other function. But inevitably, at some point, you'll need to test something that depends on something else.</p>&#13;&#13;
    <p class="normal">The question in this case is <em class="italic">should the other component be part of the test or not?</em></p>&#13;&#13;
    <p class="normal">This is not an easy question to answer. Some developers think that all unit tests should be purely about a single function or method, and therefore, any dependency should not be part of the test. But, on a more practical level, there are sometimes pieces of code that form a unit that it's easier to test in conjunction than separately.</p>&#13;&#13;
    <p class="normal">For example, think about a function that:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">For values lower than 0, returns zero.</li>&#13;&#13;
      <li class="bullet">For values greater than 100, returns 10.</li>&#13;&#13;
      <li class="bullet">For values between, it returns the square root of the value. Note that for the edges, it returns the square root of them (0 for 0 and 10 for 100).</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">This is very similar to the previous function, <code class="Code-In-Text--PACKT-">parameter_tdd,</code> but this time we need the help of an external library to produce the square root of a number. Let's take a look at the code.</p>&#13;&#13;
    <p class="normal">It's divided into two files. <code class="Code-In-Text--PACKT-">dependent.py</code> contains the definition of the function.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">import math&#13;&#13;
def parameter_dependent(value):&#13;&#13;
    if value &lt; 0:&#13;&#13;
        return 0&#13;&#13;
    if value &lt;= 100:&#13;&#13;
        return math.sqrt(value)&#13;&#13;
    return 10&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The code is pretty similar to the code in the <code class="Code-In-Text--PACKT-">parameter_tdd</code> example. The module <code class="Code-In-Text--PACKT-">math.sqrt</code> returns the square root of a number.</p>&#13;&#13;
    <p class="normal">And the tests are in <code class="Code-In-Text--PACKT-">test_dependent.py</code>.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">from dependent import parameter_dependent&#13;&#13;
def test_negative():&#13;&#13;
    assert parameter_dependent(-1) == 0&#13;&#13;
def test_zero():&#13;&#13;
    assert parameter_dependent(0) == 0&#13;&#13;
def test_twenty_five():&#13;&#13;
    assert parameter_dependent(25) == 5&#13;&#13;
def test_hundred():&#13;&#13;
    assert parameter_dependent(100) == 10&#13;&#13;
def test_hundred_and_one():&#13;&#13;
    assert parameter_dependent(101) == 10&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">In this case, we are <a id="_idIndexMarker701"/>completely using the external library and testing it at the same time that we are testing our code. For this simple example, this is a perfectly valid option, though that may not be the case for other cases.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">The code is available in GitHub at <a href="https://github.com/PacktPublishing/Python-Architecture-Patterns/tree/main/chapter_10_testing_and_tdd">https://github.com/PacktPublishing/Python-Architecture-Patterns/tree/main/chapter_10_testing_and_tdd</a>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">For example, the external dependency could be making external HTTP calls that need to be captured to prevent making them while running tests and to have control over the returned values, or other big pieces of functionality that should be tested in isolation.</p>&#13;&#13;
    <p class="normal">To detach a function from its dependencies, there are two different approaches. We will show them using <code class="Code-In-Text--PACKT-">parameter_dependent</code> as a baseline.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Again, in this case, the tests work perfectly fine with the dependency included, as it's simple and doesn't produce side effects like external calls, etc.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">We will see <a id="_idIndexMarker702"/>next how to mock the external calls.</p>&#13;&#13;
    <h2 id="_idParaDest-211" class="title">Mocking</h2>&#13;&#13;
    <p class="normal">Mocking is a<a id="_idIndexMarker703"/> practice that internally replaces the dependencies, replacing <a id="_idIndexMarker704"/>them with fake calls, under the control of the test itself. This way, we can introduce a known response for any external dependency, and not call the actual code.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">Internally, mocking is implemented using what is known as <em class="italic">monkey-patching</em>, which is the dynamic replacement of existing libraries with alternatives. While this can be achieved in different ways in different programming languages, it's especially popular in dynamic languages like Python or Ruby. Monkey-patching can be used for other purposes than testing, though it should be used with care, as it can change the behavior of libraries and can be quite disconcerting for debugging.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">To be able to mock the code, in our test code, we need to prepare the mock as part of the Arrange step. There are different libraries to mock calls, but the easiest is to use the <code class="Code-In-Text--PACKT-">unittest.mock</code> library included as part of the standard library.</p>&#13;&#13;
    <p class="normal">The easiest usage of <code class="Code-In-Text--PACKT-">mock</code> is to patch an external library:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">from unittest.mock import patch&#13;&#13;
from dependent import parameter_dependent&#13;&#13;
@patch('math.sqrt')&#13;&#13;
def test_twenty_five(mock_sqrt):&#13;&#13;
    mock_sqrt.return_value = 5&#13;&#13;
    assert parameter_dependent(25) == 5&#13;&#13;
    mock_sqrt.assert_called_once_with(25)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The <code class="Code-In-Text--PACKT-">patch</code> decorator intercepts the calls to the defined library, <code class="Code-In-Text--PACKT-">math.sqrt</code>, and replaces it with a <code class="Code-In-Text--PACKT-">mock</code> object that passes to the function, here called <code class="Code-In-Text--PACKT-">mock_sqrt</code>.</p>&#13;&#13;
    <p class="normal">This object is a bit special. It basically allows any calls, accesses almost any method or attributes (except predefined ones), and keeps returning a mock object. This makes the mock object something really flexible that will adapt to whatever code surrounds it. When necessary, the returning value can be set calling <code class="Code-In-Text--PACKT-">.return_value</code>, as we show in the first line.</p>&#13;&#13;
    <p class="normal">We are, in essence, saying that calls to <code class="Code-In-Text--PACKT-">mock_sqrt</code> will return the value 5. So, we are preparing the output of the external call, so we can control it.</p>&#13;&#13;
    <p class="normal">Finally, we check that we called the mock <code class="Code-In-Text--PACKT-">mock_sqrt</code> once, with the input (<code class="Code-In-Text--PACKT-">25</code>) using the method <code class="Code-In-Text--PACKT-">assert_called_once_with</code>.</p>&#13;&#13;
    <p class="normal">In essence, we are:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">Preparing <a id="_idIndexMarker705"/>the<a id="_idIndexMarker706"/> mock so it replaces <code class="Code-In-Text--PACKT-">math.sqrt</code></li>&#13;&#13;
      <li class="bullet">Setting the value that it will return when called</li>&#13;&#13;
      <li class="bullet">Checking that the call works as expected</li>&#13;&#13;
      <li class="bullet">Double-checking that the mock was called with the right value</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">For other tests, for example, we can check that the mock was not called, indicating that the external dependence wasn't called.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">@patch('math.sqrt')&#13;&#13;
def test_hundred_and_one(mock_sqrt):&#13;&#13;
    assert parameter_dependent(101) == 10&#13;&#13;
    mock_sqrt.assert_not_called()&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">There are multiple <code class="Code-In-Text--PACKT-">assert</code> functions that allow you to detect how the mock has been used. Some examples:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">The <code class="Code-In-Text--PACKT-">called</code> attribute returning <code class="Code-In-Text--PACKT-">True</code> or <code class="Code-In-Text--PACKT-">False</code> based on whether the mock has been called or not, allowing you to write:&#13;&#13;
        <pre class="programlisting gen"><code class="hljs"><code class="Code-In-Text--PACKT-">assert mock_sqrt.called is True</code>&#13;&#13;
</code></pre>&#13;&#13;
      </li>&#13;&#13;
      <li class="bullet">The <code class="Code-In-Text--PACKT-">call_count</code> attribute returning the number of times a mock has been called.</li>&#13;&#13;
      <li class="bullet">The <code class="Code-In-Text--PACKT-">assert_called_with()</code> method to check the number of times that it has been called. It will raise an exception if the last call is not produced in the specified way.</li>&#13;&#13;
      <li class="bullet">The <code class="Code-In-Text--PACKT-">assert_any_call()</code> method to check whether any of the calls have been produced in the specified way.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">With that<a id="_idIndexMarker707"/> information, the <a id="_idIndexMarker708"/>full file for testing, <code class="Code-In-Text--PACKT-">test_dependent_mocked_test.py</code>, will be like this.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">from unittest.mock import patch&#13;&#13;
from dependent import parameter_dependent&#13;&#13;
@patch('math.sqrt')&#13;&#13;
def test_negative(mock_sqrt):&#13;&#13;
    assert parameter_dependent(-1) == 0&#13;&#13;
    mock_sqrt.assert_not_called()&#13;&#13;
@patch('math.sqrt')&#13;&#13;
def test_zero(mock_sqrt):&#13;&#13;
    mock_sqrt.return_value = 0&#13;&#13;
    assert parameter_dependent(0) == 0&#13;&#13;
    mock_sqrt.assert_called_once_with(0)&#13;&#13;
@patch('math.sqrt')&#13;&#13;
def test_twenty_five(mock_sqrt):&#13;&#13;
    mock_sqrt.return_value = 5&#13;&#13;
    assert parameter_dependent(25) == 5&#13;&#13;
    mock_sqrt.assert_called_with(25)&#13;&#13;
@patch('math.sqrt')&#13;&#13;
def test_hundred(mock_sqrt):&#13;&#13;
    mock_sqrt.return_value = 10&#13;&#13;
    assert parameter_dependent(100) == 10&#13;&#13;
    mock_sqrt.assert_called_with(100)&#13;&#13;
@patch('math.sqrt')&#13;&#13;
def test_hundred_and_one(mock_sqrt):&#13;&#13;
    assert parameter_dependent(101) == 10&#13;&#13;
    mock_sqrt.assert_not_called()&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">If the mock needs to return different values, you can define the <code class="Code-In-Text--PACKT-">side_effect</code> attribute of the mock as a list or tuple. <code class="Code-In-Text--PACKT-">side_effect</code> is similar to <code class="Code-In-Text--PACKT-">return_value</code>, but it has a few differences, as we'll see.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">@patch('math.sqrt')&#13;&#13;
def test_multiple_returns_mock(mock_sqrt):&#13;&#13;
    mock_sqrt.side_effect = (5, 10)&#13;&#13;
    assert parameter_dependent(25) == 5&#13;&#13;
    assert parameter_dependent(100) == 10&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal"><code class="Code-In-Text--PACKT-">side_effect</code> can also <a id="_idIndexMarker709"/>be<a id="_idIndexMarker710"/> used to produce an exception, if needed.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">import pytest&#13;&#13;
from unittest.mock import patch&#13;&#13;
from dependent import parameter_dependent&#13;&#13;
@patch('math.sqrt')&#13;&#13;
def test_exception_raised_mock(mock_sqrt):&#13;&#13;
    mock_sqrt.side_effect = ValueError('Error on the external library')&#13;&#13;
    with pytest.raises(ValueError):&#13;&#13;
        parameter_dependent(25)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The <code class="Code-In-Text--PACKT-">with</code> section asserts that the expected <code class="Code-In-Text--PACKT-">Exception</code> is raised in the block. If not, it shows an error.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">In <code class="Code-In-Text--PACKT-">unittest</code>, checking a raised exception can be done with a similar <code class="Code-In-Text--PACKT-">with</code> block.</p>&#13;&#13;
      <p class="Tip--PACKT-"><code class="Code-In-Text--PACKT-">with self.assertRaises(ValueError):</code></p>&#13;&#13;
      <p class="Tip--PACKT-"><code class="Code-In-Text--PACKT-"> parameter_dependent(25)</code></p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Mocking is not the only way to handle dependencies for tests. We will see a different approach next.</p>&#13;&#13;
    <h2 id="_idParaDest-212" class="title">Dependency injection</h2>&#13;&#13;
    <p class="normal">While mocking<a id="_idIndexMarker711"/> replaces the dependency without the original code noticing, by patching it externally, dependency injection is a technique to make that dependency explicit when calling the function under test, so it can be replaced with a testing substitute.</p>&#13;&#13;
    <p class="normal">In essence, it's a way of designing the code that makes dependencies explicit by requiring them as input parameters.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Information-Box--PACKT-">Dependency injection, while useful for testing, is not only aimed at that. By adding the dependencies explicitly, it also reduces the need for a function to know how to initialize a particular dependency, instead relying on the interface of the dependency. It creates a separation between "initializing" a dependency (which should be taken care of externally) and "using" it (which is the only part that the dependent code will do). This differentiation will become clearer later when we see an OOP example.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Let's see<a id="_idIndexMarker712"/> how this changes the code under test.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def parameter_dependent(value, sqrt_func):&#13;&#13;
    if value &lt; 0:&#13;&#13;
        return 0&#13;&#13;
    if value &lt;= 100:&#13;&#13;
        return sqrt_func(value)&#13;&#13;
    return 10&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Notice how now the <code class="Code-In-Text--PACKT-">sqrt</code> function is an input parameter. </p>&#13;&#13;
    <p class="normal">If we want to use the <code class="Code-In-Text--PACKT-">parameter_dependent</code> function in a normal scenario, we will have to produce the dependency, for example.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">import math&#13;&#13;
def test_good_dependency():&#13;&#13;
    assert parameter_dependent(25, math.sqrt) == 5&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">And if we want to perform tests, we can do it by replacing the <code class="Code-In-Text--PACKT-">math.sqrt</code> function with a specific function, and then using it. For example:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def test_twenty_five():&#13;&#13;
    def good_dependency(number):&#13;&#13;
        return 5&#13;&#13;
    assert parameter_dependent(25, good_dependency) == 5&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">We can also provoke an error if calling the dependency to ensure that in some tests the dependency is not used, for example.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def test_negative():&#13;&#13;
    def bad_dependency(number):&#13;&#13;
        raise Exception('Function called')&#13;&#13;
    assert parameter_dependent(-1, bad_dependency) == 0&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Note how this approach is more explicit than mocking. The code to test becomes, in essence, totally <a id="_idIndexMarker713"/>functional as it doesn't have external dependencies.</p>&#13;&#13;
    <h2 id="_idParaDest-213" class="title">Dependency injection in OOP</h2>&#13;&#13;
    <p class="normal">Dependency<a id="_idIndexMarker714"/> injection <a id="_idIndexMarker715"/>can also be used with OOP. In this case, we can start with code that is like this.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">class Writer:&#13;&#13;
    def __init__(self):&#13;&#13;
        self.path = settings.WRITER_PATH&#13;&#13;
    def write(self, filename, data):&#13;&#13;
        with open(self.path + filename, 'w') as fp:&#13;&#13;
            fp.write(data)&#13;&#13;
class Model:&#13;&#13;
    def __init__(self, data):&#13;&#13;
        self.data = data&#13;&#13;
        self.filename = settings.MODEL_FILE&#13;&#13;
        self.writer = Writer()&#13;&#13;
    def save(self):&#13;&#13;
        self.writer.write(self.filename, self.data)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">As we can see, the <code class="Code-In-Text--PACKT-">settings</code> class stores different elements that are required on where the data will be stored. The model receives some data and then saves it. The code in operation will require minimal initialization.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">    model = Model('test')&#13;&#13;
    model.save()&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The model receives some data and then saves it. The code in operation requires minimal initialization, but at the same time, it's not explicit.</p>&#13;&#13;
    <p class="normal">To use <a id="_idIndexMarker716"/>dependency injection principles, the code will need to be<a id="_idIndexMarker717"/> written in this way:</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">class WriterInjection:&#13;&#13;
    def __init__(self, path):&#13;&#13;
        self.path = path&#13;&#13;
    def write(self, filename, data):&#13;&#13;
        with open(self.path + filename, 'w') as fp:&#13;&#13;
            fp.write(data)&#13;&#13;
class ModelInjection:&#13;&#13;
    def __init__(self, data, filename, writer):&#13;&#13;
        self.data = data&#13;&#13;
        self.filename = filename&#13;&#13;
        self.writer = writer&#13;&#13;
    def save(self):&#13;&#13;
        self.writer.write(self.filename, self.data)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">In this case, every value that is a dependency is provided explicitly. In the definition of the code, the <code class="Code-In-Text--PACKT-">settings</code> module is not present anywhere, but instead, that will be specified when the class is instantiated. The code will now need to define the configuration directly.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">    writer = WriterInjection('./')&#13;&#13;
    model = ModelInjection('test', 'model_injection.txt', writer)&#13;&#13;
    model.save()&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">We can compare how to test both cases, as seen in the file <code class="Code-In-Text--PACKT-">test_dependency_injection_test.py</code>. The first test is mocking, as we saw before, the <code class="Code-In-Text--PACKT-">write</code> method of the <code class="Code-In-Text--PACKT-">Writer</code> class to assert that it has been called correctly.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">@patch('class_injection.Writer.write')&#13;&#13;
def test_model(mock_write):&#13;&#13;
    model = Model('test_model')&#13;&#13;
    model.save()&#13;&#13;
    mock_write.assert_called_with('model.txt', 'test_model')&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Compared to that, the dependency injection example doesn't require a mock through monkey-patching. It<a id="_idIndexMarker718"/> just creates its own <code class="Code-In-Text--PACKT-">Writer</code> that simulates the<a id="_idIndexMarker719"/> interface.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def test_modelinjection():&#13;&#13;
    EXPECTED_DATA = 'test_modelinjection'&#13;&#13;
    EXPECTED_FILENAME = 'model_injection.txt'&#13;&#13;
    class MockWriter:&#13;&#13;
        def write(self, filename, data):&#13;&#13;
            self.filename = filename&#13;&#13;
            self.data = data&#13;&#13;
    writer = MockWriter()&#13;&#13;
    model = ModelInjection(EXPECTED_DATA, EXPECTED_FILENAME,&#13;&#13;
                           writer)&#13;&#13;
    model.save()&#13;&#13;
    assert writer.data == EXPECTED_DATA&#13;&#13;
    assert writer.filename == EXPECTED_FILENAME&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This second style is more verbose, but it shows some of the differences when writing code in this way:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet">No monkey-patching mock is required. Monkey-patching can be quite fragile, as it's meddling with internal code that's not supposed to be exposed. While in testing this interference is not the same as doing it for regular code running, it's still something that can be messy and have unintended effects, especially if the internal code changes in some unforeseen way. <p class="bullet">Keep in mind that mocks will likely involve, at some point, relating to second-level dependencies, which can start having strange or complicated effects requiring you to spend time handling that extra complexity.</p>&#13;&#13;
      </li>&#13;&#13;
      <li class="bullet">The way of writing the code is different in itself. Code produced with dependency injection is, as we've seen, more modular and composed of smaller elements. This tends to create smaller and more combinable modules that play along together, with fewer unknown dependencies, as they are always explicit.</li>&#13;&#13;
      <li class="bullet">Be careful, though, as this requires a certain amount of discipline and mental framing to produce truly loosely coupled modules. If this is not considered when designing the interfaces, the resulting code will instead be artificially divided, resulting in tightly coupled code across different modules. Developing this discipline requires certain training; do not expect it to come naturally to all developers.</li>&#13;&#13;
      <li class="bullet">The code can sometimes be more difficult to debug, as the configuration will be separated from the rest of the code, sometimes making it difficult to understand the flow of the code. The complexity can be produced at the interaction of classes, which may be more difficult to understand and test. Typically, the upfront effort to develop code in this style is a bit greater as well.</li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Dependency injection is a very popular technique in certain software circles and programming languages. Mocking is more difficult in less dynamic languages than Python, and<a id="_idIndexMarker720"/> also<a id="_idIndexMarker721"/> different programming languages have their own sets of ideas on how to structure code. For example, dependency injection is very popular in Java, where there are specific tools to work in this style.</p>&#13;&#13;
    <h1 id="_idParaDest-214" class="title">Advanced pytest</h1>&#13;&#13;
    <p class="normal">While we've <a id="_idIndexMarker722"/>described the basic functionalities for <code class="Code-In-Text--PACKT-">pytest</code>, we barely scratched the surface in terms of the number of possibilities that it presents to help generate testing code.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">Pytest is a big and comprehensive tool. It is worth learning how to use it. Here, we will only scratch<a id="_idIndexMarker723"/> the surface. Be sure to check the official documentation at <a href="https://docs.pytest.org/">https://docs.pytest.org/</a>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">Without being exhaustive, we will see some useful possibilities of the tool.</p>&#13;&#13;
    <h2 id="_idParaDest-215" class="title">Grouping tests</h2>&#13;&#13;
    <p class="normal">Sometimes it is <a id="_idIndexMarker724"/>useful to group tests together so they are related to specific things, like modules, or to run them in unison. The simplest way of grouping tests together is to join them into a single class.</p>&#13;&#13;
    <p class="normal">For example, going back to the test examples before, we could structure tests into two classes, as we see in <code class="Code-In-Text--PACKT-">test_group_classes.py</code>.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">from tdd_example import parameter_tdd&#13;&#13;
class TestEdgesCases():&#13;&#13;
    def test_negative(self):&#13;&#13;
        assert parameter_tdd(-1) == 0&#13;&#13;
    def test_zero(self):&#13;&#13;
        assert parameter_tdd(0) == 0&#13;&#13;
    def test_ten(self):&#13;&#13;
        assert parameter_tdd(10) == 100&#13;&#13;
    def test_eleven(self):&#13;&#13;
        assert parameter_tdd(11) == 100&#13;&#13;
class TestRegularCases():&#13;&#13;
    def test_five(self):&#13;&#13;
        assert parameter_tdd(5) == 25&#13;&#13;
    def test_seven(self):&#13;&#13;
        assert parameter_tdd(7) == 49&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This is an easy way to divide tests and allows you to run them independently:</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ pytest -v test_group_classes.py&#13;&#13;
======================== test session starts =========================&#13;&#13;
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/local/opt/python@3.9/bin/python3.9&#13;&#13;
collected 6 items&#13;&#13;
test_group_classes.py::TestEdgesCases::test_negative PASSED      [16%]&#13;&#13;
test_group_classes.py::TestEdgesCases::test_zero PASSED          [33%]&#13;&#13;
test_group_classes.py::TestEdgesCases::test_ten PASSED           [50%]&#13;&#13;
test_group_classes.py::TestEdgesCases::test_eleven PASSED        [66%]&#13;&#13;
test_group_classes.py::TestRegularCases::test_five PASSED        [83%]&#13;&#13;
test_group_classes.py::TestRegularCases::test_seven PASSED       [100%]&#13;&#13;
========================= 6 passed in 0.02s ==========================&#13;&#13;
$ pytest -k TestRegularCases -v test_group_classes.py&#13;&#13;
========================= test session starts ========================&#13;&#13;
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/local/opt/python@3.9/bin/python3.9&#13;&#13;
collected 6 items / 4 deselected / 2 selected&#13;&#13;
test_group_classes.py::TestRegularCases::test_five PASSED        [50%]&#13;&#13;
test_group_classes.py::TestRegularCases::test_seven PASSED       [100%]&#13;&#13;
================== 2 passed, 4 deselected in 0.02s ===================&#13;&#13;
$ pytest -v test_group_classes.py::TestRegularCases&#13;&#13;
========================= test session starts ========================&#13;&#13;
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/local/opt/python@3.9/bin/python3.9&#13;&#13;
cachedir: .pytest_cache&#13;&#13;
rootdir: /Users/jaime/Dropbox/Packt/architecture_book/chapter_09_testing_and_tdd/advanced_pytest&#13;&#13;
plugins: celery-4.4.7&#13;&#13;
collected 2 items&#13;&#13;
test_group_classes.py::TestRegularCases::test_five PASSED        [50%]&#13;&#13;
test_group_classes.py::TestRegularCases::test_seven PASSED       [100%]&#13;&#13;
========================== 2 passed in 0.02s =========================&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Another possibility is to use markers. Markers are indicators that can be added through a decorator in <a id="_idIndexMarker725"/>the tests, for example, in <code class="Code-In-Text--PACKT-">test_markers.py</code>.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">import pytest&#13;&#13;
from tdd_example import parameter_tdd&#13;&#13;
@pytest.mark.edge&#13;&#13;
def test_negative():&#13;&#13;
    assert parameter_tdd(-1) == 0&#13;&#13;
@pytest.mark.edge&#13;&#13;
def test_zero():&#13;&#13;
    assert parameter_tdd(0) == 0&#13;&#13;
def test_five():&#13;&#13;
    assert parameter_tdd(5) == 25&#13;&#13;
def test_seven():&#13;&#13;
    assert parameter_tdd(7) == 49&#13;&#13;
@pytest.mark.edge&#13;&#13;
def test_ten():&#13;&#13;
    assert parameter_tdd(10) == 100&#13;&#13;
@pytest.mark.edge&#13;&#13;
def test_eleven():&#13;&#13;
    assert parameter_tdd(11) == 100 &#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">See that we are defining a decorator, <code class="Code-In-Text--PACKT-">@pytest.mark.edge</code>, on all the tests that checks the edge of the values. </p>&#13;&#13;
    <p class="normal">If we execute the<a id="_idIndexMarker726"/> tests, we can use the parameter <code class="Code-In-Text--PACKT-">-m</code> to run only the ones with a certain tag.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con"> $ pytest -m edge -v test_markers.py&#13;&#13;
========================= test session starts ========================&#13;&#13;
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/local/opt/python@3.9/bin/python3.9&#13;&#13;
collected 6 items / 2 deselected / 4 selected&#13;&#13;
test_markers.py::test_negative PASSED                            [25%]&#13;&#13;
test_markers.py::test_zero PASSED                                [50%]&#13;&#13;
test_markers.py::test_ten PASSED                                 [75%]&#13;&#13;
test_markers.py::test_eleven PASSED                              [100%]&#13;&#13;
========================== warnings summary ==========================&#13;&#13;
test_markers.py:5&#13;&#13;
  test_markers.py:5: PytestUnknownMarkWarning: Unknown pytest.mark.edge - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/mark.html&#13;&#13;
    @pytest.mark.edge&#13;&#13;
test_markers.py:10&#13;&#13;
...&#13;&#13;
-- Docs: https://docs.pytest.org/en/stable/warnings.html&#13;&#13;
============ 4 passed, 2 deselected, 4 warnings in 0.02s =============&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The warning <code class="Code-In-Text--PACKT-">PytestUnknownMarkWarning: Unknown pytest.mark.edge</code> is produced if the marker <code class="Code-In-Text--PACKT-">edge</code> is not registered. </p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">Be aware that the GitHub code includes the <code class="Code-In-Text--PACKT-">pytest.ini</code> code. You won't see the warning if the <code class="Code-In-Text--PACKT-">pytest.ini</code> file is present, for example, if you clone the whole repo.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">This is very useful for finding typos, like accidentally writing <code class="Code-In-Text--PACKT-">egde</code> or similar. To avoid this warning, you'll need to add a <code class="Code-In-Text--PACKT-">pytest.ini</code> config file with the definition of the markers, like this.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">[pytest]&#13;&#13;
markers =&#13;&#13;
       edge: tests related to edges in intervals&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Now, running<a id="_idIndexMarker727"/> the tests shows no warning.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ pytest -m edge -v test_markers.py&#13;&#13;
========================= test session starts =========================&#13;&#13;
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/local/opt/python@3.9/bin/python3.9&#13;&#13;
cachedir: .pytest_cache&#13;&#13;
rootdir: /Users/jaime/Dropbox/Packt/architecture_book/chapter_09_testing_and_tdd/advanced_pytest, configfile: pytest.ini&#13;&#13;
plugins: celery-4.4.7&#13;&#13;
collected 6 items / 2 deselected / 4 selected&#13;&#13;
test_markers.py::test_negative PASSED                            [25%]&#13;&#13;
test_markers.py::test_zero PASSED                                [50%]&#13;&#13;
test_markers.py::test_ten PASSED                                 [75%]&#13;&#13;
test_markers.py::test_eleven PASSED                              [100%]&#13;&#13;
=================== 4 passed, 2 deselected in 0.02s ===================&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Note that markers can be used across the full test suite, including multiple files. That allows for making markers to identify common patterns across the tests, for example, creating a quick test suite with the most important tests to run with the marker <code class="Code-In-Text--PACKT-">basic</code>.</p>&#13;&#13;
    <p class="normal">There are also some predefined markers with some built-in features. The most common ones are <code class="Code-In-Text--PACKT-">skip</code> (which will skip the test) and <code class="Code-In-Text--PACKT-">xfail</code> (which will reverse the test, meaning that it expects <a id="_idIndexMarker728"/>it to fail).</p>&#13;&#13;
    <h2 id="_idParaDest-216" class="title">Using fixtures</h2>&#13;&#13;
    <p class="normal">The use of fixtures<a id="_idIndexMarker729"/> is the preferred way to set up tests in <code class="Code-In-Text--PACKT-">pytest</code>. A<a id="_idIndexMarker730"/> fixture, in essence, is a context created to set up a test.</p>&#13;&#13;
    <p class="normal">Fixtures are used as input for the test functions, so they can be set up and create specific environments for the test to be created.</p>&#13;&#13;
    <p class="normal">For example, let's take a look at a simple function that counts the number of occurrences of a character in a string.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def count_characters(char_to_count, string_to_count):&#13;&#13;
    number = 0&#13;&#13;
    for char in string_to_count:&#13;&#13;
        if char == char_to_count:&#13;&#13;
            number += 1&#13;&#13;
    return number&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">That's a pretty simple loop that iterates through the string and counts the matching characters.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">This is equivalent to using the function <code class="Code-In-Text--PACKT-">.count()</code> for a string, but this is included to present a working function. It could be refactored afterward!</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">A regular test to cover the functionalities could be as follows.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def test_counting():&#13;&#13;
    assert count_characters('a', 'Barbara Ann') == 3&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Pretty straightforward. Now let's see how we can define a fixture to define a setup, in case we want to replicate it.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">import pytest&#13;&#13;
@pytest.fixture()&#13;&#13;
def prepare_string():&#13;&#13;
    # Setup the values to return&#13;&#13;
    prepared_string = 'Ba, ba, ba, Barbara Ann'&#13;&#13;
    # Return the value&#13;&#13;
    yield prepared_string&#13;&#13;
    # Teardown any value&#13;&#13;
    del prepared_string&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">First of all, the fixture is decorated with <code class="Code-In-Text--PACKT-">pytest.fixture</code> to mark it as such. A fixture is divided into three steps:</p>&#13;&#13;
    <ul>&#13;&#13;
      <li class="bullet"><strong class="keyword">Setup</strong>: Here, we <a id="_idIndexMarker731"/>simply defined a string, but this will probably be the biggest part, where the values are prepared.</li>&#13;&#13;
      <li class="bullet"><strong class="keyword">Return the value</strong>: If we use the <code class="Code-In-Text--PACKT-">yield</code> functionality, we will be able to go to the next step; if not, the fixture will finish here.</li>&#13;&#13;
      <li class="bullet"><strong class="keyword">Teardown and clean up values</strong>: Here, we simply delete the variable as an example, though this will happen automatically later.<p>&#13;&#13;
          <p class="Tip--PACKT-">Later, we will see a more complex fixture. Here, we are just presenting the concept.</p>&#13;&#13;
        </p>&#13;&#13;
      </li>&#13;&#13;
    </ul>&#13;&#13;
    <p class="normal">Defining the fixture<a id="_idIndexMarker732"/> this way will allow us to reuse it easily in <a id="_idIndexMarker733"/>different test functions, just using the name as the input parameter.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def test_counting_fixture(prepare_string):&#13;&#13;
    assert count_characters('a', prepare_string) == 6&#13;&#13;
def test_counting_fixture2(prepare_string):&#13;&#13;
    assert count_characters('r', prepare_string) == 2&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Note how the <code class="Code-In-Text--PACKT-">prepare_string</code> parameter is automatically providing the value that we defined with <code class="Code-In-Text--PACKT-">yield</code>. If we run the tests, we can see the effect. Even more, we can use the parameter <code class="Code-In-Text--PACKT-">--setup-show</code> to see the setup and tear down all of the fixtures.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ pytest -v test_fixtures.py -k counting_fixture --setup-show&#13;&#13;
======================== test session starts ========================&#13;&#13;
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/local/opt/python@3.9/bin/python3.9&#13;&#13;
plugins: celery-4.4.7&#13;&#13;
collected 3 items / 1 deselected / 2 selected&#13;&#13;
test_fixtures.py::test_counting_fixture&#13;&#13;
        SETUP    F prepare_string&#13;&#13;
        test_fixtures.py::test_counting_fixture (fixtures used: prepare_string)PASSED&#13;&#13;
        TEARDOWN F prepare_string&#13;&#13;
test_fixtures.py::test_counting_fixture2&#13;&#13;
        SETUP    F prepare_string&#13;&#13;
        test_fixtures.py::test_counting_fixture2 (fixtures used: prepare_string)PASSED&#13;&#13;
        TEARDOWN F prepare_string&#13;&#13;
=================== 2 passed, 1 deselected in 0.02s ===================&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">This fixture was very simple and did not do anything that couldn't be done defining the string, but fixtures can be used to connect to a database or prepare files, taking into account that they can clean them up at the end. </p>&#13;&#13;
    <p class="normal">For example, complicating the same example a bit, instead of counting from a string, it should count<a id="_idIndexMarker734"/> from a file, so the function needs to open a file, read it, and<a id="_idIndexMarker735"/> count the characters. The function will be like this.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def count_characters_from_file(char_to_count, file_to_count):&#13;&#13;
    '''&#13;&#13;
    Open a file and count the characters in the text contained&#13;&#13;
    in the file&#13;&#13;
    '''&#13;&#13;
    number = 0&#13;&#13;
    with open(file_to_count) as fp:&#13;&#13;
        for line in fp:&#13;&#13;
            for char in line:&#13;&#13;
                if char == char_to_count:&#13;&#13;
                    number += 1&#13;&#13;
    return number&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The fixture should then create a file, return it, and then remove it as part of the teardown. Let's take a look at it.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">import os&#13;&#13;
import time&#13;&#13;
import pytest&#13;&#13;
@pytest.fixture()&#13;&#13;
def prepare_file():&#13;&#13;
    data = [&#13;&#13;
        'Ba, ba, ba, Barbara Ann',&#13;&#13;
        'Ba, ba, ba, Barbara Ann',&#13;&#13;
        'Barbara Ann',&#13;&#13;
        'take my hand',&#13;&#13;
    ]&#13;&#13;
    filename = f'./test_file_{time.time()}.txt'&#13;&#13;
    # Setup the values to return&#13;&#13;
    with open(filename, 'w') as fp:&#13;&#13;
        for line in data:&#13;&#13;
            fp.write(line)&#13;&#13;
    # Return the value&#13;&#13;
    yield filename&#13;&#13;
    # Delete the file as teardown&#13;&#13;
    os.remove(filename)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Note that in <a id="_idIndexMarker736"/>the<a id="_idIndexMarker737"/> filename, we define the name adding the timestamp when it's generated. This means that each of the files that will be generated by this fixture will be unique.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">   filename = f'./test_file_{time.time()}.txt'&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The file then gets created and the data is written.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">    with open(filename, 'w') as fp:&#13;&#13;
        for line in data:&#13;&#13;
            fp.write(line)&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">The name of the file, which, as we've seen, is unique, gets yielded. Finally, the file is deleted in the teardown.</p>&#13;&#13;
    <p class="normal">The tests are similar to the previous ones, as most of the complexity is stored in the fixture.</p>&#13;&#13;
    <pre class="programlisting code"><code class="hljs-code">def test_counting_fixture(prepare_file):&#13;&#13;
    assert count_characters_from_file('a', prepare_file) == 17&#13;&#13;
def test_counting_fixture2(prepare_file):&#13;&#13;
    assert count_characters_from_file('r', prepare_file) == 6&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">When running it, we see it works as expected, and we can check that the teardown step deletes the<a id="_idIndexMarker738"/> testing<a id="_idIndexMarker739"/> files after each test.</p>&#13;&#13;
    <pre class="programlisting con"><code class="hljs-con">$ pytest -v test_fixtures2.py&#13;&#13;
========================= test session starts =========================&#13;&#13;
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/local/opt/python@3.9/bin/python3.9&#13;&#13;
collected 2 items&#13;&#13;
test_fixtures2.py::test_counting_fixture PASSED                  [50%]&#13;&#13;
test_fixtures2.py::test_counting_fixture2 PASSED                 [100%]&#13;&#13;
========================== 2 passed in 0.02s ==========================&#13;&#13;
</code></pre>&#13;&#13;
    <p class="normal">Fixtures don't need to be defined in the same file. They can also be stored in a special file called <code class="Code-In-Text--PACKT-">conftest.py</code>, which will automatically be shared by <code class="Code-In-Text--PACKT-">pytest</code> across all the tests.</p>&#13;&#13;
    <p>&#13;&#13;
      <p class="Tip--PACKT-">Fixtures can also be combined, they can be set to be used automatically, and there are already built-in fixtures to work with temporal data and directories or capture output. There are also a lot of plugins for useful fixtures in PyPI, installable as third-party modules, covering functionality like connecting to databases or interacting with other external resources. Be sure to check the Pytest documentation and to search before implementing your own fixture to see if you can leverage an already existing module: <a href="https://docs.pytest.org/en/latest/explanation/fixtures.html#about-fixtures">https://docs.pytest.org/en/latest/explanation/fixtures.html#about-fixtures</a>.</p>&#13;&#13;
    </p>&#13;&#13;
    <p class="normal">In this chapter, we only scratched the surface in terms of the possibilities of <code class="Code-In-Text--PACKT-">pytest</code>. It is a fantastic tool and one that I encourage you to learn about. It will pay off greatly to efficiently run tests and design them in the best possible way. Testing is a critical part of a project and it's <a id="_idIndexMarker740"/>one <a id="_idIndexMarker741"/>of the development stages where developers spend most of their time.</p>&#13;&#13;
    <h1 id="_idParaDest-217" class="title">Summary</h1>&#13;&#13;
    <p class="normal">In this chapter, we went through the whys and hows of tests to describe how a good testing strategy is required to produce high-quality software and prevent problems once the code is in use by customers.</p>&#13;&#13;
    <p class="normal">We started by describing the general principles behind testing, how to make tests that provide more value than their cost, and the different levels of testing to ensure this. We saw the three main levels of tests, which we called unit tests (parts of a single component), system tests (the whole system), and integration tests in the middle (a whole component or several components, but not all).</p>&#13;&#13;
    <p class="normal">We continued by describing different strategies to ensure that our tests are great ones, and how to structure them using the Arrange-Act-Assert pattern, for ease of writing and understanding them after they are written.</p>&#13;&#13;
    <p class="normal">Later, we described in detail the principles behind Test-Driven Development, a technique that puts tests at the center of development, which mandates writing the tests before the code, working in small increments, and running the tests over and over to create a good test suite that protects against unexpected behavior. We also analyzed the limits and caveats of working in a TDD fashion and provided an example of what the flow looks like.</p>&#13;&#13;
    <p class="normal">We continued by presenting ways of creating unit tests in Python, both using the standard <code class="Code-In-Text--PACKT-">unittest</code> module and by introducing the more powerful <code class="Code-In-Text--PACKT-">pytest</code>. We also presented a section with advanced usage of <code class="Code-In-Text--PACKT-">pytest</code> to show a bit of what this great third-party module is capable of.</p>&#13;&#13;
    <p class="normal">We described how to test external dependencies, something that is critically important when writing unit tests to isolate functionality. We also described how to mock dependencies and how to work under the dependency injection principles.</p>&#13;&#13;
  </div>&#13;&#13;
</div></body></html>