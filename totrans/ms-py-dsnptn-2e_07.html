<html><head></head><body>
		<div><h1 id="_idParaDest-180" class="chapter-number"><a id="_idTextAnchor191"/>7</h1>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor192"/>Concurrency and Asynchronous Patterns</h1>
			<p>In the previous chapter, we covered architectural design patterns: patterns that help with solving some unique challenges that come with complex projects. Next, we need to discuss concurrency and asynchronous patterns, another important category in our solutions catalog.</p>
			<p>Concurrency<a id="_idIndexMarker714"/> allows your program to manage multiple operations simultaneously, leveraging the full power of modern processors. It’s akin to a chef preparing multiple dishes in parallel, each step orchestrated so that all dishes are ready at the same time. Asynchronous programming, on the<a id="_idIndexMarker715"/> other hand, lets your application move on to other tasks while waiting for operations to complete, such as sending a food order to the kitchen and serving other customers until the order is ready.</p>
			<p>In this chapter, we’re going to cover the following main topics:</p>
			<ul>
				<li>The Thread Pool pattern</li>
				<li>The Worker Model pattern</li>
				<li>The Future and Promise pattern</li>
				<li>The Observer pattern in reactive programming</li>
				<li>Other concurrency and asynchronous patterns</li>
			</ul>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor193"/>Technical requirements</h1>
			<p>See the requirements presented in <a href="B21896_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>. The additional technical requirements for the code discussed in this chapter are the following:</p>
			<ul>
				<li>Faker, using <code>pip </code><code>install faker</code></li>
				<li>ReactiveX, using <code>pip </code><code>install reactivex</code></li>
			</ul>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor194"/>The Thread Pool pattern</h1>
			<p>First, it’s important to understand what a thread is. In computing, a thread<a id="_idIndexMarker716"/> is the smallest unit of processing that can be scheduled by an operating system.</p>
			<p>Threads are like tracks of execution that can run on a computer at the same time, which enables many activities to be done simultaneously and thus improve performance. They are particularly important in applications that need multitasking, such as serving multiple web requests or carrying out multiple computations.</p>
			<p>Now, onto the<a id="_idIndexMarker717"/> Thread Pool pattern itself. Imagine you have many tasks to <a id="_idIndexMarker718"/>complete but starting each task (which means in this case, creating a thread) can be expensive in terms of resources and time. It’s like hiring a new employee every time you have a job to do and then letting them go when the job is done. This process can be inefficient and costly. By maintaining a collection, or a pool, of worker threads that can be created for once and then reused upon several jobs, the Thread Pool pattern helps reduce this inefficiency. When one thread finishes a task, it does not terminate but goes back to the pool, awaiting another task that it can be used again for.</p>
			<p class="callout-heading">What are worker threads?</p>
			<p class="callout">A worker thread is a <a id="_idIndexMarker719"/>thread of execution of a particular task or set of tasks. Worker threads are used to offload processing tasks from the main thread, helping to keep applications responsive by performing time-consuming or resource-intensive tasks asynchronously.</p>
			<p>In addition to faster application <a id="_idIndexMarker720"/>performance, there are two benefits:</p>
			<ul>
				<li><strong class="bold">Reduced overhead</strong>: By reusing threads, the application avoids the overhead of creating and destroying threads for each task</li>
				<li><strong class="bold">Better resource management</strong>: The thread pool limits the number of threads, preventing resource exhaustion that could occur if too many threads were created</li>
			</ul>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor195"/>Real-world examples</h2>
			<p>In<a id="_idIndexMarker721"/> real life, imagine a small <a id="_idIndexMarker722"/>restaurant with a limited number of chefs (threads) who cook meals (tasks) for customers. The restaurant can only accommodate a certain number of chefs working at once due to kitchen space (system resources). When a new order comes in, if all chefs are busy, the order waits in a queue until there is an available chef. This way, the restaurant efficiently manages the flow of orders with its available chefs, ensuring all are utilized effectively without overwhelming the kitchen or needing to hire more staff for each new order.</p>
			<p>There are also many examples in software:</p>
			<ul>
				<li>Web servers often use thread pools to handle incoming client requests. This allows them to serve multiple clients simultaneously without the overhead of creating a new thread for each request.</li>
				<li>Databases use thread pools to manage connections, ensuring that a pool of connections is always available for incoming queries.</li>
				<li>Task schedulers use thread pools to execute scheduled tasks such as <em class="italic">cron</em> jobs, backups, or updates.</li>
			</ul>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor196"/>Use cases for the Thread Pool pattern</h2>
			<p>There are three use cases<a id="_idIndexMarker723"/> where the Thread Pool pattern helps:</p>
			<ul>
				<li><strong class="bold">Batch processing</strong>: When you have many tasks that can be performed in parallel, a thread pool can distribute them among its worker threads</li>
				<li><strong class="bold">Load balancing</strong>: Thread pools can be used to distribute workload evenly among worker threads, ensuring that no single thread takes on too much work</li>
				<li><strong class="bold">Resource optimization</strong>: By reusing threads, the thread pool minimizes system resource usage, such as memory and CPU time</li>
			</ul>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor197"/>Implementing the Thread Pool pattern</h2>
			<p>First, let’s <a id="_idIndexMarker724"/>stop to break down how a<a id="_idIndexMarker725"/> thread pool, for a given application, works:</p>
			<ol>
				<li>When the application starts, the thread pool creates a certain number of worker threads. This is the initialization. This number of threads can be fixed or dynamically adjusted based on the application’s needs.</li>
				<li>Then, we have the task submission step. When there’s a task to be done, it’s submitted to the pool rather than directly creating a new thread. The task can be anything that needs to be executed, such as processing user input, handling network requests, or performing calculations.</li>
				<li>The following step is the task execution. The pool assigns the task to one of the available worker threads. If all threads are busy, the task might wait in a queue until a thread becomes available.</li>
				<li>Once a thread completes its task, it doesn’t die. Instead, it returns to the pool, ready to be assigned a new task.</li>
			</ol>
			<p>For our example, let’s see some code where we create a thread pool with five worker threads to handle a set of tasks. We are going to use the <code>ThreadPoolExecutor</code> class from the <code>concurrent.futures</code> module.</p>
			<p>We start by importing what we need for the example, as follows:</p>
			<pre class="source-code">
from concurrent.futures import ThreadPoolExecutor
import time</pre>			<p>Then, we create a function to simulate the tasks, by simply using <code>time.sleep(1)</code> in this case:</p>
			<pre class="source-code">
def task(n):
    print(f"Executing task {n}")
    time.sleep(1)
    print(f"Task {n} completed")</pre>			<p>Then, we use <a id="_idIndexMarker726"/>an instance of the <code>ThreadPoolExecutor</code> class, created with a maximum number of worker threads of 5, and we submit 10 tasks to the thread pool. So, the worker threads pick up these tasks and execute them. Once a worker thread completes a task, it picks up another from the queue. The code is as follows:</p>
			<pre class="source-code">
with ThreadPoolExecutor(max_workers=5) as executor:
    for i in range(10):
        executor.submit(task, i)</pre>			<p>When<a id="_idIndexMarker727"/> running the example code, using the <code>ch07/thread_pool.py</code> Python command, you should get the following output:</p>
			<pre class="source-code">
<strong class="bold">Executing task 0</strong>
<strong class="bold">Executing task 1</strong>
<strong class="bold">Executing task 2</strong>
<strong class="bold">Executing task 3</strong>
<strong class="bold">Executing task 4</strong>
<strong class="bold">Task 0 completed</strong>
<strong class="bold">Task 4 completed</strong>
<strong class="bold">Task 3 completed</strong>
<strong class="bold">Task 1 completed</strong>
<strong class="bold">Executing task 6</strong>
<strong class="bold">Executing task 7</strong>
<strong class="bold">Executing task 8</strong>
<strong class="bold">Task 2 completed</strong>
<strong class="bold">Executing task 5</strong>
<strong class="bold">Executing task 9</strong>
<strong class="bold">Task 8 completed</strong>
<strong class="bold">Task 6 completed</strong>
<strong class="bold">Task 9 completed</strong>
<strong class="bold">Task 5 completed</strong>
<strong class="bold">Task 7 completed</strong></pre>			<p>We see that the<a id="_idIndexMarker728"/> tasks were completed in an order different from the order of submission. This shows that they were executed concurrently<a id="_idIndexMarker729"/> using the threads available in the thread pool.</p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor198"/>The Worker Model pattern</h1>
			<p>The idea <a id="_idIndexMarker730"/>behind the Worker Model <a id="_idIndexMarker731"/>pattern is to divide a large task or many tasks into smaller, manageable units of work, called workers, that can be processed in parallel. This approach to concurrency and parallel processing not only accelerates processing time but also enhances the application’s performance.</p>
			<p>The workers could be threads within a single application (as we have just seen in the Thread Pool pattern), separate processes on the same machine, or even different machines in a distributed system.</p>
			<p>The benefits<a id="_idIndexMarker732"/> of the Worker Model pattern are the following:</p>
			<ul>
				<li><strong class="bold">Scalability</strong>: Easily scales with the addition of more workers, which can be particularly beneficial in distributed systems where tasks can be processed on multiple machines</li>
				<li><strong class="bold">Efficiency</strong>: By distributing tasks across multiple workers, the system can make better use of available computing resources, processing tasks in parallel</li>
				<li><strong class="bold">Flexibility</strong>: The Worker Model pattern can accommodate a range of processing strategies, from simple thread-based workers to complex distributed systems spanning multiple servers</li>
			</ul>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor199"/>Real-world examples</h2>
			<p>Consider a <a id="_idIndexMarker733"/>delivery service where <a id="_idIndexMarker734"/>packages (tasks) are delivered by a team of couriers (workers). Each courier picks up a package from the distribution center (task queue) and delivers it. The number of couriers can vary depending on demand; more couriers can be added during busy periods and reduced when it’s quieter.</p>
			<p>In big data processing, the Worker Model pattern is often employed where each worker is responsible for mapping or reducing a part of the data.</p>
			<p>In systems such as RabbitMQ or Kafka, the Worker Model pattern is used to process messages from a queue concurrently.</p>
			<p>We can also<a id="_idIndexMarker735"/> cite image processing services. Services that need to process multiple images simultaneously often use the Worker Model pattern to distribute the load among multiple workers.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor200"/>Use cases for the Worker Model pattern</h2>
			<p>One use case<a id="_idIndexMarker736"/> for the Worker Model pattern is <em class="italic">data transformation</em>. When you have a large dataset that needs to be transformed, you can distribute the work among multiple workers.</p>
			<p>Another one is <em class="italic">task parallelism</em>. In applications where different tasks are independent of each other, the Worker Model pattern can be very effective.</p>
			<p>A third use case is <em class="italic">distributed computing</em>, where the Worker Model pattern can be extended to multiple machines, making it suitable for distributed computing environments.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor201"/>Implementing the Worker Model pattern</h2>
			<p>Before discussing an <a id="_idIndexMarker737"/>implementation example, let’s understand how the Worker Model pattern works. Three components are involved in the Worker Model pattern: workers, a task queue, and, optionally, a dispatcher:</p>
			<ul>
				<li><strong class="bold">The workers</strong>: The <a id="_idIndexMarker738"/>primary actors in this model. Each worker can perform a piece of the task independently of the others. Depending on the implementation, a worker might process one task at a time or handle multiple tasks concurrently.</li>
				<li><strong class="bold">The task queue</strong>: A central component where tasks are stored awaiting processing. Workers <a id="_idIndexMarker739"/>typically pull tasks from this queue, ensuring that tasks are distributed efficiently among them. The queue acts as a buffer, decoupling task submission from task processing.</li>
				<li><strong class="bold">The dispatcher</strong>: In some implementations, a dispatcher component assigns tasks to workers<a id="_idIndexMarker740"/> based on availability, load, or priority. This can help optimize task distribution and resource utilization.</li>
			</ul>
			<p>Let’s now see an <a id="_idIndexMarker741"/>example where we execute a function in parallel.</p>
			<p>We start by importing what we need for the example, as follows:</p>
			<pre class="source-code">
from multiprocessing import Process, Queue
import time</pre>			<p>Then, we <a id="_idIndexMarker742"/>create a <code>worker()</code> function that we are going to run tasks with. It takes as a parameter the <code>task_queue</code> object that contains the tasks to execute. The code is as follows:</p>
			<pre class="source-code">
def worker(task_queue):
    while not task_queue.empty():
        task = task_queue.get()
        print(f"Worker {task} is processing")
        time.sleep(1)
        print(f"Worker {task} completed")</pre>			<p>In the <code>main()</code> function, we start by creating a queue of tasks, an instance of <code>multiprocessing.Queue</code>. Then, we create 10 tasks and add them to the queue:</p>
			<pre class="source-code">
def main():
    task_queue = Queue()
    for i in range(10):
        task_queue.put(i)</pre>			<p>Five worker processes are then created, using the <code>multiprocessing.Process</code> class, and started. Each worker picks up a task from the queue, to execute it, and then picks up another until the queue is empty. Then, we start each worker process (using <code>p.start()</code>) in a loop, which means that the associated task will get executed concurrently. After that, we create another <a id="_idTextAnchor202"/>loop where we use the process’ <code>.join()</code> method so that <a id="_idIndexMarker743"/>the program waits <a id="_idIndexMarker744"/>for those processes to complete their work. That part of the code is as follows:</p>
			<pre class="source-code">
    processes = [
        Process(target=worker, args=(task_queue,))
        for _ in range(5)
    ]
    # Start the worker processes
    for p in processes:
        p.start()
    # Wait for all worker processes to finish
    for p in pr<a id="_idTextAnchor203"/>ocesses:
        p.join()
    print("All tasks completed.")</pre>			<p>When running the example code, using the <code>ch07/worker_model.py</code> Python command, you should get the following output, where you can see that the 5 workers process tasks from the task queue in a concurrent way until all 10 tasks are completed:</p>
			<pre class="source-code">
<strong class="bold">Worker 0 is processing</strong>
<strong class="bold">Worker 1 is processing</strong>
<strong class="bold">Worker 2 is processing</strong>
<strong class="bold">Worker 3 is processing</strong>
<strong class="bold">Worker 4 is processing</strong>
<strong class="bold">Worker 0 completed</strong>
<strong class="bold">Worker 5 is processing</strong>
<strong class="bold">Worker 1 completed</strong>
<strong class="bold">Worker 6 is processing</strong>
<strong class="bold">Worker 2 completed</strong>
<strong class="bold">Worker 7 is processing</strong>
<strong class="bold">Worker 3 completed</strong>
<strong class="bold">Worker 8 is processing</strong>
<strong class="bold">Worker 4 completed</strong>
<strong class="bold">Worker 9 is processing</strong>
<strong class="bold">Worker 5 completed</strong>
<strong class="bold">Worker 6 completed</strong>
<strong class="bold">Worker 7 completed</strong>
<strong class="bold">Worker 8 completed</strong>
<strong class="bold">Worker 9 completed</strong>
<strong class="bold">All tasks completed.</strong></pre>			<p>This <a id="_idIndexMarker745"/>demonstrates our <a id="_idIndexMarker746"/>implementation of the Worker Model pattern. This pattern is particularly useful for scenarios where tasks are independent and can be processed in parallel.</p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor204"/>The Future and Promise pattern</h1>
			<p>In the<a id="_idIndexMarker747"/> asynchronous programming paradigm, a Future represents a value that is not yet known but will be provided eventually. When a function initiates an asynchronous operation, instead of blocking until the operation completes and a result is available, it immediately returns a Future. This <code>Future</code> object acts as a placeholder for the actual result available later.</p>
			<p>Futures<a id="_idIndexMarker748"/> are commonly used for I/O operations, network requests, and other time-consuming tasks that run asynchronously. They allow the program to continue executing other tasks rather than waiting for the operation to be completed. That property is referred to as <em class="italic">non-blocking</em>.</p>
			<p>Once the Future is fulfilled, the result can be accessed through the Future, often via callbacks, polling, or blocking until the result is available.</p>
			<p>A Promise is the writable, controlling counterpart to a Future. It represents the producer side of the asynchronous operation, which will eventually provide a result to its associated Future. When the operation completes, the Promise is fulfilled with a value or rejected with an error, which then resolves the Future.</p>
			<p>Promises<a id="_idIndexMarker749"/> can be chained, allowing a sequence of asynchronous operations to be performed clearly and concisely.</p>
			<p>By allowing a program to continue<a id="_idIndexMarker750"/> execution without waiting for asynchronous operations, applications become more responsive. Another benefit is <em class="italic">composability</em>: multiple asynchronous operations can be combined, sequenced, or executed in parallel in a clean and manageable way.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor205"/>Real-world examples</h2>
			<p>Ordering a <a id="_idIndexMarker751"/>custom dining table from a carpenter provides a tangible example of the Future and Promise pattern. When you place the order, you receive an estimated completion date and design sketch (Future), representing the carpenter’s promise to deliver the table. As the carpenter works, this promise moves toward fulfillment. The delivery of the completed table resolves the Future, marking the fulfillment of the carpenter’s promise to you.</p>
			<p>We can also find several examples in the digital realm, such as the following:</p>
			<ul>
				<li><strong class="bold">Online shopping order tracking</strong>: When you place an order online, the website <a id="_idIndexMarker752"/>immediately provides you with an order confirmation and a tracking number (Future). As your order is processed, shipped, and delivered, status updates (Promise fulfillment) are reflected in real time on the tracking page, eventually resolving to a final delivery status.</li>
				<li><strong class="bold">Food delivery apps</strong>: Upon ordering your meal through a food delivery app, you’re given an estimated delivery time (Future). The app continuously updates the order status—from preparation through pickup and delivery (Promise being fulfilled)—until the food arrives at your door, at which point the Future is resolved with the completion of your order.</li>
				<li><strong class="bold">Customer support tickets</strong>: When you submit a support ticket on a website, you immediately receive a ticket number and a message stating that someone will get back to you (Future). Behind the scenes, the support team addresses tickets based on priority or in the order they were received. Once your ticket is addressed, you<a id="_idIndexMarker753"/> receive a response, fulfilling the Promise made when you first submitted the ticket.</li>
			</ul>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor206"/>Use cases for the Future and Promise pattern</h2>
			<p>There are at least four <a id="_idIndexMarker754"/>use cases where the Future and Promise pattern is recommended:</p>
			<ol>
				<li><strong class="bold">Data pipelines</strong>: In data processing pipelines, data is often transformed through multiple stages before reaching its final form. By representing each stage with a Future, you can effectively manage the asynchronous flow of data. For example, the output of one stage can serve as the input for the next, but because each stage returns a Future, subsequent stages don’t have to block while waiting for the previous ones to complete.</li>
				<li><strong class="bold">Task scheduling</strong>: Task scheduling systems, such as those in an operating system or a high-level application, can use Futures to represent tasks that are scheduled to run at a future time. When a task is scheduled, a Future is returned to represent the eventual completion of that task. This allows the system or the application to keep track of the task’s state without blocking execution.</li>
				<li><strong class="bold">Complex database queries or transactions</strong>: Executing database queries asynchronously is crucial for maintaining application responsiveness, particularly in web applications where user experience is paramount. By using Futures <a id="_idIndexMarker755"/>to represent the outcome of database operations, applications can initiate a query and immediately return control to the user interface or the calling function. The Future will eventually resolve with the query result, allowing the application to update the UI or process the data without having frozen or become unresponsive while waiting for the database response.</li>
				<li><strong class="bold">File I/O operations</strong>: File I/O operations can significantly impact application performance, particularly<a id="_idIndexMarker756"/> if executed synchronously on the main thread. By applying the Future and Promise pattern, file I/O operations are offloaded to a background process, with a Future returned to represent the completion of the operation. This approach allows the application to continue running other tasks or responding to user interactions while the file is being read from or written to. Once the I/O operation completes, the Future resolves, and the application can process or display the file data.</li>
			</ol>
			<p>In each of these use cases, the Future and Promise pattern facilitates asynchronous operation, allowing applications to remain responsive and efficient by not blocking the main thread with long-running tasks.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor207"/>Implementing the Future and Promise pattern – using concurrent.futures</h2>
			<p>To understand<a id="_idIndexMarker757"/> how to implement the Future and Promise pattern, you must first understand the three steps of its mechanism. Let’s break those down next:</p>
			<ol>
				<li><strong class="bold">Initiation</strong>: The initiation step involves starting an asynchronous operation using a function where, instead of waiting for the operation to complete, the function immediately returns a “Future” object. This object acts as a placeholder for the result that will be available later. Internally, the asynchronous function creates a “Promise” object. This object is responsible for handling the outcome of the asynchronous operation. The Promise is linked to the Future, meaning the state of the Promise (whether it’s fulfilled or rejected) will directly affect the Future.</li>
				<li><strong class="bold">Execution</strong>: During the execution step, the operation proceeds independently of the main <a id="_idIndexMarker758"/>program flow. This allows the program to remain responsive and continue with other tasks. Once the asynchronous task completes, its result needs to be communicated back to the part of the program that initiated the operation. The outcome of the operation (be it a successful result or an error) is passed to the previously created Promise.</li>
				<li><strong class="bold">Resolution</strong>: If the operation is successful, the Promise is “fulfilled” with the result. If the operation fails, the Promise is “rejected” with an error. The fulfillment or rejection of the Promise resolves the Future. Using the result is often done through a callback or continuation function, which is a piece of code that specifies what to do with the result. The Future provides mechanisms (for example, methods or operators) to specify these callbacks, which will execute once the<a id="_idIndexMarker759"/> Future is resolved.</li>
			</ol>
			<p>In our example, we use an instance of the <code>ThreadPoolExecutor</code> class to execute tasks asynchronously. The submit method returns a <code>Future</code> object that will eventually contain the result of the computation. We start by importing what we need, as follows:</p>
			<pre class="source-code">
from concurrent.futures import ThreadPoolExecutor, as_completed</pre>			<p>Then, we define a function for the task to be executed:</p>
			<pre class="source-code">
def square(x):
    return x * x</pre>			<p>We submit tasks and get <code>Future</code> objects, then we collect the completed Futures. The <code>as_completed</code> function allows us to iterate over completed <code>Future</code> objects and retrieve their results:</p>
			<pre class="source-code">
with ThreadPoolExecutor() as executor:
    future1 = executor.submit(square, 2)
    future2 = executor.submit(square, 3)
    future3 = executor.submit(square, 4)
    futures = [future1, future2, future3]
    for future in as_completed(futures):
        print(f"Result: {future.result()}")</pre>			<p>When<a id="_idIndexMarker760"/> running the example, using the <code>ch07/future_and_promise/future.py</code> Python command, you<a id="_idIndexMarker761"/> should get the following output:</p>
			<pre class="source-code">
<strong class="bold">Result: 16</strong>
<strong class="bold">Result: 4</strong>
<strong class="bold">Result: 9</strong></pre>			<p>This demonstrates our implementation.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor208"/>Implementing the Future and Promise pattern – using asyncio</h2>
			<p>Python’s <code>asyncio</code> library<a id="_idIndexMarker762"/> provides another way to execute tasks using asynchronous programming. It is particularly useful for I/O-bound tasks. Let’s see a second example using this technique.</p>
			<p class="callout-heading">What is asyncio?</p>
			<p class="callout">The <code>asyncio</code> library<a id="_idIndexMarker763"/> provides support for asynchronous I/O, event loops, coroutines, and other concurrency-related tasks. So, using <code>asyncio</code>, developers can write code that efficiently handles I/O-bound operations.</p>
			<p class="callout-heading">Coroutines and async/await</p>
			<p class="callout">A coroutine<a id="_idIndexMarker764"/> is a special kind of function that can pause and resume its execution at certain points, allowing other coroutines to run in the meantime. Coroutines are declared with the <code>async</code> keyword. Also, a coroutine can be awaited from other coroutines, using the <code>await</code> keyword.</p>
			<p>We import the <code>asyncio</code> module, which contains everything we need:</p>
			<pre class="source-code">
import asyncio</pre>			<p>Then, we create a function for the task of computing and returning the square of a number. We also want an I/O-bound operation, so we use <code>asyncio.sleep()</code>. Notice that in the <code>asyncio</code> style of programming, such a function is defined using the combined keywords <code>async def</code> – it is a coroutine. The <code>asyncio.sleep()</code> function itself is a <a id="_idIndexMarker765"/>coroutine, so we make<a id="_idIndexMarker766"/> sure to use the <code>await</code> keyword when calling it:</p>
			<pre class="source-code">
async def square(x):
    # Simulate some IO-bound operation
    await asyncio.sleep(1)
    return x * x</pre>			<p>Then, we move to creating our <code>main()</code> function. We use the <code>asyncio.ensure_future()</code> function to create the <code>Future</code> objects we want, passing it <code>square(x)</code>, with <code>x</code> being the number to square. We create three <code>Future</code> objects, <code>future1</code>, <code>future2</code>, and <code>future3</code>. Then, we use the <code>asyncio.gather()</code> coroutine to wait for our Futures to complete and gather the results. The code for the <code>main()</code> function is as follows:</p>
			<pre class="source-code">
async def main():
    fut1 = asyncio.ensure_future(square(2))
    fut2 = asyncio.ensure_future(square(3))
    fut3 = asyncio.ensure_future(square(4))
    results = await asyncio.gather(fut1, fut2, fut3)
    for result in results:
        print(f"Result: {result}")</pre>			<p>At the end of our code file, we have the usual <code>if __name__ == "__main__":</code>  block. What is new here, since we are writing <code>asyncio</code>-based code, is that we need to run <code>asyncio</code>’s event loop, by calling <code>asyncio.run(main())</code>:</p>
			<pre class="source-code">
if __name__ == "__main__":
    asyncio.run(main())</pre>			<p>To test the<a id="_idIndexMarker767"/> example, run<a id="_idIndexMarker768"/> the <code>ch07/future_and_promise/async.py</code> Python command. You should get an output like the following:</p>
			<pre class="source-code">
<strong class="bold">Result: 4</strong>
<strong class="bold">Result: 9</strong>
<strong class="bold">Result: 16</strong></pre>			<p>The order of the results may vary, depending on who is running the program and when. In fact, it is not predictable. You may have noticed similar behavior in our previous examples. This is generally the case with concurrency or asynchronous code.</p>
			<p>This simple example shows that <code>asyncio</code> is a suitable choice for the Future and Promise pattern when we need to efficiently handle I/O-bound tasks (in scenarios such as web scraping or API calls).</p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor209"/>The Observer pattern in reactive programming</h1>
			<p>The <a id="_idIndexMarker769"/>Observer pattern (covered in <a href="B21896_05.xhtml#_idTextAnchor121"><em class="italic">Chapter 5</em></a>, <em class="italic">Behavioral Design Patterns</em>) is useful for notifying an object or a group of objects when the state of a <a id="_idIndexMarker770"/>given object changes. This type of traditional Observer allows us to react to some object change events. It provides a nice solution for many cases, but in a situation where we must deal with many events, some depending on each other, the traditional way could lead to complicated, difficult-to-maintain code. That is where another paradigm called reactive programming gives us an interesting option. In simple terms, the concept of reactive programming is to react to many events (streams of events) while keeping our code clean.</p>
			<p>Let’s focus on <a id="_idIndexMarker771"/>ReactiveX (<a href="http://reactivex.io">http://reactivex.io</a>), which is a part of reactive programming. At the heart of ReactiveX is a concept known as an Observable. According to its official website, ReactiveX is about providing an API for asynchronous programming with what are called observable streams. This concept is added to the idea of the Observer, which we already discussed.</p>
			<p>Imagine an <a id="_idIndexMarker772"/>Observable like a river that flows data or events down to an Observer. This Observable sends out items one after another. These items travel through a path made up of different steps or operations until they reach an Observer, who takes them in or consumes them.</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor210"/>Real-world examples</h2>
			<p>An airport’s flight information display <a id="_idIndexMarker773"/>system is analogous to an Observable in reactive programming. Such a system continuously streams updates about flight statuses, including arrivals, departures, delays, and cancellations. This analogy illustrates how observers (travelers, airline staff, and airport services subscribed to receive updates) subscribe to an Observable (the flight display system) and react to a continuous stream of updates, allowing for dynamic responses to real-time information.</p>
			<p>A spreadsheet application can also be seen as an example of reactive programming, based on its internal behavior. In virtually all spreadsheet applications, interactively changing any one cell in the sheet will result in immediately reevaluating all formulas that directly or indirectly depend on that cell and updating the display to reflect these reevaluations.</p>
			<p>The<a id="_idIndexMarker774"/> ReactiveX idea is implemented in a variety of languages, including Java (RxJava), Python (RxPY), and JavaScript (RxJS). The Angular framework uses RxJS to implement the Observable pattern.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor211"/>Use cases for the Observer pattern in reactive programming</h2>
			<p>One use case<a id="_idIndexMarker775"/> is the idea of a collection pipeline, discussed by Martin Fowler on his blog (<a href="https://martinfowler.com/articles/collection-pipeline">https://martinfowler.com/articles/collection-pipeline</a>).</p>
			<p class="callout-heading">Collection pipeline, described by Martin Fowler</p>
			<p class="callout">Collection pipelines<a id="_idIndexMarker776"/> are a programming pattern where you organize some computation as a sequence of operations that compose by taking a collection as the output of one operation and feeding it into the next.</p>
			<p>We can also use an Observable to do operations such as “map and reduce” or “groupby” on sequences of objects when processing data.</p>
			<p>Finally, Observables can be created for diverse functions such as button events, requests, and Twitter feeds.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor212"/>Implementing the Observer pattern in reactive programming</h2>
			<p>For this<a id="_idIndexMarker777"/> example, we<a id="_idIndexMarker778"/> decided to build a stream of a list of (fake) people’s names (in the <code>ch07/observer_rx/people.txt</code>) text file, and an observable based on it.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">A first example text file containing fake names of people is provided (<code>ch07/observer_rx/people.txt</code>) as part of the book’s example files. But a new one can be generated whenever needed using a helper script (<code>ch07/observer_rx/peoplelist.py</code>), which will be presented in a minute.</p>
			<p>An example of such a list of names will look like this:</p>
			<pre class="source-code">
Peter Brown, Gabriel Hunt, Gary Martinez, Heather Fernandez, Juan White, Alan George, Travis Davidson, David Adams, Christopher Morris, Brittany Thomas, Brian Allen, Stefanie Lutz, Craig West, William Phillips, Kirsten Michael, Daniel Brennan, Derrick West, Amy Vazquez, Carol Howard, Taylor Abbott,</pre>			<p>Back to our implementation. We start by importing what we need:</p>
			<pre class="source-code">
from pathlib import Path
import reactivex as rx
from reactivex import operators as ops</pre>			<p>We define a function, <code>firstnames_from_db()</code>, which returns an Observable from the text file (reading the content of the file) containing the names, with transformations (as we have already seen) using <code>flat_map()</code>, <code>filter()</code>, and <code>map()</code> methods, and a new operation, <code>group_by()</code>, to emit items from another sequence—the first name found in the <a id="_idIndexMarker779"/>file, with its <a id="_idIndexMarker780"/>number of occurrence:</p>
			<pre class="source-code">
def firstnames_from_db(path: Path):
    file = path.open()
    # collect and push stored people firstnames
    return rx.from_iterable(file).pipe(
        ops.flat_map(
            lambda content: rx.from_iterable(
                content.split(", ")
            )
        ),
        ops.filter(lambda name: name != ""),
        ops.map(lambda name: name.split()[0]),
        ops.group_by(lambda firstname: firstname),
        ops.flat_map(
            lambda grp: grp.pipe(
                ops.count(),
                ops.map(lambda ct: (grp.key, ct)),
            )
        ),
    )</pre>			<p>Then, in<a id="_idIndexMarker781"/> the <code>main()</code> function, we define an Observable that emits data every 5 seconds, merging its emission <a id="_idIndexMarker782"/>with what is returned from <code>firstnames_from_db(db_file)</code>, after setting <code>db_file</code> to the people names text file, as follows:</p>
			<pre class="source-code">
def main():
    db_path = Path(__file__).parent / Path("people.txt")
    # Emit data every 5 seconds
    rx.interval(5.0).pipe(
        ops.flat_map(lambda i: firstnames_from_db(db_path))
    ).subscribe(lambda val: print(str(val)))
    # Keep alive until user presses any key
    input("Starting... Press any key and ENTER, to quit\n")</pre>			<p>Here is a recap of the example (complete code in the <code>ch07/observer_rx/rx_peoplelist.py</code> file):</p>
			<ol>
				<li>We import the modules and classes we need.</li>
				<li>We define a <code>firstnames_from_db()</code> function, which returns an Observable from the text<a id="_idTextAnchor213"/> file that is the source of the data. We collect and push the stored people’s first names from that file.</li>
				<li>Finally, in the <code>main()</code> function, we define an Observable that emits data every 5 seconds, merging its emission with what is returned from calling the <code>firstnames_from_db()</code> function.</li>
			</ol>
			<p>To test the example, run the <code>ch07/observer_rx/rx_peoplelist.py</code> Python command. You should<a id="_idIndexMarker783"/> get an output like the <a id="_idIndexMarker784"/>following (only an extract is shown here):</p>
			<pre class="source-code">
<strong class="bold">Starting... Press any key and ENTER, to quit</strong>
<strong class="bold">('Peter', 1)</strong>
<strong class="bold">('Gabriel', 1)</strong>
<strong class="bold">('Gary', 1)</strong>
<strong class="bold">('Heather', 1)</strong>
<strong class="bold">('Juan', 1)</strong>
<strong class="bold">('Alan', 1)</strong>
<strong class="bold">('Travis', 1)</strong>
<strong class="bold">('David', 1)</strong>
<strong class="bold">('Christopher', 1)</strong>
<strong class="bold">('Brittany', 1)</strong>
<strong class="bold">('Brian', 1)</strong>
<strong class="bold">('Stefanie', 1)</strong>
<strong class="bold">('Craig', 1)</strong>
<strong class="bold">('William', 1)</strong>
<strong class="bold">('Kirsten', 1)</strong>
<strong class="bold">('Daniel', 1)</strong>
<strong class="bold">('Derrick', 1)</strong></pre>			<p>Once you press a<a id="_idIndexMarker785"/> key and press <em class="italic">Enter</em> on the keyboard, the emission is interrupted, and the program stops.</p>
			<h3>Handling new streams of data</h3>
			<p>Our test <a id="_idIndexMarker786"/>worked, but in a sense, it was static; the stream of data was limited to what is currently in the<a id="_idIndexMarker787"/> text file. What we need now is to generate several streams of data. The technique we can use to generate the type of fake data in the text file is based on a third-party module called Faker (<a href="https://pypi.org/project/Faker">https://pypi.org/project/Faker</a>). The code that produces the data is provided to you, for free (in the <code>ch07/observer_rx/peoplelist.py</code> file), as follows:</p>
			<pre class="source-code">
from faker import Faker
import sys
fake = Faker()
args = sys.argv[1:]
if len(args) == 1:
    output_filename = args[0]
    persons = []
    for _ in range(0, 20):
        p = {"firstname": fake.first_name(), "lastname": fake.last_name()}
        persons.append(p)
    persons = iter(persons)
    data = [f"{p['firstname']} {p['lastname']}" for p in persons]
    data = ", ".join(data) + ", "
    with open(output_filename, "a") as f:
        f.write(data)
else:
    print("You need to pass the output filepath!")</pre>			<p>Now, let’s <a id="_idIndexMarker788"/>see what happens<a id="_idIndexMarker789"/> when we execute both programs (<code>ch07/observer_rx/peoplelist.py</code> and <code>ch07/observer_rx/rx_peoplelis.py</code>):</p>
			<ul>
				<li>From one command-line window or terminal, you can generate people’s names, passing the right file path to the script; you would execute the following command: <code>python </code><code>ch07/observer_rx/peoplelist.py ch07/observer_rx/people.txt</code>.</li>
				<li>From a second shell window, you can run the program that implements the Observable via the <code>python </code><code>ch07/observer_rx/rx_peoplelist.py</code> command.</li>
			</ul>
			<p>So, what is the output from both commands?</p>
			<p>A new version of the <code>people.txt</code> file is created (with the random names in it, separated by a comma), to replace the existing file. And, each time you rerun that command (<code>python ch07/observer_rx/peoplelist.py</code>), a new set of names is added to the file.</p>
			<p>The second<a id="_idIndexMarker790"/> command <a id="_idIndexMarker791"/>gives an output like the one you got with the first execution; the difference is that now it is not the same set of data that is emitted repeatedly. Now, new data can be generated in the source and emitted.</p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor214"/>Other concurrency and asynchronous patterns</h1>
			<p>There are some other <a id="_idIndexMarker792"/>concurrency and asynchronous patterns developers may use. We can cite the following:</p>
			<ul>
				<li><strong class="bold">The Actor model</strong>: A conceptual<a id="_idIndexMarker793"/> model to deal with concurrent <a id="_idIndexMarker794"/>computation. It defines some rules for how actor instances should behave: an actor can make local decisions, create more actors, send more messages, and determine how to respond to the next message received.</li>
				<li><code>asyncio</code> library).</li>
				<li><strong class="bold">Message passing</strong>: Used in <a id="_idIndexMarker797"/>parallel computing, <strong class="bold">object-oriented programming</strong> (<strong class="bold">OOP</strong>), and <strong class="bold">inter-process communication</strong> (<strong class="bold">IPC</strong>), where software entities<a id="_idIndexMarker798"/> communicate and coordinate<a id="_idIndexMarker799"/> their <a id="_idIndexMarker800"/>actions by passing messages to each other.</li>
				<li><strong class="bold">Backpressure</strong>: A mechanism to <a id="_idIndexMarker801"/>manage the flow of data through software systems and prevent overwhelming <a id="_idIndexMarker802"/>components. It allows systems to gracefully handle overload by signaling the producer to slow down until the consumer can catch up.</li>
			</ul>
			<p>Each of these patterns has its use cases and trade-offs. It is interesting to know they exist, but we cannot discuss all the available patterns and techniques.</p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor215"/>Summary</h1>
			<p>In this chapter, we discussed concurrency and asynchronous patterns, patterns useful for writing efficient, responsive software that can handle multiple tasks at once.</p>
			<p>The Thread Pool pattern is a powerful tool in concurrent programming, offering a way to manage resources efficiently and improve application performance. It helps us improve application performance but also reduces overhead and better manages resources because the thread pool limits the number of threads.</p>
			<p>While the Thread Pool pattern focuses on reusing a fixed number of threads to execute tasks, the Worker Model pattern is more about the dynamic distribution of tasks across potentially scalable and flexible worker entities. This pattern is particularly useful for scenarios where tasks are independent and can be processed in parallel.</p>
			<p>The Future and Promise pattern facilitates asynchronous operation, allowing applications to remain responsive and efficient by not blocking the main thread with long-running tasks.</p>
			<p>We also discussed the Observer pattern in reactive programming. The core idea of this pattern is to react to a stream of data and events, as with the streams of water we see in nature. We have lots of examples of this idea in the computing world. We have discussed an example of ReactiveX, which serves as an introduction for the reader to approach this programming paradigm and continue their own research via the ReactiveX official documentation.</p>
			<p>Lastly, we touched upon the fact that there are other concurrency and asynchronous patterns. Each of these patterns has its use cases and trade-offs, but we cannot cover them all in a single book.</p>
			<p>In the next chapter, we will discuss performance design patterns.</p>
		</div>
	</body></html>