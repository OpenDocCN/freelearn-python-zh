<html><head></head><body>
		<div id="_idContainer023">
			<h1 id="_idParaDest-180" class="chapter-number"><a id="_idTextAnchor191"/>7</h1>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor192"/>Concurrency and Asynchronous Patterns</h1>
			<p>In the previous chapter, we covered architectural design patterns: patterns that help with solving some unique challenges that come with complex projects. Next, we need to discuss concurrency and asynchronous patterns, another important category in our <span class="No-Break">solutions catalog.</span></p>
			<p>Concurrency<a id="_idIndexMarker714"/> allows your program to manage multiple operations simultaneously, leveraging the full power of modern processors. It’s akin to a chef preparing multiple dishes in parallel, each step orchestrated so that all dishes are ready at the same time. Asynchronous programming, on the<a id="_idIndexMarker715"/> other hand, lets your application move on to other tasks while waiting for operations to complete, such as sending a food order to the kitchen and serving other customers until the order <span class="No-Break">is ready.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>The Thread <span class="No-Break">Pool pattern</span></li>
				<li>The Worker <span class="No-Break">Model pattern</span></li>
				<li>The Future and <span class="No-Break">Promise pattern</span></li>
				<li>The Observer pattern in <span class="No-Break">reactive programming</span></li>
				<li>Other concurrency and <span class="No-Break">asynchronous patterns</span></li>
			</ul>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor193"/>Technical requirements</h1>
			<p>See the requirements presented in <a href="B21896_01.xhtml#_idTextAnchor017"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. The additional technical requirements for the code discussed in this chapter are <span class="No-Break">the following:</span></p>
			<ul>
				<li>Faker, using <strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install faker</strong></span></li>
				<li>ReactiveX, using <strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install reactivex</strong></span></li>
			</ul>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor194"/>The Thread Pool pattern</h1>
			<p>First, it’s important to understand what a thread is. In computing, a thread<a id="_idIndexMarker716"/> is the smallest unit of processing that can be scheduled by an <span class="No-Break">operating system.</span></p>
			<p>Threads are like tracks of execution that can run on a computer at the same time, which enables many activities to be done simultaneously and thus improve performance. They are particularly important in applications that need multitasking, such as serving multiple web requests or carrying out <span class="No-Break">multiple computations.</span></p>
			<p>Now, onto the<a id="_idIndexMarker717"/> Thread Pool pattern itself. Imagine you have many tasks to <a id="_idIndexMarker718"/>complete but starting each task (which means in this case, creating a thread) can be expensive in terms of resources and time. It’s like hiring a new employee every time you have a job to do and then letting them go when the job is done. This process can be inefficient and costly. By maintaining a collection, or a pool, of worker threads that can be created for once and then reused upon several jobs, the Thread Pool pattern helps reduce this inefficiency. When one thread finishes a task, it does not terminate but goes back to the pool, awaiting another task that it can be used <span class="No-Break">again for.</span></p>
			<p class="callout-heading">What are worker threads?</p>
			<p class="callout">A worker thread is a <a id="_idIndexMarker719"/>thread of execution of a particular task or set of tasks. Worker threads are used to offload processing tasks from the main thread, helping to keep applications responsive by performing time-consuming or resource-intensive <span class="No-Break">tasks asynchronously.</span></p>
			<p>In addition to faster application <a id="_idIndexMarker720"/>performance, there are <span class="No-Break">two benefits:</span></p>
			<ul>
				<li><strong class="bold">Reduced overhead</strong>: By reusing threads, the application avoids the overhead of creating and destroying threads for <span class="No-Break">each task</span></li>
				<li><strong class="bold">Better resource management</strong>: The thread pool limits the number of threads, preventing resource exhaustion that could occur if too many threads <span class="No-Break">were created</span></li>
			</ul>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor195"/>Real-world examples</h2>
			<p>In<a id="_idIndexMarker721"/> real life, imagine a small <a id="_idIndexMarker722"/>restaurant with a limited number of chefs (threads) who cook meals (tasks) for customers. The restaurant can only accommodate a certain number of chefs working at once due to kitchen space (system resources). When a new order comes in, if all chefs are busy, the order waits in a queue until there is an available chef. This way, the restaurant efficiently manages the flow of orders with its available chefs, ensuring all are utilized effectively without overwhelming the kitchen or needing to hire more staff for each <span class="No-Break">new order.</span></p>
			<p>There are also many examples <span class="No-Break">in software:</span></p>
			<ul>
				<li>Web servers often use thread pools to handle incoming client requests. This allows them to serve multiple clients simultaneously without the overhead of creating a new thread for <span class="No-Break">each request.</span></li>
				<li>Databases use thread pools to manage connections, ensuring that a pool of connections is always available for <span class="No-Break">incoming queries.</span></li>
				<li>Task schedulers use thread pools to execute scheduled tasks such as <em class="italic">cron</em> jobs, backups, <span class="No-Break">or updates.</span></li>
			</ul>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor196"/>Use cases for the Thread Pool pattern</h2>
			<p>There are three use cases<a id="_idIndexMarker723"/> where the Thread Pool <span class="No-Break">pattern helps:</span></p>
			<ul>
				<li><strong class="bold">Batch processing</strong>: When you have many tasks that can be performed in parallel, a thread pool can distribute them among its <span class="No-Break">worker threads</span></li>
				<li><strong class="bold">Load balancing</strong>: Thread pools can be used to distribute workload evenly among worker threads, ensuring that no single thread takes on too <span class="No-Break">much work</span></li>
				<li><strong class="bold">Resource optimization</strong>: By reusing threads, the thread pool minimizes system resource usage, such as memory and <span class="No-Break">CPU time</span></li>
			</ul>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor197"/>Implementing the Thread Pool pattern</h2>
			<p>First, let’s <a id="_idIndexMarker724"/>stop to break down how a<a id="_idIndexMarker725"/> thread pool, for a given <span class="No-Break">application, works:</span></p>
			<ol>
				<li>When the application starts, the thread pool creates a certain number of worker threads. This is the initialization. This number of threads can be fixed or dynamically adjusted based on the <span class="No-Break">application’s needs.</span></li>
				<li>Then, we have the task submission step. When there’s a task to be done, it’s submitted to the pool rather than directly creating a new thread. The task can be anything that needs to be executed, such as processing user input, handling network requests, or <span class="No-Break">performing calculations.</span></li>
				<li>The following step is the task execution. The pool assigns the task to one of the available worker threads. If all threads are busy, the task might wait in a queue until a thread <span class="No-Break">becomes available.</span></li>
				<li>Once a thread completes its task, it doesn’t die. Instead, it returns to the pool, ready to be assigned a <span class="No-Break">new task.</span></li>
			</ol>
			<p>For our example, let’s see some code where we create a thread pool with five worker threads to handle a set of tasks. We are going to use the <strong class="source-inline">ThreadPoolExecutor</strong> class from the <span class="No-Break"><strong class="source-inline">concurrent.futures</strong></span><span class="No-Break"> module.</span></p>
			<p>We start by importing what we need for the example, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from concurrent.futures import ThreadPoolExecutor
import time</pre>			<p>Then, we create a function to simulate the tasks, by simply using <strong class="source-inline">time.sleep(1)</strong> in <span class="No-Break">this case:</span></p>
			<pre class="source-code">
def task(n):
    print(f"Executing task {n}")
    time.sleep(1)
    print(f"Task {n} completed")</pre>			<p>Then, we use <a id="_idIndexMarker726"/>an instance of the <strong class="source-inline">ThreadPoolExecutor</strong> class, created with a maximum number of worker threads of 5, and we submit 10 tasks to the thread pool. So, the worker threads pick up these tasks and execute them. Once a worker thread completes a task, it picks up another from the queue. The code is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
with ThreadPoolExecutor(max_workers=5) as executor:
    for i in range(10):
        executor.submit(task, i)</pre>			<p>When<a id="_idIndexMarker727"/> running the example code, using the <strong class="source-inline">ch07/thread_pool.py</strong> Python command, you should get the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
<strong class="bold">Executing task 0</strong>
<strong class="bold">Executing task 1</strong>
<strong class="bold">Executing task 2</strong>
<strong class="bold">Executing task 3</strong>
<strong class="bold">Executing task 4</strong>
<strong class="bold">Task 0 completed</strong>
<strong class="bold">Task 4 completed</strong>
<strong class="bold">Task 3 completed</strong>
<strong class="bold">Task 1 completed</strong>
<strong class="bold">Executing task 6</strong>
<strong class="bold">Executing task 7</strong>
<strong class="bold">Executing task 8</strong>
<strong class="bold">Task 2 completed</strong>
<strong class="bold">Executing task 5</strong>
<strong class="bold">Executing task 9</strong>
<strong class="bold">Task 8 completed</strong>
<strong class="bold">Task 6 completed</strong>
<strong class="bold">Task 9 completed</strong>
<strong class="bold">Task 5 completed</strong>
<strong class="bold">Task 7 completed</strong></pre>			<p>We see that the<a id="_idIndexMarker728"/> tasks were completed in an order different from the order of submission. This shows that they were executed concurrently<a id="_idIndexMarker729"/> using the threads available in the <span class="No-Break">thread pool.</span></p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor198"/>The Worker Model pattern</h1>
			<p>The idea <a id="_idIndexMarker730"/>behind the Worker Model <a id="_idIndexMarker731"/>pattern is to divide a large task or many tasks into smaller, manageable units of work, called workers, that can be processed in parallel. This approach to concurrency and parallel processing not only accelerates processing time but also enhances the <span class="No-Break">application’s performance.</span></p>
			<p>The workers could be threads within a single application (as we have just seen in the Thread Pool pattern), separate processes on the same machine, or even different machines in a <span class="No-Break">distributed system.</span></p>
			<p>The benefits<a id="_idIndexMarker732"/> of the Worker Model pattern are <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Scalability</strong>: Easily scales with the addition of more workers, which can be particularly beneficial in distributed systems where tasks can be processed on <span class="No-Break">multiple machines</span></li>
				<li><strong class="bold">Efficiency</strong>: By distributing tasks across multiple workers, the system can make better use of available computing resources, processing tasks <span class="No-Break">in parallel</span></li>
				<li><strong class="bold">Flexibility</strong>: The Worker Model pattern can accommodate a range of processing strategies, from simple thread-based workers to complex distributed systems spanning <span class="No-Break">multiple servers</span></li>
			</ul>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor199"/>Real-world examples</h2>
			<p>Consider a <a id="_idIndexMarker733"/>delivery service where <a id="_idIndexMarker734"/>packages (tasks) are delivered by a team of couriers (workers). Each courier picks up a package from the distribution center (task queue) and delivers it. The number of couriers can vary depending on demand; more couriers can be added during busy periods and reduced when <span class="No-Break">it’s quieter.</span></p>
			<p>In big data processing, the Worker Model pattern is often employed where each worker is responsible for mapping or reducing a part of <span class="No-Break">the data.</span></p>
			<p>In systems such as RabbitMQ or Kafka, the Worker Model pattern is used to process messages from a <span class="No-Break">queue concurrently.</span></p>
			<p>We can also<a id="_idIndexMarker735"/> cite image processing services. Services that need to process multiple images simultaneously often use the Worker Model pattern to distribute the load among <span class="No-Break">multiple workers.</span></p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor200"/>Use cases for the Worker Model pattern</h2>
			<p>One use case<a id="_idIndexMarker736"/> for the Worker Model pattern is <em class="italic">data transformation</em>. When you have a large dataset that needs to be transformed, you can distribute the work among <span class="No-Break">multiple workers.</span></p>
			<p>Another one is <em class="italic">task parallelism</em>. In applications where different tasks are independent of each other, the Worker Model pattern can be <span class="No-Break">very effective.</span></p>
			<p>A third use case is <em class="italic">distributed computing</em>, where the Worker Model pattern can be extended to multiple machines, making it suitable for distributed <span class="No-Break">computing environments.</span></p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor201"/>Implementing the Worker Model pattern</h2>
			<p>Before discussing an <a id="_idIndexMarker737"/>implementation example, let’s understand how the Worker Model pattern works. Three components are involved in the Worker Model pattern: workers, a task queue, and, optionally, <span class="No-Break">a dispatcher:</span></p>
			<ul>
				<li><strong class="bold">The workers</strong>: The <a id="_idIndexMarker738"/>primary actors in this model. Each worker can perform a piece of the task independently of the others. Depending on the implementation, a worker might process one task at a time or handle multiple <span class="No-Break">tasks concurrently.</span></li>
				<li><strong class="bold">The task queue</strong>: A central component where tasks are stored awaiting processing. Workers <a id="_idIndexMarker739"/>typically pull tasks from this queue, ensuring that tasks are distributed efficiently among them. The queue acts as a buffer, decoupling task submission from <span class="No-Break">task processing.</span></li>
				<li><strong class="bold">The dispatcher</strong>: In some implementations, a dispatcher component assigns tasks to workers<a id="_idIndexMarker740"/> based on availability, load, or priority. This can help optimize task distribution and <span class="No-Break">resource utilization.</span></li>
			</ul>
			<p>Let’s now see an <a id="_idIndexMarker741"/>example where we execute a function <span class="No-Break">in parallel.</span></p>
			<p>We start by importing what we need for the example, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from multiprocessing import Process, Queue
import time</pre>			<p>Then, we <a id="_idIndexMarker742"/>create a <strong class="source-inline">worker()</strong> function that we are going to run tasks with. It takes as a parameter the <strong class="source-inline">task_queue</strong> object that contains the tasks to execute. The code is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
def worker(task_queue):
    while not task_queue.empty():
        task = task_queue.get()
        print(f"Worker {task} is processing")
        time.sleep(1)
        print(f"Worker {task} completed")</pre>			<p>In the <strong class="source-inline">main()</strong> function, we start by creating a queue of tasks, an instance of <strong class="source-inline">multiprocessing.Queue</strong>. Then, we create 10 tasks and add them to <span class="No-Break">the queue:</span></p>
			<pre class="source-code">
def main():
    task_queue = Queue()
    for i in range(10):
        task_queue.put(i)</pre>			<p>Five worker processes are then created, using the <strong class="source-inline">multiprocessing.Process</strong> class, and started. Each worker picks up a task from the queue, to execute it, and then picks up another until the queue is empty. Then, we start each worker process (using <strong class="source-inline">p.start()</strong>) in a loop, which means that the associated task will get executed concurrently. After that, we create another <a id="_idTextAnchor202"/>loop where we use the process’ <strong class="source-inline">.join()</strong> method so that <a id="_idIndexMarker743"/>the program waits <a id="_idIndexMarker744"/>for those processes to complete their work. That part of the code is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
    processes = [
        Process(target=worker, args=(task_queue,))
        for _ in range(5)
    ]
    # Start the worker processes
    for p in processes:
        p.start()
    # Wait for all worker processes to finish
    for p in pr<a id="_idTextAnchor203"/>ocesses:
        p.join()
    print("All tasks completed.")</pre>			<p>When running the example code, using the <strong class="source-inline">ch07/worker_model.py</strong> Python command, you should get the following output, where you can see that the 5 workers process tasks from the task queue in a concurrent way until all 10 tasks <span class="No-Break">are completed:</span></p>
			<pre class="source-code">
<strong class="bold">Worker 0 is processing</strong>
<strong class="bold">Worker 1 is processing</strong>
<strong class="bold">Worker 2 is processing</strong>
<strong class="bold">Worker 3 is processing</strong>
<strong class="bold">Worker 4 is processing</strong>
<strong class="bold">Worker 0 completed</strong>
<strong class="bold">Worker 5 is processing</strong>
<strong class="bold">Worker 1 completed</strong>
<strong class="bold">Worker 6 is processing</strong>
<strong class="bold">Worker 2 completed</strong>
<strong class="bold">Worker 7 is processing</strong>
<strong class="bold">Worker 3 completed</strong>
<strong class="bold">Worker 8 is processing</strong>
<strong class="bold">Worker 4 completed</strong>
<strong class="bold">Worker 9 is processing</strong>
<strong class="bold">Worker 5 completed</strong>
<strong class="bold">Worker 6 completed</strong>
<strong class="bold">Worker 7 completed</strong>
<strong class="bold">Worker 8 completed</strong>
<strong class="bold">Worker 9 completed</strong>
<strong class="bold">All tasks completed.</strong></pre>			<p>This <a id="_idIndexMarker745"/>demonstrates our <a id="_idIndexMarker746"/>implementation of the Worker Model pattern. This pattern is particularly useful for scenarios where tasks are independent and can be processed <span class="No-Break">in parallel.</span></p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor204"/>The Future and Promise pattern</h1>
			<p>In the<a id="_idIndexMarker747"/> asynchronous programming paradigm, a Future represents a value that is not yet known but will be provided eventually. When a function initiates an asynchronous operation, instead of blocking until the operation completes and a result is available, it immediately returns a Future. This <strong class="source-inline">Future</strong> object acts as a placeholder for the actual result <span class="No-Break">available later.</span></p>
			<p>Futures<a id="_idIndexMarker748"/> are commonly used for I/O operations, network requests, and other time-consuming tasks that run asynchronously. They allow the program to continue executing other tasks rather than waiting for the operation to be completed. That property is referred to <span class="No-Break">as </span><span class="No-Break"><em class="italic">non-blocking</em></span><span class="No-Break">.</span></p>
			<p>Once the Future is fulfilled, the result can be accessed through the Future, often via callbacks, polling, or blocking until the result <span class="No-Break">is available.</span></p>
			<p>A Promise is the writable, controlling counterpart to a Future. It represents the producer side of the asynchronous operation, which will eventually provide a result to its associated Future. When the operation completes, the Promise is fulfilled with a value or rejected with an error, which then resolves <span class="No-Break">the Future.</span></p>
			<p>Promises<a id="_idIndexMarker749"/> can be chained, allowing a sequence of asynchronous operations to be performed clearly <span class="No-Break">and concisely.</span></p>
			<p>By allowing a program to continue<a id="_idIndexMarker750"/> execution without waiting for asynchronous operations, applications become more responsive. Another benefit is <em class="italic">composability</em>: multiple asynchronous operations can be combined, sequenced, or executed in parallel in a clean and <span class="No-Break">manageable way.</span></p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor205"/>Real-world examples</h2>
			<p>Ordering a <a id="_idIndexMarker751"/>custom dining table from a carpenter provides a tangible example of the Future and Promise pattern. When you place the order, you receive an estimated completion date and design sketch (Future), representing the carpenter’s promise to deliver the table. As the carpenter works, this promise moves toward fulfillment. The delivery of the completed table resolves the Future, marking the fulfillment of the carpenter’s promise <span class="No-Break">to you.</span></p>
			<p>We can also find several examples in the digital realm, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Online shopping order tracking</strong>: When you place an order online, the website <a id="_idIndexMarker752"/>immediately provides you with an order confirmation and a tracking number (Future). As your order is processed, shipped, and delivered, status updates (Promise fulfillment) are reflected in real time on the tracking page, eventually resolving to a final <span class="No-Break">delivery status.</span></li>
				<li><strong class="bold">Food delivery apps</strong>: Upon ordering your meal through a food delivery app, you’re given an estimated delivery time (Future). The app continuously updates the order status—from preparation through pickup and delivery (Promise being fulfilled)—until the food arrives at your door, at which point the Future is resolved with the completion of <span class="No-Break">your order.</span></li>
				<li><strong class="bold">Customer support tickets</strong>: When you submit a support ticket on a website, you immediately receive a ticket number and a message stating that someone will get back to you (Future). Behind the scenes, the support team addresses tickets based on priority or in the order they were received. Once your ticket is addressed, you<a id="_idIndexMarker753"/> receive a response, fulfilling the Promise made when you first submitted <span class="No-Break">the ticket.</span></li>
			</ul>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor206"/>Use cases for the Future and Promise pattern</h2>
			<p>There are at least four <a id="_idIndexMarker754"/>use cases where the Future and Promise pattern <span class="No-Break">is recommended:</span></p>
			<ol>
				<li><strong class="bold">Data pipelines</strong>: In data processing pipelines, data is often transformed through multiple stages before reaching its final form. By representing each stage with a Future, you can effectively manage the asynchronous flow of data. For example, the output of one stage can serve as the input for the next, but because each stage returns a Future, subsequent stages don’t have to block while waiting for the previous ones <span class="No-Break">to complete.</span></li>
				<li><strong class="bold">Task scheduling</strong>: Task scheduling systems, such as those in an operating system or a high-level application, can use Futures to represent tasks that are scheduled to run at a future time. When a task is scheduled, a Future is returned to represent the eventual completion of that task. This allows the system or the application to keep track of the task’s state without <span class="No-Break">blocking execution.</span></li>
				<li><strong class="bold">Complex database queries or transactions</strong>: Executing database queries asynchronously is crucial for maintaining application responsiveness, particularly in web applications where user experience is paramount. By using Futures <a id="_idIndexMarker755"/>to represent the outcome of database operations, applications can initiate a query and immediately return control to the user interface or the calling function. The Future will eventually resolve with the query result, allowing the application to update the UI or process the data without having frozen or become unresponsive while waiting for the <span class="No-Break">database response.</span></li>
				<li><strong class="bold">File I/O operations</strong>: File I/O operations can significantly impact application performance, particularly<a id="_idIndexMarker756"/> if executed synchronously on the main thread. By applying the Future and Promise pattern, file I/O operations are offloaded to a background process, with a Future returned to represent the completion of the operation. This approach allows the application to continue running other tasks or responding to user interactions while the file is being read from or written to. Once the I/O operation completes, the Future resolves, and the application can process or display the <span class="No-Break">file data.</span></li>
			</ol>
			<p>In each of these use cases, the Future and Promise pattern facilitates asynchronous operation, allowing applications to remain responsive and efficient by not blocking the main thread with <span class="No-Break">long-running tasks.</span></p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor207"/>Implementing the Future and Promise pattern – using concurrent.futures</h2>
			<p>To understand<a id="_idIndexMarker757"/> how to implement the Future and Promise pattern, you must first understand the three steps of its mechanism. Let’s break those <span class="No-Break">down next:</span></p>
			<ol>
				<li><strong class="bold">Initiation</strong>: The initiation step involves starting an asynchronous operation using a function where, instead of waiting for the operation to complete, the function immediately returns a “Future” object. This object acts as a placeholder for the result that will be available later. Internally, the asynchronous function creates a “Promise” object. This object is responsible for handling the outcome of the asynchronous operation. The Promise is linked to the Future, meaning the state of the Promise (whether it’s fulfilled or rejected) will directly affect <span class="No-Break">the Future.</span></li>
				<li><strong class="bold">Execution</strong>: During the execution step, the operation proceeds independently of the main <a id="_idIndexMarker758"/>program flow. This allows the program to remain responsive and continue with other tasks. Once the asynchronous task completes, its result needs to be communicated back to the part of the program that initiated the operation. The outcome of the operation (be it a successful result or an error) is passed to the previously <span class="No-Break">created Promise.</span></li>
				<li><strong class="bold">Resolution</strong>: If the operation is successful, the Promise is “fulfilled” with the result. If the operation fails, the Promise is “rejected” with an error. The fulfillment or rejection of the Promise resolves the Future. Using the result is often done through a callback or continuation function, which is a piece of code that specifies what to do with the result. The Future provides mechanisms (for example, methods or operators) to specify these callbacks, which will execute once the<a id="_idIndexMarker759"/> Future <span class="No-Break">is resolved.</span></li>
			</ol>
			<p>In our example, we use an instance of the <strong class="source-inline">ThreadPoolExecutor</strong> class to execute tasks asynchronously. The submit method returns a <strong class="source-inline">Future</strong> object that will eventually contain the result of the computation. We start by importing what we need, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from concurrent.futures import ThreadPoolExecutor, as_completed</pre>			<p>Then, we define a function for the task to <span class="No-Break">be executed:</span></p>
			<pre class="source-code">
def square(x):
    return x * x</pre>			<p>We submit tasks and get <strong class="source-inline">Future</strong> objects, then we collect the completed Futures. The <strong class="source-inline">as_completed</strong> function allows us to iterate over completed <strong class="source-inline">Future</strong> objects and retrieve <span class="No-Break">their results:</span></p>
			<pre class="source-code">
with ThreadPoolExecutor() as executor:
    future1 = executor.submit(square, 2)
    future2 = executor.submit(square, 3)
    future3 = executor.submit(square, 4)
    futures = [future1, future2, future3]
    for future in as_completed(futures):
        print(f"Result: {future.result()}")</pre>			<p>When<a id="_idIndexMarker760"/> running the example, using the <strong class="source-inline">ch07/future_and_promise/future.py</strong> Python command, you<a id="_idIndexMarker761"/> should get the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
<strong class="bold">Result: 16</strong>
<strong class="bold">Result: 4</strong>
<strong class="bold">Result: 9</strong></pre>			<p>This demonstrates <span class="No-Break">our implementation.</span></p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor208"/>Implementing the Future and Promise pattern – using asyncio</h2>
			<p>Python’s <strong class="source-inline">asyncio</strong> library<a id="_idIndexMarker762"/> provides another way to execute tasks using asynchronous programming. It is particularly useful for I/O-bound tasks. Let’s see a second example using <span class="No-Break">this technique.</span></p>
			<p class="callout-heading">What is asyncio?</p>
			<p class="callout">The <strong class="source-inline">asyncio</strong> library<a id="_idIndexMarker763"/> provides support for asynchronous I/O, event loops, coroutines, and other concurrency-related tasks. So, using <strong class="source-inline">asyncio</strong>, developers can write code that efficiently handles <span class="No-Break">I/O-bound operations.</span></p>
			<p class="callout-heading">Coroutines and async/await</p>
			<p class="callout">A coroutine<a id="_idIndexMarker764"/> is a special kind of function that can pause and resume its execution at certain points, allowing other coroutines to run in the meantime. Coroutines are declared with the <strong class="source-inline">async</strong> keyword. Also, a coroutine can be awaited from other coroutines, using the <span class="No-Break"><strong class="source-inline">await</strong></span><span class="No-Break"> keyword.</span></p>
			<p>We import the <strong class="source-inline">asyncio</strong> module, which contains everything <span class="No-Break">we need:</span></p>
			<pre class="source-code">
import asyncio</pre>			<p>Then, we create a function for the task of computing and returning the square of a number. We also want an I/O-bound operation, so we use <strong class="source-inline">asyncio.sleep()</strong>. Notice that in the <strong class="source-inline">asyncio</strong> style of programming, such a function is defined using the combined keywords <strong class="source-inline">async def</strong> – it is a coroutine. The <strong class="source-inline">asyncio.sleep()</strong> function itself is a <a id="_idIndexMarker765"/>coroutine, so we make<a id="_idIndexMarker766"/> sure to use the <strong class="source-inline">await</strong> keyword when <span class="No-Break">calling it:</span></p>
			<pre class="source-code">
async def square(x):
    # Simulate some IO-bound operation
    await asyncio.sleep(1)
    return x * x</pre>			<p>Then, we move to creating our <strong class="source-inline">main()</strong> function. We use the <strong class="source-inline">asyncio.ensure_future()</strong> function to create the <strong class="source-inline">Future</strong> objects we want, passing it <strong class="source-inline">square(x)</strong>, with <strong class="source-inline">x</strong> being the number to square. We create three <strong class="source-inline">Future</strong> objects, <strong class="source-inline">future1</strong>, <strong class="source-inline">future2</strong>, and <strong class="source-inline">future3</strong>. Then, we use the <strong class="source-inline">asyncio.gather()</strong> coroutine to wait for our Futures to complete and gather the results. The code for the <strong class="source-inline">main()</strong> function is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
async def main():
    fut1 = asyncio.ensure_future(square(2))
    fut2 = asyncio.ensure_future(square(3))
    fut3 = asyncio.ensure_future(square(4))
    results = await asyncio.gather(fut1, fut2, fut3)
    for result in results:
        print(f"Result: {result}")</pre>			<p>At the end of our code file, we have the usual <strong class="source-inline">if __name__ == "__main__":</strong>  block. What is new here, since we are writing <strong class="source-inline">asyncio</strong>-based code, is that we need to run <strong class="source-inline">asyncio</strong>’s event loop, by <span class="No-Break">calling </span><span class="No-Break"><strong class="source-inline">asyncio.run(main())</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
if __name__ == "__main__":
    asyncio.run(main())</pre>			<p>To test the<a id="_idIndexMarker767"/> example, run<a id="_idIndexMarker768"/> the <strong class="source-inline">ch07/future_and_promise/async.py</strong> Python command. You should get an output like <span class="No-Break">the following:</span></p>
			<pre class="source-code">
<strong class="bold">Result: 4</strong>
<strong class="bold">Result: 9</strong>
<strong class="bold">Result: 16</strong></pre>			<p>The order of the results may vary, depending on who is running the program and when. In fact, it is not predictable. You may have noticed similar behavior in our previous examples. This is generally the case with concurrency or <span class="No-Break">asynchronous code.</span></p>
			<p>This simple example shows that <strong class="source-inline">asyncio</strong> is a suitable choice for the Future and Promise pattern when we need to efficiently handle I/O-bound tasks (in scenarios such as web scraping or <span class="No-Break">API calls).</span></p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor209"/>The Observer pattern in reactive programming</h1>
			<p>The <a id="_idIndexMarker769"/>Observer pattern (covered in <a href="B21896_05.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Behavioral Design Patterns</em>) is useful for notifying an object or a group of objects when the state of a <a id="_idIndexMarker770"/>given object changes. This type of traditional Observer allows us to react to some object change events. It provides a nice solution for many cases, but in a situation where we must deal with many events, some depending on each other, the traditional way could lead to complicated, difficult-to-maintain code. That is where another paradigm called reactive programming gives us an interesting option. In simple terms, the concept of reactive programming is to react to many events (streams of events) while keeping our <span class="No-Break">code clean.</span></p>
			<p>Let’s focus on <a id="_idIndexMarker771"/>ReactiveX (<a href="http://reactivex.io">http://reactivex.io</a>), which is a part of reactive programming. At the heart of ReactiveX is a concept known as an Observable. According to its official website, ReactiveX is about providing an API for asynchronous programming with what are called observable streams. This concept is added to the idea of the Observer, which we <span class="No-Break">already discussed.</span></p>
			<p>Imagine an <a id="_idIndexMarker772"/>Observable like a river that flows data or events down to an Observer. This Observable sends out items one after another. These items travel through a path made up of different steps or operations until they reach an Observer, who takes them in or <span class="No-Break">consumes them.</span></p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor210"/>Real-world examples</h2>
			<p>An airport’s flight information display <a id="_idIndexMarker773"/>system is analogous to an Observable in reactive programming. Such a system continuously streams updates about flight statuses, including arrivals, departures, delays, and cancellations. This analogy illustrates how observers (travelers, airline staff, and airport services subscribed to receive updates) subscribe to an Observable (the flight display system) and react to a continuous stream of updates, allowing for dynamic responses to <span class="No-Break">real-time information.</span></p>
			<p>A spreadsheet application can also be seen as an example of reactive programming, based on its internal behavior. In virtually all spreadsheet applications, interactively changing any one cell in the sheet will result in immediately reevaluating all formulas that directly or indirectly depend on that cell and updating the display to reflect <span class="No-Break">these reevaluations.</span></p>
			<p>The<a id="_idIndexMarker774"/> ReactiveX idea is implemented in a variety of languages, including Java (RxJava), Python (RxPY), and JavaScript (RxJS). The Angular framework uses RxJS to implement the <span class="No-Break">Observable pattern.</span></p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor211"/>Use cases for the Observer pattern in reactive programming</h2>
			<p>One use case<a id="_idIndexMarker775"/> is the idea of a collection pipeline, discussed by Martin Fowler on his <span class="No-Break">blog (</span><a href="https://martinfowler.com/articles/collection-pipeline"><span class="No-Break">https://martinfowler.com/articles/collection-pipeline</span></a><span class="No-Break">).</span></p>
			<p class="callout-heading">Collection pipeline, described by Martin Fowler</p>
			<p class="callout">Collection pipelines<a id="_idIndexMarker776"/> are a programming pattern where you organize some computation as a sequence of operations that compose by taking a collection as the output of one operation and feeding it into <span class="No-Break">the next.</span></p>
			<p>We can also use an Observable to do operations such as “map and reduce” or “groupby” on sequences of objects when <span class="No-Break">processing data.</span></p>
			<p>Finally, Observables can be created for diverse functions such as button events, requests, and <span class="No-Break">Twitter feeds.</span></p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor212"/>Implementing the Observer pattern in reactive programming</h2>
			<p>For this<a id="_idIndexMarker777"/> example, we<a id="_idIndexMarker778"/> decided to build a stream of a list of (fake) people’s names (in the <strong class="source-inline">ch07/observer_rx/people.txt</strong>) text file, and an observable based <span class="No-Break">on it.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">A first example text file containing fake names of people is provided (<strong class="source-inline">ch07/observer_rx/people.txt</strong>) as part of the book’s example files. But a new one can be generated whenever needed using a helper script (<strong class="source-inline">ch07/observer_rx/peoplelist.py</strong>), which will be presented in <span class="No-Break">a minute.</span></p>
			<p>An example of such a list of names will look <span class="No-Break">like this:</span></p>
			<pre class="source-code">
Peter Brown, Gabriel Hunt, Gary Martinez, Heather Fernandez, Juan White, Alan George, Travis Davidson, David Adams, Christopher Morris, Brittany Thomas, Brian Allen, Stefanie Lutz, Craig West, William Phillips, Kirsten Michael, Daniel Brennan, Derrick West, Amy Vazquez, Carol Howard, Taylor Abbott,</pre>			<p>Back to our implementation. We start by importing what <span class="No-Break">we need:</span></p>
			<pre class="source-code">
from pathlib import Path
import reactivex as rx
from reactivex import operators as ops</pre>			<p>We define a function, <strong class="source-inline">firstnames_from_db()</strong>, which returns an Observable from the text file (reading the content of the file) containing the names, with transformations (as we have already seen) using <strong class="source-inline">flat_map()</strong>, <strong class="source-inline">filter()</strong>, and <strong class="source-inline">map()</strong> methods, and a new operation, <strong class="source-inline">group_by()</strong>, to emit items from another sequence—the first name found in the <a id="_idIndexMarker779"/>file, with its <a id="_idIndexMarker780"/>number <span class="No-Break">of occurrence:</span></p>
			<pre class="source-code">
def firstnames_from_db(path: Path):
    file = path.open()
    # collect and push stored people firstnames
    return rx.from_iterable(file).pipe(
        ops.flat_map(
            lambda content: rx.from_iterable(
                content.split(", ")
            )
        ),
        ops.filter(lambda name: name != ""),
        ops.map(lambda name: name.split()[0]),
        ops.group_by(lambda firstname: firstname),
        ops.flat_map(
            lambda grp: grp.pipe(
                ops.count(),
                ops.map(lambda ct: (grp.key, ct)),
            )
        ),
    )</pre>			<p>Then, in<a id="_idIndexMarker781"/> the <strong class="source-inline">main()</strong> function, we define an Observable that emits data every 5 seconds, merging its emission <a id="_idIndexMarker782"/>with what is returned from <strong class="source-inline">firstnames_from_db(db_file)</strong>, after setting <strong class="source-inline">db_file</strong> to the people names text file, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
def main():
    db_path = Path(__file__).parent / Path("people.txt")
    # Emit data every 5 seconds
    rx.interval(5.0).pipe(
        ops.flat_map(lambda i: firstnames_from_db(db_path))
    ).subscribe(lambda val: print(str(val)))
    # Keep alive until user presses any key
    input("Starting... Press any key and ENTER, to quit\n")</pre>			<p>Here is a recap of the example (complete code in the <span class="No-Break"><strong class="source-inline">ch07/observer_rx/rx_peoplelist.py</strong></span><span class="No-Break"> file):</span></p>
			<ol>
				<li>We import the modules and classes <span class="No-Break">we need.</span></li>
				<li>We define a <strong class="source-inline">firstnames_from_db()</strong> function, which returns an Observable from the text<a id="_idTextAnchor213"/> file that is the source of the data. We collect and push the stored people’s first names from <span class="No-Break">that file.</span></li>
				<li>Finally, in the <strong class="source-inline">main()</strong> function, we define an Observable that emits data every 5 seconds, merging its emission with what is returned from calling the <span class="No-Break"><strong class="source-inline">firstnames_from_db()</strong></span><span class="No-Break"> function.</span></li>
			</ol>
			<p>To test the example, run the <strong class="source-inline">ch07/observer_rx/rx_peoplelist.py</strong> Python command. You should<a id="_idIndexMarker783"/> get an output like the <a id="_idIndexMarker784"/>following (only an extract is <span class="No-Break">shown here):</span></p>
			<pre class="source-code">
<strong class="bold">Starting... Press any key and ENTER, to quit</strong>
<strong class="bold">('Peter', 1)</strong>
<strong class="bold">('Gabriel', 1)</strong>
<strong class="bold">('Gary', 1)</strong>
<strong class="bold">('Heather', 1)</strong>
<strong class="bold">('Juan', 1)</strong>
<strong class="bold">('Alan', 1)</strong>
<strong class="bold">('Travis', 1)</strong>
<strong class="bold">('David', 1)</strong>
<strong class="bold">('Christopher', 1)</strong>
<strong class="bold">('Brittany', 1)</strong>
<strong class="bold">('Brian', 1)</strong>
<strong class="bold">('Stefanie', 1)</strong>
<strong class="bold">('Craig', 1)</strong>
<strong class="bold">('William', 1)</strong>
<strong class="bold">('Kirsten', 1)</strong>
<strong class="bold">('Daniel', 1)</strong>
<strong class="bold">('Derrick', 1)</strong></pre>			<p>Once you press a<a id="_idIndexMarker785"/> key and press <em class="italic">Enter</em> on the keyboard, the emission is interrupted, and the <span class="No-Break">program stops.</span></p>
			<h3>Handling new streams of data</h3>
			<p>Our test <a id="_idIndexMarker786"/>worked, but in a sense, it was static; the stream of data was limited to what is currently in the<a id="_idIndexMarker787"/> text file. What we need now is to generate several streams of data. The technique we can use to generate the type of fake data in the text file is based on a third-party module called Faker (<a href="https://pypi.org/project/Faker">https://pypi.org/project/Faker</a>). The code that produces the data is provided to you, for free (in the <strong class="source-inline">ch07/observer_rx/peoplelist.py</strong> file), <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from faker import Faker
import sys
fake = Faker()
args = sys.argv[1:]
if len(args) == 1:
    output_filename = args[0]
    persons = []
    for _ in range(0, 20):
        p = {"firstname": fake.first_name(), "lastname": fake.last_name()}
        persons.append(p)
    persons = iter(persons)
    data = [f"{p['firstname']} {p['lastname']}" for p in persons]
    data = ", ".join(data) + ", "
    with open(output_filename, "a") as f:
        f.write(data)
else:
    print("You need to pass the output filepath!")</pre>			<p>Now, let’s <a id="_idIndexMarker788"/>see what happens<a id="_idIndexMarker789"/> when we execute both programs (<strong class="source-inline">ch07/observer_rx/peoplelist.py</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">ch07/observer_rx/rx_peoplelis.py</strong></span><span class="No-Break">):</span></p>
			<ul>
				<li>From one command-line window or terminal, you can generate people’s names, passing the right file path to the script; you would execute the following command: <strong class="source-inline">python </strong><span class="No-Break"><strong class="source-inline">ch07/observer_rx/peoplelist.py ch07/observer_rx/people.txt</strong></span><span class="No-Break">.</span></li>
				<li>From a second shell window, you can run the program that implements the Observable via the <strong class="source-inline">python </strong><span class="No-Break"><strong class="source-inline">ch07/observer_rx/rx_peoplelist.py</strong></span><span class="No-Break"> command.</span></li>
			</ul>
			<p>So, what is the output from <span class="No-Break">both commands?</span></p>
			<p>A new version of the <strong class="source-inline">people.txt</strong> file is created (with the random names in it, separated by a comma), to replace the existing file. And, each time you rerun that command (<strong class="source-inline">python ch07/observer_rx/peoplelist.py</strong>), a new set of names is added to <span class="No-Break">the file.</span></p>
			<p>The second<a id="_idIndexMarker790"/> command <a id="_idIndexMarker791"/>gives an output like the one you got with the first execution; the difference is that now it is not the same set of data that is emitted repeatedly. Now, new data can be generated in the source <span class="No-Break">and emitted.</span></p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor214"/>Other concurrency and asynchronous patterns</h1>
			<p>There are some other <a id="_idIndexMarker792"/>concurrency and asynchronous patterns developers may use. We can cite <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">The Actor model</strong>: A conceptual<a id="_idIndexMarker793"/> model to deal with concurrent <a id="_idIndexMarker794"/>computation. It defines some rules for how actor instances should behave: an actor can make local decisions, create more actors, send more messages, and determine how to respond to the next <span class="No-Break">message received.</span></li>
				<li><strong class="bold">Coroutines</strong>: General control structures where flow control is cooperatively passed between two different routines without returning. Coroutines<a id="_idIndexMarker795"/> facilitate asynchronous<a id="_idIndexMarker796"/> programming by allowing execution to be suspended and resumed. As we have seen in one of our examples, Python has coroutines built in (via the <span class="No-Break"><strong class="source-inline">asyncio</strong></span><span class="No-Break"> library).</span></li>
				<li><strong class="bold">Message passing</strong>: Used in <a id="_idIndexMarker797"/>parallel computing, <strong class="bold">object-oriented programming</strong> (<strong class="bold">OOP</strong>), and <strong class="bold">inter-process communication</strong> (<strong class="bold">IPC</strong>), where software entities<a id="_idIndexMarker798"/> communicate and coordinate<a id="_idIndexMarker799"/> their <a id="_idIndexMarker800"/>actions by passing messages to <span class="No-Break">each other.</span></li>
				<li><strong class="bold">Backpressure</strong>: A mechanism to <a id="_idIndexMarker801"/>manage the flow of data through software systems and prevent overwhelming <a id="_idIndexMarker802"/>components. It allows systems to gracefully handle overload by signaling the producer to slow down until the consumer can <span class="No-Break">catch up.</span></li>
			</ul>
			<p>Each of these patterns has its use cases and trade-offs. It is interesting to know they exist, but we cannot discuss all the available patterns <span class="No-Break">and techniques.</span></p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor215"/>Summary</h1>
			<p>In this chapter, we discussed concurrency and asynchronous patterns, patterns useful for writing efficient, responsive software that can handle multiple tasks <span class="No-Break">at once.</span></p>
			<p>The Thread Pool pattern is a powerful tool in concurrent programming, offering a way to manage resources efficiently and improve application performance. It helps us improve application performance but also reduces overhead and better manages resources because the thread pool limits the number <span class="No-Break">of threads.</span></p>
			<p>While the Thread Pool pattern focuses on reusing a fixed number of threads to execute tasks, the Worker Model pattern is more about the dynamic distribution of tasks across potentially scalable and flexible worker entities. This pattern is particularly useful for scenarios where tasks are independent and can be processed <span class="No-Break">in parallel.</span></p>
			<p>The Future and Promise pattern facilitates asynchronous operation, allowing applications to remain responsive and efficient by not blocking the main thread with <span class="No-Break">long-running tasks.</span></p>
			<p>We also discussed the Observer pattern in reactive programming. The core idea of this pattern is to react to a stream of data and events, as with the streams of water we see in nature. We have lots of examples of this idea in the computing world. We have discussed an example of ReactiveX, which serves as an introduction for the reader to approach this programming paradigm and continue their own research via the ReactiveX <span class="No-Break">official documentation.</span></p>
			<p>Lastly, we touched upon the fact that there are other concurrency and asynchronous patterns. Each of these patterns has its use cases and trade-offs, but we cannot cover them all in a <span class="No-Break">single book.</span></p>
			<p>In the next chapter, we will discuss performance <span class="No-Break">design patterns.</span></p>
		</div>
	</body></html>