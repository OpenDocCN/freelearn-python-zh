- en: SLAM for Robot Navigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will deep dive into robot navigation, a ubiquitous task
    in robotics engineering. Typical use cases include self-driving cars and transporting
    materials in a factory. You will find that the map we generated previously by
    applying **SLAM (Simultaneous localization and mapping)** is used for path planning
    along the way. Given an initial pose, the robot will travel along the optimal
    path and should be capable of reacting to dynamic events, that is, it should be
    able to avoid the obstacles (static or dynamic) that appeared after the map was
    built.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is a natural extension of the previous one. In the previous chapter,
    you gained a practical understanding of SLAM and navigation, and you did that
    inside the Gazebo simulator using a virtual model of GoPiGo3\. Now, you are ready
    to complete the exercise again with a physical robot. By doing so, you will discover
    how many details and practical questions arise when you complete a robotic task
    in a real environment. Simulation is a good start, but the real proof that your
    robot performs as expected is by executing the task in an actual scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing **Laser Distance Sensor** (**LDS**) for your robot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a navigation application in ROS, including explanations about common
    algorithms that are used in navigation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practicing navigation with GoPiGo3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main sensor for the navigation task will be the low-cost LDS by EAI model
    YDLIDAR X4 ([https://www.aliexpress.com/item/32908156152.html](https://es.aliexpress.com/item/32908156152.html)),
    which we've simulated already within Gazebo. We will dedicate a large portion
    of this chapter to learning how to set up the LDS, understand how it works, and
    what practical information it provides to the robot.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will make use of the code located in the `Chapter9_GoPiGo_SLAM` folder
    ([https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter9_GoPiGo_SLAM](https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter9_GoPiGo3_SLAM)).
    Copy its files to the ROS workspace so that they''re available and leave the rest
    outside the `src` folder. This way, you will have a cleaner ROS environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The code in the aforementioned folder contains two new ROS packages, each one
    located within a folder that has the same name:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ydlidar`, the officially supported ROS package for the selected LDS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gopigo3_navigation`, the top-level package for performing navigation with
    GoPiGo3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will use both on the laptop environment, but in the robot – that is, the
    Raspberry Pi – you will only need `ydlidar` since the computationally expensive
    task of navigation is recommended to be run on the laptop. This way, GoPiGo3 will
    receive the drive command through the familiar `cmd_vel` topic and publish a 360°
    range scan from the LDS through the `/scan` topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, you need to rebuild the workspace separately, both for the robot
    and the laptop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Check that the packages have been installed correctly by selecting them and
    listing the files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we have to point the ROS master to the robot.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the ROS master to be in the robot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since you''ll be working with the physical robot once more, you need to reconfigure
    the ROS master URI so that it points to GoPiGo3\. So that your laptop reflects
    such a configuration, open your local `.bashrc` file and uncomment the line at
    the end that specifies what URL to point to in order to find the ROS master:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Close any open Terminals, open a new one, and check the `ROS_MASTER_URI` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You should find that the environment variable has reverted to the default server
    (localhost) and default port (`11311`). Now, we are ready to switch to the virtual
    robot. If, for some reason, `gopigo3.local` does not resolve the robot IP, set
    up its IPv4 address directly. You can get it from the robot OS like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in the `.bashrc` file, modify the following line accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Close the Terminal on your laptop and open a new one so that the configuration
    takes effect. Then, check for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can get familiar with our new sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing an LDS for your robot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before you begin, you should take some time to review all the documentation
    provided by the manufacturer EAI. You can find all the resources at [http://www.ydlidar.com/download](http://www.ydlidar.com/download).
    Pay special attention to the following items:'
  prefs: []
  type: TYPE_NORMAL
- en: The YDLIDAR X4 user manual, to get familiar with the hardware and install it
    safely with your robot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The YDLIDAR X4 ROS manual, located within the compressed `ROS.zip` file. The
    `ros` folder inside corresponds to the ROS package, but you should clone it from
    GitHub to make sure you get the latest version and stay updated. Follow the instructions
    at [https://github.com/EAIBOT/ydlidar](https://github.com/EAIBOT/ydlidar) to get
    the most recent version of the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EAI has removed** CAD (**short for **Computer-Aided Design)** models from the
    download page.
  prefs: []
  type: TYPE_NORMAL
- en: The YDLIDAR X4 development manual, which describes the communication protocol
    so that you can build your own driver to control the device.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, you are ready to get started with the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up YDLIDAR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow the instructions provided in the user manual to physically connect the
    device to your laptop or to the robot. The following screenshot shows what it
    looks like once the sensor itself has been wired to the control board via the
    set of five colored cables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24067c30-5a0b-49fa-8260-a9c9115d8499.png)'
  prefs: []
  type: TYPE_IMG
- en: Although the software instructions are also provided in the manual, we will
    list all the steps here since they refer to the core integration with ROS. First,
    we will integrate with the laptop, and then with the Raspberry Pi of the robot.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating with the remote PC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with any other hardware we integrate with ROS, we follow the standard procedure
    of cloning the package supplied by the manufacturer and building it with our workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: By running `catkin_make`, the `ydlidar_client` and  `ydlidar_node` nodes will
    be available.
  prefs: []
  type: TYPE_NORMAL
- en: This code is also bundled with the rest of the YDLIDAR models at [https://github.com/YDLIDAR/ydlidar_ros](https://github.com/YDLIDAR/ydlidar_ros).
    For a specific model, you just have to select the corresponding branch, X4\. In
    our case, this is `git clone https://github.com/YDLIDAR/ydlidar_ros -b X4 --single-branch`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After connecting X4 to a USB port of the laptop, change the permissions in
    order to access the new LDS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command assumes that your user is `ubuntu`. If it isn''t, replace
    it with your actual user. Then, initiate the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This script creates a symbolic link to the `/dev/ydlidar--> /dev/ttyUSB0` device.
    The next step is to run a test inside ROS to check everything works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Running the YDLIDAR ROS package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we are going to launch the laser scan node and visualize the results with
    a console client, before doing the same with RViz.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the YDLIDAR node with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For this part of this chapter, you should temporarily point the ROS master to
    the laptop, not the robot. Remember that you can do this for single Terminals
    by specifying `$ export ROS_MASTER_URI=http://localhost:11311` in each. Once you
    close any of these, the temporal definition will be thrown away.
  prefs: []
  type: TYPE_NORMAL
- en: 'From another Terminal, list the scan data using a client node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the YDLIDAR node''s scan result in the console, as well as the
    ROS graph (obtained by running `rqt_graph` in a separate Terminal, `T3`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d7ecd57-76e2-4e01-9739-feaa02a10687.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that `base_link_to_laser4` provides the coordinate frame transformation
    in the `/tf` topic, while `ydlidar_node` provides the sensor data feed in the
    `/scan` topic, which is visualized in the Terminal thanks to the `ydlidar_client` node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, launch RViz to see the distribution of red points at the positions
    where obstacles were found:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will repeat this exercise with the LDS connected to the robot.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating with Raspberry Pi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will repeat the process we described in the preceding section, *Setting
    up YDLIDAR*, in order to connect the LDS to the Raspberry Pi. After attaching
    the sensor to a USB port of the Raspberry Pi, open a Terminal in the robot and
    follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the repository and rebuild the workspace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When you''ve connected YDLIDAR to a USB port, check that the connection has
    been established properly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, change the permissions so that your normal user, `pi`, has access to
    the new device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, initiate the device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This script creates a symbolic link to the `/dev/ydlidar--> /dev/ttyUSB0` device.
    If this is not the case, you can do this by hand, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This way, you make sure that the `ydlidar_node` node finds the device.
  prefs: []
  type: TYPE_NORMAL
- en: Checking that YDLIDAR works with GoPiGo3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just like we did with the laptop, use the `ydlidar_client` script to check
    that you have received data from the sensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The letter `r` in the preceding code snippet stands for the Terminals in the
    Raspberry Pi. If you receive data in `r2`, then this will be proof that the sensor
    is sending its readings to ROS.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing scan data in the Raspberry Pi desktop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s check the RViz visualization in the Raspberry Pi, just like we
    did for the laptop. For this, you need to use **VNC (Virtual Network Computing)**,
    as we explained in [Chapter 6](0b20bdff-f1dc-42e8-ae83-fc290da31381.xhtml), *Programming
    in ROS – Commands and Tools*, in the *Setting up the physical robot* section.
    Set up a VNC server (`x11vnc`). Once connected from the remote laptop, launch
    the following four Terminals in the Raspberry Pi desktop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the whole screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44cabf11-2872-45b7-991d-47c7d85181e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The laser scan view in RViz (top-right window in the preceding screenshot)
    is provided by `lidar_view.launch`. The ROS graph (bottom-right window) shows
    that the `key_teleop` node allows you to teleoperate the robot with the arrow
    keys by publishing messages in the `/cmd_vel` topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c3c1b3a-1548-472d-8e2b-e9235f9533e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at what the RViz window is showing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd42aa49-815b-402b-8637-8418a7cb70c1.png)'
  prefs: []
  type: TYPE_IMG
- en: The arrow marked as **GoPiGo3** shows the location of the robot in a corner
    of the room. The external straight red lines stand for the walls, while the arrow
    pointing to **me** shows the contour of myself as I am leaving the room through
    the access door (the free space – no red points – in front of me).
  prefs: []
  type: TYPE_NORMAL
- en: Grouping launch files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For efficiency, we should offload the Raspberry Pi from visualization tasks
    and move them to the remote laptop. In order to so, we need to rework the launch
    files so that GoPiGo3 strictly runs the code that''s necessary for the robot to
    work, that is, the `gopigo3_driver.py` part of the `mygopipo` package we described
    in [Chapter 6](0b20bdff-f1dc-42e8-ae83-fc290da31381.xhtml), *Programming in ROS –
    Commands and Tools*, plus the `lidar.launch` part of the `ydlidar` package. These
    two components can be launched with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The launch files in `r1` and `r2` can be grouped into one, like so. We will
    call this script `gopigo3_ydlidar.launch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Thanks to this grouping, all the code of GoPiGo3 can be run with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This launches the `ydlidar` and `gopigo3` nodes, which provide a software interface
    so that we can talk to the robot sensors and actuators. This also creates the
    following ROS graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30a81460-081c-4b52-b632-fda7b362a4b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, to listen for the scan data, you need to execute the YDLIDAR client in
    the robot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The ROS graph looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29e2c41b-4c9f-422e-9cd4-1a4423258768.png)'
  prefs: []
  type: TYPE_IMG
- en: The `rqt_graph` command that throws the preceding graph can be executed either
    from the Raspberry Pi or a remote laptop. Since our goal is to offload the Raspberry
    Pi, you should run it from the laptop. In such cases, you won't need the desktop
    interface of the Raspberry Pi anymore.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding graph shows that `ydlidar_node` publishes laser scan data in the `/scan` topic,
    which it is read by the `ydlidar_client` node and is printed in the Terminal where
    the node was launched from, that is, `r2`.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing scan data from the remote laptop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final step is to get the RViz laser scan data on the desktop of the laptop.
    This is what we will accomplish in this section.
  prefs: []
  type: TYPE_NORMAL
- en: In the following paragraphs, the letter `r` in the code snippets stands for
    the Terminals in the robot, while `T` refers to the Terminals in the laptop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to build the ROS environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, launch the processes in the robot using the unified launch file that
    we built in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'From the laptop, find the content of the last message that was published in
    the `/scan` topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Ranges are provided in the `ranges` array field for 720 orientations, corresponding
    to an angle resolution of 0.5° for a 360° coverage. Then, find which message type
    it is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, inspect the message structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, run the ROS visualization node in the laptop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `T1` Terminal will launch the visualization in RViz, while `T2` will let
    you teleoperate the robot to check how its perception of the environment changes
    as it moves by modifying the ranges of the laser scan. The visualization provided
    by `display.launch` adds the URDF model of YDLIDAR to RViz. The black circle in
    the following diagram represents the sensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73dd3803-f380-454d-8536-a2a93e6e1b7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Be aware that since the URDF model only includes the sensor, it doesn't move
    like the physical GoPiGo3 robot moves. The scan data – the red points – will change
    according to the robot's motion, but the virtual sensor will remain in the initial
    position, which is not its actual location anymore (unless you stop `T1` and launch
    it again). Hence, at this point, it is more coherent that you use `display_scan.launch` (which
    does not include a URDF model, just the scan data), instead of `display.launch`.
    In the *Practising navigation with GoPiGo3* section, you will link the URDF models
    of GoPiGo3 and the LDS sensor so that RViz shows the motion of the robot.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Running the YDLIDAR ROS package* section, you will run a distributed
    system, where the Raspberry Pi collects sensor data and the remote laptop provides
    a visualization of it.
  prefs: []
  type: TYPE_NORMAL
- en: Processing YDLIDAR data from a remote laptop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it''s time to interpret the scan data. This can be accomplished with a
    simple snippet called `scan.py`, which is provided with the ROS package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Type the following command into a Terminal on a laptop to see it in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code lists the detected range along the main axes, *X* and *Y*,
    on the screen. Keep the following photograph in mind regarding the reference frame
    of the sensor, which was extracted from the X4 documentation. The angle is measured
    clockwise, taking the *X* axis as its origin. In the following photograph, you
    can see the LDS mounted on the GoPiGo3 and the *X* and *Y* axes directions drawn
    on top:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f21d8ec6-c80b-46e7-a667-be60c3ebd066.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Going back to the screenshot in the *Visualizing data from the remote laptop *section,
    you can guess how the robot is oriented in the room. Take into account that the
    green axis corresponds to *X* and that the red lines corresponds to *Y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73c88c4e-7c9e-4689-8521-e4c74a1b5c73.png)'
  prefs: []
  type: TYPE_IMG
- en: The callback function ranges along the main axes (*+X (0°)*, *+Y (-90°)*, *-X
    (180°)*, *-Y (90)°*), where you can detect obstacles for the right (*+X*), front
    (*+Y*), left (*-X*), or back (*-Y*), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a navigation application in ROS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An application that provides a robot with navigation capabilities has to take
    into account the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensing**: This provides us with the ability to acquire motion data so that
    the robot is able to estimate its position in real time. This kind of information
    is known as **robot** **odometry**. There are two main sources of sensor data:
    the encoders, which let us know the rotation of the robot wheels, and the IMU
    sensor, which provides acceleration and rotation information about the robot as
    a whole. Generally speaking, data from encoders is used the most, although it
    may be combined with IMU data to improve the accuracy of the pose estimation.
    This is an advanced topic called **fusion sensor**, which is out of the scope
    of this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Localization/pose estimation**: As a result of odometry and the current map
    of the environment, the **AMCL (Adaptive Monte Carlo localization)** algorithm
    allows us to update the robot pose estimation in real time, as we introduced in
    the previous chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Path planning**: Given a target pose, such planning consists of creating
    a global optimum path of the whole map and a local path that covers a small area
    around the robot so that it is able to follow a precise path while avoiding obstacles.
    Local path planning is dynamic; that is, as the robot moves, the area around the
    robot changes accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Move/obstacle avoidance**: As we previously, there is a global optimum path
    that is combined with a local path, and this happens for every position of the
    robot as it moves to the target location. This is like making a zoom window of
    the surroundings. Hence, the local path is calculated by taking the global path
    and the close obstacles into account (for example, a person crossing in front
    of the robot). Local path planning is able to avoid such obstacles without losing
    the global path. This local zoom window is built using the real-time information
    provided by the LDS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a result of the aforementioned points, the following data has to be available
    to ROS so that navigation is possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Odometry**: It is published by the `gopigo3` node in the `/odom` topic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coordinate transformation**: The position of the sensors in the robot frame
    of reference is published in the `/tf` topic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scan data**: The distances from the robot to the obstacles around it are
    obtained from the LDS and made available in the `/scan` topic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Map**: The occupancy grid that''s built when executing SLAM is saved to a
    `map.pgm` file, with the configuration in the `map.yml` file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target pose**: This will be specified by the user in an RViz window once
    the ROS navigation''s setup has been launched.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity commands**: This is the final output of the algorithm. Commands
    are published in the `/cmd_vel` topic that the `gopigo3` node is subscribed to.
    Then, the robot moves accordingly to follow the planned path.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given the preceding topics and concepts, the steps to create a navigation application
    in ROS are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a map of the environment. Taking the data from the LDS, the robot will
    create a map of the environment based on the range of data coming from the sensor.
    It will use the SLAM technique we discussed in the previous chapter to do so.
    This process of building the map follows a practical sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start ROS in the physical robot, meaning that the necessary nodes will be exposing
    the topics where sensor data is published, as well as the topic that will receive
    motion commands. The set of rules to publish the motion commands as a function
    of the acquired sensor data conforms to what we will call the **robot application
    logic**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish the connection from the remote PC. If it's been configured properly,
    it should be automatic when launching ROS in the laptop. This topic was covered
    in the *Technical requirements* section at the beginning of this chapter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Launch the SLAM process from the laptop. This will allow ROS to acquire real-time
    range data from the LDS so that it can start building a map of the environment.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Teleoperate the robot and check the zones that are mapped and the ones to be
    scanned in an RViz visualization. In this case, the robot application logic named
    in the first bullet is driven by you as a human, where you decide what motion
    GoPiGo3 has to perform at every instance. You may also automate teleoperation
    by letting the robot wander around randomly (remember the *Simulating the LDS*
    section of the previous chapter, where you let GoPiGo3 autonomously explore the
    environment while applying a set of rules to surround the obstacles it might encounter
    on its way). In this case, the robot application logic is implemented in a Python
    script and there is no human intervention.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the environment has been fully explored, you have to save the map so that
    it can be used in the next step for autonomous navigation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Launch the navigation task by telling the robot the target location you want
    it to move to. This process of autonomous navigation follows the following sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start ROS in the physical robot. In this case, the robot application logic is
    part of the navigation task, which is intended to be performed autonomously by
    GoPiGo3, without any human intervention.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Load the map of the environment that was created in the first part of the navigation
    application.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Indicate a target pose to the robot, something you can directly perform on an
    RViz visualization, which shows the map of the environment.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let the robot navigate by itself to the target location, checking that it is
    able to plan an optimum path while avoiding the obstacles it may encounter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We will illustrate this process with a real-world example in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Practicing navigation with GoPiGo3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll cover the steps that we followed in the *Practising SLAM
    and navigation with GoPiGo3* section of the previous chapter by substituting the
    virtual robot and the Gazebo simulator with the actual GoPiGo3 and the physical
    environment, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Building a map of the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s consider a physical environment that''s simple enough for our
    learning purposes. This can be seen in the following photograph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d993b2ff-272a-45a0-ba4e-1b21bdc59ddd.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Be aware that this almost-square space has three limiting sides and one step
    that cannot be detected by the laser sensor because it is below the floor level
    of the robot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going to ROS, the first step consists of mapping the environment so that the
    robot can localize its surroundings and navigate around it. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch all the ROS nodes in the robot. From a remote Terminal connected to
    the Raspberry Pi, this means running the ROS launch files that control the drives
    and the LDS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall the *Integrating with Raspberry Pi* section: grouping launch files is
    how we built a unique launch file to run the robot configuration in one shot.
    This ensures that GoPiGo3 is ready to interact with ROS in the laptop, where all
    the processing related to the map of the environment and the navigation command
    will be done.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the SLAM mapping ROS package, whose launch file includes a RViz visualization
    that overimposes the virtual model of the robot with the actual scan data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Teleoperate the robot to make it cover as much of the surface of the virtual
    environment as possible. We can do this as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As you explore the robot''s surroundings, you should see something similar
    to the following in the RViz window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca2f5b14-4c2f-43dc-8a7b-a404317152cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, you can see the three limiting walls of the square space. The rest of
    the map shows the first obstacles the laser finds in the remaining directions.
    Remember that the step in the fourth side cannot be detected because it is below
    the floor level of the robot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve finished exploring, save the map we generated into the two files
    we specified previously, that is, `.pgm` and `.yaml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Again, you will have the map files in the root folders of your workspace, that
    is, `test_map.pgm` and `test_map.yaml`. Now, we are ready to make GoPiGo3 navigate
    in the physical environment.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating GoPiGo3 in the real world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This second part requires that you stop all ROS processes on the laptop, but
    not necessarily on GoPiGo3\. Remember that, in the robot, you have the minimum
    ROS configuration so that the robot is able to perceive the environment (LDS X4
    sensor) and move around (drives). All the remaining logic for navigation will
    run in the laptop. Hence, close any open Terminals in your PC and start the new
    phase by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the ROS nodes in the robot if you stopped the Terminal previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch the AMCL navigation by providing the cost map that the robot built previously.
    To do so, you have to reference the `.yaml` map file you created previously. Make
    sure that the corresponding `.pgm` file has the same name and is placed in the
    same location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This launch file includes an RViz visualization that will let us interact with
    the map so that we can set a target location, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10af7274-4e17-49a2-8085-c039ec9303ec.png)'
  prefs: []
  type: TYPE_IMG
- en: As in the case of the Gazebo simulation, the goal location is set by pressing the **2D
    Nav Goal** button at the top right of the RViz window and selecting the target
    pose, which is composed of both the position and orientation (a green arrow in
    RViz lets you define it graphically). As soon as you pick such a location, the
    AMCL algorithm starts path planning and sends motion commands via the `/cmd_vel`
    topic. Consequently, the robot moves to the specified location as the sequence
    of commands is executed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you finally completed an autonomous task using GoPiGo3\. This
    is only the entry point to the fascinating field of artificial intelligence applied
    to robotics. The most obvious functionality to be built on top of robot navigation
    is self-driving, which is the functionality that is currently being implemented
    by many vehicle manufacturers to make safer and more comfortable vehicles for
    the end users.
  prefs: []
  type: TYPE_NORMAL
- en: In the fourth and last part of this book, you will learn how machine learning
    techniques are applied nowadays to build smarter robots.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which of these sensors is of the LDS type?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) LIDAR
  prefs: []
  type: TYPE_NORMAL
- en: B) Ultrasonic distance sensor
  prefs: []
  type: TYPE_NORMAL
- en: C) Capacitive sensors
  prefs: []
  type: TYPE_NORMAL
- en: Where does the ROS master node have to live to perform navigation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) In the robot
  prefs: []
  type: TYPE_NORMAL
- en: B) In the robot and the laptop
  prefs: []
  type: TYPE_NORMAL
- en: C) In either the robot or the laptop
  prefs: []
  type: TYPE_NORMAL
- en: What will happen if an obstacle is placed in the environment after the map has
    been built?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) The robot will not detect it and may crash with it if it interferes with
    the planned path.
  prefs: []
  type: TYPE_NORMAL
- en: B) The local path planning will be taken into account to provide a modified
    path that avoids the obstacle.
  prefs: []
  type: TYPE_NORMAL
- en: C) You should rebuild the map with the new conditions before proceeding to the
    navigation task.
  prefs: []
  type: TYPE_NORMAL
- en: Can you perform navigation without previously running SLAM with the robot?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) No, because you have to build the map with the same robot that you will use
    for navigation.
  prefs: []
  type: TYPE_NORMAL
- en: B) Yes, the only condition is that you provide a premade map of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: C) No, SLAM and navigation are two sides of the same coin.
  prefs: []
  type: TYPE_NORMAL
- en: What is the odometry of a robot?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) The total distance it has covered since the ROS application was started.
  prefs: []
  type: TYPE_NORMAL
- en: B) The use of data from motion sensors to estimate the changes in the robot's
    pose over time.
  prefs: []
  type: TYPE_NORMAL
- en: C) The use of data from motion sensors to estimate the current robot's pose.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main resource that you can read in order to deepen your knowledge of SLAM
    is the official documentation of the ROS Navigation Stack, which is located at
    [http://wiki.ros.org/navigation](http://wiki.ros.org/navigation). For those of
    you who are interested, here are some additional references:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ROS Navigation: Concepts and Tutorial, Federal University of Technology*, Longhi
    R., Schneider A., Fabro J., Becker T., and Amilgar V. (2018), Parana, Curitiba,
    Brazil.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lidar design, use, and calibration concepts for correct environmental detection*,
    in IEEE Transactions on Robotics and Automation, M. D. Adams (2000) vol. 16, no.
    6, pp. 753-761, Dec. 2000, doi: 10.1109/70.897786. URL: [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=897786&isnumber=19436](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=897786&isnumber=19436).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The LIDAR Odometry in the SLAM*, V. Kirnos, V. Antipov, A. Priorov, and V.
    Kokovkina, 23rd Conference of Open Innovations Association (FRUCT), Bologna, 2018,
    pp. 180-185. doi: 10.23919/FRUCT.2018.8588026, URL: [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8588026&isnumber=8587913](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8588026&isnumber=8587913).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
