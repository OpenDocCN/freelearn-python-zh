- en: '6'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recursions and Reductions
  prefs: []
  type: TYPE_NORMAL
- en: Many functional programming language compilers will optimize a recursive function
    to transform a recursive call in the tail of the function to an iteration. This
    tail-call optimization will dramatically improve performance. Python doesn‚Äôt do
    this automatic tail-call optimization. One consequence is pure recursion suffers
    from limitations. Lacking an automated optimization, we need to do the tail-call
    optimization manually. This means rewriting recursion to use an explicit iteration.
    There are two common ways to do this, and we‚Äôll consider them both in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In previous chapters, we‚Äôve looked at several related kinds of processing design
    patterns; some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Mapping and filtering, which create collections from collections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reductions that create a scalar value from a collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distinction is exemplified by functions such as `map()` and `filter()` that
    accomplish the first kind of collection processing. There are some more specialized
    reduction functions, which include `min()`, `max()`, `len()`, and `sum()`. There‚Äôs
    a general-purpose reduction function as well, `functools.reduce()`.
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôll also consider creating a `collections.Counter()` object as a kind of reduction
    operator. It doesn‚Äôt produce a single scalar value per se, but it does create
    a new organization of the data that eliminates some of the original structure.
    At heart, it‚Äôs a kind of count-group-by operation that has more in common with
    a counting reduction than with a mapping.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we‚Äôll look at reduction functions in more detail. From a purely
    functional perspective, a reduction can be defined recursively. The tail-call
    optimization techniques available in Python apply elegantly to reductions.
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôll review a number of built-in reduction algorithms including `sum()`, `count()`,
    `max()`, and `min()`. We‚Äôll look at the `collections.Counter()` creation and related
    `itertools.groupby()` reductions. We‚Äôll also look at how parsing (and lexical
    scanning) are proper reductions since they transform sequences of tokens (or sequences
    of characters) into higher-order collections with more complex properties.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Simple numerical recursions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can consider all numeric operations to be defined by recursions. For more
    details, read about the Peano axioms that define the essential features of numbers
    at [https://www.britannica.com/science/Peano-axioms](https://www.britannica.com/science/Peano-axioms).
  prefs: []
  type: TYPE_NORMAL
- en: From these axioms, we can see that addition is defined recursively using more
    primitive notions of the next number, or the successor of a number n, S(n).
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the presentation, we‚Äôll assume that we can define a predecessor
    function, P(n), such that n = S(P(n)) = P(S(n)), as long as n‚â†0\. This formalizes
    the idea that a number is the successor of the number‚Äôs predecessor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Addition between two natural numbers could be defined recursively as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ add(a,b) = b if a = 0 |( add(P(a),S(b)) if a ‚ÅÑ= 0 ](img/file48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we use the more typical notations of n + 1 and n‚àí 1 instead of S(n) and P(n),
    we can more easily see how the rule add(a,b) = add(a ‚àí 1,b + 1) when a‚â†0 works.
  prefs: []
  type: TYPE_NORMAL
- en: 'This translates neatly into Python, as shown in the following function definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We‚Äôve rearranged the abstract mathematical notation into concrete Python.
  prefs: []
  type: TYPE_NORMAL
- en: There‚Äôs no good reason to provide our own functions in Python to do simple addition.
    We rely on Python‚Äôs underlying implementation to properly handle arithmetic of
    various kinds. Our point here is that fundamental scalar arithmetic can be defined
    recursively, and the definition translates to Python.
  prefs: []
  type: TYPE_NORMAL
- en: This suggests that more complicated operations, defined recursively, can also
    be translated to Python. The translation can be manually optimized to create working
    code that matches the abstract definitions, reducing questions about possible
    bugs in the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'A recursive definition must include at least two cases: a non-recursive (or
    base) case where the value of the function is defined directly, and the recursive
    case where the value of the function is computed from a recursive evaluation of
    the function with different argument values.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to be sure the recursion will terminate, it‚Äôs important to see how
    the recursive case computes values that approach the defined non-recursive base
    case. Pragmatically, there are often constraints on the argument values that we‚Äôve
    omitted from the functions here. For example, the `add()` function in the preceding
    command snippet could be expanded to include `assert``¬†a>=0``¬†and``¬†b>=0` to establish
    two necessary constraints on the input values.
  prefs: []
  type: TYPE_NORMAL
- en: Without these constraints, starting with `a` equal to -1 won‚Äôt approach the
    non-recursive case of `a``¬†==``¬†0` as we keep subtracting 1 from `a`.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Implementing manual tail-call optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For some functions, the recursive definition is the most succinct and expressive.
    A common example is the `factorial()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see how this is rewritten as a simple recursive function in Python from
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( | { 1 if n = 0 n! = |( n √ó (n‚àí 1)! if n ‚ÅÑ= 0 ](img/file49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding formula can be implemented in Python by using the following function
    definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This implementation has the advantage of simplicity. The recursion limits in
    Python artificially constrain us; we can‚Äôt do anything larger than about `fact(997)`.
    The value of 1000! has 2,568 digits and generally exceeds our floating-point capacity;
    on some systems the floating-point limit is near 10^(300). Pragmatically, it‚Äôs
    common to switch to a log gamma function instead of working with immense numbers.
  prefs: []
  type: TYPE_NORMAL
- en: See [https://functions.wolfram.com/GammaBetaErf/LogGamma/introductions/Gammas/ShowAll.html](https://functions.wolfram.com/GammaBetaErf/LogGamma/introductions/Gammas/ShowAll.html)
    for more on log gamma functions.
  prefs: []
  type: TYPE_NORMAL
- en: We can expand Python‚Äôs call stack limit to stretch this to the limits of memory.
    It‚Äôs better, however, to manually optimize these kinds of functions to eliminate
    the recursion.
  prefs: []
  type: TYPE_NORMAL
- en: This function demonstrates a typical tail recursion. The last expression in
    the function is a call to the function with a new argument value. An optimizing
    compiler can replace the function call stack management with a loop that executes
    very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the function involves an incremental change from n to n ‚àí 1\.
    This means that we‚Äôre generating a sequence of numbers and then doing a reduction
    to compute their product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stepping outside purely functional processing, we can define an imperative
    `facti()` calculation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This version of the factorial function will compute values beyond 1000! (2000!,
    for example, has 5,736 digits). This example isn‚Äôt purely functional. We‚Äôve optimized
    the tail recursion into a stateful `for` statement depending on the `i` variable
    to maintain the state of the computation.
  prefs: []
  type: TYPE_NORMAL
- en: In general, we‚Äôre obliged to do this in Python because Python can‚Äôt automatically
    do the tail-call optimization. There are situations, however, where this kind
    of optimization isn‚Äôt actually helpful. We‚Äôll look at a few of them.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Leaving recursion in place
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In some cases, the recursive definition is actually optimal. Some recursions
    involve a divide and conquer strategy that minimizes the work. One example of
    this is the algorithm for doing exponentiation by squaring. This works for computing
    values that have a positive integer exponent, like 2^(64). We can state it formally
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| ||| 1 if n = 0 n { (n‚àí1) a = || a √ó a if a is odd ||( n2 2 (a ) if a
    is even ](img/file50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We‚Äôve broken the process into three cases, easily written in Python as a recursion.
    Look at the following function definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For odd numbers, the `fastexp()` method is defined recursively. The exponent
    `n` is reduced by 1\. A simple tail-recursion optimization would work for this
    case. It would not work for the even case, however.
  prefs: []
  type: TYPE_NORMAL
- en: For even numbers, the `fastexp()` recursion uses `n``¬†//``¬†2`, chopping the
    problem into half of its original size. Since the problem size is reduced by a
    factor of 2, this case results in a significant speed-up of the processing.
  prefs: []
  type: TYPE_NORMAL
- en: We can‚Äôt trivially reframe this kind of function into a tail-call optimization
    loop. Since it‚Äôs already optimal, we don‚Äôt really need to optimize it further.
    The recursion limit in Python would impose the constraint of n ‚â§ 2^(1000), a generous
    upper bound.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Handling difficult tail-call optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can look at the definition of Fibonacci numbers recursively. The following
    is one widely used definition for the n^(th) Fibonacci number, F[n]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| ||| 0 if n = 0 { Fn = | 1 if n = 1 |||( Fn‚àí 1 + Fn‚àí 2 if n ‚â• 2 ](img/file51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A given Fibonacci number, F[n], is defined as the sum of the previous two numbers,
    F[n‚àí1] + F[n‚àí2]. This is an example of multiple recursion: it can‚Äôt be trivially
    optimized as a simple tail recursion. However, if we don‚Äôt optimize it to a tail
    recursion, we‚Äôll find it to be too slow to be useful.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a na√Øve implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This suffers from a terrible multiple recursion problem. When computing the
    `fib(n)` value, we must compute the `fib(n-1)` and `fib(n-2)` values. The computation
    of the `fib(n-1)` value involves a duplicate calculation of the `fib(n-2)` value.
    The two recursive uses of the `fib()` function will more than duplicate the amount
    of computation being done.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the left-to-right Python evaluation rules, we can evaluate values
    up to about `fib(1000)`. However, we have to be patient. Very patient. (Trying
    to find the actual upper bound with the default stack size means waiting a long
    time before the `RecursionError` is raised.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is one alternative, which restates the entire algorithm to use
    stateful variables instead of a simple recursion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Our stateful version of this function counts up from 0, unlike the recursion,
    which counts down from the initial value of `n`. This version is considerably
    faster than the recursive version.
  prefs: []
  type: TYPE_NORMAL
- en: What‚Äôs important here is that we couldn‚Äôt trivially optimize the `fib()` function
    recursion with an obvious rewrite. In order to replace the recursion with an imperative
    version, we had to look closely at the algorithm to determine how many stateful
    intermediate variables were required.
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise for the reader, try using the `@cache` decorator from the `functools`
    module. What impact does this have?
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.4 Processing collections through recursion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When working with a collection, we can also define the processing recursively.
    We can, for example, define the `map()` function recursively. The formalism could
    be stated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ [] if len(C ) = 0 map (f,C ) = | ( map(f,C [:‚àí1]) + [f (C ‚àí1)] if len(C
    ) > 0 ](img/file52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We‚Äôve defined the mapping of a function, f, to an empty collection as an empty
    sequence, `[]`. We‚Äôve also specified that applying a function to a collection
    can be defined recursively with a three-step expression. First, recursively perform
    the mapping of the function to all of the collection except the last element,
    creating a sequence object. Then apply the function to the last element. Finally,
    append the last calculation to the previously built sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a purely recursive function version of this `map()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The value of the `mapr(f,[])` method is defined to be an empty list object.
    The value of the `mapr()` function with a non-empty list will apply the function
    to the last element in the list and append this to the list built recursively
    from the `mapr()` function applied to the head of the list.
  prefs: []
  type: TYPE_NORMAL
- en: We have to emphasize that this `mapr()` function actually creates a list object.
    The built-in `map()` function is an iterator; it doesn‚Äôt create a list object.
    It yields the result values as they are computed. Also, the work is done in right-to-left
    order, which is not the way Python normally works. This is only observable when
    using a function that has side effects, something we‚Äôd like to avoid doing.
  prefs: []
  type: TYPE_NORMAL
- en: While this is an elegant formalism, it still lacks the tail-call optimization
    required. An optimization will allow us to exceed the default recursion limit
    of 1,000 and also performs much more quickly than this na√Øve recursion.
  prefs: []
  type: TYPE_NORMAL
- en: The use of `Callable[[Any],``¬†Any]` is a weak type hint. To be more clear, it
    can help to define a domain type variable and a range type variable. We‚Äôll include
    this detail in the optimized example.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.5 Tail-call optimization for collections
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We have two general ways to handle collections: we can use a higher-order function
    that returns a generator expression, or we can create a function that uses a `for`
    statement to process each item in a collection. These two patterns are very similar.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a higher-order function that behaves like the built-in `map()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We‚Äôve returned a generator expression that produces the required mapping. This
    uses the explicit `for` in the generator expression as a kind of tail-call optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The source of data, `C`, has a type hint of `Iterable[DomT]` to emphasize that
    some type, `DomT`, will form the domain for the mapping. The transformation function
    has a hint of `Callable[[DomT],``¬†RngT]` to make it clear that it transforms from
    some domain type to a range type. The function `float()`, for example, can transform
    values from the string domain to the float range. The result has the hint of `Iterator[RngT]`
    to show that it iterates over the range type, `RngT`; the result type of the callable
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a generator function with the same signature and result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This uses a complete `for` statement for the tail-call optimization. The results
    are identical. This version is slightly slower because it involves multiple statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In both cases, the result is an iterator over the results. We must do something
    else to materialize a sequence object from an iterable source. For example, here
    is the `list()` function being used to create a sequence from the iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For performance and scalability, this kind of tail-call optimization is required
    in Python programs. It makes the code less than purely functional. However, the
    benefit far outweighs the lack of purity. In order to reap the benefits of succinct
    and expressive functional design, it is helpful to treat these less-than-pure
    functions as if they were proper recursions.
  prefs: []
  type: TYPE_NORMAL
- en: What this means, pragmatically, is that we must avoid cluttering up a collection
    processing function with additional stateful processing. The central tenets of
    functional programming are still valid even if some elements of our programs are
    less than purely functional.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.6 Using the assignment (sometimes called the ‚Äùwalrus‚Äù) operator in recursions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In some cases, recursions involve conditional processing that can be optimized
    using the ‚Äùwalrus‚Äù or assignment operator, `:=`. The use of assignment means that
    we‚Äôre introducing stateful variables. If we‚Äôre careful of the scope of those variables,
    the possibility of terribly complex algorithms is reduced.
  prefs: []
  type: TYPE_NORMAL
- en: 'We reviewed the `fast_exp()` function shown below in the [Leaving recursion
    in place](#x1-1290002) section. This function used three separate cases to implement
    a divide and conquer strategy. In the case of raising a number, `a`, to an even
    power, we can use t = a^(![n 2](img/file53.jpg)) to compute t √ó t = a^n:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This uses the `:=` walrus operator to compute a partial answer, `fastexp_w(a,``¬†q)`,
    and save it into a temporary variable, `t`. This is used later in the same statement
    to compute `t``¬†*``¬†t`.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, when we perform tail-call optimization on a recursion, the
    body of the `for` statement will have ordinary assignment statements. It isn‚Äôt
    often necessary to exploit the walrus operator.
  prefs: []
  type: TYPE_NORMAL
- en: The assignment operator is often used in situations like regular expression
    matching, where we want to save the match object as well as make a decision. It‚Äôs
    very common to see `if``¬†(match``¬†:=``¬†pattern.match(text)):` as a way to both
    attempt a regular expression match, save the resulting match object, and confirm
    it‚Äôs not a `None` object.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Reductions and folding a collection from many items to one
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can consider the `sum()` function to have the following kind of definition.
    We could say that the sum of a collection is 0 for an empty collection. For a
    non-empty collection, the sum is the first element plus the sum of the remaining
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| { 0 if n = 0 sum ([c0,c1,c2,...,cn]) = | ( c0 + sum ([c1,c2,...,cn])
    if n > 0 ](img/file54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use a slightly simplified notation called the Bird-Meertens Formalism.
    This uses ‚äï‚àï[c[0],c[1],...c[n]] to show how some arbitrary binary operator, ‚äï,
    can be applied to a sequence of values. It‚Äôs used as follows to summarize a recursive
    definition into something a little easier to work with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![sum ([c0,c1,c2,...,cn]) = + ‚àï[c0,c1,c2,...,cn] = 0+ c0 + c1 + ...+ cn ](img/file55.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We‚Äôve effectively folded the + operator between each item of the sequence. Implicitly,
    the processing will be done left to right. This could be called a ‚Äùfold left‚Äù
    way of reducing a collection to a single value. We could also imagine grouping
    the operators from right to left, calling this a ‚Äùfold right.‚Äù While some compiled
    languages will perform this optimization, Python works strictly from left to right
    when given a sequence of similar precedence operators.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, a product function can be defined recursively as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This is a tiny rewrite from a mathematical notation to Python. However, it is
    less than optimal because all of the slices will create a large number of intermediate
    list objects. It‚Äôs also limited to only working with explicit collections; it
    can‚Äôt work easily with iterable objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can revise this slightly to work with an iterable, which avoids creating
    any intermediate collection objects. The following is a properly recursive product
    function that works with any iterator as a source of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This doesn‚Äôt work with iterable collections. We can‚Äôt interrogate an iterator
    with the `len()` function to see how many elements it has. All we can do is attempt
    to extract the head of the iterator. If there are no items in the iterator, then
    any attempt to get the head will raise the `StopIteration` exception. If there
    is an item, then we can multiply this item by the product of the remaining items
    in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we must explicitly create an iterator from a materialized sequence
    object, using the `iter()` function. In other contexts, we might have an iterable
    result that we can use. Following is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This recursive definition does not rely on explicit state or other imperative
    features of Python. While it‚Äôs more purely functional, it is still limited to
    working with collections of under 1,000 items. (While we can extend the stack
    size, it‚Äôs far better to optimize this properly.) Pragmatically, we can use the
    following kind of imperative structure for reduction functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This avoids any recursion limits. It includes the required tail-call optimization.
    Furthermore, this will work equally well with any iterable. This means a `Sequence`
    object, or an iterator.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Tail-call optimization using deques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The heart of recursion is a stack of function calls. Evaluating `fact(5)`, for
    example, is `5*fact(4)`. The value of `fact(4)` is `5*fact(3)`. There is a stack
    of pending computations until `fact(0)` has a value of 1\. Then the stack of computations
    is completed, revealing the final result.
  prefs: []
  type: TYPE_NORMAL
- en: Python manages the stack of calls for us. It imposes an arbitrary default limit
    of 1,000 calls on the stack, to prevent a program with a bug in the recursion
    from running forever.
  prefs: []
  type: TYPE_NORMAL
- en: We can manage the stack manually, also. This gives us another way to optimize
    recursions. We can‚Äîexplicitly‚Äîcreate a stack of pending work. We can then do a
    final summarization of the pending work, emptying the items from the stack.
  prefs: []
  type: TYPE_NORMAL
- en: For something as simple as computing a factorial value, the stacking and unstacking
    can seem like needless overhead. For more complex applications, like examining
    the hierarchical file system, it seems more appropriate to mix processing files
    with putting directories onto a stack for later consideration.
  prefs: []
  type: TYPE_NORMAL
- en: We need a function to traverse a directory hierarchy without an explicit recursion.
    The core concept is that a directory is a collection of entries, and each entry
    is either a file, a sub-directory, or some other filesystem object we don‚Äôt want
    to touch (e.g., a mount point, symbolic link, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can say a node in the directory tree is a collection of entries: N = e[0],e[1],e[2],...,e[n].
    Each entry is either another directory, e ‚ààùîª, or a file, e ‚ààùîΩ.'
  prefs: []
  type: TYPE_NORMAL
- en: We can perform mappings on each file in the tree to process each file‚Äôs content.
    We might perform a filter operation to create an iterator over files with a specific
    property. We can also perform a reduction to count the number of files with a
    property. In this example, we‚Äôll count the occurrences of a specific substring
    throughout the contents of files in a directory tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, we want a function p(f) that will provide the count of `"print"`
    in a node of the directory tree. It could be defined like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{|‚Äúprint‚Äù ‚àà N | if N ‚àà ùîΩ p(N ) = ‚àë |( e‚ààN p(e) if N ‚àà ùîª ](img/file56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This shows how to apply the p(N) function to each element of a directory tree.
    When the element is a file, e ‚ààùîΩ, we can count instances of ‚Äùprint‚Äù. When the
    element is a directory, e ‚ààùîª, we need to apply the p(N) function recursively to
    each entry, e[x], in the directory. While directory trees can‚Äôt be deep enough
    to break Python‚Äôs stack size limit, this kind of algorithm reveals an alternative
    tail-call optimization. It is an opportunity to use an explicit stack.
  prefs: []
  type: TYPE_NORMAL
- en: The `collections.deque` class is a marvelous way to build stacks and queues.
    The name comes from ‚Äùdouble-ended queue,‚Äù sometimes spelled dequeue. The data
    structure can be used as either a last-in-first-out (LIFO) stack or a first-in-first-out
    (FIFO). In this example, we use the `append()` and `pop()` methods, which enforce
    LIFO stack behavior. While this is much like a list, there are some optimizations
    in the `deque` implementation that can make it slightly faster than the generic
    list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a stack data structure lets us work with a hierarchy of indefinite size
    without running into Python‚Äôs internal stack depth limitation and raising `RecursionError`
    exceptions. The following function will traverse a file hierarchy looking at Python
    source files (with a suffix of `.py`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We seeded the stack of pending tasks with the initial directory. The essential
    algorithm is to unstack a directory and visit each entry in the directory. For
    entries that are files with the proper suffix, the processing is performed: counting
    the occurrences of ‚Äùprint‚Äù. For entries that are directories, the directory is
    put into the stack as a pending task. Note that directories with a leading dot
    in their name need to be ignored. For the code in this book, those directories
    include caches used by tools like mypy, pytest, and tox. We want to skip over
    those cache directories.'
  prefs: []
  type: TYPE_NORMAL
- en: The processing performed on each file is part of the `all_print()` function.
    This can be refactored as a separate function that‚Äôs applied to each node as part
    of a reduction. Rewriting the `all_print()` function to be a proper higher-order
    function is left as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is we have two strategies for transforming a formal recursion
    into a usefully optimized function. We can reframe the recursion into an iteration,
    or we can introduce an explicit stack.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will apply the idea of a reduction (and the associated
    tail-call optimizations) to creating groups of items and computing a reduction
    for the groups.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Group-by reduction from many items to fewer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea of a reduction can apply in many ways. We‚Äôve looked at the essential
    recursive definition of a reduction that produces a single value from a collection
    of values. This leads us to optimizing the recursion so we have the ability to
    compute summaries without the overheads of a naive Pythonic implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Creating subgroups in Python isn‚Äôt difficult, but it can help to understand
    the formalisms that support it. This understanding can help to avoid implementations
    that perform extremely poorly.
  prefs: []
  type: TYPE_NORMAL
- en: A very common operation is a reduction that groups values by some key or indicator.
    The raw data is grouped by some column‚Äôs value, and reductions (sometimes called
    aggregate functions) are applied to other columns.
  prefs: []
  type: TYPE_NORMAL
- en: In SQL, this is often called the `GROUP``¬†BY` clause of the `SELECT` statement.
    The SQL aggregate functions include `SUM`, `COUNT`, `MAX`, and `MIN`, and often
    many more.
  prefs: []
  type: TYPE_NORMAL
- en: Python offers us several ways to group data before computing a reduction of
    the grouped values. We‚Äôll start by looking at two ways to get simple counts of
    grouped data. Then we‚Äôll look at ways to compute different summaries of grouped
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We‚Äôll use the trip data that we computed in [Chapter¬†4](Chapter_04.xhtml#x1-740004),
    [Working with Collections](Chapter_04.xhtml#x1-740004). This data started as a
    sequence of latitude-longitude waypoints. We restructured it to create legs represented
    by three-tuples of start, end, and distance for each leg. The data looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We‚Äôd like to know the most common distance. Since the data is real-valued, and
    continuous, each distance is a unique value. We need to constrain these values
    from the continuous domain to a discrete set of distances. For example, quantizing
    each leg to the nearest multiple of five nautical miles. This creates bands of
    0 to 5 miles, over 5 to 10 miles, etc. Once we‚Äôve created discrete integer values,
    we can count the number of legs in each of these bands.
  prefs: []
  type: TYPE_NORMAL
- en: 'These quantized distances can be produced with a generator expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This will divide each distance by 5‚Äîdiscarding any fractions‚Äîthen multiply the
    truncated result by 5 to compute a number that represents the distance rounded
    down to the nearest 5 nautical miles.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don‚Äôt use the values assigned to the `start` and `stop` variables. It‚Äôs
    common practice to assign these values to the `_` variable. This can lead to some
    confusion because this can obscure the structure of the triple. It would look
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This approach can be helpful for removing some visual clutter.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Building a mapping with Counter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A mapping like the `collections.Counter` class is a great optimization for
    doing reductions that create counts (or totals) grouped by some value in the collection.
    The following expression creates a mapping from distance to frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The resulting `summary` object is stateful; it can be updated. The expression
    to create the groups, `Counter()`, looks like a function, making it a good fit
    for a design based on functional programming ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we print the `summary.most_common()` value, we‚Äôll see the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The most common distance was about 30 nautical miles. We can also apply functions
    like `min()` and `max()` to find the shortest recorded and longest legs as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that your output may vary slightly from what‚Äôs shown. The results of the
    `most_common()` function are in order of frequency; equal-frequency bins may be
    in any order. These five lengths may not always be in the order shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This slight variability makes testing with the doctest tool a little bit more
    complex. One helpful trick for testing with counters is to use a dictionary to
    validate the results in general; the comparison between actual and expected no
    longer relies on the vagaries of internal hash computations.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Building a mapping by sorting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An alternative to `Counter` is to sort the original collection, and then use
    a recursive loop to identify when each group begins. This involves materializing
    the raw data, performing a sort that could‚Äîat worst‚Äîdo O(nlog n) operations, and
    then doing a reduction to get the sums or counts for each key.
  prefs: []
  type: TYPE_NORMAL
- en: In order to work in a general way with Python objects that can be sorted, we
    need to define the protocol required for sorting. We‚Äôll call the protocol `SupportsRichComparisonT`
    because we can sort any kinds of objects that implement the rich comparison operators,
    `<` and `>`. This isn‚Äôt a particular class of objects; it‚Äôs a protocol that any
    number of classes might implement. We formalize the idea of a protocol that classes
    must support using the `typing.Protocol` type definition. It could be also be
    called an interface that a class must implement. Python‚Äôs flexibility stems from
    having a fairly large number of protocols that many different classes support.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a common algorithm for creating groups from sorted data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The internal `group()` function steps through the sorted sequence of legs.
    If a given item key has already been seen‚Äîit matches the value in `previous`‚Äîthen
    the `counter` variable is incremented. If a given item does not match the previous
    value, then there‚Äôs been a change in value: emit the previous value and the count,
    and begin a new accumulation of counts for the new value.'
  prefs: []
  type: TYPE_NORMAL
- en: The definition of `group()` provides two important type hints. The source data
    is an iterable over some type, shown with the type variable `SupportsRichComparisonT`.
    In this specific case, it‚Äôs pretty clear that the values in use will be of type
    `int`; however, the algorithm will work for any Python type. The resulting iterable
    from the `group()` function will preserve the type of the source data, and this
    is made explicit by using the same type variable, `SupportsRichComparisonT`.
  prefs: []
  type: TYPE_NORMAL
- en: The final line of the `group_sort()` function creates a dictionary from the
    grouped items. This dictionary will be similar to a `Counter` dictionary. The
    primary difference is that a `Counter()` function will have a `most_common()`
    method function, which a default dictionary lacks.
  prefs: []
  type: TYPE_NORMAL
- en: We can also do this with `itertools.groupby()`. We‚Äôll look at this function
    closely in [Chapter¬†8](Chapter_08.xhtml#x1-1700008), [The Itertools Module](Chapter_08.xhtml#x1-1700008).
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3 Grouping or partitioning data by key values
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are no limits to the kinds of reductions we might want to apply to grouped
    data. We might have data with a number of independent and dependent variables.
    We can consider partitioning the data by an independent variable and computing
    summaries such as the maximum, minimum, average, and standard deviation of the
    values in each partition.
  prefs: []
  type: TYPE_NORMAL
- en: The essential trick to doing more sophisticated reductions is to collect all
    of the data values into each group. The `Counter()` function merely collects counts
    of identical items. For deeper analysis, we want to create sequences of the original
    members of the group.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at our trip data, each five-mile bin could contain the entire collection
    of legs of that distance, not merely a count of the legs. We can consider the
    partitioning as a recursion or as a stateful application of `defaultdict(list)`
    objects. We‚Äôll look at the recursive definition of a `groupby()` function, since
    it‚Äôs easy to design.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, the `groupby(C,``¬†key)` computation for an empty collection, `[]`,
    is the empty dictionary, `dict()`. Or, more usefully, the empty `defaultdict(list)`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a non-empty collection, we need to work with item `C[0]`, the head, and
    recursively process sequence `C[1:]`, the tail. We can use slice expressions,
    or we can use the `head,` `*tail``¬†=``¬†C` statement to do this parsing of the
    collection, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If we have a `defaultdict` object named `groups`, we need to use the expression
    `groups[key(head)].append(head)` to include the head element in the `groups` dictionary.
    After this, we need to evaluate the `groupby(tail,``¬†key)` expression to process
    the remaining elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The interior function `group_into()` handles the essential recursive definition.
    An empty value for `collection` returns the provided dictionary, `group_dict`.
    A non-empty collection is partitioned into a head and tail. The head is used to
    update the `group_dict` dictionary. The tail is then used, recursively, to update
    the dictionary with all remaining elements.
  prefs: []
  type: TYPE_NORMAL
- en: The type hints make an explicit distinction between the type of the source objects
    `SeqItemT` and the type of the key `ItemKeyT`. The function provided as the `key`
    parameter must be a callable that returns a value of the key type `ItemKeyT`,
    given an object of the source type `SeqItemT`. In many of the examples, a function
    to extract the distance from a `Leg` object will be be shown. This is a `Callable[[SeqItemT],``¬†ItemKeyT]`
    where the source type `SeqItemT` is the `Leg` object and the key type `ItemKeyT`
    is the float value.
  prefs: []
  type: TYPE_NORMAL
- en: '`bound=Hashable` is an additional constraint. This defines an ‚Äùupper bound‚Äù
    on the possible types, alerting mypy that any type that could be assigned to this
    type variable must implement the protocol for `Hashable`. The essential, immutable
    Python types of numbers, strings, and tuples all meet this bound. A mutable object
    like a dictionary, set, or list, will not meet the upper bound, leading to warnings
    from mypy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can‚Äôt easily use Python‚Äôs default values to collapse this into a single
    function. We explicitly cannot use the following incorrect command snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If we try this, all uses of the `group_by()` function share one common `defaultdict(list)`
    object. This does not work because Python builds the default value just once.
    Mutable objects as default values rarely do what we want. The common practice
    is to provide a `None` value, and use an explicit `if` statement to create each
    unique, empty instance of `defaultdict(list)` as needed. We‚Äôve shown how to use
    a wrapper function definition to avoid the `if` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can group the data by distance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We‚Äôve defined a reusable lambda that puts our distances into bins, each of which
    is 5 nautical miles in size. We then grouped the data using the provided lambda.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine the binned data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is what the output looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Having looked at a recursive definition, we can turn to looking at making a
    tail-call optimization to build a group-by algorithm using iteration. This will
    work with larger collections of data, because it can exceed the internal stack
    size limitation.
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôll start with doing tail-call optimization on the `group_into()` function.
    We‚Äôll rename this to `partition()` because partitioning is another way of looking
    at grouping.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `partition()` function can be written as an iteration as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: When doing the tail-call optimization, the essential line of the code in the
    imperative version will match the recursive definition. We‚Äôve put a comment under
    the changed line to emphasize the rewrite is intended to have the same outcome.
    The rest of the structure represents the tail-call optimization we‚Äôve adopted
    as a common way to work around the Python limitations.
  prefs: []
  type: TYPE_NORMAL
- en: The type hints emphasize the distinction between the source type `SeqT` and
    the key type `KeyT`. The source data can be anything, but the keys are limited
    to types that have proper hash values.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.4 Writing more general group-by reductions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once we have partitioned the raw data, we can compute various kinds of reductions
    on the data elements in each partition. We might, for example, want the northernmost
    point for the start of each leg in the distance bins.
  prefs: []
  type: TYPE_NORMAL
- en: 'We‚Äôll introduce some helper functions to decompose the tuple as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Each of these helper functions expects a tuple object to be provided using the
    `*` operator to map each element of the tuple to a separate parameter of the lambda.
    Once the tuple is expanded into the `s`, `e`, and `p` parameters, it‚Äôs reasonably
    obvious to return the proper parameter by name. It‚Äôs much clearer than trying
    to interpret the `tuple_arg[2]` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is how we use these helper functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Our initial `point` object is a nested three tuple with (0)‚Äîa starting position,
    (1)‚Äîthe ending position, and (2)‚Äîthe distance. We extracted various fields using
    our helper functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given these helpers, we can locate the northernmost starting position for the
    legs in each bin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The data that we grouped by distance included each leg of the given distance.
    We supplied all of the legs in each bin to the `max()` function. The `key` function
    we provided to the `max()` function extracted just the latitude of the starting
    point of the leg.
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us a short list of the northernmost legs of each distance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 6.3.5 Writing higher-order reductions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We‚Äôll look at an example of a higher-order reduction algorithm here. This will
    introduce a rather complex topic. The simplest kind of reduction develops a single
    value from a collection of values. Python has a number of built-in reductions,
    including `any()`, `all()`, `max()`, `min()`, `sum()`, and `len()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we noted in [Chapter¬†4](Chapter_04.xhtml#x1-740004), [Working with Collections](Chapter_04.xhtml#x1-740004),
    we can do a great deal of statistical calculation if we start with a few reductions
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to define mean, standard deviation, normalized values, correction,
    and even least-squares linear regression, building on these base reduction functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last of our reductions, `sum_x2()`, shows how we can apply existing reductions
    to create higher-order functions. We might change our approach to be more like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We‚Äôve added a function, `function()`, as a parameter; the function can transform
    the data. This overall function, `sum_f()`, computes the sum of the transformed
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can apply this function in three different ways to compute the three
    essential sums as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We‚Äôve plugged in a small lambda to compute ‚àë [x‚ààX]x‚Å∞ = ‚àë [x‚ààX]1, which is the
    count, ‚àë [x‚ààX]x¬π = ‚àë [x‚ààX]x, the sum, and ‚àë [x‚ààX]x¬≤, the sum of the squares, which
    we can use to compute standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common extension to this includes a filter to reject raw data that is unknown
    or unsuitable in some way. We might use the following function to reject bad data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function definition for computing a mean will reject `None` values
    in a simple way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This shows how we can provide two distinct combinations of lambdas to our `sum_filter_f()`
    function. The filter argument is a lambda that rejects `None` values; we‚Äôve called
    it `valid` to emphasize its meaning. The function argument is a lambda that implements
    a count or a sum operation. We can easily add a lambda to compute a sum of squares.
  prefs: []
  type: TYPE_NORMAL
- en: The reuse of a common `valid` rule assures that the various computations are
    all identical in applying any filters to the source data. This can be combined
    with a user-selected filter criteria to provide a tidy plug-in to compute a number
    of statistics related to a user‚Äôs requested subset of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.6 Writing file parsers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can often consider a file parser to be a kind of reduction. Many languages
    have two levels of definition: the lower-level tokens in the language and the
    higher-level structures built from those tokens. When looking at an XML file,
    the tags, tag names, and attribute names form this lower-level syntax; the structures
    which are described by XML form a higher-level syntax.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The lower-level lexical scanning is a kind of reduction that takes individual
    characters and groups them into tokens. This fits well with Python‚Äôs generator
    function design pattern. We can often write functions that look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: For well-known file formats, we‚Äôll use existing file parsers. For data in CSV,
    JSON, XML, or TOML format, we don‚Äôt need to write file parsers. Most of these
    modules have a `load()` method that produces useful Python objects.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, we‚Äôll need to combine the results of this parsing into higher-level
    objects, useful for our specific application. While the CSV parser provides individual
    rows, these might need to be used to create `NamedTuple` instances, or perhaps
    some other class of immutable Python objects. Our examples of trip data, starting
    in [Chapter¬†4](Chapter_04.xhtml#x1-740004), [Working with Collections](Chapter_04.xhtml#x1-740004),
    are combined into higher-level objects, legs of a journey, by an algorithm that
    combines waypoints into pairs. When we introduce more complex decision-making,
    we make a transition from restructuring into parsing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to provide useful waypoints in the first place, we needed to parse
    a source file. In these examples, the input was a KML file; KML is an XML representation
    of geographic information. The essential features of the parser look similar to
    the following definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The bulk of the `row_iter_kml()` function is the XML parsing that allows us
    to use the `doc.findall()` function to iterate through the `<ns0:coordinates>`
    tags in the document. We‚Äôve used a function named `comma_split()` to parse the
    text of this tag into a three-tuple of values.
  prefs: []
  type: TYPE_NORMAL
- en: The `cast()` function is only present to provide evidence to mypy that the value
    of `coordinates.text` is a `str` object. The default definition of the text attribute
    is `Union[str,` `bytes]`; in this application, the data will be `str` exclusively.
    The `cast()` function doesn‚Äôt do any runtime processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function focused on working with the normalized XML structure. The document
    is close to the database designer‚Äôs definitions of first normal form: each attribute
    is atomic (a single value), and each row in the XML data has the same columns
    with data of a consistent type. The data values aren‚Äôt fully atomic, however:
    we have to split the points on the , to separate longitude, latitude, and altitude
    into atomic string values. However, the text value for these XML tags is internally
    consistent, making it a close fit with first normal form.'
  prefs: []
  type: TYPE_NORMAL
- en: A large volume of data‚ÄîXML tags, attributes, and other punctuation‚Äîis reduced
    to a somewhat smaller volume, including just floating-point latitude and longitude
    values. For this reason, we can think of parsers as a kind of reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We‚Äôll need a higher-level set of conversions to map the tuples of text into
    floating-point numbers. Also, we‚Äôd like to discard altitude, and reorder longitude
    and latitude. This will produce the application-specific tuple we need. We can
    use functions as follows for this conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The essential tool is the `float_lat_lon()` function. This is a higher-order
    function that returns a generator expression. The generator uses the `map()` function
    to apply the `float()` function conversion to the results of the `pick_lat_lon()`
    function, and the `*row` argument to assign each member of the row tuple to a
    different parameter of the `pick_lat_lon()` function. This only works when each
    row is a three-tuple. The `pick_lat_lon()` function then returns a two-tuple of
    the selected items in the required order.
  prefs: []
  type: TYPE_NORMAL
- en: 'The source includes XML that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this parser as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This will build a tuple-of-tuples representation of each waypoint along the
    path in the original KML file. The result will be a flat sequence of pairs that
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The `float_lat_lon()` function uses a low-level XML parser to extract rows of
    text data from the original representation. It uses a higher-level parser to transform
    the text items into more useful tuples of floating-point values suitable for the
    target application.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing CSV files
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In [Chapter¬†3](Chapter_03.xhtml#x1-510003), [Functions, Iterators, and Generators](Chapter_03.xhtml#x1-510003),
    we saw another example where we parsed a CSV file that was not in a normalized
    form: we had to discard header rows to make it useful. To do this, we used a function
    that extracted the header and returned an iterator over the remaining rows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The columns are separated by tab characters. Plus, there are three rows of headers
    that we can discard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs another version of that CSV-based parser. We‚Äôve broken it into three
    functions. The first, `row_iter_csv()` function, returns the iterator over the
    rows in a tab-delimited file. The function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This is a small wrapper around the CSV parsing process. When we look back at
    the previous parsers for XML and plain text, this was the kind of thing that was
    missing from those parsers. Producing an iterable over row tuples can be a common
    feature of parsers for normalized data.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a row of tuples, we can pass rows that contain usable data and
    reject rows that contain other metadata, such as titles and column names. We‚Äôll
    introduce a helper function that we can use to do some of the parsing, plus a
    `filter()` function to validate a row of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This function handles the conversion of a single string to float values, converting
    bad data to a `None` value. The type hint of `float``¬†|``¬†None` expresses the
    idea of having a value of the given type or having a value of the same type as
    `None`. This can also be stated as `Union[float,``¬†None]` to show how the result
    is a union of different alternative types.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can embed the `float_none()` function in a mapping so that we convert all
    columns of a row to a float or `None` value. A lambda for this looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Two type hints are used to make the definition of the `float_row()` function
    explicit. The `R_Float` hint defines the floating-point version of a row of data
    that may include `None` values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a row-level validator based on the use of the `all()` function
    to ensure that all values are `float` (or none of the values are `None`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This lambda is a kind of reduction, transforming a row of floating-point values
    to a Boolean value if all values are not ‚Äùfalsy‚Äù (that is, neither `None` nor
    zero) and there are exactly eight values.
  prefs: []
  type: TYPE_NORMAL
- en: The simplistic `all_numeric()` function conflates zero and `None`. A more sophisticated
    test would rely on something such as `not``¬†any(item``¬†is``¬†None``¬†for``¬†item``¬†in``¬†row)`.
    The rewrite is left as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: The essential design is to create row-based elements that can be combined to
    create more complete algorithms for parsing an input file. The foundational functions
    iterate over tuples of text. These are combined to convert and validate the converted
    data. For the cases where files are either in first normal form (all rows are
    the same) or a simple validator can reject the extraneous rows, this design pattern
    works out nicely.
  prefs: []
  type: TYPE_NORMAL
- en: All parsing problems aren‚Äôt quite this simple, however. Some files have important
    data in header or trailer rows that must be preserved, even though it doesn‚Äôt
    match the format of the rest of the file. These non-normalized files will require
    a more sophisticated parser design.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing plain text files with headers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In [Chapter¬†3](Chapter_03.xhtml#x1-510003), [Functions, Iterators, and Generators](Chapter_03.xhtml#x1-510003),
    the `Crayola.GPL` file was presented without showing the parser. This file looks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We can parse a text file using regular expressions. We need to use a filter
    to read (and parse) header rows. We also want to return an iterable sequence of
    data rows. This rather complex two-part parsing is based entirely on the two-part‚Äîhead
    and tail‚Äîfile structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a low-level parser that handles both the four lines of the header
    and the long tail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The `Head_Body` type definition summarizes the overall goal of the row iterator.
    The result is a two-tuple. The first item is a two-tuple with details from the
    file header. The second item is an iterator that provides the text items for a
    color definition. This `Head_Body` type hint is used in two places in this function
    definition.
  prefs: []
  type: TYPE_NORMAL
- en: The `header_pat` regular expression parses all four lines of the header. There
    are instances of `()` in the expression to extract the name and column information
    from the header.
  prefs: []
  type: TYPE_NORMAL
- en: There are two internal functions for parsing different parts of the file. The
    `read_head()` function parses the header lines and returns interesting text and
    a `TextIO` object that can be used for the rest of the parsing. It does this by
    reading four lines and merging them into a single long string. This is then parsed
    with the `header_pat` regular expression.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of returning the iterator from one function to be used in another function
    is a pattern for passing an explicitly stateful object from one function to another.
    It seems helpful to make sure all of the arguments for the `read_tail()` function
    are the results from the `read_head()` function.
  prefs: []
  type: TYPE_NORMAL
- en: The `read_tail()` function parses the iterator over the remaining lines. These
    lines are merely split on spaces, since that fits the description of the GPL file
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, visit the following link: [https://code.google.com/p/grafx2/issues/detail?id=518](https://code.google.com/p/grafx2/issues/detail?id=518).'
  prefs: []
  type: TYPE_NORMAL
- en: Once we‚Äôve transformed each line of the file into a canonical tuple-of-strings
    format, we can apply the higher level of parsing to this data. This involves conversion
    and (if necessary) validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a higher-level parser command snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This function will work with the output of the lower-level `row_iter_gpl()`
    parser: it requires the headers and the iterator over individual rows. This function
    will use the multiple assignment feature of the `for` clause in the generator
    to separate the color numbers and the remaining words into four variables, `r`,
    `g`, `b`, and `name`. The use of the `*name` parameter ensures that all remaining
    values will be assigned to the `name` variable as a tuple. The `"``¬†".join(name)`
    expression then concatenates the words into a single space-separated string.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is how we can use this two-tier parser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We‚Äôve applied the higher-level parser to the results of the lower-level parser.
    This will return the headers and a tuple built from the sequence of `Color` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we‚Äôve looked at two significant functional programming topics.
    We‚Äôve looked at recursions in some detail. Many functional programming language
    compilers will optimize a recursive function to transform a call in the tail of
    the function to a loop. This is sometimes called tail recursion elimination. More
    commonly, it‚Äôs known as tail-call optimization. In Python, we must do the tail-call
    optimization manually by using an explicit `for` statement, replacing a purely
    functional recursion.
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôve also looked at reduction algorithms, including `sum()`, `count()`, `max()`,
    and `min()` functions. We looked at the `collections.Counter()` function and related
    `groupby()` reductions.
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôve also looked at how parsing (and lexical scanning) are similar to reductions
    since they transform sequences of tokens (or sequences of characters) into higher-order
    collections with more complex properties. We‚Äôve examined a design pattern that
    decomposes parsing into a lower level and tries to produce tuples of raw strings,
    and a higher level that creates more useful application objects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we‚Äôll look at some techniques appropriate to working with
    named tuples and other immutable data structures. We‚Äôll look at techniques that
    make stateful objects unnecessary. While stateful objects aren‚Äôt purely functional,
    the idea of a class hierarchy can be used to package related method definitions.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Exercises
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter‚Äôs exercises are based on code available from Packt Publishing on
    GitHub. See [https://github.com/PacktPublishing/Functional-Python-Programming-3rd-Edition](https://github.com/PacktPublishing/Functional-Python-Programming-3rd-Edition).
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the reader will notice that the code provided on GitHub includes
    partial solutions to some of the exercises. These serve as hints, allowing the
    reader to explore alternative solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, exercises will need unit test cases to confirm they actually
    solve the problem. These are often identical to the unit test cases already provided
    in the GitHub repository. The reader should replace the book‚Äôs example function
    name with their own solution to confirm that it works.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1 Multiple recursion and caching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Handling difficult tail-call optimization](#x1-1300003), we looked at a
    naive definition of a function to compute Fibonacci numbers, the `fib()` function.
    The `functools.cache` decorator can have a profound impact on the performance
    of this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Implement both versions and describe the impact of caching on the time required
    to compute large Fibonacci numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2 Refactor the all_print() function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Tail-call optimization using deques](#x1-1350001), we showed a function
    that used a `collections.deque` to visit all nodes in a directory tree, summing
    the value for each node that is a proper file. This can be done with a list as
    well as a `deque`, with some minor code changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function embedded a specific computation. This computation (finding all
    occurrences of ‚Äùprint‚Äù) really should have been a separate function. The body
    of the `all_print()` function should be refactored into two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: A generic directory traverse that applies a function to each text file with
    the expected suffix and sums the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that counts instances of ‚Äùprint‚Äù in a given Python file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.5.3 Parsing CSV files
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: See the [Parsing CSV files](#x1-1430006) section, earlier in this chapter. In
    that example, the simplistic `all_numeric()` function conflates zero and `None`.
  prefs: []
  type: TYPE_NORMAL
- en: Create a test case for this function that will show that it does not handle
    zero correctly, treating it as `None`. Once the test case is defined, rewrite
    the `all_numeric()` function to distinguish between zero and `None`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it‚Äôs common practice in Python to use the `is` operator when comparing
    with `None`. This specifically avoids some subtle problems that can arise when
    a class has an implementation of `__eq__()` that doesn‚Äôt handle `None` as a properly
    distinct object.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.4 Classification of state, Part III
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: See [Chapter¬†5](Chapter_05.xhtml#x1-1000005), [Higher-Order Functions](Chapter_05.xhtml#x1-1000005),
    the [Classification of state](Chapter_05.xhtml#x1-1220001) exercise.
  prefs: []
  type: TYPE_NORMAL
- en: There‚Äôs a third way to consume status details and summarize them.
  prefs: []
  type: TYPE_NORMAL
- en: Write a reduce computation. This starts with an initial state of Running. As
    each service‚Äôs three-tuple is folded into the result, there is a comparison between
    the state and the three-tuple. If the three-tuple has a non-responsive service,
    the state advances to Stopped. If the three-tuple has a slow or not working service,
    the state advances to Degraded. If no problems are found, the initial value becomes
    the final health of the overall system.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to provide a `status_add(previous,``¬†this_service)` function. This
    can be used in the context of `status``¬†=``¬†reduce(status_add,``¬†service_status_sequence,``¬†"Running")`
    to compute the current status of the sequence of services.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.5 Diesel engine data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A diesel engine has had some repairs that raised doubts about the accuracy of
    the tachometer. After some heroic effort, the following table of data was collected
    showing the observed reading on the engine‚Äôs tachometer, and the actual RPMs measured
    with an optical device on the engine.
  prefs: []
  type: TYPE_NORMAL
- en: '| Sample | Tach | Engine |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1000 | 883 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1500 | 1242 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1500 | 1217 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1600 | 1306 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1750 | 1534 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 2000 | 1805 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 2000 | 1720 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: If needed, create a CSV file with the data. If you have access to the GitHub
    repository for this book, this is available in the `engine.csv` file.
  prefs: []
  type: TYPE_NORMAL
- en: Create a `NamedTuple` for each sample and write some functions to acquire this
    data in a useful form. Once the data is available, see the [Using sums and counts
    for statistics](Chapter_04.xhtml#x1-850001) section of [Chapter¬†4](Chapter_04.xhtml#x1-740004),
    [Working with Collections](Chapter_04.xhtml#x1-740004), for a definition of a
    correlation function.
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to apply this correlation function to the engine and tach values
    to see if the values correlate. If they do, it suggests that the engine‚Äôs instruments
    can be recalibrated. If they don‚Äôt correlate, something else is wrong with the
    engine.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the [Chapter¬†4](Chapter_04.xhtml#x1-740004), [Working with Collections](Chapter_04.xhtml#x1-740004),
    correlation example may have assumptions about data types that don‚Äôt necessarily
    apply to the `NamedTuple` defined earlier. If necessary, rewrite the type hints
    or your `NamedTuple` definition. Note that it can be difficult to write perfectly
    generic type hints, and it often takes a bit of work to resolve the differences.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community Discord space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Join our Python Discord workspace to discuss and know more about the book:
    [https://packt.link/dHrHU](https://packt.link/dHrHU)'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
