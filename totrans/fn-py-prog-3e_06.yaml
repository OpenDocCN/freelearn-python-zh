- en: '6'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recursions and Reductions
  prefs: []
  type: TYPE_NORMAL
- en: Many functional programming language compilers will optimize a recursive function
    to transform a recursive call in the tail of the function to an iteration. This
    tail-call optimization will dramatically improve performance. Python doesn’t do
    this automatic tail-call optimization. One consequence is pure recursion suffers
    from limitations. Lacking an automated optimization, we need to do the tail-call
    optimization manually. This means rewriting recursion to use an explicit iteration.
    There are two common ways to do this, and we’ll consider them both in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In previous chapters, we’ve looked at several related kinds of processing design
    patterns; some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Mapping and filtering, which create collections from collections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reductions that create a scalar value from a collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distinction is exemplified by functions such as `map()` and `filter()` that
    accomplish the first kind of collection processing. There are some more specialized
    reduction functions, which include `min()`, `max()`, `len()`, and `sum()`. There’s
    a general-purpose reduction function as well, `functools.reduce()`.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also consider creating a `collections.Counter()` object as a kind of reduction
    operator. It doesn’t produce a single scalar value per se, but it does create
    a new organization of the data that eliminates some of the original structure.
    At heart, it’s a kind of count-group-by operation that has more in common with
    a counting reduction than with a mapping.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll look at reduction functions in more detail. From a purely
    functional perspective, a reduction can be defined recursively. The tail-call
    optimization techniques available in Python apply elegantly to reductions.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll review a number of built-in reduction algorithms including `sum()`, `count()`,
    `max()`, and `min()`. We’ll look at the `collections.Counter()` creation and related
    `itertools.groupby()` reductions. We’ll also look at how parsing (and lexical
    scanning) are proper reductions since they transform sequences of tokens (or sequences
    of characters) into higher-order collections with more complex properties.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Simple numerical recursions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can consider all numeric operations to be defined by recursions. For more
    details, read about the Peano axioms that define the essential features of numbers
    at [https://www.britannica.com/science/Peano-axioms](https://www.britannica.com/science/Peano-axioms).
  prefs: []
  type: TYPE_NORMAL
- en: From these axioms, we can see that addition is defined recursively using more
    primitive notions of the next number, or the successor of a number n, S(n).
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the presentation, we’ll assume that we can define a predecessor
    function, P(n), such that n = S(P(n)) = P(S(n)), as long as n≠0\. This formalizes
    the idea that a number is the successor of the number’s predecessor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Addition between two natural numbers could be defined recursively as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ add(a,b) = b if a = 0 |( add(P(a),S(b)) if a ⁄= 0 ](img/file48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we use the more typical notations of n + 1 and n− 1 instead of S(n) and P(n),
    we can more easily see how the rule add(a,b) = add(a − 1,b + 1) when a≠0 works.
  prefs: []
  type: TYPE_NORMAL
- en: 'This translates neatly into Python, as shown in the following function definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We’ve rearranged the abstract mathematical notation into concrete Python.
  prefs: []
  type: TYPE_NORMAL
- en: There’s no good reason to provide our own functions in Python to do simple addition.
    We rely on Python’s underlying implementation to properly handle arithmetic of
    various kinds. Our point here is that fundamental scalar arithmetic can be defined
    recursively, and the definition translates to Python.
  prefs: []
  type: TYPE_NORMAL
- en: This suggests that more complicated operations, defined recursively, can also
    be translated to Python. The translation can be manually optimized to create working
    code that matches the abstract definitions, reducing questions about possible
    bugs in the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'A recursive definition must include at least two cases: a non-recursive (or
    base) case where the value of the function is defined directly, and the recursive
    case where the value of the function is computed from a recursive evaluation of
    the function with different argument values.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to be sure the recursion will terminate, it’s important to see how
    the recursive case computes values that approach the defined non-recursive base
    case. Pragmatically, there are often constraints on the argument values that we’ve
    omitted from the functions here. For example, the `add()` function in the preceding
    command snippet could be expanded to include `assert`` a>=0`` and`` b>=0` to establish
    two necessary constraints on the input values.
  prefs: []
  type: TYPE_NORMAL
- en: Without these constraints, starting with `a` equal to -1 won’t approach the
    non-recursive case of `a`` ==`` 0` as we keep subtracting 1 from `a`.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Implementing manual tail-call optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For some functions, the recursive definition is the most succinct and expressive.
    A common example is the `factorial()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see how this is rewritten as a simple recursive function in Python from
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( | { 1 if n = 0 n! = |( n × (n− 1)! if n ⁄= 0 ](img/file49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding formula can be implemented in Python by using the following function
    definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This implementation has the advantage of simplicity. The recursion limits in
    Python artificially constrain us; we can’t do anything larger than about `fact(997)`.
    The value of 1000! has 2,568 digits and generally exceeds our floating-point capacity;
    on some systems the floating-point limit is near 10^(300). Pragmatically, it’s
    common to switch to a log gamma function instead of working with immense numbers.
  prefs: []
  type: TYPE_NORMAL
- en: See [https://functions.wolfram.com/GammaBetaErf/LogGamma/introductions/Gammas/ShowAll.html](https://functions.wolfram.com/GammaBetaErf/LogGamma/introductions/Gammas/ShowAll.html)
    for more on log gamma functions.
  prefs: []
  type: TYPE_NORMAL
- en: We can expand Python’s call stack limit to stretch this to the limits of memory.
    It’s better, however, to manually optimize these kinds of functions to eliminate
    the recursion.
  prefs: []
  type: TYPE_NORMAL
- en: This function demonstrates a typical tail recursion. The last expression in
    the function is a call to the function with a new argument value. An optimizing
    compiler can replace the function call stack management with a loop that executes
    very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the function involves an incremental change from n to n − 1\.
    This means that we’re generating a sequence of numbers and then doing a reduction
    to compute their product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stepping outside purely functional processing, we can define an imperative
    `facti()` calculation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This version of the factorial function will compute values beyond 1000! (2000!,
    for example, has 5,736 digits). This example isn’t purely functional. We’ve optimized
    the tail recursion into a stateful `for` statement depending on the `i` variable
    to maintain the state of the computation.
  prefs: []
  type: TYPE_NORMAL
- en: In general, we’re obliged to do this in Python because Python can’t automatically
    do the tail-call optimization. There are situations, however, where this kind
    of optimization isn’t actually helpful. We’ll look at a few of them.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Leaving recursion in place
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In some cases, the recursive definition is actually optimal. Some recursions
    involve a divide and conquer strategy that minimizes the work. One example of
    this is the algorithm for doing exponentiation by squaring. This works for computing
    values that have a positive integer exponent, like 2^(64). We can state it formally
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| ||| 1 if n = 0 n { (n−1) a = || a × a if a is odd ||( n2 2 (a ) if a
    is even ](img/file50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We’ve broken the process into three cases, easily written in Python as a recursion.
    Look at the following function definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For odd numbers, the `fastexp()` method is defined recursively. The exponent
    `n` is reduced by 1\. A simple tail-recursion optimization would work for this
    case. It would not work for the even case, however.
  prefs: []
  type: TYPE_NORMAL
- en: For even numbers, the `fastexp()` recursion uses `n`` //`` 2`, chopping the
    problem into half of its original size. Since the problem size is reduced by a
    factor of 2, this case results in a significant speed-up of the processing.
  prefs: []
  type: TYPE_NORMAL
- en: We can’t trivially reframe this kind of function into a tail-call optimization
    loop. Since it’s already optimal, we don’t really need to optimize it further.
    The recursion limit in Python would impose the constraint of n ≤ 2^(1000), a generous
    upper bound.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Handling difficult tail-call optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can look at the definition of Fibonacci numbers recursively. The following
    is one widely used definition for the n^(th) Fibonacci number, F[n]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| ||| 0 if n = 0 { Fn = | 1 if n = 1 |||( Fn− 1 + Fn− 2 if n ≥ 2 ](img/file51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A given Fibonacci number, F[n], is defined as the sum of the previous two numbers,
    F[n−1] + F[n−2]. This is an example of multiple recursion: it can’t be trivially
    optimized as a simple tail recursion. However, if we don’t optimize it to a tail
    recursion, we’ll find it to be too slow to be useful.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a naïve implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This suffers from a terrible multiple recursion problem. When computing the
    `fib(n)` value, we must compute the `fib(n-1)` and `fib(n-2)` values. The computation
    of the `fib(n-1)` value involves a duplicate calculation of the `fib(n-2)` value.
    The two recursive uses of the `fib()` function will more than duplicate the amount
    of computation being done.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the left-to-right Python evaluation rules, we can evaluate values
    up to about `fib(1000)`. However, we have to be patient. Very patient. (Trying
    to find the actual upper bound with the default stack size means waiting a long
    time before the `RecursionError` is raised.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is one alternative, which restates the entire algorithm to use
    stateful variables instead of a simple recursion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Our stateful version of this function counts up from 0, unlike the recursion,
    which counts down from the initial value of `n`. This version is considerably
    faster than the recursive version.
  prefs: []
  type: TYPE_NORMAL
- en: What’s important here is that we couldn’t trivially optimize the `fib()` function
    recursion with an obvious rewrite. In order to replace the recursion with an imperative
    version, we had to look closely at the algorithm to determine how many stateful
    intermediate variables were required.
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise for the reader, try using the `@cache` decorator from the `functools`
    module. What impact does this have?
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.4 Processing collections through recursion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When working with a collection, we can also define the processing recursively.
    We can, for example, define the `map()` function recursively. The formalism could
    be stated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ [] if len(C ) = 0 map (f,C ) = | ( map(f,C [:−1]) + [f (C −1)] if len(C
    ) > 0 ](img/file52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We’ve defined the mapping of a function, f, to an empty collection as an empty
    sequence, `[]`. We’ve also specified that applying a function to a collection
    can be defined recursively with a three-step expression. First, recursively perform
    the mapping of the function to all of the collection except the last element,
    creating a sequence object. Then apply the function to the last element. Finally,
    append the last calculation to the previously built sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a purely recursive function version of this `map()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The value of the `mapr(f,[])` method is defined to be an empty list object.
    The value of the `mapr()` function with a non-empty list will apply the function
    to the last element in the list and append this to the list built recursively
    from the `mapr()` function applied to the head of the list.
  prefs: []
  type: TYPE_NORMAL
- en: We have to emphasize that this `mapr()` function actually creates a list object.
    The built-in `map()` function is an iterator; it doesn’t create a list object.
    It yields the result values as they are computed. Also, the work is done in right-to-left
    order, which is not the way Python normally works. This is only observable when
    using a function that has side effects, something we’d like to avoid doing.
  prefs: []
  type: TYPE_NORMAL
- en: While this is an elegant formalism, it still lacks the tail-call optimization
    required. An optimization will allow us to exceed the default recursion limit
    of 1,000 and also performs much more quickly than this naïve recursion.
  prefs: []
  type: TYPE_NORMAL
- en: The use of `Callable[[Any],`` Any]` is a weak type hint. To be more clear, it
    can help to define a domain type variable and a range type variable. We’ll include
    this detail in the optimized example.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.5 Tail-call optimization for collections
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We have two general ways to handle collections: we can use a higher-order function
    that returns a generator expression, or we can create a function that uses a `for`
    statement to process each item in a collection. These two patterns are very similar.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a higher-order function that behaves like the built-in `map()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We’ve returned a generator expression that produces the required mapping. This
    uses the explicit `for` in the generator expression as a kind of tail-call optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The source of data, `C`, has a type hint of `Iterable[DomT]` to emphasize that
    some type, `DomT`, will form the domain for the mapping. The transformation function
    has a hint of `Callable[[DomT],`` RngT]` to make it clear that it transforms from
    some domain type to a range type. The function `float()`, for example, can transform
    values from the string domain to the float range. The result has the hint of `Iterator[RngT]`
    to show that it iterates over the range type, `RngT`; the result type of the callable
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a generator function with the same signature and result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This uses a complete `for` statement for the tail-call optimization. The results
    are identical. This version is slightly slower because it involves multiple statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In both cases, the result is an iterator over the results. We must do something
    else to materialize a sequence object from an iterable source. For example, here
    is the `list()` function being used to create a sequence from the iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For performance and scalability, this kind of tail-call optimization is required
    in Python programs. It makes the code less than purely functional. However, the
    benefit far outweighs the lack of purity. In order to reap the benefits of succinct
    and expressive functional design, it is helpful to treat these less-than-pure
    functions as if they were proper recursions.
  prefs: []
  type: TYPE_NORMAL
- en: What this means, pragmatically, is that we must avoid cluttering up a collection
    processing function with additional stateful processing. The central tenets of
    functional programming are still valid even if some elements of our programs are
    less than purely functional.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.6 Using the assignment (sometimes called the ”walrus”) operator in recursions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In some cases, recursions involve conditional processing that can be optimized
    using the ”walrus” or assignment operator, `:=`. The use of assignment means that
    we’re introducing stateful variables. If we’re careful of the scope of those variables,
    the possibility of terribly complex algorithms is reduced.
  prefs: []
  type: TYPE_NORMAL
- en: 'We reviewed the `fast_exp()` function shown below in the [Leaving recursion
    in place](#x1-1290002) section. This function used three separate cases to implement
    a divide and conquer strategy. In the case of raising a number, `a`, to an even
    power, we can use t = a^(![n 2](img/file53.jpg)) to compute t × t = a^n:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This uses the `:=` walrus operator to compute a partial answer, `fastexp_w(a,`` q)`,
    and save it into a temporary variable, `t`. This is used later in the same statement
    to compute `t`` *`` t`.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, when we perform tail-call optimization on a recursion, the
    body of the `for` statement will have ordinary assignment statements. It isn’t
    often necessary to exploit the walrus operator.
  prefs: []
  type: TYPE_NORMAL
- en: The assignment operator is often used in situations like regular expression
    matching, where we want to save the match object as well as make a decision. It’s
    very common to see `if`` (match`` :=`` pattern.match(text)):` as a way to both
    attempt a regular expression match, save the resulting match object, and confirm
    it’s not a `None` object.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Reductions and folding a collection from many items to one
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can consider the `sum()` function to have the following kind of definition.
    We could say that the sum of a collection is 0 for an empty collection. For a
    non-empty collection, the sum is the first element plus the sum of the remaining
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| { 0 if n = 0 sum ([c0,c1,c2,...,cn]) = | ( c0 + sum ([c1,c2,...,cn])
    if n > 0 ](img/file54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use a slightly simplified notation called the Bird-Meertens Formalism.
    This uses ⊕∕[c[0],c[1],...c[n]] to show how some arbitrary binary operator, ⊕,
    can be applied to a sequence of values. It’s used as follows to summarize a recursive
    definition into something a little easier to work with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![sum ([c0,c1,c2,...,cn]) = + ∕[c0,c1,c2,...,cn] = 0+ c0 + c1 + ...+ cn ](img/file55.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We’ve effectively folded the + operator between each item of the sequence. Implicitly,
    the processing will be done left to right. This could be called a ”fold left”
    way of reducing a collection to a single value. We could also imagine grouping
    the operators from right to left, calling this a ”fold right.” While some compiled
    languages will perform this optimization, Python works strictly from left to right
    when given a sequence of similar precedence operators.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, a product function can be defined recursively as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This is a tiny rewrite from a mathematical notation to Python. However, it is
    less than optimal because all of the slices will create a large number of intermediate
    list objects. It’s also limited to only working with explicit collections; it
    can’t work easily with iterable objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can revise this slightly to work with an iterable, which avoids creating
    any intermediate collection objects. The following is a properly recursive product
    function that works with any iterator as a source of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This doesn’t work with iterable collections. We can’t interrogate an iterator
    with the `len()` function to see how many elements it has. All we can do is attempt
    to extract the head of the iterator. If there are no items in the iterator, then
    any attempt to get the head will raise the `StopIteration` exception. If there
    is an item, then we can multiply this item by the product of the remaining items
    in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we must explicitly create an iterator from a materialized sequence
    object, using the `iter()` function. In other contexts, we might have an iterable
    result that we can use. Following is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This recursive definition does not rely on explicit state or other imperative
    features of Python. While it’s more purely functional, it is still limited to
    working with collections of under 1,000 items. (While we can extend the stack
    size, it’s far better to optimize this properly.) Pragmatically, we can use the
    following kind of imperative structure for reduction functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This avoids any recursion limits. It includes the required tail-call optimization.
    Furthermore, this will work equally well with any iterable. This means a `Sequence`
    object, or an iterator.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Tail-call optimization using deques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The heart of recursion is a stack of function calls. Evaluating `fact(5)`, for
    example, is `5*fact(4)`. The value of `fact(4)` is `5*fact(3)`. There is a stack
    of pending computations until `fact(0)` has a value of 1\. Then the stack of computations
    is completed, revealing the final result.
  prefs: []
  type: TYPE_NORMAL
- en: Python manages the stack of calls for us. It imposes an arbitrary default limit
    of 1,000 calls on the stack, to prevent a program with a bug in the recursion
    from running forever.
  prefs: []
  type: TYPE_NORMAL
- en: We can manage the stack manually, also. This gives us another way to optimize
    recursions. We can—explicitly—create a stack of pending work. We can then do a
    final summarization of the pending work, emptying the items from the stack.
  prefs: []
  type: TYPE_NORMAL
- en: For something as simple as computing a factorial value, the stacking and unstacking
    can seem like needless overhead. For more complex applications, like examining
    the hierarchical file system, it seems more appropriate to mix processing files
    with putting directories onto a stack for later consideration.
  prefs: []
  type: TYPE_NORMAL
- en: We need a function to traverse a directory hierarchy without an explicit recursion.
    The core concept is that a directory is a collection of entries, and each entry
    is either a file, a sub-directory, or some other filesystem object we don’t want
    to touch (e.g., a mount point, symbolic link, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can say a node in the directory tree is a collection of entries: N = e[0],e[1],e[2],...,e[n].
    Each entry is either another directory, e ∈𝔻, or a file, e ∈𝔽.'
  prefs: []
  type: TYPE_NORMAL
- en: We can perform mappings on each file in the tree to process each file’s content.
    We might perform a filter operation to create an iterator over files with a specific
    property. We can also perform a reduction to count the number of files with a
    property. In this example, we’ll count the occurrences of a specific substring
    throughout the contents of files in a directory tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, we want a function p(f) that will provide the count of `"print"`
    in a node of the directory tree. It could be defined like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{|“print” ∈ N | if N ∈ 𝔽 p(N ) = ∑ |( e∈N p(e) if N ∈ 𝔻 ](img/file56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This shows how to apply the p(N) function to each element of a directory tree.
    When the element is a file, e ∈𝔽, we can count instances of ”print”. When the
    element is a directory, e ∈𝔻, we need to apply the p(N) function recursively to
    each entry, e[x], in the directory. While directory trees can’t be deep enough
    to break Python’s stack size limit, this kind of algorithm reveals an alternative
    tail-call optimization. It is an opportunity to use an explicit stack.
  prefs: []
  type: TYPE_NORMAL
- en: The `collections.deque` class is a marvelous way to build stacks and queues.
    The name comes from ”double-ended queue,” sometimes spelled dequeue. The data
    structure can be used as either a last-in-first-out (LIFO) stack or a first-in-first-out
    (FIFO). In this example, we use the `append()` and `pop()` methods, which enforce
    LIFO stack behavior. While this is much like a list, there are some optimizations
    in the `deque` implementation that can make it slightly faster than the generic
    list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a stack data structure lets us work with a hierarchy of indefinite size
    without running into Python’s internal stack depth limitation and raising `RecursionError`
    exceptions. The following function will traverse a file hierarchy looking at Python
    source files (with a suffix of `.py`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We seeded the stack of pending tasks with the initial directory. The essential
    algorithm is to unstack a directory and visit each entry in the directory. For
    entries that are files with the proper suffix, the processing is performed: counting
    the occurrences of ”print”. For entries that are directories, the directory is
    put into the stack as a pending task. Note that directories with a leading dot
    in their name need to be ignored. For the code in this book, those directories
    include caches used by tools like mypy, pytest, and tox. We want to skip over
    those cache directories.'
  prefs: []
  type: TYPE_NORMAL
- en: The processing performed on each file is part of the `all_print()` function.
    This can be refactored as a separate function that’s applied to each node as part
    of a reduction. Rewriting the `all_print()` function to be a proper higher-order
    function is left as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is we have two strategies for transforming a formal recursion
    into a usefully optimized function. We can reframe the recursion into an iteration,
    or we can introduce an explicit stack.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will apply the idea of a reduction (and the associated
    tail-call optimizations) to creating groups of items and computing a reduction
    for the groups.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Group-by reduction from many items to fewer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea of a reduction can apply in many ways. We’ve looked at the essential
    recursive definition of a reduction that produces a single value from a collection
    of values. This leads us to optimizing the recursion so we have the ability to
    compute summaries without the overheads of a naive Pythonic implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Creating subgroups in Python isn’t difficult, but it can help to understand
    the formalisms that support it. This understanding can help to avoid implementations
    that perform extremely poorly.
  prefs: []
  type: TYPE_NORMAL
- en: A very common operation is a reduction that groups values by some key or indicator.
    The raw data is grouped by some column’s value, and reductions (sometimes called
    aggregate functions) are applied to other columns.
  prefs: []
  type: TYPE_NORMAL
- en: In SQL, this is often called the `GROUP`` BY` clause of the `SELECT` statement.
    The SQL aggregate functions include `SUM`, `COUNT`, `MAX`, and `MIN`, and often
    many more.
  prefs: []
  type: TYPE_NORMAL
- en: Python offers us several ways to group data before computing a reduction of
    the grouped values. We’ll start by looking at two ways to get simple counts of
    grouped data. Then we’ll look at ways to compute different summaries of grouped
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the trip data that we computed in [Chapter 4](Chapter_04.xhtml#x1-740004),
    [Working with Collections](Chapter_04.xhtml#x1-740004). This data started as a
    sequence of latitude-longitude waypoints. We restructured it to create legs represented
    by three-tuples of start, end, and distance for each leg. The data looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We’d like to know the most common distance. Since the data is real-valued, and
    continuous, each distance is a unique value. We need to constrain these values
    from the continuous domain to a discrete set of distances. For example, quantizing
    each leg to the nearest multiple of five nautical miles. This creates bands of
    0 to 5 miles, over 5 to 10 miles, etc. Once we’ve created discrete integer values,
    we can count the number of legs in each of these bands.
  prefs: []
  type: TYPE_NORMAL
- en: 'These quantized distances can be produced with a generator expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This will divide each distance by 5—discarding any fractions—then multiply the
    truncated result by 5 to compute a number that represents the distance rounded
    down to the nearest 5 nautical miles.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t use the values assigned to the `start` and `stop` variables. It’s
    common practice to assign these values to the `_` variable. This can lead to some
    confusion because this can obscure the structure of the triple. It would look
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This approach can be helpful for removing some visual clutter.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Building a mapping with Counter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A mapping like the `collections.Counter` class is a great optimization for
    doing reductions that create counts (or totals) grouped by some value in the collection.
    The following expression creates a mapping from distance to frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The resulting `summary` object is stateful; it can be updated. The expression
    to create the groups, `Counter()`, looks like a function, making it a good fit
    for a design based on functional programming ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we print the `summary.most_common()` value, we’ll see the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The most common distance was about 30 nautical miles. We can also apply functions
    like `min()` and `max()` to find the shortest recorded and longest legs as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that your output may vary slightly from what’s shown. The results of the
    `most_common()` function are in order of frequency; equal-frequency bins may be
    in any order. These five lengths may not always be in the order shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This slight variability makes testing with the doctest tool a little bit more
    complex. One helpful trick for testing with counters is to use a dictionary to
    validate the results in general; the comparison between actual and expected no
    longer relies on the vagaries of internal hash computations.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Building a mapping by sorting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An alternative to `Counter` is to sort the original collection, and then use
    a recursive loop to identify when each group begins. This involves materializing
    the raw data, performing a sort that could—at worst—do O(nlog n) operations, and
    then doing a reduction to get the sums or counts for each key.
  prefs: []
  type: TYPE_NORMAL
- en: In order to work in a general way with Python objects that can be sorted, we
    need to define the protocol required for sorting. We’ll call the protocol `SupportsRichComparisonT`
    because we can sort any kinds of objects that implement the rich comparison operators,
    `<` and `>`. This isn’t a particular class of objects; it’s a protocol that any
    number of classes might implement. We formalize the idea of a protocol that classes
    must support using the `typing.Protocol` type definition. It could be also be
    called an interface that a class must implement. Python’s flexibility stems from
    having a fairly large number of protocols that many different classes support.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a common algorithm for creating groups from sorted data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The internal `group()` function steps through the sorted sequence of legs.
    If a given item key has already been seen—it matches the value in `previous`—then
    the `counter` variable is incremented. If a given item does not match the previous
    value, then there’s been a change in value: emit the previous value and the count,
    and begin a new accumulation of counts for the new value.'
  prefs: []
  type: TYPE_NORMAL
- en: The definition of `group()` provides two important type hints. The source data
    is an iterable over some type, shown with the type variable `SupportsRichComparisonT`.
    In this specific case, it’s pretty clear that the values in use will be of type
    `int`; however, the algorithm will work for any Python type. The resulting iterable
    from the `group()` function will preserve the type of the source data, and this
    is made explicit by using the same type variable, `SupportsRichComparisonT`.
  prefs: []
  type: TYPE_NORMAL
- en: The final line of the `group_sort()` function creates a dictionary from the
    grouped items. This dictionary will be similar to a `Counter` dictionary. The
    primary difference is that a `Counter()` function will have a `most_common()`
    method function, which a default dictionary lacks.
  prefs: []
  type: TYPE_NORMAL
- en: We can also do this with `itertools.groupby()`. We’ll look at this function
    closely in [Chapter 8](Chapter_08.xhtml#x1-1700008), [The Itertools Module](Chapter_08.xhtml#x1-1700008).
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3 Grouping or partitioning data by key values
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are no limits to the kinds of reductions we might want to apply to grouped
    data. We might have data with a number of independent and dependent variables.
    We can consider partitioning the data by an independent variable and computing
    summaries such as the maximum, minimum, average, and standard deviation of the
    values in each partition.
  prefs: []
  type: TYPE_NORMAL
- en: The essential trick to doing more sophisticated reductions is to collect all
    of the data values into each group. The `Counter()` function merely collects counts
    of identical items. For deeper analysis, we want to create sequences of the original
    members of the group.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at our trip data, each five-mile bin could contain the entire collection
    of legs of that distance, not merely a count of the legs. We can consider the
    partitioning as a recursion or as a stateful application of `defaultdict(list)`
    objects. We’ll look at the recursive definition of a `groupby()` function, since
    it’s easy to design.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, the `groupby(C,`` key)` computation for an empty collection, `[]`,
    is the empty dictionary, `dict()`. Or, more usefully, the empty `defaultdict(list)`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a non-empty collection, we need to work with item `C[0]`, the head, and
    recursively process sequence `C[1:]`, the tail. We can use slice expressions,
    or we can use the `head,` `*tail`` =`` C` statement to do this parsing of the
    collection, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If we have a `defaultdict` object named `groups`, we need to use the expression
    `groups[key(head)].append(head)` to include the head element in the `groups` dictionary.
    After this, we need to evaluate the `groupby(tail,`` key)` expression to process
    the remaining elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The interior function `group_into()` handles the essential recursive definition.
    An empty value for `collection` returns the provided dictionary, `group_dict`.
    A non-empty collection is partitioned into a head and tail. The head is used to
    update the `group_dict` dictionary. The tail is then used, recursively, to update
    the dictionary with all remaining elements.
  prefs: []
  type: TYPE_NORMAL
- en: The type hints make an explicit distinction between the type of the source objects
    `SeqItemT` and the type of the key `ItemKeyT`. The function provided as the `key`
    parameter must be a callable that returns a value of the key type `ItemKeyT`,
    given an object of the source type `SeqItemT`. In many of the examples, a function
    to extract the distance from a `Leg` object will be be shown. This is a `Callable[[SeqItemT],`` ItemKeyT]`
    where the source type `SeqItemT` is the `Leg` object and the key type `ItemKeyT`
    is the float value.
  prefs: []
  type: TYPE_NORMAL
- en: '`bound=Hashable` is an additional constraint. This defines an ”upper bound”
    on the possible types, alerting mypy that any type that could be assigned to this
    type variable must implement the protocol for `Hashable`. The essential, immutable
    Python types of numbers, strings, and tuples all meet this bound. A mutable object
    like a dictionary, set, or list, will not meet the upper bound, leading to warnings
    from mypy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can’t easily use Python’s default values to collapse this into a single
    function. We explicitly cannot use the following incorrect command snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If we try this, all uses of the `group_by()` function share one common `defaultdict(list)`
    object. This does not work because Python builds the default value just once.
    Mutable objects as default values rarely do what we want. The common practice
    is to provide a `None` value, and use an explicit `if` statement to create each
    unique, empty instance of `defaultdict(list)` as needed. We’ve shown how to use
    a wrapper function definition to avoid the `if` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can group the data by distance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We’ve defined a reusable lambda that puts our distances into bins, each of which
    is 5 nautical miles in size. We then grouped the data using the provided lambda.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine the binned data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is what the output looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Having looked at a recursive definition, we can turn to looking at making a
    tail-call optimization to build a group-by algorithm using iteration. This will
    work with larger collections of data, because it can exceed the internal stack
    size limitation.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with doing tail-call optimization on the `group_into()` function.
    We’ll rename this to `partition()` because partitioning is another way of looking
    at grouping.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `partition()` function can be written as an iteration as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: When doing the tail-call optimization, the essential line of the code in the
    imperative version will match the recursive definition. We’ve put a comment under
    the changed line to emphasize the rewrite is intended to have the same outcome.
    The rest of the structure represents the tail-call optimization we’ve adopted
    as a common way to work around the Python limitations.
  prefs: []
  type: TYPE_NORMAL
- en: The type hints emphasize the distinction between the source type `SeqT` and
    the key type `KeyT`. The source data can be anything, but the keys are limited
    to types that have proper hash values.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.4 Writing more general group-by reductions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once we have partitioned the raw data, we can compute various kinds of reductions
    on the data elements in each partition. We might, for example, want the northernmost
    point for the start of each leg in the distance bins.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll introduce some helper functions to decompose the tuple as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Each of these helper functions expects a tuple object to be provided using the
    `*` operator to map each element of the tuple to a separate parameter of the lambda.
    Once the tuple is expanded into the `s`, `e`, and `p` parameters, it’s reasonably
    obvious to return the proper parameter by name. It’s much clearer than trying
    to interpret the `tuple_arg[2]` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is how we use these helper functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Our initial `point` object is a nested three tuple with (0)—a starting position,
    (1)—the ending position, and (2)—the distance. We extracted various fields using
    our helper functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given these helpers, we can locate the northernmost starting position for the
    legs in each bin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The data that we grouped by distance included each leg of the given distance.
    We supplied all of the legs in each bin to the `max()` function. The `key` function
    we provided to the `max()` function extracted just the latitude of the starting
    point of the leg.
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us a short list of the northernmost legs of each distance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 6.3.5 Writing higher-order reductions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ll look at an example of a higher-order reduction algorithm here. This will
    introduce a rather complex topic. The simplest kind of reduction develops a single
    value from a collection of values. Python has a number of built-in reductions,
    including `any()`, `all()`, `max()`, `min()`, `sum()`, and `len()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we noted in [Chapter 4](Chapter_04.xhtml#x1-740004), [Working with Collections](Chapter_04.xhtml#x1-740004),
    we can do a great deal of statistical calculation if we start with a few reductions
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to define mean, standard deviation, normalized values, correction,
    and even least-squares linear regression, building on these base reduction functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last of our reductions, `sum_x2()`, shows how we can apply existing reductions
    to create higher-order functions. We might change our approach to be more like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We’ve added a function, `function()`, as a parameter; the function can transform
    the data. This overall function, `sum_f()`, computes the sum of the transformed
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can apply this function in three different ways to compute the three
    essential sums as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We’ve plugged in a small lambda to compute ∑ [x∈X]x⁰ = ∑ [x∈X]1, which is the
    count, ∑ [x∈X]x¹ = ∑ [x∈X]x, the sum, and ∑ [x∈X]x², the sum of the squares, which
    we can use to compute standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common extension to this includes a filter to reject raw data that is unknown
    or unsuitable in some way. We might use the following function to reject bad data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function definition for computing a mean will reject `None` values
    in a simple way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This shows how we can provide two distinct combinations of lambdas to our `sum_filter_f()`
    function. The filter argument is a lambda that rejects `None` values; we’ve called
    it `valid` to emphasize its meaning. The function argument is a lambda that implements
    a count or a sum operation. We can easily add a lambda to compute a sum of squares.
  prefs: []
  type: TYPE_NORMAL
- en: The reuse of a common `valid` rule assures that the various computations are
    all identical in applying any filters to the source data. This can be combined
    with a user-selected filter criteria to provide a tidy plug-in to compute a number
    of statistics related to a user’s requested subset of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.6 Writing file parsers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can often consider a file parser to be a kind of reduction. Many languages
    have two levels of definition: the lower-level tokens in the language and the
    higher-level structures built from those tokens. When looking at an XML file,
    the tags, tag names, and attribute names form this lower-level syntax; the structures
    which are described by XML form a higher-level syntax.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The lower-level lexical scanning is a kind of reduction that takes individual
    characters and groups them into tokens. This fits well with Python’s generator
    function design pattern. We can often write functions that look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: For well-known file formats, we’ll use existing file parsers. For data in CSV,
    JSON, XML, or TOML format, we don’t need to write file parsers. Most of these
    modules have a `load()` method that produces useful Python objects.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, we’ll need to combine the results of this parsing into higher-level
    objects, useful for our specific application. While the CSV parser provides individual
    rows, these might need to be used to create `NamedTuple` instances, or perhaps
    some other class of immutable Python objects. Our examples of trip data, starting
    in [Chapter 4](Chapter_04.xhtml#x1-740004), [Working with Collections](Chapter_04.xhtml#x1-740004),
    are combined into higher-level objects, legs of a journey, by an algorithm that
    combines waypoints into pairs. When we introduce more complex decision-making,
    we make a transition from restructuring into parsing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to provide useful waypoints in the first place, we needed to parse
    a source file. In these examples, the input was a KML file; KML is an XML representation
    of geographic information. The essential features of the parser look similar to
    the following definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The bulk of the `row_iter_kml()` function is the XML parsing that allows us
    to use the `doc.findall()` function to iterate through the `<ns0:coordinates>`
    tags in the document. We’ve used a function named `comma_split()` to parse the
    text of this tag into a three-tuple of values.
  prefs: []
  type: TYPE_NORMAL
- en: The `cast()` function is only present to provide evidence to mypy that the value
    of `coordinates.text` is a `str` object. The default definition of the text attribute
    is `Union[str,` `bytes]`; in this application, the data will be `str` exclusively.
    The `cast()` function doesn’t do any runtime processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function focused on working with the normalized XML structure. The document
    is close to the database designer’s definitions of first normal form: each attribute
    is atomic (a single value), and each row in the XML data has the same columns
    with data of a consistent type. The data values aren’t fully atomic, however:
    we have to split the points on the , to separate longitude, latitude, and altitude
    into atomic string values. However, the text value for these XML tags is internally
    consistent, making it a close fit with first normal form.'
  prefs: []
  type: TYPE_NORMAL
- en: A large volume of data—XML tags, attributes, and other punctuation—is reduced
    to a somewhat smaller volume, including just floating-point latitude and longitude
    values. For this reason, we can think of parsers as a kind of reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll need a higher-level set of conversions to map the tuples of text into
    floating-point numbers. Also, we’d like to discard altitude, and reorder longitude
    and latitude. This will produce the application-specific tuple we need. We can
    use functions as follows for this conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The essential tool is the `float_lat_lon()` function. This is a higher-order
    function that returns a generator expression. The generator uses the `map()` function
    to apply the `float()` function conversion to the results of the `pick_lat_lon()`
    function, and the `*row` argument to assign each member of the row tuple to a
    different parameter of the `pick_lat_lon()` function. This only works when each
    row is a three-tuple. The `pick_lat_lon()` function then returns a two-tuple of
    the selected items in the required order.
  prefs: []
  type: TYPE_NORMAL
- en: 'The source includes XML that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this parser as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This will build a tuple-of-tuples representation of each waypoint along the
    path in the original KML file. The result will be a flat sequence of pairs that
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The `float_lat_lon()` function uses a low-level XML parser to extract rows of
    text data from the original representation. It uses a higher-level parser to transform
    the text items into more useful tuples of floating-point values suitable for the
    target application.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing CSV files
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In [Chapter 3](Chapter_03.xhtml#x1-510003), [Functions, Iterators, and Generators](Chapter_03.xhtml#x1-510003),
    we saw another example where we parsed a CSV file that was not in a normalized
    form: we had to discard header rows to make it useful. To do this, we used a function
    that extracted the header and returned an iterator over the remaining rows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The columns are separated by tab characters. Plus, there are three rows of headers
    that we can discard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s another version of that CSV-based parser. We’ve broken it into three
    functions. The first, `row_iter_csv()` function, returns the iterator over the
    rows in a tab-delimited file. The function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This is a small wrapper around the CSV parsing process. When we look back at
    the previous parsers for XML and plain text, this was the kind of thing that was
    missing from those parsers. Producing an iterable over row tuples can be a common
    feature of parsers for normalized data.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a row of tuples, we can pass rows that contain usable data and
    reject rows that contain other metadata, such as titles and column names. We’ll
    introduce a helper function that we can use to do some of the parsing, plus a
    `filter()` function to validate a row of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This function handles the conversion of a single string to float values, converting
    bad data to a `None` value. The type hint of `float`` |`` None` expresses the
    idea of having a value of the given type or having a value of the same type as
    `None`. This can also be stated as `Union[float,`` None]` to show how the result
    is a union of different alternative types.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can embed the `float_none()` function in a mapping so that we convert all
    columns of a row to a float or `None` value. A lambda for this looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Two type hints are used to make the definition of the `float_row()` function
    explicit. The `R_Float` hint defines the floating-point version of a row of data
    that may include `None` values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a row-level validator based on the use of the `all()` function
    to ensure that all values are `float` (or none of the values are `None`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This lambda is a kind of reduction, transforming a row of floating-point values
    to a Boolean value if all values are not ”falsy” (that is, neither `None` nor
    zero) and there are exactly eight values.
  prefs: []
  type: TYPE_NORMAL
- en: The simplistic `all_numeric()` function conflates zero and `None`. A more sophisticated
    test would rely on something such as `not`` any(item`` is`` None`` for`` item`` in`` row)`.
    The rewrite is left as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: The essential design is to create row-based elements that can be combined to
    create more complete algorithms for parsing an input file. The foundational functions
    iterate over tuples of text. These are combined to convert and validate the converted
    data. For the cases where files are either in first normal form (all rows are
    the same) or a simple validator can reject the extraneous rows, this design pattern
    works out nicely.
  prefs: []
  type: TYPE_NORMAL
- en: All parsing problems aren’t quite this simple, however. Some files have important
    data in header or trailer rows that must be preserved, even though it doesn’t
    match the format of the rest of the file. These non-normalized files will require
    a more sophisticated parser design.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing plain text files with headers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In [Chapter 3](Chapter_03.xhtml#x1-510003), [Functions, Iterators, and Generators](Chapter_03.xhtml#x1-510003),
    the `Crayola.GPL` file was presented without showing the parser. This file looks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We can parse a text file using regular expressions. We need to use a filter
    to read (and parse) header rows. We also want to return an iterable sequence of
    data rows. This rather complex two-part parsing is based entirely on the two-part—head
    and tail—file structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a low-level parser that handles both the four lines of the header
    and the long tail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The `Head_Body` type definition summarizes the overall goal of the row iterator.
    The result is a two-tuple. The first item is a two-tuple with details from the
    file header. The second item is an iterator that provides the text items for a
    color definition. This `Head_Body` type hint is used in two places in this function
    definition.
  prefs: []
  type: TYPE_NORMAL
- en: The `header_pat` regular expression parses all four lines of the header. There
    are instances of `()` in the expression to extract the name and column information
    from the header.
  prefs: []
  type: TYPE_NORMAL
- en: There are two internal functions for parsing different parts of the file. The
    `read_head()` function parses the header lines and returns interesting text and
    a `TextIO` object that can be used for the rest of the parsing. It does this by
    reading four lines and merging them into a single long string. This is then parsed
    with the `header_pat` regular expression.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of returning the iterator from one function to be used in another function
    is a pattern for passing an explicitly stateful object from one function to another.
    It seems helpful to make sure all of the arguments for the `read_tail()` function
    are the results from the `read_head()` function.
  prefs: []
  type: TYPE_NORMAL
- en: The `read_tail()` function parses the iterator over the remaining lines. These
    lines are merely split on spaces, since that fits the description of the GPL file
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, visit the following link: [https://code.google.com/p/grafx2/issues/detail?id=518](https://code.google.com/p/grafx2/issues/detail?id=518).'
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve transformed each line of the file into a canonical tuple-of-strings
    format, we can apply the higher level of parsing to this data. This involves conversion
    and (if necessary) validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a higher-level parser command snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This function will work with the output of the lower-level `row_iter_gpl()`
    parser: it requires the headers and the iterator over individual rows. This function
    will use the multiple assignment feature of the `for` clause in the generator
    to separate the color numbers and the remaining words into four variables, `r`,
    `g`, `b`, and `name`. The use of the `*name` parameter ensures that all remaining
    values will be assigned to the `name` variable as a tuple. The `"`` ".join(name)`
    expression then concatenates the words into a single space-separated string.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is how we can use this two-tier parser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We’ve applied the higher-level parser to the results of the lower-level parser.
    This will return the headers and a tuple built from the sequence of `Color` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we’ve looked at two significant functional programming topics.
    We’ve looked at recursions in some detail. Many functional programming language
    compilers will optimize a recursive function to transform a call in the tail of
    the function to a loop. This is sometimes called tail recursion elimination. More
    commonly, it’s known as tail-call optimization. In Python, we must do the tail-call
    optimization manually by using an explicit `for` statement, replacing a purely
    functional recursion.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also looked at reduction algorithms, including `sum()`, `count()`, `max()`,
    and `min()` functions. We looked at the `collections.Counter()` function and related
    `groupby()` reductions.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also looked at how parsing (and lexical scanning) are similar to reductions
    since they transform sequences of tokens (or sequences of characters) into higher-order
    collections with more complex properties. We’ve examined a design pattern that
    decomposes parsing into a lower level and tries to produce tuples of raw strings,
    and a higher level that creates more useful application objects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll look at some techniques appropriate to working with
    named tuples and other immutable data structures. We’ll look at techniques that
    make stateful objects unnecessary. While stateful objects aren’t purely functional,
    the idea of a class hierarchy can be used to package related method definitions.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Exercises
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter’s exercises are based on code available from Packt Publishing on
    GitHub. See [https://github.com/PacktPublishing/Functional-Python-Programming-3rd-Edition](https://github.com/PacktPublishing/Functional-Python-Programming-3rd-Edition).
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the reader will notice that the code provided on GitHub includes
    partial solutions to some of the exercises. These serve as hints, allowing the
    reader to explore alternative solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, exercises will need unit test cases to confirm they actually
    solve the problem. These are often identical to the unit test cases already provided
    in the GitHub repository. The reader should replace the book’s example function
    name with their own solution to confirm that it works.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1 Multiple recursion and caching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Handling difficult tail-call optimization](#x1-1300003), we looked at a
    naive definition of a function to compute Fibonacci numbers, the `fib()` function.
    The `functools.cache` decorator can have a profound impact on the performance
    of this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Implement both versions and describe the impact of caching on the time required
    to compute large Fibonacci numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2 Refactor the all_print() function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Tail-call optimization using deques](#x1-1350001), we showed a function
    that used a `collections.deque` to visit all nodes in a directory tree, summing
    the value for each node that is a proper file. This can be done with a list as
    well as a `deque`, with some minor code changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function embedded a specific computation. This computation (finding all
    occurrences of ”print”) really should have been a separate function. The body
    of the `all_print()` function should be refactored into two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: A generic directory traverse that applies a function to each text file with
    the expected suffix and sums the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that counts instances of ”print” in a given Python file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.5.3 Parsing CSV files
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: See the [Parsing CSV files](#x1-1430006) section, earlier in this chapter. In
    that example, the simplistic `all_numeric()` function conflates zero and `None`.
  prefs: []
  type: TYPE_NORMAL
- en: Create a test case for this function that will show that it does not handle
    zero correctly, treating it as `None`. Once the test case is defined, rewrite
    the `all_numeric()` function to distinguish between zero and `None`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it’s common practice in Python to use the `is` operator when comparing
    with `None`. This specifically avoids some subtle problems that can arise when
    a class has an implementation of `__eq__()` that doesn’t handle `None` as a properly
    distinct object.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.4 Classification of state, Part III
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: See [Chapter 5](Chapter_05.xhtml#x1-1000005), [Higher-Order Functions](Chapter_05.xhtml#x1-1000005),
    the [Classification of state](Chapter_05.xhtml#x1-1220001) exercise.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a third way to consume status details and summarize them.
  prefs: []
  type: TYPE_NORMAL
- en: Write a reduce computation. This starts with an initial state of Running. As
    each service’s three-tuple is folded into the result, there is a comparison between
    the state and the three-tuple. If the three-tuple has a non-responsive service,
    the state advances to Stopped. If the three-tuple has a slow or not working service,
    the state advances to Degraded. If no problems are found, the initial value becomes
    the final health of the overall system.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to provide a `status_add(previous,`` this_service)` function. This
    can be used in the context of `status`` =`` reduce(status_add,`` service_status_sequence,`` "Running")`
    to compute the current status of the sequence of services.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.5 Diesel engine data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A diesel engine has had some repairs that raised doubts about the accuracy of
    the tachometer. After some heroic effort, the following table of data was collected
    showing the observed reading on the engine’s tachometer, and the actual RPMs measured
    with an optical device on the engine.
  prefs: []
  type: TYPE_NORMAL
- en: '| Sample | Tach | Engine |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1000 | 883 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1500 | 1242 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1500 | 1217 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1600 | 1306 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1750 | 1534 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 2000 | 1805 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 2000 | 1720 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: If needed, create a CSV file with the data. If you have access to the GitHub
    repository for this book, this is available in the `engine.csv` file.
  prefs: []
  type: TYPE_NORMAL
- en: Create a `NamedTuple` for each sample and write some functions to acquire this
    data in a useful form. Once the data is available, see the [Using sums and counts
    for statistics](Chapter_04.xhtml#x1-850001) section of [Chapter 4](Chapter_04.xhtml#x1-740004),
    [Working with Collections](Chapter_04.xhtml#x1-740004), for a definition of a
    correlation function.
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to apply this correlation function to the engine and tach values
    to see if the values correlate. If they do, it suggests that the engine’s instruments
    can be recalibrated. If they don’t correlate, something else is wrong with the
    engine.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the [Chapter 4](Chapter_04.xhtml#x1-740004), [Working with Collections](Chapter_04.xhtml#x1-740004),
    correlation example may have assumptions about data types that don’t necessarily
    apply to the `NamedTuple` defined earlier. If necessary, rewrite the type hints
    or your `NamedTuple` definition. Note that it can be difficult to write perfectly
    generic type hints, and it often takes a bit of work to resolve the differences.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community Discord space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Join our Python Discord workspace to discuss and know more about the book:
    [https://packt.link/dHrHU](https://packt.link/dHrHU)'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
