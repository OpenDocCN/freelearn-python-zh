- en: '6'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '6'
- en: Recursions and Reductions
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 递归和归约
- en: Many functional programming language compilers will optimize a recursive function
    to transform a recursive call in the tail of the function to an iteration. This
    tail-call optimization will dramatically improve performance. Python doesn’t do
    this automatic tail-call optimization. One consequence is pure recursion suffers
    from limitations. Lacking an automated optimization, we need to do the tail-call
    optimization manually. This means rewriting recursion to use an explicit iteration.
    There are two common ways to do this, and we’ll consider them both in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 许多函数式编程语言编译器会将递归函数优化，将函数尾部的递归调用转换为迭代。这种尾调用优化将显著提高性能。Python 不进行这种自动尾调用优化。一个后果是纯递归受到限制。缺乏自动优化，我们需要手动进行尾调用优化。这意味着重写递归以使用显式迭代。有两种常见的方法来做这件事，我们将在本章中考虑它们。
- en: 'In previous chapters, we’ve looked at several related kinds of processing design
    patterns; some of them are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了多种相关的处理设计模式；其中一些如下：
- en: Mapping and filtering, which create collections from collections
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 映射和过滤，它们从集合创建集合
- en: Reductions that create a scalar value from a collection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从集合创建标量值的归约
- en: The distinction is exemplified by functions such as `map()` and `filter()` that
    accomplish the first kind of collection processing. There are some more specialized
    reduction functions, which include `min()`, `max()`, `len()`, and `sum()`. There’s
    a general-purpose reduction function as well, `functools.reduce()`.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这种区别可以通过像 `map()` 和 `filter()` 这样的函数来体现，这些函数完成了第一种集合处理。还有一些更专业的归约函数，包括 `min()`、`max()`、`len()`
    和 `sum()`。还有一个通用归约函数，`functools.reduce()`。
- en: We’ll also consider creating a `collections.Counter()` object as a kind of reduction
    operator. It doesn’t produce a single scalar value per se, but it does create
    a new organization of the data that eliminates some of the original structure.
    At heart, it’s a kind of count-group-by operation that has more in common with
    a counting reduction than with a mapping.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将考虑创建一个 `collections.Counter()` 对象作为归约操作符的一种。它本身并不产生单个标量值，但它确实创建了一种新的数据组织方式，消除了原始结构的一些部分。本质上，它是一种计数分组操作，与计数归约比与映射有更多的共同点。
- en: In this chapter, we’ll look at reduction functions in more detail. From a purely
    functional perspective, a reduction can be defined recursively. The tail-call
    optimization techniques available in Python apply elegantly to reductions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更详细地探讨归约函数。从纯函数的角度来看，归约可以递归地定义。Python 中可用的尾调用优化技术非常适合归约。
- en: We’ll review a number of built-in reduction algorithms including `sum()`, `count()`,
    `max()`, and `min()`. We’ll look at the `collections.Counter()` creation and related
    `itertools.groupby()` reductions. We’ll also look at how parsing (and lexical
    scanning) are proper reductions since they transform sequences of tokens (or sequences
    of characters) into higher-order collections with more complex properties.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回顾一些内置的归约算法，包括 `sum()`、`count()`、`max()` 和 `min()`。我们将探讨 `collections.Counter()`
    的创建和相关 `itertools.groupby()` 归约。我们还将探讨解析（和词法扫描）是如何作为适当的归约的，因为它们将标记序列（或字符序列）转换为具有更复杂属性的更高阶集合。
- en: 6.1 Simple numerical recursions
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 简单数值递归
- en: We can consider all numeric operations to be defined by recursions. For more
    details, read about the Peano axioms that define the essential features of numbers
    at [https://www.britannica.com/science/Peano-axioms](https://www.britannica.com/science/Peano-axioms).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将所有数值运算都定义为递归。有关更多详细信息，请阅读定义数字基本特征的佩亚诺公理，见[https://www.britannica.com/science/Peano-axioms](https://www.britannica.com/science/Peano-axioms)。
- en: From these axioms, we can see that addition is defined recursively using more
    primitive notions of the next number, or the successor of a number n, S(n).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些公理中，我们可以看到加法是通过使用更原始的下一个数的概念，即数 n 的后继，S(n) 来递归定义的。
- en: To simplify the presentation, we’ll assume that we can define a predecessor
    function, P(n), such that n = S(P(n)) = P(S(n)), as long as n≠0\. This formalizes
    the idea that a number is the successor of the number’s predecessor.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化说明，我们假设我们可以定义一个前驱函数，P(n)，使得 n = S(P(n)) = P(S(n))，只要 n≠0。这形式化了这样一个观点：一个数是它前驱数的后继。
- en: 'Addition between two natural numbers could be defined recursively as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 两个自然数之间的加法可以递归地定义为如下：
- en: '![ ( |{ add(a,b) = b if a = 0 |( add(P(a),S(b)) if a ⁄= 0 ](img/file48.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![ ( |{ add(a,b) = b if a = 0 |( add(P(a),S(b)) if a ⁄= 0 ](img/file48.jpg)'
- en: If we use the more typical notations of n + 1 and n− 1 instead of S(n) and P(n),
    we can more easily see how the rule add(a,b) = add(a − 1,b + 1) when a≠0 works.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用更典型的n + 1和n− 1的表示法，而不是S(n)和P(n)，我们可以更容易地看到当a≠0时，add(a,b) = add(a − 1,b
    + 1)的规则是如何工作的。
- en: 'This translates neatly into Python, as shown in the following function definition:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下函数定义中得到了很好的体现：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We’ve rearranged the abstract mathematical notation into concrete Python.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将抽象的数学符号重新排列成了具体的Python代码。
- en: There’s no good reason to provide our own functions in Python to do simple addition.
    We rely on Python’s underlying implementation to properly handle arithmetic of
    various kinds. Our point here is that fundamental scalar arithmetic can be defined
    recursively, and the definition translates to Python.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中提供我们自己的函数来进行简单的加法没有很好的理由。我们依赖于Python的底层实现来正确处理各种算术运算。我们在这里的要点是，基本标量算术可以递归定义，并且定义可以翻译成Python。
- en: This suggests that more complicated operations, defined recursively, can also
    be translated to Python. The translation can be manually optimized to create working
    code that matches the abstract definitions, reducing questions about possible
    bugs in the implementation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明更复杂的递归定义的操作也可以翻译成Python。这种翻译可以通过手动优化来创建与抽象定义相匹配的运行代码，从而减少关于实现中可能出现的错误的问题。
- en: 'A recursive definition must include at least two cases: a non-recursive (or
    base) case where the value of the function is defined directly, and the recursive
    case where the value of the function is computed from a recursive evaluation of
    the function with different argument values.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 递归定义必须至少包括两种情况：一个非递归（或基本）情况，其中函数的值直接定义，以及递归情况，其中函数的值是通过递归评估具有不同参数值的函数来计算的。
- en: In order to be sure the recursion will terminate, it’s important to see how
    the recursive case computes values that approach the defined non-recursive base
    case. Pragmatically, there are often constraints on the argument values that we’ve
    omitted from the functions here. For example, the `add()` function in the preceding
    command snippet could be expanded to include `assert`` a>=0`` and`` b>=0` to establish
    two necessary constraints on the input values.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保递归能够终止，重要的是要看到递归情况是如何计算接近定义的非递归基本情况的值的。从实用角度来看，我们通常省略了函数中的参数值约束。例如，前面命令片段中的`add()`函数可以扩展以包括`assert
    a>=0 and b>=0`，以建立对输入值的两个必要约束。
- en: Without these constraints, starting with `a` equal to -1 won’t approach the
    non-recursive case of `a`` ==`` 0` as we keep subtracting 1 from `a`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 没有这些限制，从`a`等于-1开始，当我们不断从`a`中减去1时，不会接近`a == 0`的非递归情况。
- en: 6.1.1 Implementing manual tail-call optimization
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 实现手动尾调用优化
- en: For some functions, the recursive definition is the most succinct and expressive.
    A common example is the `factorial()` function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些函数，递归定义是最简洁和表达性最强的。一个常见的例子是`factorial()`函数。
- en: 'We can see how this is rewritten as a simple recursive function in Python from
    the following formula:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从以下公式中看到，这如何被重写为Python中的一个简单递归函数：
- en: '![ ( | { 1 if n = 0 n! = |( n × (n− 1)! if n ⁄= 0 ](img/file49.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![ ( | { 1 if n = 0 n! = |( n × (n− 1)! if n ⁄= 0 ](img/file49.jpg)'
- en: 'The preceding formula can be implemented in Python by using the following function
    definition:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公式可以通过以下函数定义在Python中实现：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This implementation has the advantage of simplicity. The recursion limits in
    Python artificially constrain us; we can’t do anything larger than about `fact(997)`.
    The value of 1000! has 2,568 digits and generally exceeds our floating-point capacity;
    on some systems the floating-point limit is near 10^(300). Pragmatically, it’s
    common to switch to a log gamma function instead of working with immense numbers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实现具有简单性的优势。Python中的递归限制人为地限制了我们的能力；我们无法进行大于约`fact(997)`的操作。1000!的值有2,568位数字，通常超过了我们的浮点数容量；在某些系统中，浮点数限制接近10^(300)。从实用角度来看，通常切换到对数伽马函数而不是处理巨大的数字。
- en: See [https://functions.wolfram.com/GammaBetaErf/LogGamma/introductions/Gammas/ShowAll.html](https://functions.wolfram.com/GammaBetaErf/LogGamma/introductions/Gammas/ShowAll.html)
    for more on log gamma functions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有关对数伽马函数的更多信息，请参阅[https://functions.wolfram.com/GammaBetaErf/LogGamma/introductions/Gammas/ShowAll.html](https://functions.wolfram.com/GammaBetaErf/LogGamma/introductions/Gammas/ShowAll.html)。
- en: We can expand Python’s call stack limit to stretch this to the limits of memory.
    It’s better, however, to manually optimize these kinds of functions to eliminate
    the recursion.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将Python的调用栈限制扩展到内存的极限。然而，手动优化这些函数以消除递归是更好的选择。
- en: This function demonstrates a typical tail recursion. The last expression in
    the function is a call to the function with a new argument value. An optimizing
    compiler can replace the function call stack management with a loop that executes
    very quickly.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数演示了一个典型的尾递归。函数中的最后一个表达式是对具有新参数值的函数的调用。优化编译器可以用执行非常快的循环来替换函数调用栈管理。
- en: In this example, the function involves an incremental change from n to n − 1\.
    This means that we’re generating a sequence of numbers and then doing a reduction
    to compute their product.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，函数涉及从 n 到 n - 1 的增量变化。这意味着我们在生成一系列数字后，再进行归约以计算它们的乘积。
- en: 'Stepping outside purely functional processing, we can define an imperative
    `facti()` calculation as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 超出纯粹函数式处理，我们可以定义一个命令式的 `facti()` 计算如下：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This version of the factorial function will compute values beyond 1000! (2000!,
    for example, has 5,736 digits). This example isn’t purely functional. We’ve optimized
    the tail recursion into a stateful `for` statement depending on the `i` variable
    to maintain the state of the computation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶乘函数版本将计算超过1000!（例如，2000!有5,736位）。这个例子并不纯粹是函数式的。我们将尾递归优化为一个依赖于 `i` 变量的状态 `for`
    语句，以保持计算状态。
- en: In general, we’re obliged to do this in Python because Python can’t automatically
    do the tail-call optimization. There are situations, however, where this kind
    of optimization isn’t actually helpful. We’ll look at a few of them.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们不得不在Python中这样做，因为Python无法自动进行尾调用优化。然而，在某些情况下，这种优化实际上并不 helpful。我们将探讨其中的一些情况。
- en: 6.1.2 Leaving recursion in place
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 保持递归不变
- en: 'In some cases, the recursive definition is actually optimal. Some recursions
    involve a divide and conquer strategy that minimizes the work. One example of
    this is the algorithm for doing exponentiation by squaring. This works for computing
    values that have a positive integer exponent, like 2^(64). We can state it formally
    as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，递归定义实际上是最佳的。一些递归涉及分治策略，这可以最小化工作量。其中一个例子是平方幂的指数算法。这适用于计算具有正整数指数的值，如 2^(64)。我们可以如下形式化地陈述它：
- en: '![ (| ||| 1 if n = 0 n { (n−1) a = || a × a if a is odd ||( n2 2 (a ) if a
    is even ](img/file50.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![ (| ||| 1 if n = 0 n { (n−1) a = || a × a if a is odd ||( n2 2 (a ) if a
    is even ](img/file50.jpg)'
- en: 'We’ve broken the process into three cases, easily written in Python as a recursion.
    Look at the following function definition:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将过程分解为三个情况，可以很容易地用Python作为递归编写。看看以下函数定义：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For odd numbers, the `fastexp()` method is defined recursively. The exponent
    `n` is reduced by 1\. A simple tail-recursion optimization would work for this
    case. It would not work for the even case, however.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于奇数，`fastexp()` 方法定义为递归。指数 `n` 减少了1。对于这种情况，简单的尾递归优化是可行的。然而，对于偶数情况，则不可行。
- en: For even numbers, the `fastexp()` recursion uses `n`` //`` 2`, chopping the
    problem into half of its original size. Since the problem size is reduced by a
    factor of 2, this case results in a significant speed-up of the processing.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于偶数，`fastexp()` 递归使用 `n // 2`，将问题规模减半。由于问题规模减少了2倍，这种情况会导致处理速度显著提升。
- en: We can’t trivially reframe this kind of function into a tail-call optimization
    loop. Since it’s already optimal, we don’t really need to optimize it further.
    The recursion limit in Python would impose the constraint of n ≤ 2^(1000), a generous
    upper bound.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能简单地重构这种函数为尾调用优化循环。由于它已经是最优的，我们实际上不需要进一步优化它。Python中的递归限制将导致 n ≤ 2^(1000)，这是一个相当宽松的上限。
- en: 6.1.3 Handling difficult tail-call optimization
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 处理困难的尾调用优化
- en: 'We can look at the definition of Fibonacci numbers recursively. The following
    is one widely used definition for the n^(th) Fibonacci number, F[n]:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以递归地查看斐波那契数的定义。以下是对第 n 个斐波那契数 F[n] 的一个广泛使用的定义：
- en: '![ (| ||| 0 if n = 0 { Fn = | 1 if n = 1 |||( Fn− 1 + Fn− 2 if n ≥ 2 ](img/file51.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![ (| ||| 0 if n = 0 { Fn = | 1 if n = 1 |||( Fn− 1 + Fn− 2 if n ≥ 2 ](img/file51.jpg)'
- en: 'A given Fibonacci number, F[n], is defined as the sum of the previous two numbers,
    F[n−1] + F[n−2]. This is an example of multiple recursion: it can’t be trivially
    optimized as a simple tail recursion. However, if we don’t optimize it to a tail
    recursion, we’ll find it to be too slow to be useful.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个给定的斐波那契数F[n]定义为前两个数的和，即F[n−1] + F[n−2]。这是一个多次递归的例子：它不能简单地作为简单的尾递归进行优化。然而，如果我们不将其优化为尾递归，我们会发现它太慢而无法使用。
- en: 'The following is a naïve implementation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的实现：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This suffers from a terrible multiple recursion problem. When computing the
    `fib(n)` value, we must compute the `fib(n-1)` and `fib(n-2)` values. The computation
    of the `fib(n-1)` value involves a duplicate calculation of the `fib(n-2)` value.
    The two recursive uses of the `fib()` function will more than duplicate the amount
    of computation being done.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这存在一个可怕的多次递归问题。在计算`fib(n)`值时，我们必须计算`fib(n-1)`和`fib(n-2)`的值。`fib(n-1)`值的计算涉及到`fib(n-2)`值的重复计算。`fib()`函数的两次递归使用将超过重复计算的工作量。
- en: Because of the left-to-right Python evaluation rules, we can evaluate values
    up to about `fib(1000)`. However, we have to be patient. Very patient. (Trying
    to find the actual upper bound with the default stack size means waiting a long
    time before the `RecursionError` is raised.)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Python从左到右的评估规则，我们可以评估到大约`fib(1000)`的值。然而，我们必须有耐心。非常耐心。（尝试使用默认的栈大小找到实际的界限意味着在`RecursionError`被抛出之前要等待很长时间。）
- en: 'The following is one alternative, which restates the entire algorithm to use
    stateful variables instead of a simple recursion:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个替代方案，它重新表述了整个算法，使用有状态变量而不是简单的递归：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Our stateful version of this function counts up from 0, unlike the recursion,
    which counts down from the initial value of `n`. This version is considerably
    faster than the recursive version.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这个有状态版本的函数从0开始计数，与递归从初始值`n`开始计数不同。这个版本比递归版本快得多。
- en: What’s important here is that we couldn’t trivially optimize the `fib()` function
    recursion with an obvious rewrite. In order to replace the recursion with an imperative
    version, we had to look closely at the algorithm to determine how many stateful
    intermediate variables were required.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重要的是，我们无法简单地通过明显的重写来优化`fib()`函数的递归。为了用命令式版本替换递归，我们必须仔细查看算法，以确定需要多少个有状态的中间变量。
- en: As an exercise for the reader, try using the `@cache` decorator from the `functools`
    module. What impact does this have?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对读者的练习，尝试使用`functools`模块中的`@cache`装饰器。这会产生什么影响？
- en: 6.1.4 Processing collections through recursion
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.4 通过递归处理集合
- en: 'When working with a collection, we can also define the processing recursively.
    We can, for example, define the `map()` function recursively. The formalism could
    be stated as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理集合时，我们也可以递归地定义处理。例如，我们可以递归地定义`map()`函数。形式化可以表述如下：
- en: '![ ( |{ [] if len(C ) = 0 map (f,C ) = | ( map(f,C [:−1]) + [f (C −1)] if len(C
    ) > 0 ](img/file52.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![ ( |{ [] if len(C ) = 0 map (f,C ) = | ( map(f,C [:−1]) + [f (C −1)] if len(C
    ) > 0 ](img/file52.jpg)'
- en: We’ve defined the mapping of a function, f, to an empty collection as an empty
    sequence, `[]`. We’ve also specified that applying a function to a collection
    can be defined recursively with a three-step expression. First, recursively perform
    the mapping of the function to all of the collection except the last element,
    creating a sequence object. Then apply the function to the last element. Finally,
    append the last calculation to the previously built sequence.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将函数f映射到空集合定义为空序列`[]`。我们还指定了将函数应用于集合可以通过一个三步表达式递归定义。首先，递归地对函数应用于除最后一个元素之外的所有集合元素进行映射，创建一个序列对象。然后对最后一个元素应用函数。最后，将最后的计算结果追加到之前构建的序列中。
- en: 'Following is a purely recursive function version of this `map()` function:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这个`map()`函数的纯递归函数版本：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The value of the `mapr(f,[])` method is defined to be an empty list object.
    The value of the `mapr()` function with a non-empty list will apply the function
    to the last element in the list and append this to the list built recursively
    from the `mapr()` function applied to the head of the list.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapr(f,[])`方法定义的值是一个空列表对象。对于非空列表的`mapr()`函数，将应用函数到列表的最后一个元素，并将其追加到由应用于列表头的`mapr()`函数递归构建的列表中。'
- en: We have to emphasize that this `mapr()` function actually creates a list object.
    The built-in `map()` function is an iterator; it doesn’t create a list object.
    It yields the result values as they are computed. Also, the work is done in right-to-left
    order, which is not the way Python normally works. This is only observable when
    using a function that has side effects, something we’d like to avoid doing.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须强调，这个`mapr()`函数实际上创建了一个列表对象。内置的`map()`函数是一个迭代器；它不会创建列表对象。它按计算顺序产生结果值。此外，工作是在从右到左的顺序中完成的，这不是Python通常的工作方式。这只有在使用具有副作用的功能时才会观察到，这是我们希望避免做的事情。
- en: While this is an elegant formalism, it still lacks the tail-call optimization
    required. An optimization will allow us to exceed the default recursion limit
    of 1,000 and also performs much more quickly than this naïve recursion.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个优雅的形式主义，但它仍然缺乏所需的尾调用优化。优化将使我们能够超过默认的递归限制1,000，并且比这种原始递归快得多。
- en: The use of `Callable[[Any],`` Any]` is a weak type hint. To be more clear, it
    can help to define a domain type variable and a range type variable. We’ll include
    this detail in the optimized example.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Callable[[Any], Any]`是一种弱类型提示。为了更清楚，可以定义一个域类型变量和一个范围类型变量。我们将在优化示例中包含这个细节。
- en: 6.1.5 Tail-call optimization for collections
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.5 集合的尾调用优化
- en: 'We have two general ways to handle collections: we can use a higher-order function
    that returns a generator expression, or we can create a function that uses a `for`
    statement to process each item in a collection. These two patterns are very similar.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种处理集合的一般方法：我们可以使用返回生成器表达式的高阶函数，或者我们可以创建一个使用`for`语句处理集合中每个项的函数。这两种模式非常相似。
- en: 'Following is a higher-order function that behaves like the built-in `map()`
    function:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个类似于内置`map()`函数的高阶函数：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We’ve returned a generator expression that produces the required mapping. This
    uses the explicit `for` in the generator expression as a kind of tail-call optimization.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们返回了一个生成器表达式，它产生了所需的映射。这使用了生成器表达式中的显式`for`作为尾调用优化的一种形式。
- en: The source of data, `C`, has a type hint of `Iterable[DomT]` to emphasize that
    some type, `DomT`, will form the domain for the mapping. The transformation function
    has a hint of `Callable[[DomT],`` RngT]` to make it clear that it transforms from
    some domain type to a range type. The function `float()`, for example, can transform
    values from the string domain to the float range. The result has the hint of `Iterator[RngT]`
    to show that it iterates over the range type, `RngT`; the result type of the callable
    function.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 数据源`C`有一个类型提示`Iterable[DomT]`，以强调某些类型`DomT`将形成映射的域。转换函数有一个提示`Callable[[DomT],
    RngT]`，以使其明确地从某个域类型转换到范围类型。例如，`float()`函数可以将值从字符串域转换为浮点数范围。结果有一个提示`Iterator[RngT]`，以表明它遍历范围类型`RngT`；可调用函数的结果类型。
- en: 'The following is a generator function with the same signature and result:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个具有相同签名和结果的生成器函数：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This uses a complete `for` statement for the tail-call optimization. The results
    are identical. This version is slightly slower because it involves multiple statements.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用了完整的`for`语句进行尾调用优化。结果相同。这个版本稍微慢一些，因为它涉及多个语句。
- en: 'In both cases, the result is an iterator over the results. We must do something
    else to materialize a sequence object from an iterable source. For example, here
    is the `list()` function being used to create a sequence from the iterator:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，结果是对结果的一个迭代器。我们必须做些别的事情，才能从一个可迭代源中创建一个序列对象。例如，这里使用`list()`函数从迭代器创建序列：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For performance and scalability, this kind of tail-call optimization is required
    in Python programs. It makes the code less than purely functional. However, the
    benefit far outweighs the lack of purity. In order to reap the benefits of succinct
    and expressive functional design, it is helpful to treat these less-than-pure
    functions as if they were proper recursions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了性能和可扩展性，Python程序中需要这种尾调用优化。这使得代码不如纯函数。然而，好处远远超过了纯度的缺乏。为了获得简洁和表达性强的函数式设计的益处，将这些非纯函数视为适当的递归是有帮助的。
- en: What this means, pragmatically, is that we must avoid cluttering up a collection
    processing function with additional stateful processing. The central tenets of
    functional programming are still valid even if some elements of our programs are
    less than purely functional.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这在实用意义上意味着我们必须避免在集合处理函数中添加额外的状态化处理。即使我们程序的一些元素不是完全函数式的，函数式编程的核心原则仍然有效。
- en: 6.1.6 Using the assignment (sometimes called the ”walrus”) operator in recursions
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.6 在递归中使用赋值（有时称为“walrus”）运算符
- en: In some cases, recursions involve conditional processing that can be optimized
    using the ”walrus” or assignment operator, `:=`. The use of assignment means that
    we’re introducing stateful variables. If we’re careful of the scope of those variables,
    the possibility of terribly complex algorithms is reduced.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，递归涉及可以使用“walrus”或赋值运算符`:=`进行优化的条件处理。使用赋值意味着我们正在引入状态变量。如果我们小心这些变量的作用域，那么产生极其复杂算法的可能性就会降低。
- en: 'We reviewed the `fast_exp()` function shown below in the [Leaving recursion
    in place](#x1-1290002) section. This function used three separate cases to implement
    a divide and conquer strategy. In the case of raising a number, `a`, to an even
    power, we can use t = a^(![n 2](img/file53.jpg)) to compute t × t = a^n:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[保留递归](#x1-1290002)部分回顾了下面的`fast_exp()`函数。这个函数使用了三个不同的案例来实现分而治之的策略。在将数字`a`提升到偶数次幂的情况下，我们可以使用`t
    = a^![n 2](img/file53.jpg)`来计算`t × t = a^n`：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This uses the `:=` walrus operator to compute a partial answer, `fastexp_w(a,`` q)`,
    and save it into a temporary variable, `t`. This is used later in the same statement
    to compute `t`` *`` t`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用`:=` walrus运算符来计算部分答案`fastexp_w(a, q)`并将其保存到临时变量`t`中。这将在同一语句的稍后部分用于计算`t *
    t`。
- en: For the most part, when we perform tail-call optimization on a recursion, the
    body of the `for` statement will have ordinary assignment statements. It isn’t
    often necessary to exploit the walrus operator.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于递归的大部分情况，当我们对递归进行尾调用优化时，`for`语句的主体将包含普通赋值语句。通常没有必要利用walrus运算符。
- en: The assignment operator is often used in situations like regular expression
    matching, where we want to save the match object as well as make a decision. It’s
    very common to see `if`` (match`` :=`` pattern.match(text)):` as a way to both
    attempt a regular expression match, save the resulting match object, and confirm
    it’s not a `None` object.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 赋值运算符常用于正则表达式匹配等场景，我们希望保存匹配对象并做出决策。`if(match := pattern.match(text)):`作为尝试正则表达式匹配、保存结果匹配对象并确认它不是`None`对象的一种常见方式。
- en: 6.2 Reductions and folding a collection from many items to one
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 从多个项目折叠集合到单个项目
- en: 'We can consider the `sum()` function to have the following kind of definition.
    We could say that the sum of a collection is 0 for an empty collection. For a
    non-empty collection, the sum is the first element plus the sum of the remaining
    elements:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`sum()`函数考虑为以下类型的定义。我们可以说，对于空集合，集合的和为0。对于非空集合，和是第一个元素加上剩余元素的和：
- en: '![ (| { 0 if n = 0 sum ([c0,c1,c2,...,cn]) = | ( c0 + sum ([c1,c2,...,cn])
    if n > 0 ](img/file54.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![ (| { 0 if n = 0 sum ([c0,c1,c2,...,cn]) = | ( c0 + sum ([c1,c2,...,cn])
    if n > 0 ](img/file54.jpg)'
- en: 'We can use a slightly simplified notation called the Bird-Meertens Formalism.
    This uses ⊕∕[c[0],c[1],...c[n]] to show how some arbitrary binary operator, ⊕,
    can be applied to a sequence of values. It’s used as follows to summarize a recursive
    definition into something a little easier to work with:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一种稍微简化的符号，称为Bird-Meertens形式主义。它使用⊕∕[c[0],c[1],...c[n]]来显示某些任意二元运算符⊕如何应用于一系列值。它如下所示，将递归定义总结为更容易处理的东西：
- en: '![sum ([c0,c1,c2,...,cn]) = + ∕[c0,c1,c2,...,cn] = 0+ c0 + c1 + ...+ cn ](img/file55.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![sum ([c0,c1,c2,...,cn]) = + ∕[c0,c1,c2,...,cn] = 0+ c0 + c1 + ...+ cn ](img/file55.jpg)'
- en: We’ve effectively folded the + operator between each item of the sequence. Implicitly,
    the processing will be done left to right. This could be called a ”fold left”
    way of reducing a collection to a single value. We could also imagine grouping
    the operators from right to left, calling this a ”fold right.” While some compiled
    languages will perform this optimization, Python works strictly from left to right
    when given a sequence of similar precedence operators.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有效地将序列中每个项目之间的加法运算符折叠起来。隐式地，处理将按从左到右的顺序进行。这可以称为将集合折叠为单个值的“fold left”方式。我们也可以想象从右到左分组运算符，称之为“fold
    right”。虽然一些编译型语言会执行这种优化，但Python在给定一系列具有相同优先级的运算符时，会严格从左到右工作。
- en: 'In Python, a product function can be defined recursively as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，乘积函数可以递归地定义为以下内容：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is a tiny rewrite from a mathematical notation to Python. However, it is
    less than optimal because all of the slices will create a large number of intermediate
    list objects. It’s also limited to only working with explicit collections; it
    can’t work easily with iterable objects.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对数学符号到Python的微小重写。然而，它并不理想，因为所有的切片都会创建大量中间列表对象。它也仅限于与显式集合一起工作；它不能容易地与可迭代对象一起工作。
- en: 'We can revise this slightly to work with an iterable, which avoids creating
    any intermediate collection objects. The following is a properly recursive product
    function that works with any iterator as a source of data:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以稍作修改以适应可迭代对象，这样可以避免创建任何中间集合对象。以下是一个正确递归的乘积函数，它可以与任何迭代器作为数据源一起工作：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This doesn’t work with iterable collections. We can’t interrogate an iterator
    with the `len()` function to see how many elements it has. All we can do is attempt
    to extract the head of the iterator. If there are no items in the iterator, then
    any attempt to get the head will raise the `StopIteration` exception. If there
    is an item, then we can multiply this item by the product of the remaining items
    in the sequence.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这不适用于可迭代集合。我们无法使用`len()`函数来查询迭代器有多少元素。我们所能做的就是尝试提取迭代器的头部。如果没有元素在迭代器中，那么任何获取头部尝试都会引发`StopIteration`异常。如果有元素，那么我们可以将这个元素乘以序列中剩余元素的乘积。
- en: 'Note that we must explicitly create an iterator from a materialized sequence
    object, using the `iter()` function. In other contexts, we might have an iterable
    result that we can use. Following is an example:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们必须显式地使用`iter()`函数从一个具体化的序列对象创建一个迭代器。在其他上下文中，我们可能有一个可迭代的输出结果可以使用。以下是一个示例：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This recursive definition does not rely on explicit state or other imperative
    features of Python. While it’s more purely functional, it is still limited to
    working with collections of under 1,000 items. (While we can extend the stack
    size, it’s far better to optimize this properly.) Pragmatically, we can use the
    following kind of imperative structure for reduction functions:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个递归定义不依赖于显式状态或Python的其他命令式特性。虽然它更纯粹是函数式的，但它仍然限制在处理小于1,000个项目的集合。（虽然我们可以扩展栈的大小，但正确优化这一点要好得多。）从实用主义的角度来看，我们可以使用以下类型的命令式结构来处理归约函数：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This avoids any recursion limits. It includes the required tail-call optimization.
    Furthermore, this will work equally well with any iterable. This means a `Sequence`
    object, or an iterator.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这避免了任何递归限制。它包括所需的尾调用优化。此外，这将以相同的方式与任何可迭代对象一起工作。这意味着一个`Sequence`对象，或者一个迭代器。
- en: 6.2.1 Tail-call optimization using deques
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 使用双端队列进行尾调用优化
- en: The heart of recursion is a stack of function calls. Evaluating `fact(5)`, for
    example, is `5*fact(4)`. The value of `fact(4)` is `5*fact(3)`. There is a stack
    of pending computations until `fact(0)` has a value of 1\. Then the stack of computations
    is completed, revealing the final result.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 递归的核心是函数调用的栈。例如，评估`fact(5)`是`5*fact(4)`。`fact(4)`的值是`5*fact(3)`。直到`fact(0)`的值为1，才会有一系列待处理的计算。然后计算栈完成，揭示最终结果。
- en: Python manages the stack of calls for us. It imposes an arbitrary default limit
    of 1,000 calls on the stack, to prevent a program with a bug in the recursion
    from running forever.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Python为我们管理调用栈。它对栈强加了一个任意默认限制，即1,000次调用，以防止具有递归错误的程序无限期地运行。
- en: We can manage the stack manually, also. This gives us another way to optimize
    recursions. We can—explicitly—create a stack of pending work. We can then do a
    final summarization of the pending work, emptying the items from the stack.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以手动管理栈。这为我们提供了优化递归的另一种方法。我们可以——明确地——创建一个待处理工作的栈。然后我们可以对待处理工作进行最终总结，从栈中清空项目。
- en: For something as simple as computing a factorial value, the stacking and unstacking
    can seem like needless overhead. For more complex applications, like examining
    the hierarchical file system, it seems more appropriate to mix processing files
    with putting directories onto a stack for later consideration.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像计算阶乘值这样简单的事情，堆栈的压入和弹出可能看起来像是无用的开销。对于更复杂的应用，如检查分层文件系统，将文件处理与将目录放入堆栈以供以后考虑混合起来似乎更合适。
- en: We need a function to traverse a directory hierarchy without an explicit recursion.
    The core concept is that a directory is a collection of entries, and each entry
    is either a file, a sub-directory, or some other filesystem object we don’t want
    to touch (e.g., a mount point, symbolic link, etc.).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个函数来遍历目录层次结构而不使用显式递归。核心概念是目录是一系列条目，每个条目要么是一个文件，要么是一个子目录，或者是我们不想接触的其他文件系统对象（例如，挂载点、符号链接等）。
- en: 'We can say a node in the directory tree is a collection of entries: N = e[0],e[1],e[2],...,e[n].
    Each entry is either another directory, e ∈𝔻, or a file, e ∈𝔽.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说目录树中的一个节点是一系列条目：N = e[0],e[1],e[2],...,e[n]。每个条目要么是另一个目录，e ∈𝔻，要么是一个文件，e
    ∈𝔽。
- en: We can perform mappings on each file in the tree to process each file’s content.
    We might perform a filter operation to create an iterator over files with a specific
    property. We can also perform a reduction to count the number of files with a
    property. In this example, we’ll count the occurrences of a specific substring
    throughout the contents of files in a directory tree.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在树中的每个文件上执行映射以处理每个文件的内容。我们可能执行一个过滤操作来创建具有特定属性的文件迭代器。我们还可以执行归约操作来计算具有属性的文件数量。在这个例子中，我们将计算目录树中文件内容中特定子字符串的出现次数。
- en: 'Formally, we want a function p(f) that will provide the count of `"print"`
    in a node of the directory tree. It could be defined like this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，我们希望有一个函数p(f)，它将为目录树节点中的“打印”提供计数。它可以定义如下：
- en: '![ ( |{|“print” ∈ N | if N ∈ 𝔽 p(N ) = ∑ |( e∈N p(e) if N ∈ 𝔻 ](img/file56.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![ ( |{|“打印” ∈ N | 如果 N ∈ 𝔽 p(N ) = ∑ |( e∈N p(e) if N ∈ 𝔻 ](img/file56.jpg)'
- en: This shows how to apply the p(N) function to each element of a directory tree.
    When the element is a file, e ∈𝔽, we can count instances of ”print”. When the
    element is a directory, e ∈𝔻, we need to apply the p(N) function recursively to
    each entry, e[x], in the directory. While directory trees can’t be deep enough
    to break Python’s stack size limit, this kind of algorithm reveals an alternative
    tail-call optimization. It is an opportunity to use an explicit stack.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了如何将p(N)函数应用于目录树中的每个元素。当元素是文件，e ∈𝔽，时，我们可以计算“打印”的实例。当元素是目录，e ∈𝔻，时，我们需要递归地应用p(N)函数到目录中的每个条目，e[x]。虽然目录树可能不够深以打破Python的栈大小限制，但这种算法揭示了尾调用优化的替代方案。这是一个使用显式栈的机会。
- en: The `collections.deque` class is a marvelous way to build stacks and queues.
    The name comes from ”double-ended queue,” sometimes spelled dequeue. The data
    structure can be used as either a last-in-first-out (LIFO) stack or a first-in-first-out
    (FIFO). In this example, we use the `append()` and `pop()` methods, which enforce
    LIFO stack behavior. While this is much like a list, there are some optimizations
    in the `deque` implementation that can make it slightly faster than the generic
    list.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`collections.deque`类是构建栈和队列的奇妙方式。这个名字来自“双端队列”，有时拼写为dequeue。这种数据结构可以用作后进先出（LIFO）栈或先进先出（FIFO）。在这个例子中，我们使用`append()`和`pop()`方法，这些方法强制执行LIFO栈行为。虽然这很像列表，但在`deque`实现中还有一些优化，可以使其比通用列表略快。'
- en: 'Using a stack data structure lets us work with a hierarchy of indefinite size
    without running into Python’s internal stack depth limitation and raising `RecursionError`
    exceptions. The following function will traverse a file hierarchy looking at Python
    source files (with a suffix of `.py`):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用栈数据结构让我们能够在不遇到Python内部栈深度限制并引发`RecursionError`异常的情况下处理不定大小的层次结构。以下函数将遍历文件层次结构，查看Python源文件（后缀为`.py`）：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We seeded the stack of pending tasks with the initial directory. The essential
    algorithm is to unstack a directory and visit each entry in the directory. For
    entries that are files with the proper suffix, the processing is performed: counting
    the occurrences of ”print”. For entries that are directories, the directory is
    put into the stack as a pending task. Note that directories with a leading dot
    in their name need to be ignored. For the code in this book, those directories
    include caches used by tools like mypy, pytest, and tox. We want to skip over
    those cache directories.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用初始目录填充了待处理任务的栈。基本算法是从栈中弹出目录并访问目录中的每个条目。对于具有正确后缀的文件条目，执行处理：计算“打印”的出现次数。对于目录条目，将目录作为待处理任务放入栈中。注意，名称中带有点的目录需要被忽略。对于本书中的代码，这些目录包括mypy、pytest和tox等工具使用的缓存。我们希望跳过这些缓存目录。
- en: The processing performed on each file is part of the `all_print()` function.
    This can be refactored as a separate function that’s applied to each node as part
    of a reduction. Rewriting the `all_print()` function to be a proper higher-order
    function is left as an exercise for the reader.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is we have two strategies for transforming a formal recursion
    into a usefully optimized function. We can reframe the recursion into an iteration,
    or we can introduce an explicit stack.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will apply the idea of a reduction (and the associated
    tail-call optimizations) to creating groups of items and computing a reduction
    for the groups.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Group-by reduction from many items to fewer
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea of a reduction can apply in many ways. We’ve looked at the essential
    recursive definition of a reduction that produces a single value from a collection
    of values. This leads us to optimizing the recursion so we have the ability to
    compute summaries without the overheads of a naive Pythonic implementation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Creating subgroups in Python isn’t difficult, but it can help to understand
    the formalisms that support it. This understanding can help to avoid implementations
    that perform extremely poorly.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: A very common operation is a reduction that groups values by some key or indicator.
    The raw data is grouped by some column’s value, and reductions (sometimes called
    aggregate functions) are applied to other columns.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: In SQL, this is often called the `GROUP`` BY` clause of the `SELECT` statement.
    The SQL aggregate functions include `SUM`, `COUNT`, `MAX`, and `MIN`, and often
    many more.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Python offers us several ways to group data before computing a reduction of
    the grouped values. We’ll start by looking at two ways to get simple counts of
    grouped data. Then we’ll look at ways to compute different summaries of grouped
    data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the trip data that we computed in [Chapter 4](Chapter_04.xhtml#x1-740004),
    [Working with Collections](Chapter_04.xhtml#x1-740004). This data started as a
    sequence of latitude-longitude waypoints. We restructured it to create legs represented
    by three-tuples of start, end, and distance for each leg. The data looks as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We’d like to know the most common distance. Since the data is real-valued, and
    continuous, each distance is a unique value. We need to constrain these values
    from the continuous domain to a discrete set of distances. For example, quantizing
    each leg to the nearest multiple of five nautical miles. This creates bands of
    0 to 5 miles, over 5 to 10 miles, etc. Once we’ve created discrete integer values,
    we can count the number of legs in each of these bands.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'These quantized distances can be produced with a generator expression:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This will divide each distance by 5—discarding any fractions—then multiply the
    truncated result by 5 to compute a number that represents the distance rounded
    down to the nearest 5 nautical miles.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t use the values assigned to the `start` and `stop` variables. It’s
    common practice to assign these values to the `_` variable. This can lead to some
    confusion because this can obscure the structure of the triple. It would look
    like this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This approach can be helpful for removing some visual clutter.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Building a mapping with Counter
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A mapping like the `collections.Counter` class is a great optimization for
    doing reductions that create counts (or totals) grouped by some value in the collection.
    The following expression creates a mapping from distance to frequency:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The resulting `summary` object is stateful; it can be updated. The expression
    to create the groups, `Counter()`, looks like a function, making it a good fit
    for a design based on functional programming ideas.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'If we print the `summary.most_common()` value, we’ll see the following results:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The most common distance was about 30 nautical miles. We can also apply functions
    like `min()` and `max()` to find the shortest recorded and longest legs as well.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that your output may vary slightly from what’s shown. The results of the
    `most_common()` function are in order of frequency; equal-frequency bins may be
    in any order. These five lengths may not always be in the order shown:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This slight variability makes testing with the doctest tool a little bit more
    complex. One helpful trick for testing with counters is to use a dictionary to
    validate the results in general; the comparison between actual and expected no
    longer relies on the vagaries of internal hash computations.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Building a mapping by sorting
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An alternative to `Counter` is to sort the original collection, and then use
    a recursive loop to identify when each group begins. This involves materializing
    the raw data, performing a sort that could—at worst—do O(nlog n) operations, and
    then doing a reduction to get the sums or counts for each key.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: In order to work in a general way with Python objects that can be sorted, we
    need to define the protocol required for sorting. We’ll call the protocol `SupportsRichComparisonT`
    because we can sort any kinds of objects that implement the rich comparison operators,
    `<` and `>`. This isn’t a particular class of objects; it’s a protocol that any
    number of classes might implement. We formalize the idea of a protocol that classes
    must support using the `typing.Protocol` type definition. It could be also be
    called an interface that a class must implement. Python’s flexibility stems from
    having a fairly large number of protocols that many different classes support.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a common algorithm for creating groups from sorted data:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The internal `group()` function steps through the sorted sequence of legs.
    If a given item key has already been seen—it matches the value in `previous`—then
    the `counter` variable is incremented. If a given item does not match the previous
    value, then there’s been a change in value: emit the previous value and the count,
    and begin a new accumulation of counts for the new value.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The definition of `group()` provides two important type hints. The source data
    is an iterable over some type, shown with the type variable `SupportsRichComparisonT`.
    In this specific case, it’s pretty clear that the values in use will be of type
    `int`; however, the algorithm will work for any Python type. The resulting iterable
    from the `group()` function will preserve the type of the source data, and this
    is made explicit by using the same type variable, `SupportsRichComparisonT`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: The final line of the `group_sort()` function creates a dictionary from the
    grouped items. This dictionary will be similar to a `Counter` dictionary. The
    primary difference is that a `Counter()` function will have a `most_common()`
    method function, which a default dictionary lacks.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: We can also do this with `itertools.groupby()`. We’ll look at this function
    closely in [Chapter 8](Chapter_08.xhtml#x1-1700008), [The Itertools Module](Chapter_08.xhtml#x1-1700008).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3 Grouping or partitioning data by key values
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are no limits to the kinds of reductions we might want to apply to grouped
    data. We might have data with a number of independent and dependent variables.
    We can consider partitioning the data by an independent variable and computing
    summaries such as the maximum, minimum, average, and standard deviation of the
    values in each partition.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The essential trick to doing more sophisticated reductions is to collect all
    of the data values into each group. The `Counter()` function merely collects counts
    of identical items. For deeper analysis, we want to create sequences of the original
    members of the group.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at our trip data, each five-mile bin could contain the entire collection
    of legs of that distance, not merely a count of the legs. We can consider the
    partitioning as a recursion or as a stateful application of `defaultdict(list)`
    objects. We’ll look at the recursive definition of a `groupby()` function, since
    it’s easy to design.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, the `groupby(C,`` key)` computation for an empty collection, `[]`,
    is the empty dictionary, `dict()`. Or, more usefully, the empty `defaultdict(list)`
    object.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'For a non-empty collection, we need to work with item `C[0]`, the head, and
    recursively process sequence `C[1:]`, the tail. We can use slice expressions,
    or we can use the `head,` `*tail`` =`` C` statement to do this parsing of the
    collection, as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If we have a `defaultdict` object named `groups`, we need to use the expression
    `groups[key(head)].append(head)` to include the head element in the `groups` dictionary.
    After this, we need to evaluate the `groupby(tail,`` key)` expression to process
    the remaining elements.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a function as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The interior function `group_into()` handles the essential recursive definition.
    An empty value for `collection` returns the provided dictionary, `group_dict`.
    A non-empty collection is partitioned into a head and tail. The head is used to
    update the `group_dict` dictionary. The tail is then used, recursively, to update
    the dictionary with all remaining elements.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: The type hints make an explicit distinction between the type of the source objects
    `SeqItemT` and the type of the key `ItemKeyT`. The function provided as the `key`
    parameter must be a callable that returns a value of the key type `ItemKeyT`,
    given an object of the source type `SeqItemT`. In many of the examples, a function
    to extract the distance from a `Leg` object will be be shown. This is a `Callable[[SeqItemT],`` ItemKeyT]`
    where the source type `SeqItemT` is the `Leg` object and the key type `ItemKeyT`
    is the float value.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '`bound=Hashable` is an additional constraint. This defines an ”upper bound”
    on the possible types, alerting mypy that any type that could be assigned to this
    type variable must implement the protocol for `Hashable`. The essential, immutable
    Python types of numbers, strings, and tuples all meet this bound. A mutable object
    like a dictionary, set, or list, will not meet the upper bound, leading to warnings
    from mypy.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'We can’t easily use Python’s default values to collapse this into a single
    function. We explicitly cannot use the following incorrect command snippet:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If we try this, all uses of the `group_by()` function share one common `defaultdict(list)`
    object. This does not work because Python builds the default value just once.
    Mutable objects as default values rarely do what we want. The common practice
    is to provide a `None` value, and use an explicit `if` statement to create each
    unique, empty instance of `defaultdict(list)` as needed. We’ve shown how to use
    a wrapper function definition to avoid the `if` statement.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'We can group the data by distance as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We’ve defined a reusable lambda that puts our distances into bins, each of which
    is 5 nautical miles in size. We then grouped the data using the provided lambda.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine the binned data as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following is what the output looks like:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Having looked at a recursive definition, we can turn to looking at making a
    tail-call optimization to build a group-by algorithm using iteration. This will
    work with larger collections of data, because it can exceed the internal stack
    size limitation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with doing tail-call optimization on the `group_into()` function.
    We’ll rename this to `partition()` because partitioning is another way of looking
    at grouping.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'The `partition()` function can be written as an iteration as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: When doing the tail-call optimization, the essential line of the code in the
    imperative version will match the recursive definition. We’ve put a comment under
    the changed line to emphasize the rewrite is intended to have the same outcome.
    The rest of the structure represents the tail-call optimization we’ve adopted
    as a common way to work around the Python limitations.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: The type hints emphasize the distinction between the source type `SeqT` and
    the key type `KeyT`. The source data can be anything, but the keys are limited
    to types that have proper hash values.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.4 Writing more general group-by reductions
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once we have partitioned the raw data, we can compute various kinds of reductions
    on the data elements in each partition. We might, for example, want the northernmost
    point for the start of each leg in the distance bins.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll introduce some helper functions to decompose the tuple as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Each of these helper functions expects a tuple object to be provided using the
    `*` operator to map each element of the tuple to a separate parameter of the lambda.
    Once the tuple is expanded into the `s`, `e`, and `p` parameters, it’s reasonably
    obvious to return the proper parameter by name. It’s much clearer than trying
    to interpret the `tuple_arg[2]` value.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is how we use these helper functions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Our initial `point` object is a nested three tuple with (0)—a starting position,
    (1)—the ending position, and (2)—the distance. We extracted various fields using
    our helper functions.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Given these helpers, we can locate the northernmost starting position for the
    legs in each bin:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The data that we grouped by distance included each leg of the given distance.
    We supplied all of the legs in each bin to the `max()` function. The `key` function
    we provided to the `max()` function extracted just the latitude of the starting
    point of the leg.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us a short list of the northernmost legs of each distance, as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 6.3.5 Writing higher-order reductions
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ll look at an example of a higher-order reduction algorithm here. This will
    introduce a rather complex topic. The simplest kind of reduction develops a single
    value from a collection of values. Python has a number of built-in reductions,
    including `any()`, `all()`, `max()`, `min()`, `sum()`, and `len()`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'As we noted in [Chapter 4](Chapter_04.xhtml#x1-740004), [Working with Collections](Chapter_04.xhtml#x1-740004),
    we can do a great deal of statistical calculation if we start with a few reductions
    such as the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This allows us to define mean, standard deviation, normalized values, correction,
    and even least-squares linear regression, building on these base reduction functions.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'The last of our reductions, `sum_x2()`, shows how we can apply existing reductions
    to create higher-order functions. We might change our approach to be more like
    the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We’ve added a function, `function()`, as a parameter; the function can transform
    the data. This overall function, `sum_f()`, computes the sum of the transformed
    values.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can apply this function in three different ways to compute the three
    essential sums as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We’ve plugged in a small lambda to compute ∑ [x∈X]x⁰ = ∑ [x∈X]1, which is the
    count, ∑ [x∈X]x¹ = ∑ [x∈X]x, the sum, and ∑ [x∈X]x², the sum of the squares, which
    we can use to compute standard deviation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'A common extension to this includes a filter to reject raw data that is unknown
    or unsuitable in some way. We might use the following function to reject bad data:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following function definition for computing a mean will reject `None` values
    in a simple way:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This shows how we can provide two distinct combinations of lambdas to our `sum_filter_f()`
    function. The filter argument is a lambda that rejects `None` values; we’ve called
    it `valid` to emphasize its meaning. The function argument is a lambda that implements
    a count or a sum operation. We can easily add a lambda to compute a sum of squares.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: The reuse of a common `valid` rule assures that the various computations are
    all identical in applying any filters to the source data. This can be combined
    with a user-selected filter criteria to provide a tidy plug-in to compute a number
    of statistics related to a user’s requested subset of the data.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.6 Writing file parsers
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can often consider a file parser to be a kind of reduction. Many languages
    have two levels of definition: the lower-level tokens in the language and the
    higher-level structures built from those tokens. When looking at an XML file,
    the tags, tag names, and attribute names form this lower-level syntax; the structures
    which are described by XML form a higher-level syntax.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'The lower-level lexical scanning is a kind of reduction that takes individual
    characters and groups them into tokens. This fits well with Python’s generator
    function design pattern. We can often write functions that look as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: For well-known file formats, we’ll use existing file parsers. For data in CSV,
    JSON, XML, or TOML format, we don’t need to write file parsers. Most of these
    modules have a `load()` method that produces useful Python objects.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, we’ll need to combine the results of this parsing into higher-level
    objects, useful for our specific application. While the CSV parser provides individual
    rows, these might need to be used to create `NamedTuple` instances, or perhaps
    some other class of immutable Python objects. Our examples of trip data, starting
    in [Chapter 4](Chapter_04.xhtml#x1-740004), [Working with Collections](Chapter_04.xhtml#x1-740004),
    are combined into higher-level objects, legs of a journey, by an algorithm that
    combines waypoints into pairs. When we introduce more complex decision-making,
    we make a transition from restructuring into parsing.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to provide useful waypoints in the first place, we needed to parse
    a source file. In these examples, the input was a KML file; KML is an XML representation
    of geographic information. The essential features of the parser look similar to
    the following definition:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The bulk of the `row_iter_kml()` function is the XML parsing that allows us
    to use the `doc.findall()` function to iterate through the `<ns0:coordinates>`
    tags in the document. We’ve used a function named `comma_split()` to parse the
    text of this tag into a three-tuple of values.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The `cast()` function is only present to provide evidence to mypy that the value
    of `coordinates.text` is a `str` object. The default definition of the text attribute
    is `Union[str,` `bytes]`; in this application, the data will be `str` exclusively.
    The `cast()` function doesn’t do any runtime processing.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'This function focused on working with the normalized XML structure. The document
    is close to the database designer’s definitions of first normal form: each attribute
    is atomic (a single value), and each row in the XML data has the same columns
    with data of a consistent type. The data values aren’t fully atomic, however:
    we have to split the points on the , to separate longitude, latitude, and altitude
    into atomic string values. However, the text value for these XML tags is internally
    consistent, making it a close fit with first normal form.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: A large volume of data—XML tags, attributes, and other punctuation—is reduced
    to a somewhat smaller volume, including just floating-point latitude and longitude
    values. For this reason, we can think of parsers as a kind of reduction.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll need a higher-level set of conversions to map the tuples of text into
    floating-point numbers. Also, we’d like to discard altitude, and reorder longitude
    and latitude. This will produce the application-specific tuple we need. We can
    use functions as follows for this conversion:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The essential tool is the `float_lat_lon()` function. This is a higher-order
    function that returns a generator expression. The generator uses the `map()` function
    to apply the `float()` function conversion to the results of the `pick_lat_lon()`
    function, and the `*row` argument to assign each member of the row tuple to a
    different parameter of the `pick_lat_lon()` function. This only works when each
    row is a three-tuple. The `pick_lat_lon()` function then returns a two-tuple of
    the selected items in the required order.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'The source includes XML that looks like this:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We can use this parser as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This will build a tuple-of-tuples representation of each waypoint along the
    path in the original KML file. The result will be a flat sequence of pairs that
    looks like this:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The `float_lat_lon()` function uses a low-level XML parser to extract rows of
    text data from the original representation. It uses a higher-level parser to transform
    the text items into more useful tuples of floating-point values suitable for the
    target application.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Parsing CSV files
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In [Chapter 3](Chapter_03.xhtml#x1-510003), [Functions, Iterators, and Generators](Chapter_03.xhtml#x1-510003),
    we saw another example where we parsed a CSV file that was not in a normalized
    form: we had to discard header rows to make it useful. To do this, we used a function
    that extracted the header and returned an iterator over the remaining rows.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'The data looks as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The columns are separated by tab characters. Plus, there are three rows of headers
    that we can discard.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s another version of that CSV-based parser. We’ve broken it into three
    functions. The first, `row_iter_csv()` function, returns the iterator over the
    rows in a tab-delimited file. The function looks as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This is a small wrapper around the CSV parsing process. When we look back at
    the previous parsers for XML and plain text, this was the kind of thing that was
    missing from those parsers. Producing an iterable over row tuples can be a common
    feature of parsers for normalized data.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a row of tuples, we can pass rows that contain usable data and
    reject rows that contain other metadata, such as titles and column names. We’ll
    introduce a helper function that we can use to do some of the parsing, plus a
    `filter()` function to validate a row of data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the conversion:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This function handles the conversion of a single string to float values, converting
    bad data to a `None` value. The type hint of `float`` |`` None` expresses the
    idea of having a value of the given type or having a value of the same type as
    `None`. This can also be stated as `Union[float,`` None]` to show how the result
    is a union of different alternative types.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'We can embed the `float_none()` function in a mapping so that we convert all
    columns of a row to a float or `None` value. A lambda for this looks as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Two type hints are used to make the definition of the `float_row()` function
    explicit. The `R_Float` hint defines the floating-point version of a row of data
    that may include `None` values.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a row-level validator based on the use of the `all()` function
    to ensure that all values are `float` (or none of the values are `None`):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This lambda is a kind of reduction, transforming a row of floating-point values
    to a Boolean value if all values are not ”falsy” (that is, neither `None` nor
    zero) and there are exactly eight values.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The simplistic `all_numeric()` function conflates zero and `None`. A more sophisticated
    test would rely on something such as `not`` any(item`` is`` None`` for`` item`` in`` row)`.
    The rewrite is left as an exercise for the reader.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: The essential design is to create row-based elements that can be combined to
    create more complete algorithms for parsing an input file. The foundational functions
    iterate over tuples of text. These are combined to convert and validate the converted
    data. For the cases where files are either in first normal form (all rows are
    the same) or a simple validator can reject the extraneous rows, this design pattern
    works out nicely.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: All parsing problems aren’t quite this simple, however. Some files have important
    data in header or trailer rows that must be preserved, even though it doesn’t
    match the format of the rest of the file. These non-normalized files will require
    a more sophisticated parser design.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Parsing plain text files with headers
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In [Chapter 3](Chapter_03.xhtml#x1-510003), [Functions, Iterators, and Generators](Chapter_03.xhtml#x1-510003),
    the `Crayola.GPL` file was presented without showing the parser. This file looks
    as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We can parse a text file using regular expressions. We need to use a filter
    to read (and parse) header rows. We also want to return an iterable sequence of
    data rows. This rather complex two-part parsing is based entirely on the two-part—head
    and tail—file structure.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a low-level parser that handles both the four lines of the header
    and the long tail:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The `Head_Body` type definition summarizes the overall goal of the row iterator.
    The result is a two-tuple. The first item is a two-tuple with details from the
    file header. The second item is an iterator that provides the text items for a
    color definition. This `Head_Body` type hint is used in two places in this function
    definition.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: The `header_pat` regular expression parses all four lines of the header. There
    are instances of `()` in the expression to extract the name and column information
    from the header.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: There are two internal functions for parsing different parts of the file. The
    `read_head()` function parses the header lines and returns interesting text and
    a `TextIO` object that can be used for the rest of the parsing. It does this by
    reading four lines and merging them into a single long string. This is then parsed
    with the `header_pat` regular expression.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The idea of returning the iterator from one function to be used in another function
    is a pattern for passing an explicitly stateful object from one function to another.
    It seems helpful to make sure all of the arguments for the `read_tail()` function
    are the results from the `read_head()` function.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: The `read_tail()` function parses the iterator over the remaining lines. These
    lines are merely split on spaces, since that fits the description of the GPL file
    format.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, visit the following link: [https://code.google.com/p/grafx2/issues/detail?id=518](https://code.google.com/p/grafx2/issues/detail?id=518).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve transformed each line of the file into a canonical tuple-of-strings
    format, we can apply the higher level of parsing to this data. This involves conversion
    and (if necessary) validation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a higher-level parser command snippet:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This function will work with the output of the lower-level `row_iter_gpl()`
    parser: it requires the headers and the iterator over individual rows. This function
    will use the multiple assignment feature of the `for` clause in the generator
    to separate the color numbers and the remaining words into four variables, `r`,
    `g`, `b`, and `name`. The use of the `*name` parameter ensures that all remaining
    values will be assigned to the `name` variable as a tuple. The `"`` ".join(name)`
    expression then concatenates the words into a single space-separated string.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is how we can use this two-tier parser:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We’ve applied the higher-level parser to the results of the lower-level parser.
    This will return the headers and a tuple built from the sequence of `Color` objects.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Summary
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we’ve looked at two significant functional programming topics.
    We’ve looked at recursions in some detail. Many functional programming language
    compilers will optimize a recursive function to transform a call in the tail of
    the function to a loop. This is sometimes called tail recursion elimination. More
    commonly, it’s known as tail-call optimization. In Python, we must do the tail-call
    optimization manually by using an explicit `for` statement, replacing a purely
    functional recursion.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also looked at reduction algorithms, including `sum()`, `count()`, `max()`,
    and `min()` functions. We looked at the `collections.Counter()` function and related
    `groupby()` reductions.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also looked at how parsing (and lexical scanning) are similar to reductions
    since they transform sequences of tokens (or sequences of characters) into higher-order
    collections with more complex properties. We’ve examined a design pattern that
    decomposes parsing into a lower level and tries to produce tuples of raw strings,
    and a higher level that creates more useful application objects.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll look at some techniques appropriate to working with
    named tuples and other immutable data structures. We’ll look at techniques that
    make stateful objects unnecessary. While stateful objects aren’t purely functional,
    the idea of a class hierarchy can be used to package related method definitions.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Exercises
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter’s exercises are based on code available from Packt Publishing on
    GitHub. See [https://github.com/PacktPublishing/Functional-Python-Programming-3rd-Edition](https://github.com/PacktPublishing/Functional-Python-Programming-3rd-Edition).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the reader will notice that the code provided on GitHub includes
    partial solutions to some of the exercises. These serve as hints, allowing the
    reader to explore alternative solutions.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, exercises will need unit test cases to confirm they actually
    solve the problem. These are often identical to the unit test cases already provided
    in the GitHub repository. The reader should replace the book’s example function
    name with their own solution to confirm that it works.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1 Multiple recursion and caching
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Handling difficult tail-call optimization](#x1-1300003), we looked at a
    naive definition of a function to compute Fibonacci numbers, the `fib()` function.
    The `functools.cache` decorator can have a profound impact on the performance
    of this algorithm.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Implement both versions and describe the impact of caching on the time required
    to compute large Fibonacci numbers.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2 Refactor the all_print() function
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Tail-call optimization using deques](#x1-1350001), we showed a function
    that used a `collections.deque` to visit all nodes in a directory tree, summing
    the value for each node that is a proper file. This can be done with a list as
    well as a `deque`, with some minor code changes.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'This function embedded a specific computation. This computation (finding all
    occurrences of ”print”) really should have been a separate function. The body
    of the `all_print()` function should be refactored into two functions:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: A generic directory traverse that applies a function to each text file with
    the expected suffix and sums the results.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that counts instances of ”print” in a given Python file.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.5.3 Parsing CSV files
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: See the [Parsing CSV files](#x1-1430006) section, earlier in this chapter. In
    that example, the simplistic `all_numeric()` function conflates zero and `None`.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Create a test case for this function that will show that it does not handle
    zero correctly, treating it as `None`. Once the test case is defined, rewrite
    the `all_numeric()` function to distinguish between zero and `None`.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Note that it’s common practice in Python to use the `is` operator when comparing
    with `None`. This specifically avoids some subtle problems that can arise when
    a class has an implementation of `__eq__()` that doesn’t handle `None` as a properly
    distinct object.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.4 Classification of state, Part III
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: See [Chapter 5](Chapter_05.xhtml#x1-1000005), [Higher-Order Functions](Chapter_05.xhtml#x1-1000005),
    the [Classification of state](Chapter_05.xhtml#x1-1220001) exercise.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: There’s a third way to consume status details and summarize them.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Write a reduce computation. This starts with an initial state of Running. As
    each service’s three-tuple is folded into the result, there is a comparison between
    the state and the three-tuple. If the three-tuple has a non-responsive service,
    the state advances to Stopped. If the three-tuple has a slow or not working service,
    the state advances to Degraded. If no problems are found, the initial value becomes
    the final health of the overall system.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to provide a `status_add(previous,`` this_service)` function. This
    can be used in the context of `status`` =`` reduce(status_add,`` service_status_sequence,`` "Running")`
    to compute the current status of the sequence of services.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.5 Diesel engine data
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A diesel engine has had some repairs that raised doubts about the accuracy of
    the tachometer. After some heroic effort, the following table of data was collected
    showing the observed reading on the engine’s tachometer, and the actual RPMs measured
    with an optical device on the engine.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '| Sample | Tach | Engine |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1000 | 883 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1500 | 1242 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1500 | 1217 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1600 | 1306 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1750 | 1534 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| 6 | 2000 | 1805 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| 7 | 2000 | 1720 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: If needed, create a CSV file with the data. If you have access to the GitHub
    repository for this book, this is available in the `engine.csv` file.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Create a `NamedTuple` for each sample and write some functions to acquire this
    data in a useful form. Once the data is available, see the [Using sums and counts
    for statistics](Chapter_04.xhtml#x1-850001) section of [Chapter 4](Chapter_04.xhtml#x1-740004),
    [Working with Collections](Chapter_04.xhtml#x1-740004), for a definition of a
    correlation function.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to apply this correlation function to the engine and tach values
    to see if the values correlate. If they do, it suggests that the engine’s instruments
    can be recalibrated. If they don’t correlate, something else is wrong with the
    engine.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Note that the [Chapter 4](Chapter_04.xhtml#x1-740004), [Working with Collections](Chapter_04.xhtml#x1-740004),
    correlation example may have assumptions about data types that don’t necessarily
    apply to the `NamedTuple` defined earlier. If necessary, rewrite the type hints
    or your `NamedTuple` definition. Note that it can be difficult to write perfectly
    generic type hints, and it often takes a bit of work to resolve the differences.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Join our community Discord space
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Join our Python Discord workspace to discuss and know more about the book:
    [https://packt.link/dHrHU](https://packt.link/dHrHU)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
