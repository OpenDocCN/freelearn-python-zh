- en: '*Chapter 9*: Python Programming for the Cloud'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud computing is a broad term that is used for a wide variety of use cases.
    These use cases include an offering of physical or virtual compute platforms,
    software development platforms, big data processing platforms, storage, network
    functions, software services, and many more. In this chapter, we will explore
    Python for cloud computing from two correlated aspects. First, we will investigate
    the options of using Python for building applications for cloud runtimes. Then,
    we will extend our discussion of data-intensive processing, which we started in
    [*Chapter 8*](B17189_08_Final_PG_ePub.xhtml#_idTextAnchor227), *Scaling Out Python
    using Clusters*, from clusters to cloud environments. The focus of the discussion
    in this chapter will largely center on the three public cloud platforms; that
    is, **Google Cloud Platform** (**GCP**), **Amazon Web Services** (**AWS**), and
    **Microsoft Azure**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning about the cloud options for Python applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building Python web services for cloud deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Google Cloud Platform for data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will know how to develop and deploy applications
    to a cloud platform and how to use Apache Beam in general and for Google Cloud
    Platform.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to have Python 3.7 or later installed on your computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need a service account for Google Cloud Platform. A free account will
    work fine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need the Google Cloud SDK installed on your computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Apache Beam installed on your computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sample code for this chapter can be found at [https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter09](https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: We will start our discussion by looking at the cloud options that are available
    for developing applications for cloud deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about the cloud options for Python applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud computing is the ultimate frontier for programmers these days. In this
    section, we will investigate how we can develop Python applications using a cloud
    development environment or using a specific **Software Development Kit** (**SDK**)
    for cloud deployment, and then how we can execute the Python code in a cloud environment.
    We will also investigate options regarding data-intensive processing, such as
    Apache Spark on the cloud. We will start with the cloud-based development environments.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Python development environments for the cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to setting up a Python development environment for one of the
    three main public clouds, two types of models are available:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-native **Integrated Development Environment** (**IDE**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locally installed IDE with an integration option for the cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss these two modes next.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-native IDE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several cloud-native development environments available in general
    that are not specifically attached to the three public cloud providers. These
    include **PythonAnyWhere**, **Repl.it**, **Trinket**, and **Codeanywhere**. Most
    of these cloud environments offer a free license in addition to a paid one. These
    public cloud platforms offer a mixture of tools for development environments,
    as explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS**: This offers a sophisticated cloud IDE in the form of **AWS Cloud9**,
    which can be accessed through a web browser. This cloud IDE has a rich set of
    features for developers and the option of supporting several programming languages,
    including Python. It is important to understand that AWS Cloud9 is offered as
    an application hosted on an Amazon EC2 instance (a virtual machine). There is
    no direct fee for using AWS Cloud9, but there will be a fee for using the underlying
    Amazon EC2 instance and storage space, which is very nominal for limited use.
    The AWS platform also offers tools for building and testing the code for **continuous
    integration** (**CI**) and **continuous delivery** (**CD**) goals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS CodeBuild** is another service that''s available that compiles our source
    code, runs tests, and builds software packages for deployment. It is a build server
    similar to Bamboo. **AWS CodeStar** is commonly used with AWS Cloud9 and offers
    a projects-based platform to help develop, build, and deploy software. AWS CodeStar
    offers predefined project templates to define an entire continuous delivery toolchain
    until the code is released.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Microsoft Azure**: This comes with the **Visual Studio** IDE, which is available
    online (cloud-based) if you are part of the Azure DevOps platform. Online access
    to the Visual Studio IDE is based on a paid subscription. Visual Studio IDE is
    well-known for its rich features and capabilities for offering an environment
    for team-level collaboration. Microsoft Azure offers **Azure Pipelines** for building,
    testing, and deploying your code to any platform such as Azure, AWS, and GCP.
    Azure Pipelines support many languages, such as Node.js, Python, Java, PHP, Ruby,
    C/C++, and .NET, and even mobile development toolkits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google**: Google offers **Cloud Code** to write, test, and deploy the code
    that can be written either through your browser (such as via ASW Cloud9) or using
    a local IDE of your choice. Cloud Code comes with plugins for the most popular
    IDEs, such as IntelliJ IDE, Visual Studio Code, and JetBrains PyCharm. Google
    Cloud Code is available free of charge and is targeted at container runtime environments.
    Like AWS CodeBuild and Azure Pipelines, Google offers an equivalent service that
    is also called **Cloud Build** for the continuous building, testing, and deploying
    of software in multiple environments, such as virtual machines and containers.
    Google also offers Google **Colaboratory** or **Google Colab** that offers Jupyter
    Notebooks remotely. The Google Colab option is popular among data scientists'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud also offers **Tekton** and the **Jenkins** service for building
    CI/CD development and delivery models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition to all these dedicated tools and services, these cloud platforms
    offer online as well as locally installed shell environments. These shell environments
    are also a quick way to manage code in a limited capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss the local IDE options for using Python for the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Local IDE for cloud development
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The cloud-native development environment is a great tool for having native integration
    options in the rest of your cloud ecosystem. This makes instantiating on-demand
    resources and then deploying them convenient, and doesn't require any authentication
    tokens. But this comes with some caveats. First, although the tools are mostly
    free, the underlying resources that they are using are not. The second caveat
    is that the offline availability of these cloud-native tools is not seamless.
    Developers like to write code without any online ties so that they can do this
    anywhere, such as on a train or in a park.
  prefs: []
  type: TYPE_NORMAL
- en: Due to these caveats, developers like to use local editors or IDEs for developing
    and testing the software before using additional tools to deploy on one of the
    cloud platforms. Microsoft Azure IDEs such as Visual Studio and Visual Studio
    Code are available for local machines. AWS and Google platform offer their own
    SDKs (shell-like environments) and plugins to be integrated with your IDE of choice.
    We will explore these models of development later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss the runtime environments that are available on the public
    clouds.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing cloud runtime options for Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simplest way to get a Python runtime environment is to get a Linux virtual
    machine or a container with Python installed. Once we have a virtual machine or
    a container, we can also install the Python version of our choice. For data-intensive
    workloads, the Apache Spark cluster can be set up on the compute nodes of the
    cloud. But this requires us to own all platform-related tasks and maintenance
    in case anything goes wrong. Almost all public cloud platforms offer more elegant
    solutions to simplify developers' and IT administrators' lives. These cloud providers
    offer one or more pre-built runtime environments based on the application types.
    We will discuss a few of the runtime environments that are available from the
    three public cloud providers – Amazon AWS, GCP, and Microsoft Azure.
  prefs: []
  type: TYPE_NORMAL
- en: What is a runtime environment?
  prefs: []
  type: TYPE_NORMAL
- en: A runtime environment is an execution platform that runs Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Runtime options offered by Amazon AWS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Amazon AWS offers the following runtime options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Beanstalk**: This **Platform-as-a-Service** (**PAAS**) offering can be
    used to deploy web applications that have been developed using Java, .NET, PHP,
    Node.js, Python, and many more. This service also offers the option of using Apache,
    Nginx, Passenger, or IIS as a web server. This service provides flexibility in
    managing the underlining infrastructure, which is sometimes required for deploying
    complex applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS App Runner**: This service can be used to run containerized web applications
    and microservices with an API. This service is fully managed, which means you
    have no administrative responsibilities, as well as no access to the underlying
    infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Lambda**: This is a serverless compute runtime that allows you to run
    your code without the worry of managing any underlying servers. This server supports
    multiple languages, including Python. Although Lambda code can be executed directly
    from an application, this is well-suited for cases when we must run a certain
    piece of code in case an event is triggered by the other AWS services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Batch**: This option is used to run computing jobs in large volumes in
    the form of batches. This is a cloud option from Amazon that''s an alternative
    to the Apache Spark and Hadoop MapReduce cluster options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Kinesis**: This service is also for data processing, but for real-time
    streaming data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runtime options offered by GCP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the runtime options that are available from GCP:'
  prefs: []
  type: TYPE_NORMAL
- en: '**App Engine**: This is a PaaS option from GCP that can be used to develop
    and host web applications at scale. The applications are deployed as containers
    on App Engine, but your source code is packed into a container by the deployment
    tool. This complexity is hidden from developers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CloudRun**: This option is used to host any code that has been built as a
    container. The container applications must have HTTP endpoints to be deployed
    on CloudRun. In comparison to App Engine, packaging applications to a container
    is the developer''s responsibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud Function**: This is an event-driven, serverless, and single-purpose
    solution for hosting lightweight Python code. The hosted code is typically triggered
    by listening to events on other GCP services or via direct HTTP requests. This
    is comparable to the AWS Lambda service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataflow**: This is another serverless option but mainly for data processing
    with minimal latency. This simplifies a data scientist''s life by taking away
    the complexity of the underlying processing platform and offering a data pipeline
    model based on Apache Beam.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataproc**: This service offers a computer platform based on Apache Spark,
    Apache Flink, Presto, and many more tools. This platform is suitable for those
    who have data processing jobs with dependencies on a Spark or Hadoop ecosystem.
    This service requires that we manually provision clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runtime options offered by Microsoft Azure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Microsoft Azure offers the following runtime environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**App Service**: This service is used to build and deploy web apps at scale.
    This web application can be deployed as a container or run on Windows or Linux.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Functions**: This is a serverless event-driven runtime environment
    that''s used to execute code based on a certain event or direct request. This
    is comparable to AWS Lambda and GCP CloudRun.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch**: As its name suggests, this service is used to run cloud-scale jobs
    that require hundreds or thousands of virtual machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Databricks**: Microsoft has partnered with Databricks to offer this
    Apache Spark-based platform for large-scale data processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Data Factory**: This is a serverless option from Azure that you can
    use to process streaming data and transform the data into meaningful outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we have seen, the three main cloud providers offer a variety of execution
    environments based on the applications and workloads that are available. The following
    use cases can be deployed on cloud platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: Developing web services and web applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data processing using a cloud runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservice-based applications (containers) using Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless functions or applications for the cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will address the first two use cases in the upcoming sections of this chapter.
    The remaining use cases will be discussed in the upcoming chapters as they require
    more extensive discussion. In the next section, we will start building a web service
    using Python and explore how to deploy it on the GCP App Engine runtime environment.
  prefs: []
  type: TYPE_NORMAL
- en: Building Python web services for cloud deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building an application for cloud deployment is slightly different than doing
    so for a local deployment. There are three key requirements we must consider while
    developing and deploying an application to any cloud. These requirements are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Web interface**: For most cloud deployments, applications that have a **graphical
    user interface** (**GUI**) or **application programming interface** (**API**)
    are the main candidates. Command-line interface-based applications will not get
    their usability from a cloud environment unless they are deployed in a dedicated
    virtual machine instance, and we can execute them on a VM instance using SSH or
    Telnet. This is why we selected a web interface-based application for our discussion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment setup**: All public cloud platforms support multiple languages,
    as well as different versions of a single language. For example, GCP App Engine
    supports Python versions 3.7, 3.8, and 3.9 as of June 2021\. Sometimes, cloud
    services allow you to bring your own version for deployment as well. For web applications,
    it is also important to set an entry point for accessing the code and project-level
    settings. These are typically defined in a single file (a YAML file, in the case
    of the GCP App Engine application).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requirements.txt`) manually or using the `PIP freeze` command. There are other
    elegant ways available to solve this problem as well. One such way is to package
    all third-party libraries with applications into a single file for cloud deployment,
    such as the Java web archive file (`.war` file). Another approach is to bundle
    all the dependencies containing application code and the target execution platform
    into a container and deploy the container directly on a container hosting platform.
    We will explore container-based deployment options in [*Chapter 11*](B17189_11_Final_PG_ePub.xhtml#_idTextAnchor289),
    *Using Python for Microservices Development*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are at least three options for deploying a Python web service application
    on GCP App Engine, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Google Cloud SDK via the CLI interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the GCP web console (portal) along with Cloud Shell (CLI interface)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a third-party IDE such as PyCharm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss the first option in detail and provide a summary of our experience
    with the other two options.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: To deploy a Python application in AWS and Azure, the procedural steps are the
    same in principle, but the details are different, depending on the SDK and API
    support available from each cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: Using Google Cloud SDK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss how to use Google Cloud SDK (mainly the CLI
    interface) to create and deploy a sample application. This sample application
    will be deployed on the **Google App Engine** (**GAE**) platform. GAE is a PaaS
    platform and is best suited for deploying web applications using a wide variety
    of programming languages, including Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the Google Cloud SDK for Python application deployment, we must have
    the following prerequisites on our local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install and initialize the Cloud SDK. Once installed, you can access it via
    the CLI interface and check its version with the following command. Note that
    almost all Cloud SDK commands start with `gcloud`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the Cloud SDK components to add the App Engine extension for Python
    3\. This can be done by using the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The GCP CloudBuild API must be enabled for the GCP cloud project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud billing must be enabled for the GCP cloud project, even if you are using
    a trial account, by associating your GCP billing account with the project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GCP user privileges to set up a new App Engine application and to enable
    API services should be done at the *Owner* level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will describe how to set up a GCP cloud project, create a sample web
    service application, and deploy it to the GAE.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a GCP cloud project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concept of a GCP cloud project is the same as we see in most development
    IDEs. A GCP cloud project consists of a set of project-level settings that manage
    how our code interacts with GCP services and tracks the resources in use by the
    project. A GCP project must be associated with a billing account. This is a prerequisite,
    in terms of billing, for tracking how many GCP services and resources are consumed
    on a per-project basis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will explain how to set up a project using Cloud SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to the Cloud SDK using the following command. This will take you to
    the web browser so that you can sign in, in case you have not already done so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new project called `time-wsproj`. The project''s name should be short
    and use only letters and numbers. The use of `-` is allowed for better readability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Switch your default scope of the Cloud SDK to the newly created project, if
    you haven''t done so already, by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will enable Cloud SDK to use this project as a default project for any
    command we push through the Cloud SDK CLI.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create an App Engine instance under a default project or for any project by
    using the `project` attribute with one of the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that this command will reserve cloud resources (mainly compute and storage)
    and will prompt you to select a region and zone to host the resources. You can
    select the region and zone that's closest to you and also more appropriate from
    your audience's point of view.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Enable the Cloud Build API service for the current project. As we've discussed
    already, the Google Cloud Build service is used to build the application before
    it's deployed to a Google runtime such as App Engine. The Cloud Build API service
    is easier to enable through the GCP web console as it only takes a few clicks.
    To enable it using Cloud SDK, first, we need to know the exact name of the service.
    We can get the list of available GCP services by using the `gcloud services list`
    command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This command will give you a long list of GCP services so that you can look
    for a service related to Cloud Build. You can also use `format`, attributed with
    any command, to beautify the Cloud SDK''s output. To make this even more convenient,
    you can use the Linux `grep` utility (if you are using Linux or macOS) with this
    command to filter the results and then enable the service using the `enable` command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To enable the Cloud Billing API service for our project, first, we need to
    associate a billing account with our project. Support for billing accounts in
    Cloud SDK hasn''t been achieved with `beta` command presented here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Enable the Cloud Billing API service for the current project by following the
    same steps we followed for enabling the Cloud Build API. First, we must find the
    name of the API service and then enable it using the following set of Cloud SDK
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The steps you must follow to set up a cloud project are straightforward for
    an experienced cloud user and will not take more than a few minutes. Once the
    project has been set up, we can get the project configuration details by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this command will provide the project life cycle''s status, the
    project''s name, the project''s ID, and the project''s number. The following is
    some example output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now that the project has been set up, we can start developing our Python web
    application. We will do this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Python application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For cloud deployments, we can build a Python application using an IDE or system
    editor and then emulate the App Engine runtime locally using the Cloud SDK and
    the *app-engine-python component*, which we have installed as a prerequisite.
    As an example, we will build a web service-based application that will provide
    us with the date and time through a REST API. The application can be triggered
    via an API client or using a web browser. We did not enable any authentication
    to keep the deployment simple.
  prefs: []
  type: TYPE_NORMAL
- en: To build the Python application, we will set up a Python virtual environment
    using the Python `venv` package. A virtual environment, created using the `venv`
    package, will be used to wrap the Python interpreter, core, and third-party libraries
    and scripts to keep them separate from the system Python environment and other
    Python virtual environments. Creating and managing a virtual environment in Python
    using the `venv` package has been supported in Python since v3.3\. There are other
    tools available for creating virtual environments, such as `virtualenv` and `pipenv`.
    PyPA recommends using `venv` for creating a virtual environment, so we selected
    it for most of the examples presented in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we will create a web application project directory named `time-wsproj`
    that contains the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`app.yaml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`main.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requirements.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used the same name for the directory that we used to create the cloud project
    just for convenience, but this is not a requirement. Let's look at these files
    in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: YAML file
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This file contains the deployment and runtime settings for an App Engine application,
    such as runtime version number. For Python 3, the `app.yaml` file must have at
    least a runtime parameter (`runtime: python38`). Each service in the web application
    can have its own YAML file. For the sake of simplicity, we will use only one YAML
    file. In our case, this YAML file will only contain the runtime attribute. We
    added a few more attributes to the sample YAML file for illustration purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: main.py Python file
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We selected the `Flask` library to build our sample application. `Flask` is
    a well-known library for web development, mainly because of the powerful features
    it offers, along with its ease of use. We will cover Flask in the next chapter
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'This `main.py` Python module is the entry point of our application. The complete
    code of the application is presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This module provides the following key features:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a default entrypoint called `app` that's defined in this module. The
    `app` variable is used to redirect the requests that are sent to this module.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using Flask''s annotation, we have defined handlers for three URLs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) The root `/` URL will trigger a function named `welcome`. The `welcome` function
    returns a greeting message as a string.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The `/date` URL will trigger the `today` function, which will return today's
    date in JSON format.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The `/time` URL will execute the `time` function, which will return the current
    time in JSON format.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At the end of the module, we added a `__main__` function to initiate a local
    web server that comes with Flask for testing purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requirements file
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This file contains a list of project dependencies for third-party libraries.
    The contents of this file will be used by App Engine to make the required libraries
    available to our application. In our case, we will need the Flask library to build
    our sample web application. The contents of this file for our project are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have created the project directory and made these files, we must create
    a virtual environment inside or outside the project directory and activate it
    using the source command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After activating the virtual environment, we must install the necessary dependencies,
    as per the `requirements.txt` file. We will use the `pip` utility from the same
    directory where the `requirements.txt` file resides:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the Flask library and its dependencies have been installed, the directory
    structure will look like this in our PyCharm IDE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Directory structure for a sample web application'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_09_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – Directory structure for a sample web application
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the project file and dependencies have been set up, we will start the
    web server locally using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The server will start with the following debug messages, which makes it clear
    that this server option is only for testing purposes and not for production environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Our web service application can be accessed using the following URIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://localhost:8080/`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http://localhost:8080/date`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http://localhost:8080/time`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The response from the web servers for these URIs is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Response in the web browser from our sample web service application'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_09_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – Response in the web browser from our sample web service application
  prefs: []
  type: TYPE_NORMAL
- en: The web server will be stopped before we move on to the next phase – that is,
    deploying this application to Google App Engine.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Google App Engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To deploy our web service application to GAE, we must use the following command
    from the project directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Cloud SDK will read the `app.yaml` file, which provides input for creating
    an App Engine instance for this application. During the deployment, a container
    image is created using the Cloud Build service; then, this container image is
    uploaded to GCP storage for deployment. Once it has been successfully deployed,
    we can access the web service using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This command will open the application using the default browser on your machine.
    The URL of the hosted application will vary, depending on the region and zone
    that was selected during app creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to understand that every time we execute the `deploy` command,
    it will create a new version of our application in App Engine, which means more
    resources will be consumed. We can check the versions that have been installed
    for a web application using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The older versions of the application still can be in a serving state with
    slightly different URLs assigned to them. The older versions can be stopped, started,
    or deleted using the `gcloud app versions` Cloud SDK command and the version ID.
    An application can be stopped or started using the `stop` or `start` commands,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The version ID is available when we run the `gcloud app versions list` command.
    This concludes our discussion on building and deploying a Python web application
    to Google Cloud. Next, we will summarize how we can leverage the GCP console to
    deploy the same application.
  prefs: []
  type: TYPE_NORMAL
- en: Using the GCP web console
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GCP console provides an easy-to-use web portal for accessing and managing
    GCP projects, as well as an online version of Google **Cloud Shell**. The console
    also offers customizable dashboards, visibility to cloud resources used by projects,
    billing details, activity logging, and many more features. When it comes to developing
    and deploying a web application using the GCP console, we have some features we
    can use thanks to the web UI, but most of the steps will require the use of Cloud
    Shell. This is a Cloud SDK that's available online through any browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cloud Shell is more than Cloud SDK in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: It offers access to the `gcloud` CLI, as well as the `kubectl` CLI. `kubectl`
    is used for managing resources on the GCP Kubernetes engine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With Cloud Shell, we can develop, debug, build, and deploy our applications
    using **Cloud Shell Editor**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Shell also offers an online development server for testing an application
    before deploying it to App Engine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Shell comes with tools to upload and download files between the Cloud
    Shell platform and your machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Shell comes with the ability to preview the web application on port 8080
    or a port of your choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Cloud Shell commands that are required to set up a new project, build the
    application, and deploy to App Engine are the same ones we discussed for Cloud
    SDK. That is why we will leave this for you to explore by following the same steps
    that we described in the previous section. Note that the project can be set up
    using the GCP console. The Cloud Shell interface can be enabled using the Cloud
    Shell icon on the top menu bar, on the right-hand side. Once Cloud Shell has been
    enabled, a command-line interface will appear at the bottom of the console''s
    web page. This is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – GCP console with Cloud Shell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_09_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 – GCP console with Cloud Shell
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, Cloud Shell comes with an editor tool that can be
    started by using the **Open editor** button. The following screenshot shows the
    Python file opened inside **Cloud Shell Editor**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – GCP console with Cloud Shell Editor ](img/B17189_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – GCP console with Cloud Shell Editor
  prefs: []
  type: TYPE_NORMAL
- en: Another option when it comes to building and deploying web applications is using
    third-party IDEs with Google App Engine plugins. Based on our experience, the
    plugins that are available for commonly used IDEs such as PyCharm and Eclipse
    are mostly built for Python 2 and legacy web application libraries. Directly integrating
    IDEs with GCP requires more work and evolution. At the time of writing, the best
    option is to use Cloud SDK or Cloud Shell directly in conjunction with the editor
    or IDE of your choice for application development.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered developing web applications using Python and deploying
    them to the GCP App Engine platform. Amazon offers the AWS Beanstalk service for
    web application deployment. The steps for deploying a web application in AWS Beanstalk
    are nearly the same as for GCP App Engine, except that AWS Beanstalk does not
    need projects to be set up as a prerequisite. Therefore, we can deploy applications
    faster in AWS Beanstalk.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy our web service application in AWS Beanstalk, we must provide the
    following information, either using the AWS console or using the AWS CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: Application name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform (Python version 3.7 or 3.8, in our case)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source code version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source code, along with a `requirements.txt` file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We recommend using the AWS CLI for web applications that have a dependency on
    third-party libraries. We can upload our source code as a ZIP file or as a web
    archive (`WAR` file) from our local machine or copy them from an **Amazon S3**
    location.
  prefs: []
  type: TYPE_NORMAL
- en: The exact steps of deploying a web application on AWS Beanstalk are available
    at [https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-flask.html](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-flask.html).
    Azure offers App Service for building and deploying web applications. You can
    find the steps for creating and deploying a web application on Azure at [https://docs.microsoft.com/en-us/azure/app-service/quickstart-python](https://docs.microsoft.com/en-us/azure/app-service/quickstart-python).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore building driver programs for data processing using cloud
    platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Using Google Cloud Platform for data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google Cloud Platform offers Cloud Dataflow as a data processing service to
    serve both batch and real-time data streaming applications. This service is meant
    for data scientists and analytics application developers so that they can set
    up a processing **pipeline** for data analysis and data processing. Cloud Dataflow
    uses Apache Beam under the hood. **Apache Beam** originated from Google, but it
    is now an open source project under Apache. This project offers a programming
    model for building data processing using pipelines. Such pipelines can be created
    using Apache Beam and then executed using the Cloud Dataflow service.
  prefs: []
  type: TYPE_NORMAL
- en: The Google Cloud Dataflow service is similar to `Amazon Kinesis`, Apache Storm, `Apache
    Spark`, and Facebook Flux. Before we discuss how to use Google Dataflow with Python,
    we will introduce Apache Beam and its pipeline concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Learning the fundamentals of Apache Beam
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the current era, data is like a cash cow for many organizations. A lot of
    data is generated by applications, by devices, and by human interaction with systems.
    Before consuming the data, it is important to process it. The steps that are defined
    for data processing are typically called pipelines in Apache Beam nomenclature.
    In other words, a data pipeline is a series of actions that are performed on raw
    data that originates from different sources, and then moves that data to a destination
    for consumption by analytic or business applications.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Beam is used to break the problem into small bundles of data that can
    be processed in parallel. One of the main use cases of Apache Beam is **Extract,
    Transform, and Load** (**ETL**) applications. These three ETL steps are core for
    a pipeline whenever we have to move the data from a raw form to a refined form
    for data consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'The core concepts and components of Apache Beam are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline**: A pipeline is a scheme for transforming the data from one form
    into the other as part of data processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PCollection**: A Pcollection, or Parallel Collection, is analogous to RDD
    in Apache Spark. It is a distributed dataset that contains an immutable and unordered
    bag of elements. The size of the dataset can be fixed or bounded, similar to batch
    processing, where we know how many jobs to process in one batch. The size can
    also be flexible or unbounded based on the continuously updating and streaming
    data source.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PTransforms**: These are the operations that are defined in a pipeline to
    transform the data. These operations are operated on PCollection objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SDK**: A language-specific software development kit that''s available for
    Java, Python, and Go to build pipelines and submit them to a runner for execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`run (Pipeline)` that is asynchronous by default. A few runners that are available
    are Apache Flink, Apache Spark, and Google Cloud Dataflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DoFn`, which operates on a per-element basis. The provided `DoFn` implementation
    is wrapped inside an `ParDo` object that is designed for parallel execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A simple pipeline looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Flow of a pipeline with three PTransform operations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_09_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – Flow of a pipeline with three PTransform operations
  prefs: []
  type: TYPE_NORMAL
- en: 'To design a pipeline, we must typically consider three elements:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to understand the source of the data. Is it stored in a file
    or in a database, or is it coming as a stream? Based on this, we will determine
    what type of Read Transform operation we have to implement. As part of the read
    operation or a separate operation, we also need to understand the data format
    or structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to define and design what to do with this data. This is our
    main transform operation(s). We can have multiple transform operations one after
    the other in a serial way or in parallel on the same data. The Apache Beam SDK
    provides several pre-built transforms that can be used. It also allows us to write
    our own transforms using ParDo/DoFn functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Last, we need to know what the output of our pipeline will be and where to store
    the output results. This is shown as a Write Transform in the preceding diagram.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we discussed a simple pipeline structure to explain different
    concepts related to Apache Beam and pipelines. In practice, the pipeline can be
    relatively complex. A pipeline can have multiple input data sources and multiple
    output sinks. The PTransforms operations may result in multiple PCollection objects,
    which requires PTransform operations to run in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will learn how to create a new pipeline and execute
    a pipeline using an Apache Beam runner or Cloud Dataflow runner.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Apache Beam pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will discuss how to create Apache Beam pipelines. As we''ve
    discussed already, a pipeline is a set of actions or operations that are orchestrated
    to achieve certain data processing goals. The pipeline requires an input data
    source that can contain in-memory data, local or remote files, or streaming data.
    The pseudocode for a typical pipeline will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The initial PCollection is used as input to the First PTransform operation.
    The output PCollection of First PTransform will be used as input to Second PTransform,
    and so on. The final output of the PCollection of the last PTransform will be
    captured as a Final PCollection object and be used to export the results to the
    target destination.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this concept, we will build a few example pipelines of different
    complexity levels. These examples are designed to show the roles of different
    Apache components and libraries that are used in building and executing a pipeline.
    In the end, we will build a pipeline for a famous *word count* application that
    is also referenced in the Apache Beam and GCP Dataflow documentation. It is important
    to highlight that we must install the `apache-beam` Python library using the `pip`
    utility for all the code examples in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – creating a pipeline with in-memory string data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we will create an input PCollection from an in-memory collection
    of strings, apply a couple of transform operations, and then print the results
    to the output console. The following is the complete example code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For this example, it is important to highlight a few points:'
  prefs: []
  type: TYPE_NORMAL
- en: We used `|` to write different PTransform operations in a pipeline. This is
    an overloaded operator that is more like applying a PTransform to a PCollection
    to produce another PCollection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used the `>>` operator to name each PTransform operation for logging and
    tracking purposes. The string between `|` and `>>` is used for displaying and
    logging purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We used three transform operations; all are part of the Apache Beam library:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) The first transform operation is used to create a PCollection object, which
    is a string containing five subject names.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The second transform operation is used to split the string data into a new
    PCollection using a built-in `String` object method (`split`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The third transform operation is used to print each entry in the PCollection
    to the console output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The console's output will show a list of subject names, with one name in one
    line.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2 – creating and processing a pipeline with in-memory tuple data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this code example, we will create a PCollection of tuples. Each tuple will
    have a subject name and a grade associated with it. The core PTransform operation
    of this pipeline is to separate the subject and its grade from the data. The sample
    code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In comparison to the first example, we used the `FlatMapTuple` transform operation
    with a custom function to format the tuple data. The console output will show
    each subject's name, along with its grade, in a separate line.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3 – creating a pipeline with data from a text file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the first two examples, we focused on building a simple pipeline to parse
    string data from a large string and to split tuples from a PCollection of tuples.
    In practice, we are working on a large volume of data that is either loaded from
    a file or storage system or coming from a streaming source. In this example, we
    will read data from a local text file to build our initial PCollection object
    and also output the final results to an output file. The complete code example
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code example, we applied a PTransform operation to read the text data
    from a file before applying any data processing-related PTransforms. Finally,
    we applied a PTransform operation to write the data to an output file. We used
    two new functions in this code example called `ReadFromText` and `WriteToText`,
    as explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ReadFromText`: This function is part of the Apache Beam I/O module and is
    used to read data from text files into a PCollection of strings. The file path
    or file pattern can be provided as an input argument to read from a local path.
    We can also use `gs://` to access any file in GCS storage locations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WriteToText`: This function is used to write PCollection data to a text file.
    This requires the `file_path_prefix` argument at a minimum. We can also provide
    the `file_path_suffix` argument to set the file extension. `shard_name_template`
    is set to empty to create the file with a name using the prefix and suffix arguments.
    Apache Beam supports a shard name template for defining the filename based on
    a template.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When this pipeline is executed locally, it will create a file named `subjects.txt`
    with subject names captured in it, as per the PTransform operation.
  prefs: []
  type: TYPE_NORMAL
- en: Example 4 – creating a pipeline for an Apache Beam runner with arguments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, we have learned how to create a simple pipeline, how to build a PCollection
    object from a text file, and how to write the results back to a file. In addition
    to these core steps, we need to perform a few more steps, to make sure our driver
    program is ready to submit the job to a GCP Dataflow runner or any other cloud
    runner. These additional steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, we provided the names of the input file and the output
    file pattern that are set in the driver program. In practice, we should expect
    these parameters to be provided through command-line arguments. We will use the
    `argparse` library to parse and manage command-line arguments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will add extended arguments such as setting a runner. This argument will
    be used to set the target runner of the pipeline using DirectRunner or a GCP Dataflow
    runner. Note that DirectRunner is a pipeline runtime for your local machine. It
    makes sure that those pipelines follow the Apache Beam model as closely as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also implement and use the `ParDo` function, which will utilize our
    custom-built function for parsing strings from text data. We can achieve this
    using `String` functions, but it has been added here to illustrate how to use
    `ParDo` and `DoFn` with PTransform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will build the argument parser and define the arguments we expect
    from the command line. We will set the default values for those arguments and
    set additional helping text to be shown with the `help` switch on the command
    line. The `dest` attribute is important because it is used to identify any argument
    to be used in programming statements. We will also define the `ParDo` function,
    which will be used to execute the pipeline. Some sample code is presented here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will set `DirectRunner` as our pipeline runtime and name the job to
    be executed. The sample code for this step is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will create a pipeline using the `pipeline_options` object that
    we created in the previous step. The pipeline will be reading data from an input
    text file, transforming data as per our `ParDo` function, and then saving the
    results as output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Apache Beam is a vast topic, so it is not possible to cover all its features
    without writing a few chapters on it. However, we have covered the fundamentals
    by providing code examples that will enable us to start writing simple pipelines
    that we can deploy on GCP Cloud Dataflow. We will cover this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building pipelines for Cloud Dataflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code examples we've discussed so far focused on building simple pipelines
    and executing them using DirectRunner. In this section, we will build a driver
    program to deploy a word count data processing pipeline on Google Cloud Dataflow.
    This driver program is important for Cloud Dataflow deployments because we will
    set all cloud-related parameters inside the program. Due to this, there will be
    no need to use Cloud SDK or Cloud Shell to execute additional commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'The word count pipeline will be an extended version of our `pipeline4.py` example.
    The additional components and steps required to deploy the word count pipeline
    are summarized here:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will create a new GCP cloud project using steps that are similar to
    the ones we followed for our web service application for App Engine deployment.
    We can use Cloud SDK, Cloud Shell, or the GCP console for this task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will enable Dataflow Engine API for the new project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will create a storage bucket for storing the input and output files
    and to provide temporary and staging directories for Cloud Dataflow. We can achieve
    this by using the GCP console, Cloud Shell, or Cloud SDK. We can use the following
    command if we are using Cloud Shell or Cloud SDK to create a new bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You may need to associate a service account with the newly created bucket if
    it is not under the same project as the dataflow pipeline job is and select the
    *storage object admin* role for access control.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We must install Apache Beam with the necessary `gcp` libraries. This can be
    achieved by using the `pip` utility, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We must create a key for authentication for the GCP service account used for
    the GCP cloud project. This is not required if we will be running the driver program
    from a GCP platform such as Cloud Shell. The service account key must be downloaded
    on your local machine. To make the key available to the Apache Beam SDK, we need
    to set the path of the key file (a JSON file) to an environment variable called
    `GOOGLE_APPLICATION_CREDENTIALS`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before discussing how to execute a pipeline on Cloud Dataflow, we will take
    a quick look at the sample word count driver program for this exercise. In this
    driver program, we will define command-line arguments, very similar to the ones
    we did in the previous code example (`pipeline4.py`), except we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of setting the `GOOGLE_APPLICATION_CREDENTIALS` environment variable
    through the operating system, we will set it using our driver program for ease
    of execution for testing purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will upload the `sample.txt` file to Google storage, which is the `gs//muasif/input`
    directory in our case. We will use the path to this Google storage as the default
    value of the `input` argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The complete sample code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next step, we will set up extended arguments for the pipeline options
    to execute our pipeline on the Cloud Dataflow runtime. These arguments are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime platform (runner) for pipeline execution (DataflowRunner, in this case)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCP Cloud Project ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCP region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google storage bucket paths for storing input, output, and temporary files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Job name for tracking purposes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on these arguments, we will create a pipeline options object to be used
    for pipeline execution. The sample code for these tasks is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will implement a pipeline with the pipeline options that have already
    been defined and add our PTransform operations. For this code example, we added
    an extra PTransform operation to build a pair of each word with `1`. In the follow-up
    PTransform operation, we grouped the pairs and applied the `sum` operation to
    count their frequency. This gives us the count of each word in the input text
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We set default values for each argument within the driver program. This means
    that we can execute the program directly with the `python wordcount.py` command
    or we can use the following command to pass the arguments through the CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output file will contain the results, along with the count of each word
    in the file. GCP Cloud Dataflow provides additional tools for monitoring the progress
    of submitted jobs and for understanding the resource utilization to perform the
    job. The following screenshot of the GCP console shows a list of jobs that have
    been submitted to Cloud Dataflow. The summary view shows their statuses and a
    few key metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Cloud Dataflow jobs summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_09_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – Cloud Dataflow jobs summary
  prefs: []
  type: TYPE_NORMAL
- en: 'We can navigate to the detailed job view (by clicking any job name), as shown
    in the following screenshot. This view shows the job and environment details on
    the right-hand side and a progress summary of the different PTransforms we defined
    for our pipeline. When the job is running, the status of each PTransform operation
    is updated in real time, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Cloud Dataflow job detail view with a flowchart and metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_09_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7 – Cloud Dataflow job detail view with a flowchart and metrics
  prefs: []
  type: TYPE_NORMAL
- en: A very important point to notice is that the different PTransform operations
    are named according to the strings we used with the `>>` operator. This is helpful
    for visualizing the operations conveniently. This concludes our discussion on
    building and deploying a pipeline for Google Dataflow. In comparison to Apache
    Spark, Apache Beam provides more flexibility for parallel and distributed data
    processing. With the availability of cloud data processing options, we can focus
    entirely on modeling the pipelines and leave the job of executing pipelines to
    cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, Amazon offers a similar service (AWS Kinesis) for deploying
    and executing pipelines. AWS Kinesis is more focused on data streams for real-time
    data. Like AWS Beanstalk, AWS Kinesis does not require that we set up a project
    as a prerequisite. The user guides for data processing using AWS Kinesis are available
    at [https://docs.aws.amazon.com/kinesis/](https://docs.aws.amazon.com/kinesis/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the role of Python for developing applications
    for cloud deployment in general, as well as the use of Apache Beam with Python
    for deploying data processing pipelines on Google Cloud Dataflow. We started this
    chapter by comparing three main public cloud providers in terms of what they offer
    for developing, building, and deploying different types of applications. We also
    compared the options that are available from each cloud provider for runtime environments.
    We learned that each cloud provider offers a variety of runtime engines based
    on the application or program. For example, we have separate runtime engines for
    classic web applications, container-based applications, and serverless functions.
    To explore the effectiveness of Python for cloud-native web applications, we built
    a sample application and learned how to deploy such an application on Google App
    Engine by using Cloud SDK. In the last section, we extended our discussion of
    the data process, which we started in the previous chapter. We introduced a new
    modeling approach for data processing (pipelines) using Apache Beam. Once we learned
    how to build pipelines with a few code examples, we extended our discussion to
    how to build pipelines for Cloud Dataflow deployment.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provided a comparative analysis of public cloud service offerings.
    This was followed by hands-on knowledge of building web applications and data
    processing applications for the cloud. The code examples included in this chapter
    will enable you to start creating cloud projects and writing code for Apache Beam.
    This knowledge is important for anyone who wants to solve their big data problems
    using cloud-based data processing services.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the power of Python for developing web
    applications using the Flask and Django frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How is AWS Beanstalk different from AWS App Runner?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the GCP Cloud Function service?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What services from GCP are available for data processing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an Apache Beam pipeline?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the role of PCollection in a data processing pipeline?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Flask Web Development*, by Miguel Grinberg.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advanced Guide to Python 3 Programming*, by John Hunt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Apache Beam: A Complete Guide*, by Gerardus Blokdyk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Google Cloud Platform for Developers*, by Ted Hunter, Steven Porter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Google Cloud Dataflow documentation* is available at[https://cloud.google.com/dataflow/docs](https://cloud.google.com/dataflow/docs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AWS Elastic Beanstalk documentation* is available at [https://docs.aws.amazon.com/elastic-beanstalk](https://docs.aws.amazon.com/elastic-beanstalk).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Azure App Service documentation* is available at [https://docs.microsoft.com/en-us/azure/app-service/](https://docs.microsoft.com/en-us/azure/app-service/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AWS Kinesis documentation* is available at [https://docs.aws.amazon.com/kinesis/](https://docs.aws.amazon.com/kinesis/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS Beanstalk is a general-purpose PaaS offering for deploying web applications,
    whereas AWS App Runner is a fully managed service for deploying container-based
    web applications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GCP Cloud Function is a serverless, event-driven service for executing a program.
    The specified event can be triggered from another GCP service or through an HTTP
    request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cloud Dataflow and Cloud Dataproc are two popular services for data processing
    offered by GCP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An Apache Beam pipeline is a set of actions that have been defined to load the
    data, transform the data from one form into another, and write the data to a destination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PCollection is like an RDD in Apache Spark that holds data elements. In pipeline
    data processing, a typical PTransform operation takes one or more PCollection
    objects as input and produces the results as one or more PCollection objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
