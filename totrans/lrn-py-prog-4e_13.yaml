- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Science in Brief
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ”If we have data, let’s look at data. If all we have are opinions, let’s go
    with mine.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Jim Barksdale, former Netscape CEO
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Data science** is a broad term that can have different meanings depending
    on the context, understanding, and tools, amongst other factors. To do proper
    data science, you need to, at the very least, know mathematics and statistics.
    Then, you may want to dig into other subjects, such as pattern recognition and
    machine learning, and, of course, there is a plethora of languages and tools you
    can use.'
  prefs: []
  type: TYPE_NORMAL
- en: We will not be able to talk about everything here. Therefore, to render this
    chapter meaningful, we are going to work on a project together instead.
  prefs: []
  type: TYPE_NORMAL
- en: Around 2012/2013, Fabrizio was working for a top-tier social media company in
    London. He stayed there for two years and was privileged to work with several
    very brilliant people. The company was the first in the world to have access to
    the Twitter Ads API, and they were partners with Facebook as well. That means
    a lot of data.
  prefs: []
  type: TYPE_NORMAL
- en: Their analysts were dealing with a vast number of campaigns, and they were struggling
    with the amount of work they had to do, so the development team Fabrizio was a
    part of tried to help by introducing them to Python and to the tools Python gives
    us to deal with data. It was an interesting journey that led him to mentor several
    people in the company, eventually taking him to Manila, where he gave a two-week
    intensive training course in Python and data science to the analysts over there.
  prefs: []
  type: TYPE_NORMAL
- en: The project we are going to do in this chapter is a lightweight version of the
    final example Fabrizio presented to his students in Manila. We have rewritten
    it to a size that will fit this chapter and made a few adjustments here and there
    for teaching purposes, but all the main concepts are there, so it should be fun
    and instructional for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we are going to explore the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The Jupyter Notebook and JupyterLab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'pandas and NumPy: the main libraries for data science in Python'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few concepts around pandas’s `DataFrame` class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and manipulating a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us start by talking about Roman gods.
  prefs: []
  type: TYPE_NORMAL
- en: IPython and Jupyter Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2001, Fernando Perez was a graduate student in physics at CU Boulder and
    was trying to improve the Python shell so that he could have the niceties he was
    used to when working with tools such as Mathematica and Maple. The result of these
    efforts took the name **IPython** .
  prefs: []
  type: TYPE_NORMAL
- en: That small script began as an enhanced version of the Python shell and, through
    the efforts of other coders and eventually with funding from several different
    companies, it became the successful project it is today. Some 10 years after its
    birth, a Notebook environment was created, powered by technologies such as WebSockets,
    the Tornado web server, jQuery, CodeMirror, and MathJax. The ZeroMQ library was
    also used to handle the messages between the Notebook interface and the Python
    core that lies behind it.
  prefs: []
  type: TYPE_NORMAL
- en: The IPython Notebook became so popular and widely used that, over time, numerous
    features were added to it. It can handle widgets, parallel computing, various
    media formats, and much more. Moreover, at some point, it became possible to code
    in languages other than Python from within the Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Eventually, the project was split into two: IPython has been stripped down
    to focus more on the kernel and the shell, while the Notebook has become a new
    project called **Jupyter** . Jupyter allows interactive scientific computations
    to be done in more than 40 languages. More recently, the Jupyter project has created
    **JupyterLab** , a web-based **IDE** incorporating Jupyter notebooks, interactive
    consoles, a code editor, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter’s project will all be coded and run in a Jupyter Notebook, so let
    us briefly explain what a Notebook is. A Notebook environment is a web page that
    exposes a simple menu and cells in which you can run Python code. Even though
    the cells are separate entities that you can run individually, they all share
    the same Python kernel. This means that all the names that you define in one cell
    (the variables, functions, and so on) will be available in any other cell.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, a Python kernel is a process in which Python is running. The Notebook
    web page is an interface exposed to the user for driving this kernel. The web
    page communicates with it using a fast messaging system.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from all the graphical advantages, the beauty of having such an environment
    lies in the ability to run a Python script in chunks, and this can be a tremendous
    advantage. Take a script that connects to a database to fetch data and then manipulates
    that data. If you do it in the conventional way, with a Python script, you have
    to fetch the data every time you want to experiment with it. Within a Notebook
    environment, you can fetch the data in one cell and then manipulate and experiment
    with it in other cells, so fetching it every time is not necessary.
  prefs: []
  type: TYPE_NORMAL
- en: The Notebook environment is also helpful for data science because it allows
    for the step-by-step inspection of results. You do one chunk of work and then
    verify it. You then do another chunk and verify again, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: It is also invaluable for prototyping because the results are there, right in
    front of your eyes, immediately available.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to know more about these tools, please check out [ipython.org](http://ipython.org)
    and [jupyter.org](http://jupyter.org) .
  prefs: []
  type: TYPE_NORMAL
- en: 'We have created a simple example Notebook with a `fibonacci()` function that
    gives you a list of all the Fibonacci numbers smaller than a given `N` . It looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/B30992_13_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1: A Jupyter Notebook'
  prefs: []
  type: TYPE_NORMAL
- en: Every cell has a label in square brackets, like **[1]** . If there is nothing
    between the brackets, it means that the cell has never been executed. If there
    is a number, it means that the cell has been executed, and the number represents
    the order in which the cell was executed. An asterisk, like ***** , means that
    the cell is currently being executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see in the screenshot that in the first cell we have defined the `fibonacci()`
    function and executed it. This has the effect of placing the `fibonacci` name
    in the global scope associated with the Notebook, and therefore the `fibonacci()`
    function is now available to the other cells as well. In fact, in the second cell,
    we can run `list(fibonacci(100))` and see the results output below cell **[2]**
    (the output for each cell is labeled with the same number as the cell itself).
    In the third cell, we have shown you one of the several “magic” functions you
    can find in a Notebook: `%timeit` runs the code several times and provides you
    with a benchmark for it (this is implemented using the `timeit` module, which
    we briefly introduced in *Chapter 11, Debugging and Profiling* ).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can execute a cell as many times as you want and change the order in which
    you run them. Cells are very versatile: you can also have Raw cells, which contain
    plain text, or Markdown cells, which are useful for adding formatted textual explanations
    or headings.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Markdown** is a lightweight markup language with plain text formatting syntax
    designed so that it can be converted to HTML and many other formats.'
  prefs: []
  type: TYPE_NORMAL
- en: Another useful feature is that whatever you place in the last line of a cell
    will automatically be printed for you. This means you are not forced to explicitly
    write `print(…)` every time you want to inspect a value.
  prefs: []
  type: TYPE_NORMAL
- en: Using Anaconda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As usual, you can install the libraries required for this chapter using the
    `requirements.txt` file in the source code for the chapter. Sometimes, however,
    installing data science libraries can be quite painful. If you are struggling
    to install the libraries for this chapter in your virtual environment, you could
    install Anaconda instead. Anaconda is a free and open-source distribution of the
    Python and R programming languages for data science and machine learning-related
    applications, which aims to simplify package management and deployment. You can
    download it from the [anaconda.org](http://anaconda.org) website. Once you have
    installed it, you can use the Anaconda interface to create a virtual environment
    and install the packages listed in the `requirements.in` file, which you can also
    find in the source code for the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Starting a Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have all the required libraries installed, you can start a Notebook
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you install the requirements via Anaconda, you can also launch the Notebook
    from the Anaconda interface. In either case, you will have an open page in your
    web browser at this address (the port might be different): [http://localhost:8888/](http://localhost:8888/)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also launch JupyterLab from Anaconda, or with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It will also open as a new page in your web browser.
  prefs: []
  type: TYPE_NORMAL
- en: Explore both interfaces. Create a new Notebook or open the `example.ipynb` Notebook
    we showed you above. See which interface you prefer and get comfortable with it
    before proceeding with the rest of the chapter. We have included a saved JupyterLab
    workspace containing the Notebooks used in the rest of this chapter in the source
    code for the chapter (the file is called `ch13.jupyterlab-workspace` ). You can
    use that to follow along in JupyterLab or stick to the classic Notebook interface
    if you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if you use a modern IDE to follow the chapter’s example, it’s
    likely you will be able to install a plugin to work with a notebook directly from
    inside the IDE.
  prefs: []
  type: TYPE_NORMAL
- en: To help you follow along, we will tag each code example in this chapter with
    the Notebook cell number it belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: If you familiarize yourself with the keyboard shortcuts (look in the classic
    Notebook **Help** menu or the **Settings Editor** in JupyterLab), you will be
    able to move between cells and handle their content without having to reach for
    the mouse. This will make you much faster when you work in a Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now move on and talk about the most interesting part of this chapter:
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Typically, when you deal with data, this is the path you go through: you fetch
    it; you clean and manipulate it; and then you analyze it and present results as
    values, spreadsheets, graphs, and so on. We want you to be able to perform all
    three steps of the process without having any external dependency on a data provider,
    so we are going to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the data, simulating that it comes in a format that is not perfect or
    ready to be worked on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clean it and feed it to the main tool we will use in the project, which is a
    `DataFrame` from the `pandas` library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manipulate the data in a `DataFrame` .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save a `DataFrame` to a file in different formats.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze the data and get some results out of it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting up the Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let us produce the data. We start from the `ch13-dataprep` Notebook.
    Cell `#1` takes care of the imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The only modules we have not already encountered are `random` and `faker` .
    `random` is a standard library module for generating pseudo-random numbers. `faker`
    is a third-party module for generating fake data. It is particularly useful in
    tests, when you prepare your fixtures, to get all sorts of things such as names,
    email addresses, phone numbers, and credit card details.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We want to achieve the following data structure: we are going to have a list
    of user objects. Each user object will be linked to several campaign objects.
    In Python, everything is an object, so we are using this term in a generic way.
    The user object may be a string, a dictionary, or something else.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A **campaign** in the social media world is a promotional campaign that a media
    agency runs on social media networks on behalf of a client. Remember that we are
    going to prepare this data so that it is not in perfect shape. Firstly, we instantiate
    the `Faker` that we will use to create the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will internally keep track of values that have been generated and only
    yield unique values. We use a list comprehension to generate 1,000 unique usernames.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a list of `users` . We will generate 1,000 `user` dictionaries
    with details such as `username` , `name` , `gender` , and `email` . Each `user`
    dictionary is then dumped to JSON and added to the list. This data structure is
    not optimal, of course, but we are simulating a scenario where users come to us
    like that.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `get_users()` generator function takes the number of users to create as
    a parameter. We use `fake.unique.user_name()` in a generator expression to generate
    unique usernames. The `fake.unique` property keeps track of values that have been
    generated and yields only unique values. Next, we call `random.choices()` to generate
    a list of `no_of_users` random elements from the list `["M", "F", "O"]` (to represent
    male, female, or other genders). The weights 0.43, 0.47, and 0.1 will ensure that
    roughly 43% of our users will be male, 47% female, and 10% will not identify as
    either male or female. We use `zip()` to iterate over the usernames and genders.
    For each user, we call `get_random_name()` , which uses a `match` statement to
    generate a gender-appropriate name, and then generate a fake email address, age,
    and address. We dump the user data in a JSON string and `yield` it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also note the last line in the cell. Each cell automatically prints what is
    on the last line; therefore, the output of #3 is a list with the first three users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We hope you are following along with your own Notebook. If you are, please note
    that all data is generated using random functions and values; therefore, you will
    see different results. They will change every time you execute the Notebook. Also
    note that we have had to trim most of the output in this chapter to fit onto the
    page, so you will see a lot more output in your Notebook than we have reproduced
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Analysts use spreadsheets all the time, and they create all sorts of coding
    techniques to compress as much information as possible into the campaign names.
    The format we have chosen is a simple example of that technique—there is a code
    that tells us the campaign type, then the start and end dates, then the target
    `age` and `gender` ( `"M"` for male, `"F"` for female, or `"A"` for any), and
    finally the currency.
  prefs: []
  type: TYPE_NORMAL
- en: 'All values are separated by an underscore. The code to generate these campaign
    names can be found in cell `#4` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `get_type()` function, we use `random.choice()` to get one value randomly
    out of a collection. `get_start_end_dates()` is a bit more interesting. We compute
    two random integers: the `duration` of the campaign in days (between one day and
    two years) and an `offset` (a number of days between –365 and 365). We subtract
    `offset` (as a `timedelta` ) from today’s date to get the start date and add the
    `duration` to get the end date. Finally, we return string representations of both
    dates.'
  prefs: []
  type: TYPE_NORMAL
- en: The `get_age_range()` function generates a random target age range, where both
    endpoints are multiples of five. We use the `random.randrange()` function, which
    returns a random number from a range defined by `start` , `stop` , and `step`
    parameters (these parameters have the same meaning as for the `range` object that
    we first encountered in *Chapter 3, Conditionals and Iteration* ). We generate
    random numbers `age` (a multiple of 5 between 20 and 46) and `diff` (a multiple
    of 5 between 5 and 26). We add `diff` to `age` to get the upper limit of our age
    range and return a string representation of the age range.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the functions are just some applications of `random.choice()` and
    the last one, `get_campaign_name()` , assembles all the pieces and returns the
    final campaign name.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `#5` , we write a function that creates a complete campaign object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We used a few functions from the `random` module. `random.randint()` gives you
    an integer between two extremes. It follows a uniform probability distribution,
    which means that any number in the interval has the same probability of coming
    up. To avoid having all our data look similar, we chose to use `triangular()`
    and `gauss()` , for `clicks` and `impressions` . They use different probability
    distributions so that we will have something more interesting to see in the end.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to make sure we are on the same page with the terminology: `clicks` represents
    the number of clicks on a campaign advertisement, `budget` is the total amount
    of money allocated for the campaign, `spent` is how much of that money has already
    been spent, and `impressions` is the number of times the campaign has been displayed,
    regardless of the number of clicks that were performed on the campaign. Normally,
    the number of `impressions` is greater than the number of `clicks` because an
    advertisement is often viewed without being clicked on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the data, we can put it all together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, each item in `data` is a dictionary with a `user` and a list
    of campaigns that are associated with that `user` .
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we can start cleaning the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We simulate fetching the data from a source and then inspect it. The Notebook
    is the perfect tool for inspecting your steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can vary the granularity to suit your needs. The first item in `rough_data`
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can start working on the data. The first thing we need to do to be able
    to work with this `data` is to denormalize it. **Denormalization** is a process
    of restructuring data into a single table. This involves merging data from multiple
    tables or flattening out nested data structures. It usually introduces some duplication
    of data; however, it simplifies data analysis by eliminating the need to deal
    with nested structures or to look related data up across multiple tables. In our
    case, this means transforming `data` into a list whose items are campaign dictionaries,
    augmented with their relative `user` dictionary. Users will be duplicated in each
    campaign they are associated with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The first item in `data` now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we would like to help you and offer a deterministic second part of the
    chapter, so we are going to save the data we generated here so that we (and you,
    too) will be able to load it from the next Notebook, and we should then have the
    same results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You should find the `data.json` file in the source code for the book. Now, we
    are done with `ch13-dataprep` , so we can close it and open the `ch13` notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have prepared our data, we can start analyzing it. First, we have
    another round of imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We have already seen the `json` module in *Chapter 8, Files and Data Persistence*
    . We also briefly introduced `arrow` in *Chapter 2, Built-In Data Types* . It
    is a very useful third-party library that makes working with dates and times a
    lot easier. `pandas` is the core upon which the whole project is based. **pandas**
    stands for **Python Data Analysis Library** . Among many other things, it provides
    the `DataFrame` , a matrix-like data structure with advanced processing capabilities.
    It is customary to `import pandas as pd` and also import `DataFrame` separately
    **.**
  prefs: []
  type: TYPE_NORMAL
- en: 'After the imports, we load our data into a `DataFrame` using the `pandas.read_json()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We inspect the first five rows using the `head()` method of `DataFrame` . You
    should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/B30992_13_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2: The first few rows of the DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter automatically renders the output of the `df.head()` call as HTML. To
    get a plain text representation, you can wrap `df.head()` in a `print` call.
  prefs: []
  type: TYPE_NORMAL
- en: The `DataFrame` structure allows you to perform various operations on its contents.
    You can filter by rows or columns, aggregate data, and so on. You can operate
    on entire rows or columns without suffering the time penalty you would have to
    pay if you were working on data with pure Python. This is possible because, under
    the hood, `pandas` harnesses the power of the **NumPy** library, which itself
    draws its incredible speed from the low-level implementation of its core.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy stands for Numeric Python. It is one of the most widely used libraries
    in the data science environment.
  prefs: []
  type: TYPE_NORMAL
- en: Using `DataFrame` allows us to couple the power of NumPy with spreadsheet-like
    capabilities so that we can work on our data in a way that is similar to what
    an analyst would normally do, only we do it with code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see two ways to quickly get an overview of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `count()` method returns a count of all the non-empty cells in each column.
    This is useful to help you understand how sparse your data is. In our case, we
    have no missing values, so the output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `cmp_name` | 5065 |'
  prefs: []
  type: TYPE_TB
- en: '| `cmp_bgt` | 5065 |'
  prefs: []
  type: TYPE_TB
- en: '| `cmp_spent` | 5065 |'
  prefs: []
  type: TYPE_TB
- en: '| `cmp_clicks` | 5065 |'
  prefs: []
  type: TYPE_TB
- en: '| `cmp_impr` | 5065 |'
  prefs: []
  type: TYPE_TB
- en: '| `user` | 5065 |'
  prefs: []
  type: TYPE_TB
- en: '| `dtype: int64` |  |'
  prefs: []
  type: TYPE_TB
- en: We have 5,065 rows. Given that we have 1,000 users and the number of campaigns
    per user is a random number between 2 and 8, that is in line with what we would
    expect.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `dtype: int64` line at the end of the output indicates that the values
    returned by `df.count()` are NumPy `int64` objects. Here, `dtype` stands for “data
    type” and `int64` means 64-bit integers. NumPy is largely implemented in C and,
    instead of using Python’s built-in numeric types, it uses its own types, which
    are closely related to C language data types. This allows it to perform numerical
    operations much more quickly than pure Python.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `describe` method is useful to quickly obtain a statistical summary of
    our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the output below, it gives us several measures, such as `count`
    , `mean` , `std` (standard deviation), `min` , and `max` , and shows how data
    is distributed in the various quartiles. Thanks to this method, we already have
    an idea of how our data is structured:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **cmp_bgt** | **cmp_spent** | **cmp_clicks** | **cmp_impr** |'
  prefs: []
  type: TYPE_TB
- en: '| **count** | 5065.000000 | 5065.000000 | 5065.000000 | 5065.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **mean** | 502965.054097 | 253389.854689 | 40265.781639 | 499999.474630 |'
  prefs: []
  type: TYPE_TB
- en: '| **std** | 290468.998656 | 222774.897138 | 21840.783154 | 2.023801 |'
  prefs: []
  type: TYPE_TB
- en: '| **min** | 1764.000000 | 107.000000 | 899.000000 | 499992.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **25%** | 251171.000000 | 67071.000000 | 22575.000000 | 499998.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **50%** | 500694.000000 | 187743.000000 | 36746.000000 | 499999.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **75%** | 756850.000000 | 391790.000000 | 55817.000000 | 500001.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **max** | 999565.000000 | 984705.000000 | 98379.000000 | 500007.000000 |'
  prefs: []
  type: TYPE_TB
- en: 'We can use the `sort_values()` and `head()` methods to see the campaigns with
    the highest budgets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output (we have omitted some columns to fit the output
    on the page):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **cmp_name** | **cmp_bgt** | **cmp_clicks** | **cmp_impr** |'
  prefs: []
  type: TYPE_TB
- en: '| **3186** | `GRZ_20230914_20230929_40-60_A_EUR` | 999565 | 63869 | 499998
    |'
  prefs: []
  type: TYPE_TB
- en: '| **3168** | `KTR_20250315_20260507_25-40_M_USD` | 999487 | 21097 | 500000
    |'
  prefs: []
  type: TYPE_TB
- en: '| **3624** | `GRZ_20250227_20250617_30-45_F_USD` | 999482 | 3435 | 499998 |'
  prefs: []
  type: TYPE_TB
- en: 'Calling `tail()` instead of `head()` shows us the campaigns with the lowest
    budgets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will take on some more complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Unpacking the campaign name
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Firstly, we want to get rid of the campaign name column ( `cmp_name` ). We need
    to explode it into parts and put each part in its own dedicated column. We will
    use the `apply()` method of the `Series` object to do this.
  prefs: []
  type: TYPE_NORMAL
- en: The `pandas.core.series.Series` class is a powerful wrapper around an array
    (think of it as a list with augmented capabilities). We can extract a `Series`
    object from `DataFrame` by accessing it in the same way we do with a key in a
    dictionary. We will then use the `apply()` method of the `Series` object to call
    a function on each item in the `Series` and obtain a new `Series` with the results.
    Finally, we compose the result into a new `DataFrame` , and then join that `DataFrame`
    with `df` .
  prefs: []
  type: TYPE_NORMAL
- en: We start by defining a function to split a campaign name into a tuple containing
    the type, start and end dates, target age, target gender, and currency. Note that
    we use `arrow.get()` to convert the start and end date strings to `date` objects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Next, we extract the `Series` with the campaign names from `df` and apply the
    `unpack_campaign_name()` function to each name.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can construct a new `DataFrame` from the `campaign_data` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'A quick peek at the first three rows reveals:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Type** | **Start** | **End** | **Target Age** | **Target Gender** |
    **Currency** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | `KTR` | `2025-04-04` | `2025-09-16` | `35-50` | `A` | `EUR` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `AKX` | `2024-01-30` | `2024-10-17` | `20-25` | `M` | `GBP` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `BYU` | `2023-08-28` | `2025-01-15` | `25-45` | `M` | `GBP` |'
  prefs: []
  type: TYPE_TB
- en: 'That looks better. Now, we can more easily work with the data represented by
    the column names. One important thing to remember: even if the dates are printed
    as strings, they are just the representation of the real `date` objects that are
    stored in `DataFrame` .'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can join the original `DataFrame` ( `df` ) and `campaign_df` into
    a single `DataFrame` . When joining two `DataFrame` instances, it is imperative
    that they have the same `index` , otherwise `pandas` will not be able to match
    up the rows. We took care of this by explicitly using the index from `df` when
    creating `campaign_df.`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us inspect the data to verify that everything matches correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The first few columns of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **cmp_name** | **Type** | **Start** | **End** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | `KTR_20250404_20250916_35-50_A_EUR` | `KTR` | `2025-04-04` | `2025-09-16`
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `AKX_20240130_20241017_20-25_M_GBP` | `AKX` | `2024-01-30` | `2024-10-17`
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `BYU_20230828_20250115_25-45_M_GBP` | `BYU` | `2023-08-28` | `2025-01-15`
    |'
  prefs: []
  type: TYPE_TB
- en: As you can see, `join()` was successful; the campaign name and the separated
    columns represent the same data.
  prefs: []
  type: TYPE_NORMAL
- en: Note how we access the `DataFrame` using the square brackets syntax, passing
    a list of column names. This will produce a new `DataFrame` , with those columns
    (in the same order), on which we then call the `head()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Unpacking the user data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now do the same thing for each piece of `user` JSON data. We call `apply()`
    on the `user` series, running the `unpack_user_json()` function, which takes a
    JSON `user` object and transforms it into a list of its fields. We create a new
    `DataFrame` , `user_df` , with this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we join `user_df` back with `df` (like we did with `campaign_df` ), and
    inspect the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The output shows us that everything went well.
  prefs: []
  type: TYPE_NORMAL
- en: Renaming columns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you evaluate `df.columns` in a cell, you will see that we still have ugly
    names for our columns. Let us change that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `rename()` method can be used to change the column (or row) labels. We have
    given it a dictionary mapping old column names to our preferred names. Any column
    that is not mentioned in the dictionary will remain unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: Computing some metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our next step will be to add some additional columns. For each campaign, we
    have the number of clicks and impressions, and we have the amounts spent. This
    allows us to introduce three measurement ratios: `CTR` , `CPC` , and `CPI` . They
    stand for *Click Through Rate* , *Cost Per Click* , and *Cost Per Impression*
    , respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last two are straightforward, but `CTR` is not. Suffice it to say that
    it is the ratio between clicks and impressions. It gives you a measure of how
    many clicks were performed on a campaign advertisement per impression—the higher
    this number, the more successful the advertisement is in attracting users to click
    on it. Let us write a function that calculates all three ratios and adds them
    to the `DataFrame` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we are adding those three columns with one line of code each, but
    the `DataFrame` applies the operation automatically (the division, in this case)
    to each pair of cells from the appropriate columns. So, even though it looks like
    we are only doing three divisions, there are actually *5,140 * 3* divisions because
    they are performed for each row. `pandas` does a lot of work for us while hiding
    much of the complexity of it.
  prefs: []
  type: TYPE_NORMAL
- en: The `calculate_metrics()` function takes a `DataFrame` ( `df` ) and works directly
    on it. This mode of operation is called **in-place** . This is similar to how
    the `list.sort()` method sorts a list. You could also say that this function is
    not pure, which means it has side effects, as it modifies the mutable object it
    is passed as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can take a look at the results by filtering on the relevant columns and
    calling `head()` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us that the calculations were performed correctly on each row:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Spent** | **Clicks** | **Impressions** | **CTR** | **CPC** | **CPI**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 29586 | 36632 | 500001 | 0.073264 | 0.807655 | 0.059172 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 166010 | 67325 | 499999 | 0.134650 | 2.465800 | 0.332021 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 125738 | 29989 | 499997 | 0.059978 | 4.192804 | 0.251478 |'
  prefs: []
  type: TYPE_TB
- en: 'We can also verify the accuracy of the results manually for the first row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The values match, confirming that our computations are correct. Of course, we
    would not normally need to do this, but we wanted to show you how can you perform
    such calculations. You can access a `Series` (a column) by passing its name to
    the `DataFrame` in square brackets (this is similar to looking up a key in a dictionary).
    You can then access each row in the column by its position, exactly as you would
    with a regular list or tuple.
  prefs: []
  type: TYPE_NORMAL
- en: We are almost done with our `DataFrame` . All we are missing now is a column
    that tells us the duration of the campaign and a column that tells us which day
    of the week each campaign started on. The duration is important to have, since
    it allows us to relate data such as the amount spent or number of impressions
    to the duration of the campaign (we may expect longer-running campaigns to cost
    more and have more impressions). The day of the week can also be useful; for example,
    some campaigns may be tied to events that happen on particular days of the week
    (such as sports events that take place on weekends).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`get_day_of_the_week()` takes a `date` object and formats it as a string that
    only contains the name of the corresponding day of the week. `get_duration()`
    is more interesting. First, notice that it takes an entire row, not just a single
    value. This function subtracts a campaign’s start date from the end date. When
    you subtract `date` objects, the result is a `timedelta` object, which represents
    a given amount of time. We take the value of its `.days` property to get the duration
    in days.'
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the starting day of the week for each campaign by applying `get_day_of_the_week()`
    to the `Start` column (as a `Series` object); this is similar to what we did with
    `"user"` and `"cmp_name"` . Next, we apply `get_duration()` to the whole `DataFrame`
    . Note that we instruct `pandas` to operate on the rows by passing `axis="columns"`
    . This may seem counter-intuitive but think of it as passing all the columns to
    each call of `get_duration()` .
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify the results as shown in the next cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Start** | **End** | **Duration** | **Day of Week** |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | `2025-04-04` | `2025-09-16` | 165 | `Friday` |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | `2024-01-30` | `2024-10-17` | 261 | `Tuesday` |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | `2023-08-28` | `2025-01-15` | 506 | `Monday` |'
  prefs: []
  type: TYPE_TB
- en: So, we now know that there are 165 days between the 4th of April, 2025, and
    the 16th of September, 2025, and that the 15th of January, 2025, is a Monday.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning everything up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have everything we want, it is time to do the final cleaning; remember
    we still have the `"cmp_name"` and `"user"` columns. Those are no longer needed,
    so we will remove them. We also want to reorder the columns in our `DataFrame`
    so that they are more relevant to the data it now contains. We can accomplish
    this by filtering `df` on the column list we want. The result is a new `DataFrame`
    that we can reassign to the name `df` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We have grouped the campaign information at the beginning, then the measurements,
    and finally the user data at the end. Now our `DataFrame` is clean and ready for
    us to inspect.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start creating some graphs, we want to take a snapshot of the `DataFrame`
    so that we can easily reconstruct it from a file without needing to redo all the
    steps we did to get here. Some analysts may want to have it in spreadsheet form,
    to do a different kind of analysis, so let us see how to save a `DataFrame` to
    a file.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the DataFrame to a file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can save a `DataFrame` in several formats. You can type `df.to_` and then
    press *Tab* to make autocompletion pop up, so you can see all the options.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to save our `DataFrame` in three different formats. First, CSV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, in an Excel spreadsheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `to_excel()` method requires the `openpyxl` package to be installed. It
    is included in the `requirements.txt` file for this chapter, so if you used that
    to install the requirements, you should have it in your virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, it is easy to save a `DataFrame` in many different formats.
    The good news is that the reverse is also true: it’s equally easy to load a spreadsheet
    into a `DataFrame` (just use the `pandas read_csv()` or `read_excel()` functions).'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we are going to visualize some results. From a data science
    perspective, we are not going to attempt an in-depth analysis of the data or try
    to draw any conclusions from it. It would not make much sense because the data
    is completely random. However, this example should still be enough to get you
    started with graphs and other features.
  prefs: []
  type: TYPE_NORMAL
- en: One lesson that we have learned the hard way is that appearance makes a difference
    in how people perceive your work. If you want to be taken seriously, think carefully
    about how you present your data and try to make your graphs and tables look appealing.
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` uses the Matplotlib plotting library to draw graphs. We will not be
    using it directly, except to configure the plot style and save plots to disk.
    You can learn more about this versatile plotting library at [https://matplotlib.org/](https://matplotlib.org/)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will configure the Notebook to render Matplotlib graphs as interactive
    widgets in the cell output frame. We do it with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This will allow you to pan and zoom the figure and save a (low-resolution) snapshot
    to disk.
  prefs: []
  type: TYPE_NORMAL
- en: By default (without `%matplotlib widget` ), figures will be rendered as static
    images in the cell output frame. Using the interactive widget mode requires the
    `ipympl` package to be installed. It is included in the `requirements.txt` file
    for this chapter, so if you used that to install the requirements, you should
    have it in your virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we set up some styling for our plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We use the `matplotlib.pyplot` interface to set the plot style. We have chosen
    to use a combination of the `classic` and `ggplot` style sheets. Style sheets
    are applied from left to right, so here `ggplot` will override the `classic` style
    for any style items that are defined in both. We also set the font family used
    in the plots to `serif` . The `plt.rc("savefig", dpi=300)` call, configures the
    `savefig()` method to generate high-resolution image files that are suitable for
    printing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we generate any graphs, let us run `df.describe()` ( `#26` ) again.
    The results should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/B30992_13_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.3: Some statistics for our cleaned-up data'
  prefs: []
  type: TYPE_NORMAL
- en: This kind of quick result is perfect for satisfying those managers who have
    20 seconds to dedicate to you and just want rough numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, please keep in mind that our campaigns have different currencies,
    so these numbers are meaningless. The point here is to demonstrate the `DataFrame`
    capabilities, not to perform a correct or detailed analysis of real data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, a graph is usually much better than a table with numbers because
    it is much easier to read, and it gives you immediate feedback. So, let us plot
    the four pieces of information we have on each campaign: `"Budget"` , `"Spent"`
    , `"Clicks"` , and `"Impressions"` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We extract those four columns (this will give us another `DataFrame` made with
    only those columns) and call the `hist()` method to get a histogram plot. We give
    some arguments to specify the number of bins and figure sizes, and everything
    else is done automatically. The result looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/B30992_13_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.4: Histogram plots of the campaign data'
  prefs: []
  type: TYPE_NORMAL
- en: We also call `plt.savefig("Figure13.4.png")` to save the image to a file named
    `Figure13.4.png` . This will use the `300dpi` setting we configured previously
    to generate a high-resolution image. Note that `plt.savefig()` will save the most
    recent image generated by Matplotlib. Calling it from the same cell where we generate
    the plot ensures that we save the correct image to the correct filename.
  prefs: []
  type: TYPE_NORMAL
- en: Although it is meaningless to try to interpret plots of random data, we can
    at least verify that what we see matches what we might expect, given how the data
    was generated.
  prefs: []
  type: TYPE_NORMAL
- en: '*Budget* is selected randomly from an interval, so we expect a uniform distribution.
    Looking at the graph, that is indeed what we see.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Spent* is also uniformly distributed, but its upper limit is the budget, which
    is not constant. This means we should expect a logarithmic curve that decreases
    from left to right. Again, that matches what the graph shows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Clicks* was generated with a triangular distribution with a mean 20% of the
    interval size, and you can see that the peak is right there, at about 20% to the
    left.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Impressions* was a Gaussian distribution, which assumes the famous bell shape.
    The mean was exactly in the middle and we had a standard deviation of 2. You can
    see that the graph matches those parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us also plot the metrics we calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/B30992_13_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.5: Histogram plots of computed measures'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the *CPC* is highly skewed to the left, meaning that most of
    the *CPC* values are low. The *CPI* has a similar shape but is less extreme.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, suppose you want to analyze only a particular segment of the data. We
    can apply a mask to the `DataFrame` so that we get a new `DataFrame` with only
    the rows that satisfy the mask condition. It is like applying a global, row-wise
    `if` clause:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we prepared `selector` to filter out all the rows for which the
    amount spent is less than or equal to 75% of the budget. In other words, we will
    include only those campaigns for which we have spent at least three-quarters of
    the budget. Notice that in `selector` , we are showing you an alternative way
    of asking for a `DataFrame` column, by using direct property access ( `object.property_name`
    ), instead of dictionary-like access ( `object['property_name']` ). If `property_name`
    is a valid Python name, you can use both ways interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: '`selector` is applied in the same way that we access a dictionary with a key.
    When we apply `selector` to `df` , we get a new `DataFrame` . We select only the
    relevant columns from this and call `hist()` again. This time, we set `color`
    to `green` , just to show how that can be done.'
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/B30992_13_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.6: Histogram plots of campaign data where at least 75% of the budget
    was spent'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the shapes of the graphs have not changed much, apart from the *Spent*
    graph, which is quite different. This is because we have selected only the rows
    where the amount spent is at least 75% of the budget. This means that we are including
    only the rows where the amount spent is close to the budget. The budget numbers
    come from a uniform distribution. Therefore, the *Spent* graph is now assuming
    that kind of shape. If you make the boundary even tighter and ask for 85% or more,
    you will see the *Spent* graph become increasingly similar to *Budget* .
  prefs: []
  type: TYPE_NORMAL
- en: Let us also look at a different kind of plot. We will plot the sums of `"Spent"`
    , `"Clicks"` , and `"` `Impressions"` for each day of the week.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The first line creates a new `DataFrame` , `df_weekday` , by asking for a grouping
    by `"Day of Week"` on `df` . We then aggregate, by computing the sum within each
    group. Note that we must pass `numeric_only=True` to avoid errors when attempting
    to sum columns with non-numeric data. We could also have taken a different approach
    by selecting only the columns we need ( `"Day of Week"` , `"Impressions"` , `"Spent"`
    , and `"Clicks"` ) before grouping and summing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this time we call `plot()` instead of `hist()` . This will draw a
    line graph instead of a histogram. The `subplots=True` option makes `plot` draw
    three separate graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/B30992_13_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.7: Plots of campaign data aggregated by day of the week'
  prefs: []
  type: TYPE_NORMAL
- en: These plots show that campaigns that started on Thursdays got the most clicks
    and impressions, while campaigns that started on Saturdays spent the least money.
    If this were real data, this would potentially be important information to give
    to our clients.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the days are sorted alphabetically, which makes the graph difficult
    to read. We will leave it to you as an exercise to find a way to fix this.
  prefs: []
  type: TYPE_NORMAL
- en: Let us finish this presentation section with a couple more things. First, a
    simple aggregation. We want to group by `"Target Gender"` and `"Target Age"` ,
    and compute the mean and the standard deviation `("std")` of `"Impressions"` and
    `"Spent"` within each grouping.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We prepare a dictionary to use as the configuration. Then, we perform a grouping
    on the `"Target Gender"` and `"Target Age"` columns, and we pass our configuration
    dictionary to the `agg()` method. The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | **Impressions** | **Spent** |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | **mean** | **std** | **mean** | **std** |'
  prefs: []
  type: TYPE_TB
- en: '| **Target Gender** | **Target Age** |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **A** | 20-25 | 499999.245614 | 2.189918 | 217330.771930 | 204518.652595
    |'
  prefs: []
  type: TYPE_TB
- en: '| 20-30 | 499999.465517 | 2.210148 | 252261.637931 | 228932.088945 |'
  prefs: []
  type: TYPE_TB
- en: '| 20-35 | 499998.564103 | 1.774006 | 218726.410256 | 215060.976707 |'
  prefs: []
  type: TYPE_TB
- en: '| 20-40 | 499999.459016 | 1.971241 | 255598.213115 | 222697.755231 |'
  prefs: []
  type: TYPE_TB
- en: '| 20-45 | 499999.574074 | 2.245346 | 216527.666667 | 190345.252888 |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **M** | 45-50 | 499999.480769 | 2.128153 | 276112.557692 | 226975.008137
    |'
  prefs: []
  type: TYPE_TB
- en: '| 45-55 | 499999.306122 | 2.053494 | 267137.938776 | 239249.474145 |'
  prefs: []
  type: TYPE_TB
- en: '| 45-60 | 499999.500000 | 1.984063 | 236623.312500 | 223464.578371 |'
  prefs: []
  type: TYPE_TB
- en: '| 45-65 | 499999.679245 | 1.503503 | 215634.528302 | 223308.046968 |'
  prefs: []
  type: TYPE_TB
- en: '| 45-70 | 499998.870370 | 1.822773 | 310267.944444 | 242353.980346 |'
  prefs: []
  type: TYPE_TB
- en: 'Let us do one more thing before we wrap this chapter up. We want to show you
    something called a **pivot table** . A pivot table is a way of grouping data,
    computing an aggregate value for each group, and displaying the result in table
    form. The pivot table is an essential tool for data analysis, so let us see a
    simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a pivot table that shows us the correlation between `"Target Age"`
    and `"Impressions"` , `"Clicks"` , and `"Spent"` . These last three will be subdivided
    according to `"` `Target Gender"` . The `aggfunc` argument specifies the aggregation
    function to use, it can be a function object, the name of a function, a list of
    functions, or a dictionary that maps column names to functions. In this case,
    we use `"sum"` (the default, if no function is specified is to compute the `mean`
    ). The result is a new `DataFrame` containing the pivot table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/B30992_13_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.8: A pivot table'
  prefs: []
  type: TYPE_NORMAL
- en: That brings us to the end of our data analysis project. We will leave you to
    discover more about the wonderful world of IPython, Jupyter, and data science.
    We strongly encourage you to get comfortable with the Notebook environment. It
    is much better than a console, it is practical and fun to use, and you can even
    create slides and documents with it.
  prefs: []
  type: TYPE_NORMAL
- en: Where do we go from here?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data science is indeed a fascinating subject. As we said in the introduction,
    those who want to delve into its meanders need to have a solid foundation in mathematics
    and statistics. Working with data that has been interpolated incorrectly renders
    any result about it useless. The same goes for data that has been extrapolated
    incorrectly or sampled with the wrong frequency. To give you an example, imagine
    a population of individuals that are aligned in a queue. If, for some reason,
    the gender of that population alternated between male and female, the queue would
    look something like this: F-M-F-M-F-M-F-M-F...'
  prefs: []
  type: TYPE_NORMAL
- en: If you sampled it, taking only the even elements, you would draw the conclusion
    that the population was made up only of males, while sampling the odd ones would
    tell you exactly the opposite.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this was just a silly example, but it is easy to make mistakes in
    this field, especially when dealing with big datasets where sampling is mandatory
    and, therefore, the quality of your analysis depends, first and foremost, on the
    quality of the sampling itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to data science and Python, these are the main tools you want
    to look at:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NumPy** ( [https://www.numpy.org/](https://www.numpy.org/) ): This is the
    main package for scientific computing with Python. It contains a powerful N-dimensional
    array object, sophisticated (broadcasting) functions, tools for integrating C/C++
    and Fortran code, useful linear algebra, the Fourier transform, random number
    capabilities, and much more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scikit-learn** ( [https://scikit-learn.org/](https://scikit-learn.org/) ):
    This is one of the most popular machine learning libraries in Python. It has simple
    and efficient tools for data mining and data analysis, is accessible to everybody,
    and is reusable in various contexts. It is built on NumPy, SciPy, and Matplotlib.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pandas** ( [https://pandas.pydata.org/](https://pandas.pydata.org/) ): This
    is an open-source, BSD-licensed library providing high-performance, easy-to-use
    data structures and data analysis tools. We have used it throughout this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IPython** ( [https://ipython.org/](https://ipython.org/) )/ **Jupyter** (
    [https://jupyter.org/](https://jupyter.org/) ): These provide a rich architecture
    for interactive computing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matplotlib** ( [https://matplotlib.org/](https://matplotlib.org/) ): This
    is a Python 2D plotting library that produces publication-quality figures in a
    variety of hard-copy formats and interactive environments across platforms. Matplotlib
    can be used in Python scripts, the Python and IPython shell, a Jupyter Notebook,
    web application servers, and several graphical user interface toolkits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seaborn** ( [https://seaborn.pydata.org/](https://seaborn.pydata.org/) ):
    This is a Python data visualization library based on Matplotlib. It provides a
    high-level interface for drawing attractive and informative statistical graphics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numba** ( [https://numba.pydata.org/](https://numba.pydata.org/) ): This
    gives you the power to speed up your applications with high-performance functions
    written directly in Python. With a few annotations, array-oriented and math-heavy
    Python code can be just-in-time compiled to native machine instructions, similar
    in performance to C, C++, and Fortran, without having to switch languages or Python
    interpreters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bokeh** ( [https://bokeh.pydata.org/](https://bokeh.pydata.org/) ): This
    is a Python-interactive visualization library that targets modern web browsers
    for presentation. Its goal is to provide elegant, concise construction of novel
    graphics in the style of D3.js, but also deliver this capability with high-performance
    interactivity over large or streaming datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other than these single libraries, you can also find ecosystems, such as **SciPy**
    ( [https://scipy.org/](https://scipy.org/) ) and the aforementioned **Anaconda**
    ( [https://anaconda.org/](https://anaconda.org/) ), that bundle several different
    packages to give you something that just works in an “out-of-the-box” fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Installing all these tools and their several dependencies is hard on some systems,
    so we suggest that you try out ecosystems as well to see whether you are comfortable
    with them. It may be worth it.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about data science. Rather than attempting to explain
    anything about this broad subject, we delved into a project. We familiarized ourselves
    with the Jupyter Notebook, and with different libraries, such as `pandas` , `Matplotlib`
    , and `NumPy` .
  prefs: []
  type: TYPE_NORMAL
- en: Of course, having to compress all this information into one single chapter means
    we could only touch briefly on the subjects we presented. We hope the project
    we have worked through together has been comprehensive enough to give you an idea
    of the workflow to follow when working in this field.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is dedicated to API development.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://discord.com/invite/uaKmaz7FEC](Chapter_13.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/QR_Code119001106417026468.png)'
  prefs: []
  type: TYPE_IMG
