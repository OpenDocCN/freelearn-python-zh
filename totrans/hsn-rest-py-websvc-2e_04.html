<html><head></head><body><div><h1 class="header-title">Testing and Deploying an API in a Microservice with Flask</h1>
                
            
            
                
<p>In this chapter, we will configure, write, and execute unit tests and learn a few things related to deployment. We will do the following:</p>
<ul>
<li>Set up unit tests with <kbd>pytest</kbd></li>
<li>Create a database for testing</li>
<li>Create fixtures to perform setup and teardown tasks for running clean tests</li>
<li>Write the first round of unit tests</li>
<li>Run unit tests with <kbd>pytest</kbd> and check testing coverage</li>
<li>Improve testing coverage</li>
<li>Understand strategies for deployments and scalability</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Setting up unit tests with pytest</h1>
                
            
            
                
<p>So far, we have been writing code to add features to our RESTful API. We used command-line and GUI tools to understand how all the pieces worked together and to check the results of diverse HTTP requests made to the RESTful API with Flask's development server. Now we will write unit tests that will allow us to make sure that the RESTful API works as expected. Before we can start writing unit tests, it is necessary to install many additional packages in our virtual environment, create a new PostgreSQL database that we will use for testing, and build a configuration file for the testing environment.</p>
<p>Make sure you quit Flask's development server. You just need to press <em>Ctrl</em> + <em>C</em> in the Terminal or Command Prompt window in which it is running.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now we will install many additional packages. Make sure you have activated the virtual environment named <kbd>Flask01</kbd>, which we created in <a href="dbf75cef-4962-4e40-8192-03873b774c48.xhtml" target="_blank">Chapter 1</a>, <em>Developing RESTful APIs and Microservices with Flask 1.0.2</em>. After you activate the virtual environment, it is time to run many commands, which will be the same for macOS, Linux, or Windows.</p>
<p>Now we will edit the existing <kbd>requirements.txt</kbd> file to specify the additional set of packages that our application requires to be installed in any supported platform. This way, it will be extremely easy to repeat the installation of the specified packages with their versions in any new virtual environment.</p>
<p>Use your favorite editor to edit the existing text file named <kbd>requirements.txt</kbd> within the root folder for the virtual environment. Add the following lines after the last line to declare the additional packages and the versions that our new version of the API requires. The code file for the sample is included in the <kbd>restful_python_2_04_01</kbd> folder, in the <kbd>Flask01/requirements.txt</kbd> file:</p>
<pre>pytest==4.0.1 
coverage==4.5.2 
pytest-cov==2.6.0 </pre>
<p>Each additional line added to the <kbd>requirements.txt</kbd> file indicates the package and the version that needs to be installed. The following table summarizes the packages and the version numbers that we specified as additional requirements to the previously included packages:</p>
<table style="border-collapse: collapse" border="1">
<tbody>
<tr>
<td>
<p>Package name</p>
</td>
<td>
<p>Version to be installed</p>
</td>
</tr>
<tr>
<td>
<p><kbd>pytest</kbd></p>
</td>
<td>
<p>4.0.1</p>
</td>
</tr>
<tr>
<td>
<p><kbd>coverage</kbd></p>
</td>
<td>
<p>4.5.2</p>
</td>
</tr>
<tr>
<td>
<p><kbd>pytest-cov</kbd></p>
</td>
<td>
<p>2.6.0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We will install the following Python packages in our virtual environment:</p>
<ul>
<li><kbd>pytest</kbd>: This is a very popular Python unit testing framework that makes testing easy and reduces boilerplate code.</li>
<li><kbd>coverage</kbd>: This tool measures code coverage of Python programs and we will use it to determine which parts of the code are being executed by unit tests and which parts aren't.</li>
<li><kbd>pytest-cov</kbd>: This plugin for <kbd>pytest</kbd> makes it easy to produce coverage reports that use the <kbd>coverage</kbd> tool under the hood, and provides some additional features.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now we must run the following command on macOS, Linux, or Windows to install the additional packages and the versions outlined in the previous table with <kbd>pip</kbd> using the recently edited <kbd>requirements.txt</kbd> file. Make sure you are in the folder that has the <kbd>requirements.txt</kbd> file before running the command:</p>
<pre>    <strong>pip install -r requirements.txt</strong></pre>
<p>The last lines for the output will indicate all the new packages and their dependencies have been successfully installed. If you downloaded the source code for the example and you didn't work with the previous version of the API, <kbd>pip</kbd> will also install the other packages included in the <kbd>requirements.txt</kbd> file:</p>
<pre><strong>Installing collected packages: atomicwrites, six, more-itertools, pluggy, py, attrs, pytest, coverage, pytest-cov</strong>
<strong>Successfully installed atomicwrites-1.2.1 attrs-18.2.0 coverage-4.5.2 more-itertools-4.3.0 pluggy-0.8.0 py-1.7.0 pytest-4.0.1 pytest-cov-2.6.0 six-1.12.0</strong>
  </pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Creating a database for testing</h1>
                
            
            
                
<p>Now we will create the PostgreSQL database that we will use as a repository for our testing environment. Notice that the testing computer or server must have PostgreSQL 10.5 installed on it, as explained in the previous chapters for the development environment. I assume that you are running the tests on the same computer in which you worked with the previous examples.</p>
<p>Remember to make sure that the PostgreSQL bin folder is included in the <kbd>PATH</kbd> environmental variable. You should be able to execute the <kbd>psql</kbd> command-line utility from your current Terminal, Command Prompt, or Windows PowerShell.</p>
<p>We will use the PostgreSQL command-line tools to create a new database named <kbd>test_flask_notifications</kbd>. If you already ...</p></div>



  
<div><h1 class="header-title">Creating fixtures to perform setup and teardown tasks for running clean tests</h1>
                
            
            
                
<p>Test fixtures provide a fixed baseline to enable us to reliably and repeatedly execute tests. Pytest makes it easy to declare a test fixture function by marking a function with the <kbd>@pytest.fixture</kbd> decorator. Then, whenever we use the fixture function name as an argument in a test function declaration, <kbd>pytest</kbd> will make the fixture function provide the fixture object. Now we will create the following two <kbd>pytest</kbd> fixture functions, which we will use in future test functions:</p>
<ul>
<li><kbd>application</kbd>: This test fixture function will perform the necessary setup tasks to create the Flask test app with the appropriate testing configuration and create all the necessary tables in the test database. The fixture will launch the test execution and when the test finishes, the fixture will perform the necessary teardown tasks to leave the database as it was before running the test.</li>
<li><kbd>client</kbd>: This test fixture function receives <kbd>application</kbd> as an argument, and therefore, it receives the Flask app created in the previously explained application test fixture function in this argument. Hence, the <kbd>client</kbd> test fixture function configures the application for testing, initializes the database, creates a test client for this application and returns it. We will use the test client in our test methods to easily compose and send requests to our API.</li>
</ul>
<p>Create a new <kbd>conftest.py</kbd> file within the <kbd>service</kbd> folder. Add the following lines that declare many <kbd>import</kbd> statements and the previously explained <kbd>pytest</kbd> test fixture functions. The code file for the sample is included in the <kbd>restful_python_2_04_01</kbd> folder, in the <kbd>Flask01/service/conftest.py</kbd> file:</p>
<pre>import pytest 
from app import create_app 
from models import orm 
from flask_sqlalchemy import SQLAlchemy 
from flask import Flask 
from views import service_blueprint 
 
 
<strong>@pytest.fixture</strong> 
<strong>def application():</strong> 
    # Beginning of Setup code 
    app = create_app('test_config') 
    with app.app_context():    
        orm.create_all() 
        # End of Setup code 
        # The test will start running here 
        yield app 
        # The test finished running here 
        # Beginning of Teardown code 
        orm.session.remove() 
        orm.drop_all() 
        # End of Teardown code 
 
 
<strong>@pytest.fixture</strong> 
<strong>def client(application):</strong> 
    return application.test_client() </pre>
<p>The <kbd>application</kbd> fixture function will be executed each time a test that uses either <kbd>application</kbd> or <kbd>client</kbd> as arguments. The function calls the <kbd>create_app</kbd> function, declared in the <kbd>app</kbd> module, with <kbd>'test_config'</kbd> as an argument. The function will set up a Flask app with this module as the configuration file, and therefore, the app will use the previously created configuration file that specifies the desired values for our testing database and environment.</p>
<p>The next line calls the <kbd>orm.create_all</kbd> method to create all the necessary tables in our test database configured in the <kbd>test_config.py</kbd> file. All the code after the <kbd>yield app</kbd> line works as the teardown code that is executed after <kbd>app</kbd> is used and the test is executed. The code removes the SQLAlchemy session and drops all the tables that we created in the test database before starting the execution of the test. This way, after each test finishes its execution, the test database will be empty again.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Writing the first round of unit tests</h1>
                
            
            
                
<p>Now, we will write the first round of unit tests. Specifically, we will write unit tests related to the user and notification category resources: <kbd>UserResource</kbd>, <kbd>UserListResource</kbd>, <kbd>NotificationCategoryResource</kbd>, and <kbd>NotificationCategoryListResource</kbd>.</p>
<p>Create a new <kbd>tests</kbd> subfolder within the <kbd>service</kbd> folder. Then, create a new <kbd>test_views.py</kbd> file within the new <kbd>service/tests</kbd> subfolder. Add the following lines that declare many <kbd>import</kbd> statements and the first functions that we will use in many test functions. The code file for the sample is included in the <kbd>restful_python_2_04_01</kbd> folder, in the <kbd>Flask01/service/tests/test_views.py</kbd> file:</p>
<pre>import pytest from base64 import b64encode from flask import current_app, json, ...</pre></div>



  
<div><h1 class="header-title">Running unit tests with pytest and checking testing coverage</h1>
                
            
            
                
<p>Create a new <kbd>setup.cfg</kbd> file within the <kbd>service</kbd> folder. The following lines show the code that specifies the desired configuration for <kbd>pytest</kbd> and the <kbd>coverage</kbd> tools. The code file for the sample is included in the <kbd>restful_python_2_04_01</kbd> folder, in the <kbd>Flask01/service/setup.cfg</kbd> file:</p>
<pre>[tool:pytest] 
testpaths = tests 
 
[coverage:run] 
branch = True 
source =  
    models 
    views </pre>
<p>The <kbd>tool:pytest</kbd> section specifies the configuration for <kbd>pytest</kbd>. The <kbd>testpaths</kbd> setting assigns the <kbd>tests</kbd> value to indicate that the tests are located within the <kbd>tests</kbd> subfolder.</p>
<p>The <kbd>coverage:run</kbd> section specifies the configuration for the <kbd>coverage</kbd> tool. The <kbd>branch</kbd> setting is set to <kbd>True</kbd> to enable branch coverage measurement, in addition to the default statement coverage. The <kbd>source</kbd> setting specifies the modules that we want to be considered for the coverage measurement. We just want to include the <kbd>models</kbd> and <kbd>views</kbd> modules.</p>
<p>Now we will use the <kbd>pytest</kbd> command to run tests and measure their code coverage. Make sure you run the command in the Terminal or Command Prompt window in which you have activated the virtual environment and that you are located within the <kbd>service</kbd> folder. Run the following command:</p>
<pre>    <strong>pytest --cov -s</strong></pre>
<p>The test runner will execute all the functions defined in the <kbd>test_views.py</kbd> that start with the <kbd>test_</kbd> prefix and will display the results. We will use the <kbd>-v</kbd> option to instruct <kbd>pytest</kbd> to print the test function names and statuses in verbose mode. The <kbd>--cov</kbd> option turns on test-coverage-reporting generation with the usage of the <kbd>pytest-cov</kbd> plugin.</p>
<p>The tests won't make changes to the database we have been using when working on the API. Remember that we configured the <kbd>test_flask_notifications</kbd> database as our test database.</p>
<p>The following lines show the sample output:</p>
<pre><strong>=================================== test session starts ===================================</strong>
<strong> latform darwin -- Python 3.7.1, pytest-4.0.1, py-1.7.0, pluggy-0.8.0 - <br/> - /Users/gaston/HillarPythonREST2/Flask01/bin/python3</strong>
 <strong>cachedir: .pytest_cache</strong>
 <strong>rootdir: /Users/gaston/HillarPythonREST2/Flask01/service, inifile: <br/> setup.cfg</strong>
 <strong>plugins: cov-2.6.0</strong>
 <strong>collected 5 items                                                                         </strong>
    
    <strong>tests/test_views.py::test_request_without_authentication PASSED                     <br/>    [ 20%]</strong>
    <strong>tests/test_views.py::test_create_and_retrieve_notification_category <br/>    PASSED          [ 40%]</strong>
    <strong>tests/test_views.py::test_create_duplicated_notification_category <br/>    PASSED            [ 60%]</strong>
    <strong>tests/test_views.py::test_retrieve_notification_categories_list <br/>    PASSED              [ 80%]</strong>
    <strong>tests/test_views.py::test_update_notification_category PASSED                       <br/>    [100%]</strong>
    
    <strong>---------- coverage: platform darwin, python 3.7.1-final-0 --------<br/>    ---</strong>
    <strong>Name        Stmts   Miss Branch BrPart  Cover</strong>
    <strong>---------------------------------------------</strong>
    <strong>models.py     101     27     24      7    66%</strong>
    <strong>views.py      208    112     46     10    43%</strong>
    <strong>---------------------------------------------</strong>
    <strong>TOTAL         309    139     70     17    51%</strong>
    
    
<strong>========================== 5 passed, 1 warnings in 18.15 seconds ==========================</strong>
  </pre>
<p>Pytest uses the configuration specified in the previously created <kbd>setup.cfg</kbd> file to determine which path includes the modules whose names start with the <kbd>test</kbd> prefix. In this case, the only module that matches the criteria is the <kbd>test_views</kbd> module. In the modules that match the criteria, <kbd>pytest</kbd> loads tests from all the functions whose names start with the <kbd>test</kbd> prefix.</p>
<p>The output provided details that the test runner discovered and executed five tests and all of them passed. The output displays the module and function names for each method in the <kbd>test_views</kbd> module that started with the <kbd>test_</kbd> prefix and represented a test to be executed.</p>
<p>The test code coverage measurement report provided by the <kbd>coverage</kbd> package in combination with the <kbd>pytest-cov</kbd> plugin uses the code analysis tools and the tracing hooks included in the Python standard library to determine which lines of code are executable and which of these lines have been executed. The report provides a table with the following columns:</p>
<ul>
<li><kbd>Name</kbd>: The Python module name</li>
<li><kbd>Stmts</kbd>: The count of executable statements for the Python module</li>
<li><kbd>Miss</kbd>: The number of executable statements missed, that is, the ones that weren't executed</li>
<li><kbd>Branch</kbd>: The count of possible branches for the Python module</li>
<li><kbd>BrPart</kbd>: The number of branches that were executed during tests</li>
<li><kbd>Cover</kbd>: The coverage of executable statements and branches, expressed as a percentage</li>
</ul>
<p>We definitely have incomplete coverage for the <kbd>views.py</kbd> and <kbd>models.py</kbd> modules based on the measurements shown in the report. In fact, we just wrote a few tests related to the notification categories and users, and therefore, it makes sense that the coverage is lower than 50% for the <kbd>views.py</kbd> module. We didn't create tests related to notifications.</p>
<p>We can run the <kbd>coverage</kbd> command with the <kbd>-m</kbd> command-line option to display the line numbers of the missing statements in a new <kbd>Missing</kbd> column:</p>
<pre>    <strong>coverage report -m</strong></pre>
<p>The command will use the information from the last execution and will display the missing statements and the missing branches. The next lines show a sample output that corresponds to the previous execution of the unit tests. A dash (<kbd>-</kbd>) is used to indicate a range of lines that were missed. For example, <kbd>22-23</kbd> means that lines <kbd>22</kbd> and <kbd>23</kbd> were missing statements. A dash followed by a greater than sign (<kbd>-&gt;</kbd>) indicates that the branch from the line before <kbd>-&gt;</kbd> to the line after it was missed. For example, <kbd>41-&gt;42</kbd> means that the branch from line <kbd>41</kbd> to line <kbd>42</kbd> was missed:</p>
<pre>    <strong>Name        Stmts   Miss Branch BrPart  Cover   Missing</strong>
    <strong>-------------------------------------------------------</strong>
    <strong>models.py     101     27     24      7    66%   22-23, 38, 40, 42, <br/>    44, 46, 48, 68-75, 78-80, 94, 133-143, 37-&gt;38, 39-&gt;40, 41&gt;42, 43-<br/>    &gt;44, 45-&gt;46, 47-&gt;48, 93-&gt;94</strong>
    <strong>views.py      208    112     46     10    43%   37-39, 45-52, 57-<br/>    58, 61, 65-66, 77-81, 86-88, 91-118, 121-129, 134-141, 144-175, <br/>    188-189, 192, 199-200, 203-206, 209-217, 230-231, 234, 245-250, 56-<br/>    &gt;57, 60-&gt;61, 64-&gt;65, 71-&gt;77, 187-&gt;188, 191-&gt;192, 194-&gt;201, 196-<br/>    &gt;199, 229-&gt;230, 233-&gt;234</strong>
    <strong>-------------------------------------------------------</strong>
    <strong>TOTAL         309    139     70     17    51%</strong>
  </pre>
<p>Now run the following command to get annotated HTML listings detailing missed lines. The command won't produce any output:</p>
<pre>    <strong>coverage html</strong></pre>
<p>Open the <kbd>index.html</kbd> HTML file generated in the <kbd>htmlcov</kbd> folder with your web browser. The following screenshot shows an example report that coverage generated in HTML format:</p>
<div><img src="img/aec4ebbd-55ec-4b41-b625-ca440e70f178.png" style="width:39.83em;height:11.92em;" width="1210" height="362"/></div>
<p>Click or tap <kbd>views.py</kbd> and the web browser will render a web page that displays the statements that were run, including the missing ones, the excluded ones, and the partially executed ones, with different colors. We can click or tap on the run, missing, excluded, and partial buttons to show or hide the background color that represents the status for each line of code. By default, the missing lines of code will be displayed with a pink background and the partially executed will be displayed with a yellow background. Thus, we must write unit tests that target these lines of code to improve our tests coverage. The following screenshot shows the buttons with the summary:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/9c948132-01e0-4bee-acbd-b6e8ca63d362.png" width="1229" height="521"/></p>
<p>The next screenshot shows the highlighted missing lines and the partially evaluated branches for some lines of code in the <kbd>views.py</kbd> module:</p>
<div><img src="img/419e2a16-088a-40cf-8159-205b45bac59a.png" style="width:38.83em;height:37.58em;" width="1451" height="1400"/></div>


            

            
        
    </div>



  
<div><h1 class="header-title">Improving testing coverage</h1>
                
            
            
                
<p>Now we will write additional tests functions to improve the testing coverage. Specifically, we will write unit tests related to notifications and users.</p>

<p>Open the existing <kbd>service/tests/test_views.py</kbd> file and insert the following lines after the last line. The code file for the sample is included in the <kbd>restful_python_2_04_02</kbd> folder, in the <kbd>Flask01/service/tests/test_views.py</kbd> file:</p>
<pre><strong>def create_notification(client, message, ttl,</strong><strong>notification_category): 
</strong> url = url_for('service.notificationlistresource', _external=True) data = {'message': message, 'ttl': ttl, 'notification_category': notification_category} response = client.post( url, headers=get_authentication_headers(TEST_USER_NAME, TEST_USER_PASSWORD), data=json.dumps(data)) ...</pre></div>



  
<div><h1 class="header-title">Understanding strategies for deployments and scalability</h1>
                
            
            
                
<p>Flask is a lightweight microframework for the web, and therefore, it is an ideal choice whenever we have to provide a RESTful API encapsulated in a microservice. So far, we have been working with the built-in development server provided by Werkzeug and with plain HTTP.</p>
<p>It is very important to understand that Flask's built-in development server is not suitable for production.</p>
<p>There are dozens of deployment options for Flask, and the different stacks and procedures are out of the scope of this book, which is focused on development tasks for RESTful APIs with the most popular Python frameworks. The most prominent cloud providers include instructions on how to deploy Flask applications with diverse possible configurations. In addition, there are many options to use <strong>WSGI</strong> (short for <strong>Web Server Gateway Interface</strong>) servers, which implement the web server side of the WSGI interface, allowing us to run Python web applications, such as Flask applications, in production.</p>
<p>Of course, in a production environment, we will also want to work with HTTPS instead of HTTP. We will have to configure the appropriate TLS certificates, also known as SSL certificates.</p>
<p>We used Flask to develop a RESTful web service. The key advantage of these kinds of web services is that they are stateless, that is, they shouldn't keep a client state on any server. Our API is a good example of a stateless RESTful web service with Flask. Flask-RESTful and PostgreSQL 10.5 can be containerized in a Docker container. For example, we can produce an image with our application configured to run with NGINX, uWSGI, Redis, and Flask. Thus, we can make the API run as a microservice.</p>
<p>We always have to make sure that we profile the API and the database before we deploy our first version of our API. It is very important to make sure that the generated queries run properly on the underlying database and that the most popular queries do not end up in sequential scans. It is usually necessary to add the appropriate indexes to the tables in the database.</p>
<p>We have been using basic HTTP authentication. We can improve it with a token-based authentication. We must make sure that the API runs under HTTPS in production environments.</p>
<p>It is convenient to use a different configuration file for production. However, another approach that is becoming extremely popular, especially for cloud-native applications, is storing configuration in the environment. If we want to deploy cloud-native RESTful web services and follow the guidelines established in the Twelve-Factor App, we should store config in the environment.</p>
<p class="mce-root"/>
<p>Each platform includes detailed instructions to deploy our application. All of them will require us to generate the <kbd>requirements.txt</kbd> file, which lists the application dependencies together with their versions. This way, the platforms will be able to install all the necessary dependencies listed in the file. We have been updating this file each time we needed to install a new package in our virtual environment. However, it is a good idea to run the following <kbd>pip freeze</kbd> within the root folder of our virtual environment, <kbd>Flask01</kbd>, to generate the final <kbd>requirements.txt</kbd> file:</p>
<pre>    <strong>pip freeze &gt; requirements.txt</strong></pre>
<p>The following lines show the contents of a sample generated <kbd>requirements.txt</kbd> file. Notice that the generated file also includes all the dependencies that were installed by the packages we specified in the original <kbd>requirements.txt</kbd> file:</p>
<pre>    <strong>alembic==1.0.0</strong>
    <strong>aniso8601==3.0.2</strong>
    <strong>atomicwrites==1.2.1</strong>
    <strong>attrs==18.2.0</strong>
    <strong>certifi==2018.8.24</strong>
    <strong>chardet==3.0.4</strong>
    <strong>Click==7.0</strong>
    <strong>coverage==4.5.1</strong>
    <strong>Flask==1.0.2</strong>
    <strong>Flask-HTTPAuth==3.2.4</strong>
    <strong>flask-marshmallow==0.9.0</strong>
    <strong>Flask-Migrate==2.3.0</strong>
    <strong>Flask-RESTful==0.3.6</strong>
    <strong>Flask-SQLAlchemy==2.3.2</strong>
    <strong>httpie==1.0.0</strong>
    <strong>idna==2.7</strong>
    <strong>itsdangerous==0.24</strong>
    <strong>Jinja2==2.10</strong>
    <strong>Mako==1.0.7</strong>
    <strong>MarkupSafe==1.0</strong>
    <strong>marshmallow==2.16.3</strong>
    <strong>marshmallow-sqlalchemy==0.15.0</strong>
    <strong>more-itertools==4.3.0</strong>
    <strong>passlib==1.7.1</strong>
    <strong>pluggy==0.8.0</strong>
    <strong>psycopg2==2.7.6.1</strong>
    <strong>py==1.7.0</strong>
    <strong>Pygments==2.2.0</strong>
    <strong>pytest==4.0.1</strong>
    <strong>pytest-cov==2.6.0</strong>
    <strong>python-dateutil==2.7.3</strong>
    <strong>python-editor==1.0.3</strong>
    <strong>pytz==2018.5</strong>
    <strong>requests==2.19.1</strong>
    <strong>six==1.11.0</strong>
    <strong>SQLAlchemy==1.2.12</strong>
    <strong>urllib3==1.23</strong>
    <strong>Werkzeug==0.14.1</strong>
  </pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Test your knowledge</h1>
                
            
            
                
<p>Let's see whether you can answer the following questions correctly:</p>
<ol>
<li>Pytest makes it easy to declare a test fixture function by marking a function with which of the following decorators?:
<ol>
<li><kbd>@pytest.fixture_function</kbd></li>
<li><kbd>@pytest.test_fixture</kbd></li>
<li><kbd>@pytest.fixture</kbd></li>
</ol>
</li>
</ol>
<ol start="2">
<li>By default, pytest discovers and executes functions as text functions when they start with which of the following prefixes?:
<ol>
<li><kbd>test</kbd></li>
<li><kbd>test_</kbd></li>
<li><kbd>test-</kbd></li>
</ol>
</li>
</ol>
<ol start="3">
<li>Which of the following commands displays the line numbers of the missing statements in the <kbd>Missing</kbd> column for a coverage report?:
<ol>
<li><kbd>coverage report -m</kbd></li>
<li><kbd>coverage report missing</kbd></li>
<li><kbd>coverage -missing</kbd></li>
</ol>
</li>
</ol>
<ol start="4">
<li>Pytest is a very popular Python:
<ol>
<li>Unit test framework that makes testing easy and reduces boilerplate code</li>
<li>WSGI server that we can ...</li></ol></li></ol></div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we set up a testing environment. We installed <kbd>pytest</kbd> to make it easy to discover and execute unit tests and we created a new database to be used for testing. We wrote a first round of unit tests, measured test coverage with the <kbd>pytest-cov</kbd> plugin combined with the <kbd>coverage</kbd> tool, and then we wrote additional unit tests to improve test coverage. Finally, we understood many considerations for deployment and scalability.</p>
<p>We built a complex API with Flask combined with Flask-RESTful and a PostgreSQL 10.5 database that we can run as a microservice, and we tested it. Now we will move to another popular Python web framework, Django, which is the topic for the next chapter.</p>


            

            
        
    </div>



  </body></html>