- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adding Computer Vision to A.R.E.S.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter, we will be adding computer vision to A.R.E.S. This will
    give A.R.E.S. the ability to recognize objects as well as alert us via text message
    to the presence of these objects. For our example, we will be recognizing dogs,
    although we could just as easily set up our object recognition code to recognize
    other objects.
  prefs: []
  type: TYPE_NORMAL
- en: We will start our journey by exploring **computer vision** and what it is before
    downloading and installing the **Open Source Computer Vision** (**OpenCV**) library
    and the **You Only Look Once** (**YOLO**) object detection system. After setting
    up these tools, we will explore hands-on examples.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have built a smart video streaming application
    that utilizes the video stream from the camera on A.R.E.S.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding computer vision to A.R.E.S.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending out a text alert
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need the following to complete this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate knowledge of Python programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The A.R.E.S. robot from [*Chapter 13*](B21282_13.xhtml#_idTextAnchor209)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A computer with a GUI-style operating system, such as the Raspberry Pi 5, macOS,
    or Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found here: [https://github.com/PacktPublishing/-Internet-of-Things-Programming-Projects-2nd-Edition/tree/main/Chapter14](https://github.com/PacktPublishing/-Internet-of-Things-Programming-Projects-2nd-Edition/tree/main/Chapter14).'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Computer vision began in the 1950s, evolving significantly with key milestones
    such as image processing algorithms in the 1960s and the introduction of GPUs
    in the 1990s. These advancements improved processing speeds and complex computations
    and enabled real-time image analysis and sophisticated models. Modern computer
    vision technology is a result of these developments. The following diagram shows
    where computer vision sits in terms of artificial intelligence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Artificial intelligence](img/B21282_14_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – Artificial intelligence
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14**.1* shows a set of concentric circles representing the relationship
    between different fields of artificial intelligence. At the core is computer vision,
    surrounded by **object detection**, which is a subset of deep learning, something
    that’s nested within machine learning. All are encompassed by the broader field
    of artificial intelligence.'
  prefs: []
  type: TYPE_NORMAL
- en: Not all computer vision techniques involve machine learning or deep learning,
    but object detection, a part of computer vision, often does.
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between image recognition, object recognition, object
    detection, and image classification?
  prefs: []
  type: TYPE_NORMAL
- en: In computer vision, terms such as **image recognition**, **object recognition**,
    **object detection**, and **image classification** describe specific processes.
    Image recognition detects features or patterns within an image. Object recognition
    moves beyond this to identify distinct objects within an image or video, although
    it does not specify their precise locations, concentrating on identifying *what*
    objects are present rather than *where* they are. Object detection, conversely,
    not only identifies objects but also locates them spatially, often using bounding
    boxes. Meanwhile, image classification involves analyzing an entire image to assign
    it to a specific category, such as determining whether an image shows a dog, cat,
    or car. For A.R.E.S., we want a video feed that creates a bounding box around
    an object that’s been detected. So, we will use object detection in our application.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will integrate OpenCV and the YOLO deep learning algorithm
    into A.R.E.S. so that we can use object detection to detect the presence of a
    dog.
  prefs: []
  type: TYPE_NORMAL
- en: We will start our foray into computer vision by familiarizing ourselves with
    the OpenCV library.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing OpenCV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenCV is a foundational tool in the field of computer vision that offers a
    vast range of capabilities for real-time image processing. OpenCV supports a multitude
    of applications, from simple image transformations to complex machine learning
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV not only allows for rapid prototyping but also supports full-scale application
    development across various operating systems, making it an excellent choice for
    hobbyists, educators, and commercial developers alike.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore the core functionalities of OpenCV. We will
    start by creating a Python virtual environment and a project folder.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing an image using OpenCV
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Getting started with OpenCV can be as simple as displaying an image in a window.
    This basic exercise introduces us to key functions for image loading, handling,
    and window operations in OpenCV. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start by opening a terminal window. We can use the Raspberry Pi operating system
    on our Raspberry Pi 5 or another operating system of our choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To store our project files, we must create a new directory and subdirectory
    (for images) with the following command (Linux commands are being used here):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we navigate to the new directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'venv library if it is not already installed):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With our new Python virtual environment created, we can source into it with
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: pip install opencv-python
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We close the terminal by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we launch Thonny and source our newly created Python virtual environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Sourcing our Python virtual environment](img/B21282_14_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – Sourcing our Python virtual environment
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll create a new tab. We can do this by selecting **File** and then
    **New** or by hitting *Ctrl* + *N* on our keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the following code in the editor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s take a closer look at this code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: First, we import the `OpenCV` library and give it an alias of `cv` for easier
    reference in the code.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Our code then reads the `Toronto.png` image file into the `img` variable from
    the `images` folder.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we create a window named `Downtown Toronto` and display `img` within this
    window.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, our code waits indefinitely for a key event before moving on to the next
    line of code. The `0` value means it will wait until a key is pressed.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we destroy all the windows that have been created during the session
    and ensure no window from the OpenCV UI remains open after the script is run.
    This could potentially cause a memory leak.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We save the code with a descriptive name such as `Toronto.py` in our `Chapter14`
    project folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We run the code by clicking on the green run button, hitting *F5* on our keyboard,
    or clicking on the **Run** menu option at the top and then **Run** **current script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We should see a window appear that contains an image of Toronto:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.3 – OpenCV window popup showing downtown Toronto (image: Maximillian
    Dow)](img/B21282_14_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3 – OpenCV window popup showing downtown Toronto (image: Maximillian
    Dow)'
  prefs: []
  type: TYPE_NORMAL
- en: To close the pop-up window, we hit any key on our keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Although very simple, this exercise lays the groundwork for more complex computer
    vision projects. Before we move on to using artificial intelligence, we will investigate
    using OpenCV to view the camera feed coming from A.R.E.S.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming video using OpenCV
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [*Chapter 13*](B21282_13.xhtml#_idTextAnchor209), we streamed a video from
    A.R.E.S. using the VLC media player. To utilize the video coming from A.R.E.S.,
    we can use OpenCV for real-time image and video analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To view our video feed using OpenCV, we must do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We launch Thonny and source the `ch14-env` Python virtual environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a new tab by selecting **File** and then **New** or by hitting *Ctrl*
    + *N* on our keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We enter the following code in the editor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s take a closer look at this code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We start by importing the OpenCV library.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we define the RTSP URL as a string for the video stream source.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Our code creates a `VideoCapture` object that attempts to open the video stream
    from the specified RTSP URL. If the stream can’t be opened, an error message is
    printed.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we start an infinite loop to continuously fetch frames from the stream.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we attempt to read the next frame from the stream.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We print an error message if a frame can’t be received and exit the loop.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We display the current frame in a window titled `A.R.E.S. Stream`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we allow the user to close the stream window manually if they press `q`
    on their keyboard.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, release the video capture object, freeing up resources and closing the
    video file or capturing device.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we close all OpenCV windows, cleaning up any remaining resources associated
    with the window displays.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We save the code with a descriptive name such as `video-feed.py` in our `Chapter14`
    project folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We run the code by clicking on the green run button, hitting *F5* on our keyboard,
    or clicking on the **Run** menu option at the top and then **Run** **current script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We should see a window appear, displaying the feed from the camera on A.R.E.S:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.4 – Video feed from the camera on A.R.E.S.](img/B21282_14_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 – Video feed from the camera on A.R.E.S.
  prefs: []
  type: TYPE_NORMAL
- en: To close the pop-up window, we hit `q` on our keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have some experience using OpenCV to view images and videos, let’s
    take this a step further and have it identify objects, specifically dogs, as we
    continue our computer vision journey.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by looking at neural networks and how they are used to identify
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding YOLO and neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll focus on YOLO and the various layers of neural networks
    so that we can construct object detection code that can identify dogs. Turning
    our attention to *Figure 14**.1*, we can see that object detection is a part of
    computer vision, where both deep learning and machine learning techniques are
    used.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning versus deep learning
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is a subset of artificial intelligence where algorithms use
    statistical methods to enable machines to improve with experience, typically requiring
    manual feature selection. In contrast, deep learning, a specialized subset of
    machine learning, operates with neural networks, which automatically extract and
    learn features from large volumes of data. This is ideal for complex tasks such
    as image and speech recognition. While machine learning works with less data and
    provides more model transparency, deep learning requires substantial data and
    computational power, often acting as a *black box* with less interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: To represent deep learning, YOLO uses a sophisticated neural network that assesses
    images in a single sweep.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring object detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned previously, object detection is the process of finding objects
    in an image or video feed. The following figure illustrates the sequential stages
    of an object detection algorithm, using the example of an image of a dog:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21282_14_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.5 – Stages of object detection in computer vision
  prefs: []
  type: TYPE_NORMAL
- en: First, we have the original image as the input. We proceed by decomposing it
    into input pixels and then identify the edges, corners, and contours for structural
    interpretation. Our algorithm proceeds to recognize individual object parts and
    places bounding boxes around these components. This leads to the final output,
    which highlights the detected object within the image.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how object detection works when used with neural networks,
    let’s consider an example. In the next subsection, we will use YOLO to identify
    a dog in a picture.
  prefs: []
  type: TYPE_NORMAL
- en: Using YOLO to identify a dog in a picture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will write a program using OpenCV and the YOLO deep learning
    algorithm to detect a dog in a picture.
  prefs: []
  type: TYPE_NORMAL
- en: To get started, we need to download the YOLO configuration files, the pre-trained
    weights, and the `coco.names` file, which contains the list of classes recognized
    by the model. These files are typically available on the official YOLO website
    or reputable GitHub repositories dedicated to YOLO. The configuration file (`yolov4.cfg`)
    outlines the network architecture, the weights file (`yolov4.weights`) contains
    the trained model parameters, and the class names file lists the object categories
    the YOLO model can detect, all of which are crucial for the object detection task
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: To make this easier, we have included all the files you’ll need for this exercise
    in this chapter’s GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: What is the yolo4.weights file?
  prefs: []
  type: TYPE_NORMAL
- en: The `yolov4.weights` file contains pre-trained weights for the YOLOv4 object
    detection model, enabling it to accurately detect and locate objects in images
    and videos. Since this file is too large to be included in this chapter’s GitHub
    repository, you’ll need to download it from the official YOLO website or GitHub
    repository ([https://github.com/AlexeyAB/darknet/releases](https://github.com/AlexeyAB/darknet/releases)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To create our object detection code, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We start by opening a terminal window. We can use the Raspberry Pi operating
    system on our Raspberry Pi 5 or another operating system of our choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We navigate to our `Chapter14` project directory with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To store our YOLO files, create a new directory with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For our test image, we copy the `dog.png` file from the images directory of
    this chapter’s GitHub repository to our project folder’s images directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We launch Thonny and source the `ch14-env` Python virtual environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a new tab by selecting **File** and then **New** or by hitting *Ctrl*
    + *N* on our keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the editor, we add the necessary imports for our code. Here, we’ll need
    OpenCV as our library and NumPy for its mathematical functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we load the YOLO algorithm by running the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s take a closer look at this code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: First, we initialize the YOLO network by loading the pre-trained weights (`yolov4.weights`)
    and configuration (`yolov4.cfg`). This creates a neural network ready for object
    detection.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we create an empty list intended to store the class names (for example,
    dog and cat) that YOLO can detect, once they are read from a file.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Our code then retrieves the names of all the layers in the YOLO network. These
    layer names are used to identify output layers. This is crucial for obtaining
    detection results.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We enter the following code to fetch the indices of the final layers in the
    YOLO neural network. These directly output detection results using the OpenCV
    `getUnconnectedOutLayers()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create a list of the names of the output layers of the YOLO neural network
    by indexing into the list of all layer names using the indices provided by `output_layer_indices`,
    adjusted for zero-based indexing. This corresponds to the *Bounding Boxes* stage
    of the algorithm, as outlined in *Figure 14**.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we read the `coco.names` file, which contains the list of object classes
    that the YOLO model can identify, and creates a list of class names by removing
    any leading or trailing whitespace from each line. The following code finds and
    stores the index of the `"dog"` class within that list, effectively preparing
    the program to specifically recognize and identify dogs in the images processed
    by the YOLO model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code reads the `dog.png` image from the `images` directory, scales
    it down to 40% of its original size to reduce computational load, and extracts
    its dimensions and color channel count. The resizing step is crucial because YOLO
    models typically expect a fixed input size, and resizing helps to match that requirement
    while also accelerating the detection process due to the smaller image size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must convert the resized image into a **blob** – a preprocessed image
    that’s compatible with the neural network – by normalizing pixel values and setting
    the size to 416x416 pixels, a standard input size for YOLO models. Then, we must
    set this blob as the input to the neural network. Finally, we must perform a forward
    pass through the network using the specified output layers to obtain the detection
    predictions. This includes class labels, confidences, and bounding box coordinates.
    The following snippet corresponds to the action that takes place between the *Input
    Pixels* and the *Bounding Boxes* stages shown in *Figure 14**.5*. It processes
    the image through various layers to detect objects, with the final line in the
    snippet producing the detections that lead to the *Output* stage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code analyzes the results from the neural network’s forward pass,
    filtering and processing detected objects for the `dog` class with a confidence
    level above 50%. It calculates the bounding box coordinates based on the object’s
    center, width, and height, then stores these coordinates along with the detection
    confidence and class ID in corresponding lists. This aligns with the *Bounding
    Boxes* stage shown in *Figure 14**.5*, where the processed outputs are used to
    specifically locate and classify the detected objects within the image. This sets
    the stage for the final visual representation in the *Output* phase, where these
    bounding boxes are drawn to indicate where dogs are located within the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code uses `NMSBoxes` function from OpenCV to refine the detection
    results by reducing overlap among bounding boxes, ensuring that each detected
    object is represented only once. After determining the best bounding boxes based
    on their confidence and overlap, it iterates through these optimized boxes to
    visually annotate the image. It does this by drawing a rectangle for each box
    and labeling it with the corresponding class name. This final step marks and identifies
    the detected objects (`dogs`) in the image, aligning with the *Output* stage shown
    in *Figure 14**.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the final section of our code, we display the processed image with detected
    objects marked by bounding boxes. The `cv2.imshow("Image", img)` function displays
    the image in a window titled `"Image"`. The `cv2.waitKey(0)` function pauses the
    execution of the script, waiting indefinitely for a key press to proceed, allowing
    the user to view the image for as long as needed. Finally, `cv2.destroyAllWindows()`
    closes all OpenCV windows opened by the script, ensuring a clean exit without
    leaving any GUI windows open:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We save the code with a descriptive name, such as `recognize-dog.py`, in our
    `Chapter14` project folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We run the code by clicking on the green run button, hitting *F5* on our keyboard,
    or clicking on the **Run** menu option at the top and then **Run** **current script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We should observe a pop-up window appear with a bounding box around the face
    of the dog:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.6 – YOLO library used to recognize a dog](img/B21282_14_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.6 – YOLO library used to recognize a dog
  prefs: []
  type: TYPE_NORMAL
- en: We press any key on our keyboard to close the pop-up window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see, our program can identify a dog from a picture. If we were to
    provide a picture of any object identified in the `coco.names` file (a person,
    for example), our program should be able to identify that.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a little exposure to YOLO, neural networks, and object detection,
    let’s add this functionality to A.R.E.S. We will program our application to send
    a text message whenever A.R.E.S. detects a dog.
  prefs: []
  type: TYPE_NORMAL
- en: Adding computer vision to A.R.E.S.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we explored OpenCV and YOLO, using OpenCV to view images
    and video feeds, and YOLO to identify a dog in a picture. In this section, we’ll
    apply what we’ve learned to create a smart video streaming application that represents
    the eyes of A.R.E.S. We’ll only use dogs as an example, but we could easily adapt
    this application for tracking other objects.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by encapsulating our YOLO code into a class called `DogTracker`
    before creating a video streaming application using this class with OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the DogTracker class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `DogTracker` class embodies the artificial intelligence component of A.R.E.S.
    Although it could be installed directly on the Raspberry Pi 3B+ within A.R.E.S.
    and accessed remotely via the streaming window application, we will install it
    on a computer alongside our streaming application for simplicity and improved
    performance. In our example, we will utilize a Windows PC.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the `DogTracker` class, we must do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We launch Thonny and source our `ch14-env` Python virtual environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a new tab by selecting **File** and then **New** or by hitting *Ctrl*
    + *N* on our keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we add the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we define our class and initialization method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must define our only method: `detect_dogs()`. This method processes
    video frames to detect dogs using a YOLO neural network model. It begins by resizing
    the input frame for optimal processing and creates a blob from the resized image,
    which is then fed into the neural network. The network outputs detection results,
    which include bounding boxes, confidences, and class IDs for detected objects.
    The method checks each detection to see if it meets the confidence threshold and
    corresponds to the class ID for dogs. If such detections are found, it calculates
    and stores their bounding box coordinates. NMS is then applied to refine these
    bounding boxes by reducing overlaps. If any boxes remain after this process, it
    confirms the presence of dogs, draws these boxes on the frame, and labels them.
    Finally, the method returns the processed frame, along with a Boolean indicating
    whether any dogs were detected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We save the code with a descriptive name, such as `DogDetector.py`, in our `Chapter14`
    project folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, we reorganized the `recognize-dog.py` code from the previous section into
    a class that we will use for our smart video streamer. With this class in place,
    it’s time to create our streaming application. We will use OpenCV for this.
  prefs: []
  type: TYPE_NORMAL
- en: Building a smart video streamer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `detect_dogs()` method inside the `DogDetector` class to identify
    dogs from the video stream coming from A.R.E.S. As mentioned previously, we could
    easily change our code so that we can use YOLO to identify other objects. Recognizing
    dogs presents a fun way for those of us with dogs to program A.R.E.S. as a sort
    of pet detection robot. We will install our smart video streamer onto the same
    computer as our `DogDetector` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the smart video streamer, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We launch Thonny and source our `ch14-env` Python virtual environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a new tab by selecting **File** and then **New** or by hitting *Ctrl*
    + *N* on our keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We start by adding our imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we define our variable declarations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The bulk of our code sits inside an infinite loop. This code continuously captures
    frames from a video source, checking if each frame is successfully retrieved.
    If a second passes since the last processed frame, it detects dogs in the current
    frame using the `detect_dogs()` method, updates the time marker, and displays
    the result; if the `q` key is pressed, the loop breaks, and the video capture
    and any OpenCV windows are cleanly released and closed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We save the code with a descriptive name, such as `smart-video-feed.py`, in
    our `Chapter14` project folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We run the code by clicking on the green run button, hitting *F5* on our keyboard,
    or clicking on the **Run** menu option at the top and then **Run** **current script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We should observe a pop-up window appear with a video feed from A.R.E.S. Our
    application should detect the presence of a dog:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.7 – Detecting a dog using our smart video streamer](img/B21282_14_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.7 – Detecting a dog using our smart video streamer
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have successfully added artificial intelligence in the form of
    object detection to A.R.E.S. We may adjust the size, font, and color of the frame
    in our `DogDetector` class. As impressive as getting object detection to work
    with A.R.E.S. is, we will take this a step further and introduce text notification,
    turning the smart video streaming functionality into a true IoT application.
  prefs: []
  type: TYPE_NORMAL
- en: Sending out a text alert
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make A.R.E.S. a true IoT device, we will add text functionality. This will
    give A.R.E.S. the ability to send out text alerts when an object of interest –
    in our case, a dog – is detected. We will use the Twilio service for this.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by setting up our Twilio account and testing the number we are
    assigned before we integrate text messaging functionality into A.R.E.S. We must
    ensure that we follow the upcoming steps carefully so that we can set up our Twilio
    account successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our Twilio account
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up a Twilio account involves registering on their website, where we’ll
    be provided with an account SID and an auth token to authenticate API requests.
    Once registered, we can also obtain a Twilio phone number, which is necessary
    for sending SMS messages and making calls through their service. In this section,
    we will set up our Twilio account and send a test SMS message.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up our Twilio account, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a web browser, we navigate to [www.twilio.com](https://www.twilio.com)
    and click the blue **Start for** **free** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.8 – The Twilio website](img/B21282_14_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.8 – The Twilio website
  prefs: []
  type: TYPE_NORMAL
- en: 'This will take us to the **Sign up** page. Here, we can create a Twilio account
    or use a Google account:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.9 – Twilio’s Sign up page](img/B21282_14_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.9 – Twilio’s Sign up page
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify our new account, we type in a phone number and click on the blue
    **Send code via** **SMS** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.10 – Verify page](img/B21282_14_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.10 – Verify page
  prefs: []
  type: TYPE_NORMAL
- en: 'We should receive a text message containing a verification code. We enter the
    number and click on the blue **Verify** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.11 – Verification code step](img/B21282_14_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.11 – Verification code step
  prefs: []
  type: TYPE_NORMAL
- en: 'This will take us to the **You’re all verified!** page, which will provide
    us with a **Recovery code** value. We click on the blue **Continue** button to
    go to the next page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.12 – Recovery code](img/B21282_14_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.12 – Recovery code
  prefs: []
  type: TYPE_NORMAL
- en: 'The next page allows us to customize our Twilio experience:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.13 – Customizing Twilio](img/B21282_14_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.13 – Customizing Twilio
  prefs: []
  type: TYPE_NORMAL
- en: The dashboard screen allows us to get a Twilio phone number. We click on the
    blue **Get phone number** button to continue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next page provides us with a Twilio phone number we can use. We click on
    the blue **Next** button to proceed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the next screen, we can test out our new Twilio number. The `IoT test`,
    and click on the blue **Send test** **SMS** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.14 – Testing our new Twilio phone number](img/B21282_14_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.14 – Testing our new Twilio phone number
  prefs: []
  type: TYPE_NORMAL
- en: 'We should receive a text message with our test message, **IoT test**, on the
    phone we provided the number to Twilio on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.15 – Test message successfully received, as seen on a cell phone](img/B21282_14_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.15 – Test message successfully received, as seen on a cell phone
  prefs: []
  type: TYPE_NORMAL
- en: With our Twilio account set up and our phone number tested, we are ready to
    incorporate text messaging into A.R.E.S.
  prefs: []
  type: TYPE_NORMAL
- en: Adding text message functionality to A.R.E.S.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To integrate text messaging functionality into A.R.E.S., we will develop a
    new class named `TwilioMessage` and a new version of the `smart-video-feed.py`
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.16 – Adding text functionality to A.R.E.S.](img/B21282_14_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.16 – Adding text functionality to A.R.E.S.
  prefs: []
  type: TYPE_NORMAL
- en: The `TwilioMessage` class will encapsulate communication to the Twilio server.
    As shown in *Figure 14**.16*, our new `TwilioMessage` class is called from our
    smart video streamer and sends out text messages.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by creating this class.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the TwilioMessage class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To create the `TwilioMessage` class, we must do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We launch Thonny and source our `ch14-env` Python virtual environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As we require a library from Twilio to make our code work, we will install
    it in our Python virtual environment. To do so, open the system shell by clicking
    on **Tools** | **Open system shell…** in Thonny:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.17 – Open system shell…](img/B21282_14_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.17 – Open system shell…
  prefs: []
  type: TYPE_NORMAL
- en: 'At the command prompt, we execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the library has been installed, we close the terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a new tab by selecting **File** and then **New** or by hitting *Ctrl*
    + *N* on our keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We add the following code to the editor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s take a closer look at our code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: First, we import the Twilio client and define the `TwilioMessage` class.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we initialize our class with Twilio credentials (account SID, auth token,
    and Twilio phone number).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The `send_sms()` method sends an SMS to a specified number and prints the message
    SID after sending.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In the main execution block, an instance of `TwilioMessage` is created with
    Twilio credentials, and a test SMS is sent to our number.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We save the code with a descriptive name, such as `TwilioMessage.py`, in our
    `Chapter14` project folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We run the code by clicking on the green run button, hitting *F5* on our keyboard,
    or clicking on the **Run** menu option at the top and then **Run** **current script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We should receive a text message saying, `Hello from A.R.E.S.` on our cell phone.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the `TwilioMessage` class created, it is time to modify the smart video
    streamer code so that text messages will be sent when a dog or dogs are detected.
    We will do that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the smart video streamer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final step in providing text message functionality in A.R.E.S. is to create
    a new smart video streamer script to send a text message when a dog is detected.
    To limit the number of messages sent, we will increase the time between frames
    to 5 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'To modify our smart video streamer, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In Thonny, we create a new tab by selecting **File** and then **New** or by
    hitting *Ctrl* + *N* on our keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We add the following code to the editor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We save the code with a descriptive name, such as `smart-video-sms.py`, in our
    `Chapter14` project folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We run the code by clicking on the green run button, hitting *F5* on n our keyboard,
    or clicking on the **Run** menu option at the top and then **Run** **current script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We should observe a window appear with a video feed coming from A.R.E.S. that
    provides object detection functionality when a dog is present.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We should receive a text message once a dog has been detected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.18 – A text message indicating that a dog or dogs were detected](img/B21282_14_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.18 – A text message indicating that a dog or dogs were detected
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have successfully added text message functionality to A.R.E.S.
    to alert us when an object we are interested in – in this case, a dog – is detected.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the field of computer vision and successfully integrated
    it into our A.R.E.S. robot car. By incorporating this technology, A.R.E.S. can
    now process and interpret visual data. We also added text messaging functionality
    to A.R.E.S., turning our robot car into a true IoT device.
  prefs: []
  type: TYPE_NORMAL
- en: Although not implemented, we could easily imagine how we would take A.R.E.S.
    to the next level by incorporating obstacle avoidance based on the data we get
    back from YOLO. We may also imagine how we could make A.R.E.S. follow a certain
    object if desired.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter marks the end of our IoT journey together. Throughout, we have
    explored the world of IoT while utilizing and building our programming skills
    – from using the Sense HAT to serve up web services data to utilizing LoRa for
    long-range communication to implementing advanced features such as computer vision
    in a robot car we control using the internet. This adventure has not only taught
    us technical know-how but also showed us the potential of IoT to innovate and
    solve any real-world challenges in the future.
  prefs: []
  type: TYPE_NORMAL
- en: It’s been a pleasure.
  prefs: []
  type: TYPE_NORMAL
