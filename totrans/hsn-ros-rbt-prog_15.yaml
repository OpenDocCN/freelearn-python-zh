- en: Machine Learning with OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced you to the usage of deep learning in
    order to recognize objects based on a real-time image feed coming from the Raspberry
    Pi camera. Hence, this provides the robot the ability to take smart actions related
    to the recognized object. For example, if the object is a ball, the robot could
    collect it and put it apart so that nobody has an accident by stepping on the
    ball.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will be introduced to **reinforcement learning** (**RL**),
    a field of machine learning that, nowadays, is a very active topic of research,
    having achieved the success of surpassing human performance in some scenarios,
    as shown in the recent case of the AlphaGo game ([https://deepmind.com/blog/article/alphago-zero-starting-scratch](https://deepmind.com/blog/article/alphago-zero-starting-scratch)).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will learn in a practical manner using the Python framework **OpenAI Gym**,
    which is a toolkit for developing and comparing RL algorithms. We will provide
    a conceptual approach to RL that will allow us to handle various problems within
    a programmatic way using Gym environments. In order to do this, we will differentiate
    between three main components: *scenarios*, *tasks*, and *agents*. Here, the scenario
    is the physical environment where the robot evolves, the task is the action(s)
    the robot is expected to learn, and the agent is the software program that makes
    the decisions for the action(s) to execute.'
  prefs: []
  type: TYPE_NORMAL
- en: This segregation will allow you to decouple these components and reuse them
    in a different scope. For example, you could have trained an agent so that a two-wheeled
    drive robot, such as our GoPiGo3, would be able to carry a load from one point
    to a target location and use the same agent with a different robot, for example,
    a four-wheel drive such as Summit XL ([https://www.robotnik.es/robots-moviles/summit-xl](https://www.robotnik.es/robots-moviles/summit-xl)).
    The code of the agent is the same because it abstracts the robot's concrete features.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you could use different generated scenarios to test the same robot.
    This will show the ability of the trained agent to perform under different boundary
    conditions. With these ideas in mind, this chapter will teach you the basics of
    the OpenAI Gym API, how to integrate with an ROS environment, and how to follow
    the training process by representing the results graphically.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to OpenAI Gym
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running an environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring an environment file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the simulation and plotting the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first section, you will start using the base Gym API in its native Python
    environment. In the following ones, you will learn how to add the ROS wrappers
    to train robots in Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will make use of the code in the `Chapter11_OpenAI_Gym` folder,
    located at [https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter11_OpenAI_Gym](https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter11_OpenAI_Gym). Copy
    the files of this chapter to the ROS workspace, putting them inside the `src` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, you will need to install Anaconda ([https://www.anaconda.com](https://www.anaconda.com)).
    This is the Python distribution that has become the *de facto* open source standard
    for the Data Science community. It provides a complete Python environment for
    machine learning projects.
  prefs: []
  type: TYPE_NORMAL
- en: Visit the download section of the Anaconda website at [https://www.anaconda.com/distribution/#linux](https://www.anaconda.com/distribution/#linux), and
    select the Python 2.7 bundle. We select this package because the ROS Python client
    is focused on this version; however, you should be aware that it recently came
    to the end of life in December 2019.
  prefs: []
  type: TYPE_NORMAL
- en: Open Robotics intends to create a new ROS distribution in May 2020 targeting
    Python 3: [https://discourse.ros.org/t/planning-future-ros-1-distribution-s/6538](https://discourse.ros.org/t/planning-future-ros-1-distribution-s/6538).
  prefs: []
  type: TYPE_NORMAL
- en: 'After downloading Anaconda, go to the download directory and enter the following
    command to execute the code for the installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The filename marked in bold letters should match the name of the one you have
    downloaded. If this is not the case, then run the `bash` command, replacing the
    filename with the actual one you have.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `$ conda init` command is optionally executed from the installation script,
    and, if successful, it will provide the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Conda is the package manager that ships with Anaconda. It allows you to easily
    install, remove, list, and inspect the Python packages in your Anaconda installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing Anaconda, you will see these lines added to your `~/.bashrc` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'So that the added configuration takes effect, source it in the Terminal prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding `.bashrc` line should take you into the (base) default Anaconda
    environment. We recommend that you do not activate Conda''s base environment on
    startup. To ensure this, set the `auto_activate_base` parameter to `false`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you wish to restore the default configuration, you can revert it by changing
    the value to `true`. Finally, you have the option to manually activate the default
    Anaconda environment, on demand, in a Terminal with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this base environment, you are able to install Jupyter notebooks. You can
    use them to view, run, and modify Python notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that this is the user-friendly Python runtime you had preinstalled
    with DexterOS. It was used in [Chapter 2](7a2b1b82-c666-42df-9f10-9777eabe82df.xhtml),
    *Unit Testing of GoPiGo3*, to run most of the examples. To deactivate the virtual
    environment, just run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You can find a useful cheatsheet for Conda Package Manager, which you should
    have at hand, at the following URL: [https://kapeli.com/cheat_sheets/Conda.docset/Contents/Resources/Documents/index](https://kapeli.com/cheat_sheets/Conda.docset/Contents/Resources/Documents/index).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we are ready to proceed with OpenAI Gym and its installation;
    this will be explained in a dedicated section next.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we provided a practical overview of what you can expect
    in RL when applied to robotics. In this chapter, we will provide a general view
    in which you will discover how RL is used to train smart *agents*.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will need to install OpenAI Gym and OpenAI ROS on our laptop in preparation
    for the practical examples. Then, we will explain its concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we did in the previous chapter, we are going to create a virtual environment
    for the Python setup of this chapter, which we will call `gym`. The following
    two commands allow for the creation and then the activation of `gym`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Following this, install the specific Python packages that we are going to need
    for the examples:'
  prefs: []
  type: TYPE_NORMAL
- en: The Keras package ([https://keras.io/](https://keras.io/)), which is a high-level
    neural network API that it used within OpenAI Gym. Remember that it was also used
    in the previous chapter, but we need to install it again because this is a new `gym` environment.
    Keras will let us train an agent using the **DQN** (short for **Deep Q-Network**)
    algorithm, which is deep learning-based.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You also need TensorFlow since it is used as the backend for Keras.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, you will need the Gym package ([https://github.com/openai/gym](https://github.com/openai/gym)),
    which is the implementation in Python of OpenAI Gym.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can install three of the packages in a row, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now check the version of `gym`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to this, install the Jupyter notebooks, since some of the Python
    examples are explained in this friendly format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, install an optional library called `pybox2d`. This is a 2D physics
    engine for games and simple simulations, used by some of the premade environments
    that ship with Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The technical requirements end here. The following subsections are optional
    and are intended to increase your background of different ways of managing Python
    and Anaconda. We will show you how to install OpenAI Gym from the source and host
    the package in your working folder, which is a typical way of using a Python package
    in development mode.
  prefs: []
  type: TYPE_NORMAL
- en: Without Anaconda (optional)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you do not want to use Anaconda but instead keep working in the Python environment
    that ships with Ubuntu, you can install an in-home user directory by adding the
    `--user` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This places the necessary packages in the `~/.local/lib/python2.7/site-packages` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Installing gym in development mode (optional)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This allows you to have the source code of OpenAI Gym in your working directory,
    change the files of the package, and see their effects instantly, without needing
    to reinstall the `gym` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `-e` option allows this kind of installation, and it is suitable to be used
    as developer mode. The `--user` option performs the installation locally to the
    user at the `~/.local/lib/python2.7/site-packages` location.
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep the environment clean, remove the package installation, keeping only
    the `gym` Python package in the Gym virtual environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This snippet lets you know how to manually remove a Python package from the
    system.
  prefs: []
  type: TYPE_NORMAL
- en: To reproduce the examples in this chapter, we will be following the former approach,
    that is, installing Gym as a system package within the Conda environment.
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenAI ROS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So that the code can run inside ROS, you have to install OpenAI ROS, which
    is built on top of OpenAI Gym. Execute the following command to clone the contributed
    ROS package and start the setup for ROS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Be aware that we had to install the ROS package from the source because the
    compiled binary is not available in Ubuntu. In particular, it is worth noting
    that the `rosdep install openai_ros` command does the equivalent of the general `sudo
    apt install <package>` command of Ubuntu; that is, every time you install a new
    component, it automatically includes the required dependencies. Remember that,
    for an ROS package, the dependencies are declared in the `package.xml` file located
    in the root folder of its source code.
  prefs: []
  type: TYPE_NORMAL
- en: Once the installation of OpenAI Gym is complete, we can go on to explain its
    concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Agents, artificial intelligence, and machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The concept of an agent comes from the artificial intelligence field and is
    used to designate anything that makes decisions. Well, that is, it is what a typical
    computer program does with the conditional instructions of the `if ... then ...
    else ...` type. Put simply, an *agent* is a program that can take more elaborated
    decisions rather than using pure conditionals. For example, consider a video game:
    when you play against a machine, your opponent observes your actions and decides
    what to do next to win the game. What powers the opponent''s decisions is an agent.
    Generalizing this idea, an agent can be used to solve many kinds of problems;
    for example, when to stop and start a heater to keep a room warm at a set temperature
    point.'
  prefs: []
  type: TYPE_NORMAL
- en: When instead of using analytical formulas—as in the case of using a **PID**
    controller (acronym of *Proportional–Integral–Derivative*) to address the problem
    of temperature regulation mentioned previously—you use empirical data to tell
    the *agent* what to do in *hundreds or thousands of particular situations* (the
    outside temperature, the room temperature, the number of people in the room, the
    time of the day, and so on), you are training it so that it is able to generalize
    and respond correctly when faced with a broad range of conditions. And this training
    process is what we call machine learning in general, and RL for the particular
    scope of these last two chapters of the book.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should also be aware that an *agent* that uses machine learning
    can make good decisions when the input conditions are in the range for which it
    has been trained. You cannot expect good decisions if one or more relevant conditions
    are outside of the training range. Hence the importance of the preceding paragraph
    in providing the empirical data of *hundreds or thousands of particular situations* for
    the training process.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym is a structured framework to train agents based on RL techniques.
    Once this agent is trained, it can be reused in similar problems to power the
    decision capability. To illustrate these concepts, we are going to use a simple
    mechanism, the cart pole, which is also known as the inverted pendulum.
  prefs: []
  type: TYPE_NORMAL
- en: The cart pole example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the classical control problem of the inverted pendulum that experiments
    with an unstable equilibrium (take a look at the following diagram). By applying
    lateral forces, *F*, you may compensate its tendency to fall down and get it to
    stay up (that is, when angle θ is close to zero):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a9bbe2f-0f25-4bbc-a1b5-56a0c9901cf1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: https://de.wikipedia.org/wiki/Datei:Cart-pendulum.svg'
  prefs: []
  type: TYPE_NORMAL
- en: We are going to solve this problem with OpenAI Gym, and the approach consists
    of starting with an *agent* that has no knowledge of the physics of the problem,
    that is, it does not have any idea of what lateral forces to apply so that the
    cart pole stays up. Following a trial-and-error strategy, the agent will learn
    which force directions and values are adequate for each angle of the pendulum.
    It is a quick problem to solve because you have only one degree of freedom—the angle θ—and
    one independent variable—the force, *F*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example is included with the code provided in the book repository, and
    we will take it as the base to explain the common concepts of the OpenAI Gym framework:
    environments, observations, and spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: Environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An environment is a scenario that models a problem (such as keeping a cart
    pole standing up) with a minimal interface that an agent can interact with. You
    can see the cart pole environment in action by running this snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the cart pole moving and rotating randomly, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f8cc531-5306-4eb0-8198-51889cf73587.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The content of the script is quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: After importing the `gym` module, we set up the `env` variable to the predefined
    `CartPole-v0` environment. Then, in the next line, the `.reset()` method is applied
    to `env` so that the environment is initialized.
  prefs: []
  type: TYPE_NORMAL
- en: 'The body of the script is the `for` loop, which we set to 1,000 iterations.
    In each of these iterations, the script does two things:'
  prefs: []
  type: TYPE_NORMAL
- en: It renders the state of the cart pole with `env.render()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It takes a random action to execute a step of the simulation, something that
    is done with the line `env.step(env.action_space.sample())`. The `.sample()` method
    provides a random force, *F*, to act on the base of the cart pole.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By chaining several steps together and letting the system evolve, the agent
    completes an episode, the end of which is defined by one of three possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: The pole angle is greater than ±12°.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The position of the cart with respect to the center of the track is more than
    ±2.4 units.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The episode length exceeds 200 steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This definition is part of the environment specification that can be found at [https://github.com/openai/gym/wiki/CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0).
    Let's now review the definitions of observations and spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spaces describe valid actions (*F* forces) and observations (cart pole angle θ).
    This concept of observations will be covered in detail in the next subsection.
    Every environment has two spaces attached:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Action space**: This is characterized by a set of state variables under the `env.action_space` *object. *This
    space defines the possible actions an agent is allowed to take. For the case of
    the cart pole, there is only one variable: to apply a lateral force, *F*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observation space**: This describes the physical state of the agent, that
    is, the angular position of the cart pole, θ. Operationally, it is a set of state
    variables under the `env.observation_space` object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now describe the concept of observations in order to get a full understanding
    of how they support the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Observations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given an environment, an observation consists of a set of values that define
    a given state of the environment, the angle θ. It is like taking a snapshot of
    a scene. The environment''s step function, `env.step`, returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The current state; that is to say, it sets the current value of the state variable, **θ**.
    Operationally, it is an object type variable called `observation`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward that the agent has obtained from the last action, force *F*, as was
    previously mentioned when describing the action space. The reward is like points
    in a game—a quantitative value that accumulates all the rewards (points) obtained
    from the actions performed in the current episode from the beginning, that is,
    the current score in the game analogy. If the applied force contributes to getting
    the cart pole to stay up, the reward is positive; if not, a negative reward (or
    penalty) is given. This variable is of the float type and is called `reward`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the current episode has finished, with a Boolean variable called `done`.
    When it finishes, the `env.reset` function is called to restart the environment,
    getting ready for the next episode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diagnostic information under the form of a Python dictionary object called `info`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, the agent will try to maximize its score, which is calculated as the
    cumulative sum of rewards it receives for every force it applies. This maximization
    algorithm will make it learn to keep the cart pole up.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding explanations should allow you to understand how the scripts work.
    Now we will run a training session of the cart pole so that you are able to see
    how the *good* actions are positively rewarded, encouraging the agent to build
    an effective strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Running the full cart pole example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first script that we ran, `cart-pole_env.py`, was intended to show a sequence
    of 1,000 random steps. The new script will provide feedback for every action that
    is taken by giving rewards for good actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The iterative block in the script includes the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the order of the lines, this is what is done in each step:'
  prefs: []
  type: TYPE_NORMAL
- en: Render the environment. This is the window that shows you what is happening.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the next action. This is where the accumulated experience is used to
    decide what to do, taking into account the current state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the new step; that is to say, perform the selected action and observe what
    happens. The observation returns the new state of the agent, `next_state`, the
    reward the agent obtains, and the Boolean variable, `done`, telling you if the
    episode has finished.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reward is added to the `score` variable, which accumulates all the rewards
    obtained from the beginning of the episode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the set (state, action, and reward) is stored in memory—`agent.remember`—so
    that the agent can take advantage of their past experience, promoting the actions
    in a given state that gave them more rewards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we update the current `state` variable with the output of *step 3*,
    which is the value of `next_state`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When the training finishes, a curve representing the evolution of the score
    as a function of the episode is depicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9122e7ca-773d-4b4c-9158-0fe32700455b.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see how after 40 episodes the agent starts getting good scores of around
    200\. What this means is that the agent has learned to keep the pole in equilibrium
    by applying the force in the direction that prevents it from falling down. This
    simple example takes a few minutes to achieve the target of getting 200 points
    in every new episode, so let's quickly understand how RL works.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that the cart pole problem is not representative of real scenarios.
    In actual cases, the state is defined by a set of many variables, and the possible
    actions an agent may take are also many. Real RL problems are very CPU-intensive
    and they take thousands and even millions of episodes to get a reasonably good
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will describe a more realistic problem where there are
    500 states and six possible actions. The goal of this second example is to understand
    the score maximization algorithm in its most basic version, that is, through the
    Q-learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning explained – the self-driving cab example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The problem we are going to solve consists of a self-driving cab that has to
    pick up passengers and drop them off to the right location. It must do so as quickly
    as possible while respecting the traffic rules. The graphics are based on ASCII
    characters and we are going to use real images to explain the goal. We will also
    follow a time-ordered sequence of frames:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first sequence represents the cab—the yellow square—in the start position.
    It can move up, down, left, or right except when it finds a vertical bar; that
    motion is not allowed. There are four possible taxi stands where the cab can pick
    up or drop off a passenger. They are marked with the letters **R**, **G**, **B**,
    and **Y**. The blue letter (**R**, in this case) is the pick-up location, and
    the purple letter (**G**) is the destination of where to transport the passenger
    to:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/07e86d35-1770-41cf-9c05-029469eb4236.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This second sequence in the next diagram shows a step of the simulation where
    the passenger is inside the cab. They were picked up at location **R** and are
    being transported to the destination. This state is visually recognizable because
    the square representing the taxi is filled in green (when it does not carry any
    passenger, the color remains yellow):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/6676cc83-c73e-4394-b489-560cdbb91b22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final sequence corresponds to the scenario in which the taxi leaves the
    passenger at their destination; this is represented by the letter **G**, in this
    case. When this happens, the cab color changes to yellow again:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/707db954-8f85-47b9-a85e-1453b7da5c5b.png)'
  prefs: []
  type: TYPE_IMG
- en: The goal of this problem is to train an RL agent to learn to drive the taxi
    following the shortest path for every trip. The shortest path is operationally
    implemented by a rewards policy, which gives the agent a predefined reward for
    every action it takes depending on its utility. Hence, the RL agent will try to
    maximize its total reward by associating states with *useful* actions. In this
    way, it will gradually discover a transport policy that will let it minimize (in
    global average) the time from traveling between pick-up locations to drop-off
    stands. The rewards policy will be detailed later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of using a simple example like this to explain Q-learning is
    that it allows you to apply a **model-free RL algorithm**. This is because it
    does not include any mathematical model to predict which will be the next state
    of the system based on the current state. To understand what the difference would
    be if there was a model, let''s take the example of a robot at position *x* moving
    at the speed of *v*. After a time, *t*, the new position is expected to be as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66864c44-4bcf-481c-86e2-3420294c55b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Given that the state of the robot is represented by its position, *y*, the next
    state, *y'*, shall be a function of the speed, *v*, that is, *f(v)*. If the applied
    speed is doubled—*2v*—the robot will reach a different state, *x''*, because it
    will travel double the distance for the same time step, *t*. In this case, the
    set of speed values constitutes the action space. Based on this prediction, the
    robot is able to anticipate what reward it will obtain before executing the action.
    On the other hand, for the model-free case, it is not possible to anticipate the
    reward. The only thing the robot can do is to execute the action and see what
    happens.
  prefs: []
  type: TYPE_NORMAL
- en: Having this perspective, you are aware of the didactic reason to explain Q-learning
    using a model-free RL algorithm. The agent simply learns to select the most rewarded
    action in every state—it does not need to make any prediction in advance. And
    after many attempts, it will learn an optimal transport strategy.
  prefs: []
  type: TYPE_NORMAL
- en: How to run the code for the self-driving cab
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code is a Python file located in the `Chapter12_OpenAI_Gym/taxi` folder
    of the repository. As for the cart pole, the program is written in Python and
    the filename is `Taxi-v3.ipynb`. The `.ipynb` extension is the known Jupyter notebook
    extension. We have chosen this way of coding in Python so that the example can
    be understood by just following the notebook, because you have the code and the
    explanations in one place.
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter notebooks were introduced in [Chapter 2](7a2b1b82-c666-42df-9f10-9777eabe82df.xhtml),
    *Unit Testing of GoPiGo3*. There, we have covered the practical explanations of
    the sensor and actuator using Python code in the notebook environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We suggest that you open the notebook, read it from the beginning until the
    end, and then come back here to complete your comprehension of the topic. To do
    so, follow these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Activate the `gym` environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Move to the location of the example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch the notebook server. This command will open a file explorer in a window
    of your default browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Click on the `Taxi-v3.ipynb` file and another window of the browser will open
    showing the notebook content.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After having read it, we are ready to return to the spaces (action and state)
    and reward concepts that we introduced in the previous section, covering them
    in detail for the current example.
  prefs: []
  type: TYPE_NORMAL
- en: Reward table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This table specifies what reward the agent gets for every action it takes.
    A well-designed policy incentivizes the most desired actions with greater rewards.
    For the case of the cab example, the reward table is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent receives +20 points for a successful drop-off.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It loses 1 point for every time step. This way, we encourage it to solve the
    environment as quickly as possible: all the time it is on the road it is consuming
    resources, such as fuel, so this negative reward can be understood as the fuel
    expense.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is given a 10-point penalty for every illegal action it performs (during
    the pick-up or drop-off actions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we proceed to describe the action and state spaces that the cab has to
    comply with to evolve in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Action space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The action space consists of the possible actions the agent can perform. For
    the case of the taxi example, these are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The four possible moves: move (*S*) south, (*N*) north, (*E*) east, or (*W*) west
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Picking up a passenger (*P*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropoff (*D*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, there are six possible actions in total, and we will call them **action
    variables**.
  prefs: []
  type: TYPE_NORMAL
- en: State space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The state space is composed of all the possible combinations of values of the
    **state variables** that define our problem. For the case of the taxi example,
    these variables are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The current position is defined on the basis of rows and column numbers. These
    account for 5 rows x 5 columns = 25 cells (positions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Four destinations: Marked with **R** (red; the color it shows in the accompanying
    Jupyter notebook), **B **(blue), **Y**(yellow) and **G** (green).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Five possible passenger locations with regard to the taxi:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pickup/drop-off in any of the four locations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Plus one for the passenger inside in any of the remaining cells (+1)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hence, we have a total of 25 x 4 x 5 = 500 possible states. The following represents
    one of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The movement of hitting a wall is known to the encoded environment thanks to
    the `|` character. If the wall is to be crossed, when the environment tries to
    update the state, the cab could not move and will remain in the same cell. This
    is accomplished by keeping the state unchanged. Otherwise, the `:` character lets
    the cab move to the new cell. Bear in mind that there is no additional penalty
    for hitting a wall, just the -1 of the time step.
  prefs: []
  type: TYPE_NORMAL
- en: If you introduce this new rule, the training should be somewhat faster since
    the agent will implicitly learn where there is a wall and will not insist on moving
    in that direction after hitting it several times.
  prefs: []
  type: TYPE_NORMAL
- en: Self-driving cab example using the RL algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As stated previously, we will explain the learning process using Q-learning
    because of its simplicity and physical sense. Bear in mind that the Q-learning
    algorithm lets the agent keep track of its rewards to learn the best action for
    every single state:'
  prefs: []
  type: TYPE_NORMAL
- en: Each time an action is taken from a given state, a reward is obtained according
    to P.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward associated with each pair (state, action) creates a q-table that
    is a 500 x 6 matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The q-value for a concrete pair state-action stands for the *quality* of that
    action in that state. Hence, for a completely trained model, we will have a 500
    x 6 matrix, that is, 3,000 q-values:'
  prefs: []
  type: TYPE_NORMAL
- en: Every row represents a state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum q-value in each row lets the agent know what action is the best
    to take in that state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the first step, q-values are arbitrary. Then, when the agent receives rewards
    as it interacts with the environment, the q-value for every pair (**state, action**)
    is updated according to the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding equation is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: α, or alpha, is the learning rate (0<α≤1) and is applied to the new information
    the agent discovers, that is, the first part of the second term of the sum in
    the formula, `α * reward`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: γ, or gamma, is the discount factor (0≤γ≤1) and determines how much importance
    we want to give to future rewards. If this factor is 0, it makes the agent consider
    only the immediate reward, making it behave in a greedy manner. Hence, this parameter
    ponders the utility of future actions. It applies to the second part of the second
    term of the sum: `γ * maxQ[next_state, all actions]`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we should also consider the trade-off between **exploration** and **exploitation**. Let''s
    explain what these concepts mean:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploration refers to the behavior where the robot executes an action from a
    given state for the first time. This will let it discover whether that state-action
    pair has a high reward or not. Operationally, it consists of taking a random action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, exploitation refers to the behavior of executing an action
    from a given state that was the more rewarded in the past. Operationally, it consists
    of taking the action with the maximum q-value for that state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to balance these two behaviors, and, for that, we introduce the ϵ (epsilon)
    parameter, which represents the percentage of actions that should be of the exploration type.
    This prevents the agent from following a single route, which may not necessarily
    be the best.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The script runs 100,000 episodes in less than one minute. Then, we evaluate
    the agent with 100 episodes and obtain these average values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Steps per episode: 12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Penalties per episode: 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In front of the brute-force approach (which you can find in the Jupyter notebook),
    you obtain routes that vary from hundreds to thousands of steps and more than
    1,000 penalties (remember that a penalty of 1 was given for each illegal action
    incurred).
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters and optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, how can you choose the values of alpha, gamma, and epsilon? Well, this
    strategy has to be based on both intuition and trial and error. In any case, the
    three of them should decrease over time as the agent learns the best actions:'
  prefs: []
  type: TYPE_NORMAL
- en: Decreasing the need for learning, alpha, as the agent knows more about the environment
    and may trust in the acquired experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also decreasing the discount factor, gamma, as an agent develops an end-to-end
    strategy and not only focuses on the immediate reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And, finally, decreasing the exploitation rate, epsilon, because the exploration
    gains lose priority as the environment is well known
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, you should be ready to enter OpenAI ROS to train agents that
    power robots in Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: Running an environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of the rest of the chapter is to apply what you have learned about
    RL in general problems to a specific domain such as robotics. To easily transfer
    that knowledge, we will reproduce the simple cart pole example, modeling it as
    a robot in Gazebo. The code samples are in the `cart-pole_ROS` folder of the code
    repository of this chapter. Move to that location on your laptop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside, you will find two ROS packages, each one giving its name to the folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cartpole_description` contains the Gazebo simulation framework for the cart
    pole using ROS. The structure of this package is very similar to the one described
    in [Chapter 5](74284adc-e0d7-4e40-a54b-e2e447b8e2fe.xhtml), *Simulating Robot
    Behavior with Gazebo*. Hence, it is not necessary to dive into its details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cartpole_dqn` contains the OpenAI Gym environment for the preceding Gazebo
    simulation. This is where the RL algorithm is introduced, and we will focus on
    this in the coming paragraphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The package is quite similar. Let''s enter through the launch file, `start_training.launch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The line with the `<rosparam>` tag is the one that loads the configuration of
    the training process. We will explain this in the next section. This file is `cartpole_dqn_params.yaml`
    and it is hosted inside the `config` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other line, tagged with `<node>`, launches the single ROS `cartpole_dqn` node
    that implements the training process for the cart pole under the Python script, `cartpole_dqn.py`.
    What this code performs is briefly described in the following ordered points:'
  prefs: []
  type: TYPE_NORMAL
- en: It creates the Gym environment for the cart pole.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It loads the configuration from the ROS parameter server (this point is detailed
    in the following subsection).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it initializes the learning algorithm with the loaded parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, it loops over the predefined number of episodes, each one composed
    of a fixed number of steps (both values are also part of the configuration file).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For every episode, it performs the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the environment and get the first state of the robot.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For every step in the current episode, the agent chooses an action to run that
    will be one of random versus best action selection (depending on the epsilon exploration
    parameter):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the key line of the loop, and you can see, it is the same that is used
    for the cart pole in pure Python and the taxi examples in the preceding section.
    Hence, the output of the steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`observation`: The new state of the environment resulting from applying the
    action'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward`: The value that indicates how effective the action taken is'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`done`: The Boolean variable telling you if the goal has been achieved'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we let the algorithm learn from the result by following two subsequent
    steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remember the running step: `self.remember(state, action, reward, next_state,
    done)`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Replay to optimize the action selection: `self.replay(self.batch_size)`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this case, the algorithm that we use is somewhat different from the Q-learning
    we described in the *self-driving* *cab* example. It is called DQN and makes use
    of deep learning to select the best action for a given state. This is the algorithm
    that is more extensively used in RL problems, and if you want to deepen its formulation,
    you can do so by following the last reference in the *Further reading* section
    at the end of the chapter. In brief, this is what it performs:'
  prefs: []
  type: TYPE_NORMAL
- en: The remember process in every step of an episode saves what it is running and
    acts as the memory of the agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, the replay process takes a mini-batch of the last steps of the episode
    and applies for the improvement of the neural network. Such a network provides
    the agent with the *best* action to be carried out at every given state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conceptually, what the agent does is to use its memory from previous experiences
    to guess which might be the most convenient action to maximize its total reward
    in the episode.
  prefs: []
  type: TYPE_NORMAL
- en: In the remaining sections, we will focus on the specific training and evaluation
    inside ROS with Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the environment file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *step 2* of the preceding algorithmic description of the `start_training.py` script,
    ROS parameters are loaded into the model. Their definitions come from this line
    of the `start_training.launch` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'When executing this part, the parameters in the `cartpole_dqn_params.yaml` file are
    loaded into memory and are available to the `cartpole_dqn.py` script. The more
    relevant are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`cartpole_v0` is the namespace that is declared before the definitions in the `yaml` file. The
    meaning of every parameter was covered in the *Self driving cab example using
    RL algorithm* subsection. Although the DQN algorithm is more sophisticated than
    Q-learning, the conceptual meaning of *alpha*, *gamma*, and *epsilon* is equivalent
    to both. You can remember them by reviewing the preceding Q-learning algorithm
    section.'
  prefs: []
  type: TYPE_NORMAL
- en: Running the simulation and plotting the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run this simulation scenario, we follow the standard approach of first launching
    a Gazebo environment—part of the `cartpole_description` package with the model
    of the robot—and, afterward, we will start the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The result in the Gazebo window should be similar to the following screenshot.
    Although this is a 3D environment, the model itself behaves like a 2D model, since
    the cart pole can only slide along the direction of the guide:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d13622f9-529f-4d6a-900b-41045b1d5660.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the training process, we have the launch file in the other ROS package,
    that is, `cartpole_v0_training`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Be aware that before running the launch file, you have to activate the `gym` Python
    environment, which is where you installed OpenAI Gym.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see a live plot of the evolution of the training process that shows,
    in real time, the reward obtained in each episode. After the training concludes,
    you should obtain a graph similar to this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5549d733-7555-4df1-999f-36ba7da50417.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Bear in mind that for every tick of the cart pole, a reward of +1 is given.
    Hence, the graph also represents the total number of rewards per episode. To have
    a measurement of the convergence it is more useful to plot—for every episode—the
    average number of ticks (= reward) over the last 100 episodes. For example, for
    episode 1,000 this means to take the reward (number of ticks) of episodes 901
    to 1,000 and calculate the average of these 100 values. This result is the one
    plotted for episode 1,000 in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e251153c-6d2b-4f07-a5ff-24721c0a5b48.png)'
  prefs: []
  type: TYPE_IMG
- en: In fact, the criteria for the convergence is to obtain an average reward greater
    than 800 over the last 100 episodes. You may check that the curve experiments
    with a boosting after 2,600 episodes and quickly reaches the criteria.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of this section, we will present a friendly way to access
    the ROS console log to follow the training process in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Checking your progress with the logger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you may have observed, the log output is huge and runs at a very high speed.
    ROS provides another `rqt_tool` to easily follow the log of the session. To access
    it, launch it from a Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This should display a window that is similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e81757f-4a25-442e-a8b5-15193f94b142.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the two boxes below the message feed, you can exclude or include messages
    based on your own criteria. If you want to change the log level of the node, run
    the `rqt_logger_level` utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the log level of the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a04bd37-53d1-4dba-ad5d-31d89896e698.png)'
  prefs: []
  type: TYPE_IMG
- en: The `rqt_console` tool allows you to both follow the log in real time and save
    it to a file for offline analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided you with the theoretical background that you need to apply
    RL to real robots. By dissecting the simple example of the cart pole, you should
    now understand what happens under the hood in classical RL tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, by doing this first with the native OpenAI Gym framework in Python,
    and afterward, inside ROS, you should have acquired the basic skills to perform
    an RL process with a real robot, our GoPiGo3\. This is what you will learn to
    do in the final chapter of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does an agent learn following the RL approach?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Via the experience that it gets from the reward it receives each time it
    executes an action.
  prefs: []
  type: TYPE_NORMAL
- en: B) By randomly exploring the environment and discovering the best strategy by
    trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: C) Via a neural network that gives as output a q-value as a function of the
    state of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Does an agent trained with RL have to make predictions of the expected outcome
    of an action?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Yes; this is a characteristic called model-free RL.
  prefs: []
  type: TYPE_NORMAL
- en: B) Only if it does not take the model-free RL approach.
  prefs: []
  type: TYPE_NORMAL
- en: C) No; by definition, RL methods only need to be aware of rewards and penalties
    to ensure the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: If you run the Q-learning algorithm with a learning rate, alpha, of 0.7, what
    does this mean from the point of view of the learning process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) That you keep the top 30% of the pair state-actions that provide the higher
    rewards.
  prefs: []
  type: TYPE_NORMAL
- en: B) That you keep the 30% of the values of all the elements of the Q-matrix,
    and take the remaining 70% from the result of the new action.
  prefs: []
  type: TYPE_NORMAL
- en: C) That you keep the top 70% of the acquired knowledge from every iteration
    step to the next one.
  prefs: []
  type: TYPE_NORMAL
- en: If you run the Q-learning algorithm with a discount factor, gamma, of 1, what
    does this mean from the point of view of the learning process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) That the agent will only be interested in the immediate reward.
  prefs: []
  type: TYPE_NORMAL
- en: B) That the agent will only be interested in the goal of the task.
  prefs: []
  type: TYPE_NORMAL
- en: C) That the agent will only be interested in achieving the goal once.
  prefs: []
  type: TYPE_NORMAL
- en: If you run the Q-learning algorithm with an exploration rate, epsilon, of 0.5,
    what does this mean from the point of view of the learning process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) That the behavior of the agent will be similar to that of an agent that selects
    random actions.
  prefs: []
  type: TYPE_NORMAL
- en: B) That the agent will choose a random action in 50% of the steps of the episode.
  prefs: []
  type: TYPE_NORMAL
- en: C) That the agent will choose random actions in 50% of all the episodes.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To delve deeper into the concepts explained in this chapter, you can refer
    to the following sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Reinforcement Learning: An Introduction*, Sutto R., Barto A. (2018), The MIT
    Press, licensed under the Creative Commons Attribution-NonCommercial-NoDeriv 2.0
    Generic License ([http://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf](http://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine Learning Projects*, Chapter: *Bias-Variance for Deep Reinforcement
    Learning: How To Build a Bot for Atari with OpenAI Gym* ([https://assets.digitalocean.com/books/python/machine-learning-projects-python.pdf](https://assets.digitalocean.com/books/python/machine-learning-projects-python.pdf))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning with ROS and Gazebo ([https://ai-mrkogao.github.io/reinforcement%20learning/ROSRL](https://ai-mrkogao.github.io/reinforcement%20learning/ROSRL))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing different OpenAI RL algorithms with ROS And Gazebo ([https://www.theconstructsim.com/testing-different-openai-rl-algorithms-with-ros-and-gazebo/](https://www.theconstructsim.com/testing-different-openai-rl-algorithms-with-ros-and-gazebo/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning
    using ROS and Gazebo*, Zamora I., González N., Maoral V., Hernández A. (2016),
    arXiv:1608.05742 [cs.RO]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Q-Learning with Keras and Gym* ([https://keon.github.io/deep-q-learning](https://keon.github.io/deep-q-learning))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
