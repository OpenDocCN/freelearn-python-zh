- en: Machine Learning with OpenAI Gym
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenAI Gym进行机器学习
- en: In the previous chapter, we introduced you to the usage of deep learning in
    order to recognize objects based on a real-time image feed coming from the Raspberry
    Pi camera. Hence, this provides the robot the ability to take smart actions related
    to the recognized object. For example, if the object is a ball, the robot could
    collect it and put it apart so that nobody has an accident by stepping on the
    ball.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们向您介绍了如何使用深度学习来识别来自树莓派摄像头的实时图像流中的物体。因此，这为机器人提供了执行与识别物体相关的智能动作的能力。例如，如果物体是一个球，机器人可以收集它并把它放好，以免有人踩到球而造成事故。
- en: In this chapter, you will be introduced to **reinforcement learning** (**RL**),
    a field of machine learning that, nowadays, is a very active topic of research,
    having achieved the success of surpassing human performance in some scenarios,
    as shown in the recent case of the AlphaGo game ([https://deepmind.com/blog/article/alphago-zero-starting-scratch](https://deepmind.com/blog/article/alphago-zero-starting-scratch)).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解到**强化学习**（**RL**），这是一个机器学习的领域，如今是一个非常活跃的研究主题，在某些场景中已经取得了超越人类性能的成功，如最近AlphaGo游戏的案例所示（[https://deepmind.com/blog/article/alphago-zero-starting-scratch](https://deepmind.com/blog/article/alphago-zero-starting-scratch)）。
- en: 'You will learn in a practical manner using the Python framework **OpenAI Gym**,
    which is a toolkit for developing and comparing RL algorithms. We will provide
    a conceptual approach to RL that will allow us to handle various problems within
    a programmatic way using Gym environments. In order to do this, we will differentiate
    between three main components: *scenarios*, *tasks*, and *agents*. Here, the scenario
    is the physical environment where the robot evolves, the task is the action(s)
    the robot is expected to learn, and the agent is the software program that makes
    the decisions for the action(s) to execute.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用Python框架**OpenAI Gym**以实践的方式学习，这是一个用于开发比较强化学习（RL）算法的工具包。我们将提供一个概念性的方法来处理RL，这将使我们能够使用Gym环境以编程方式处理各种问题。为了做到这一点，我们将区分三个主要组件：*场景*、*任务*和*代理*。在这里，场景是机器人演化的物理环境，任务是机器人预期学习的动作，代理是做出执行动作决策的软件程序。
- en: This segregation will allow you to decouple these components and reuse them
    in a different scope. For example, you could have trained an agent so that a two-wheeled
    drive robot, such as our GoPiGo3, would be able to carry a load from one point
    to a target location and use the same agent with a different robot, for example,
    a four-wheel drive such as Summit XL ([https://www.robotnik.es/robots-moviles/summit-xl](https://www.robotnik.es/robots-moviles/summit-xl)).
    The code of the agent is the same because it abstracts the robot's concrete features.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分离将允许您解耦这些组件，并在不同的范围内重用它们。例如，您可能已经训练了一个代理，使得一个两轮驱动的机器人，如我们的GoPiGo3，能够从一个点到目标位置携带负载，并且可以使用相同的代理与不同的机器人一起使用，例如四轮驱动的Summit
    XL（[https://www.robotnik.es/robots-moviles/summit-xl](https://www.robotnik.es/robots-moviles/summit-xl)）。代理的代码是相同的，因为它抽象了机器人的具体特征。
- en: Similarly, you could use different generated scenarios to test the same robot.
    This will show the ability of the trained agent to perform under different boundary
    conditions. With these ideas in mind, this chapter will teach you the basics of
    the OpenAI Gym API, how to integrate with an ROS environment, and how to follow
    the training process by representing the results graphically.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以使用不同的生成场景来测试同一机器人。这将展示训练好的代理在不同边界条件下执行的能力。带着这些想法，本章将向您介绍OpenAI Gym API的基础知识，如何将其与ROS环境集成，以及如何通过图形化表示来跟踪训练过程。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: An introduction to OpenAI Gym
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym简介
- en: Running an environment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行环境
- en: Configuring an environment file
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置环境文件
- en: Running the simulation and plotting the results
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行模拟并绘制结果
- en: In the first section, you will start using the base Gym API in its native Python
    environment. In the following ones, you will learn how to add the ROS wrappers
    to train robots in Gazebo.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，您将开始在原生的Python环境中使用基础Gym API。在接下来的部分中，您将学习如何添加ROS包装器以在Gazebo中训练机器人。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will make use of the code in the `Chapter11_OpenAI_Gym` folder,
    located at [https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter11_OpenAI_Gym](https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter11_OpenAI_Gym). Copy
    the files of this chapter to the ROS workspace, putting them inside the `src` folder:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用位于[https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter11_OpenAI_Gym](https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter11_OpenAI_Gym)的`Chapter11_OpenAI_Gym`文件夹中的代码。将本章的文件复制到ROS工作空间中，将它们放在`src`文件夹内：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, you will need to install Anaconda ([https://www.anaconda.com](https://www.anaconda.com)).
    This is the Python distribution that has become the *de facto* open source standard
    for the Data Science community. It provides a complete Python environment for
    machine learning projects.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要安装Anaconda ([https://www.anaconda.com](https://www.anaconda.com))。这是已成为数据科学社区*事实上的*开源标准的Python发行版。它为机器学习项目提供了一个完整的Python环境。
- en: Visit the download section of the Anaconda website at [https://www.anaconda.com/distribution/#linux](https://www.anaconda.com/distribution/#linux), and
    select the Python 2.7 bundle. We select this package because the ROS Python client
    is focused on this version; however, you should be aware that it recently came
    to the end of life in December 2019.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 访问Anaconda网站的下载部分[https://www.anaconda.com/distribution/#linux](https://www.anaconda.com/distribution/#linux)，并选择Python
    2.7捆绑包。我们选择这个包是因为ROS Python客户端专注于这个版本；然而，您应该知道它最近在2019年12月已经停止维护。
- en: Open Robotics intends to create a new ROS distribution in May 2020 targeting
    Python 3: [https://discourse.ros.org/t/planning-future-ros-1-distribution-s/6538](https://discourse.ros.org/t/planning-future-ros-1-distribution-s/6538).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Open Robotics计划在2020年5月创建一个新的ROS发行版，针对Python 3：[https://discourse.ros.org/t/planning-future-ros-1-distribution-s/6538](https://discourse.ros.org/t/planning-future-ros-1-distribution-s/6538)。
- en: 'After downloading Anaconda, go to the download directory and enter the following
    command to execute the code for the installation:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 下载Anaconda后，转到下载目录并输入以下命令以执行安装代码：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The filename marked in bold letters should match the name of the one you have
    downloaded. If this is not the case, then run the `bash` command, replacing the
    filename with the actual one you have.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 被粗体字母标记的文件名应与您下载的名称匹配。如果不是这样，请运行`bash`命令，将文件名替换为您实际拥有的名称。
- en: 'The `$ conda init` command is optionally executed from the installation script,
    and, if successful, it will provide the following output:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ conda init`命令可以从安装脚本中可选执行，如果成功，它将提供以下输出：'
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Conda is the package manager that ships with Anaconda. It allows you to easily
    install, remove, list, and inspect the Python packages in your Anaconda installation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Conda是Anaconda附带的包管理器。它允许您轻松地安装、删除、列出和检查Anaconda安装中的Python包。
- en: 'After installing Anaconda, you will see these lines added to your `~/.bashrc` file:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Anaconda后，您将在`~/.bashrc`文件中看到以下行被添加：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'So that the added configuration takes effect, source it in the Terminal prompt:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使添加的配置生效，请在终端提示符中source它：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding `.bashrc` line should take you into the (base) default Anaconda
    environment. We recommend that you do not activate Conda''s base environment on
    startup. To ensure this, set the `auto_activate_base` parameter to `false`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的`.bashrc`行应将您带到（基础）默认的Anaconda环境。我们建议您在启动时不要激活Conda的基础环境。为了确保这一点，将`auto_activate_base`参数设置为`false`：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you wish to restore the default configuration, you can revert it by changing
    the value to `true`. Finally, you have the option to manually activate the default
    Anaconda environment, on demand, in a Terminal with this command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望恢复默认配置，可以通过将值更改为`true`来还原它。最后，您可以选择在终端中手动激活默认的Anaconda环境，使用以下命令：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this base environment, you are able to install Jupyter notebooks. You can
    use them to view, run, and modify Python notebooks:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基础环境中，您能够安装Jupyter笔记本。您可以使用它们来查看、运行和修改Python笔记本：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Remember that this is the user-friendly Python runtime you had preinstalled
    with DexterOS. It was used in [Chapter 2](7a2b1b82-c666-42df-9f10-9777eabe82df.xhtml),
    *Unit Testing of GoPiGo3*, to run most of the examples. To deactivate the virtual
    environment, just run the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这是与DexterOS预安装的用户友好的Python运行时。它在[第2章](7a2b1b82-c666-42df-9f10-9777eabe82df.xhtml)，*GoPiGo3单元测试*中使用，以运行大多数示例。要取消激活虚拟环境，只需运行以下命令：
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can find a useful cheatsheet for Conda Package Manager, which you should
    have at hand, at the following URL: [https://kapeli.com/cheat_sheets/Conda.docset/Contents/Resources/Documents/index](https://kapeli.com/cheat_sheets/Conda.docset/Contents/Resources/Documents/index).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下 URL 找到一个有用的 Conda 包管理器速查表，你应该随身携带：[https://kapeli.com/cheat_sheets/Conda.docset/Contents/Resources/Documents/index](https://kapeli.com/cheat_sheets/Conda.docset/Contents/Resources/Documents/index)。
- en: At this point, we are ready to proceed with OpenAI Gym and its installation;
    this will be explained in a dedicated section next.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经准备好使用 OpenAI Gym 及其安装；这将在下一节中解释。
- en: An introduction to OpenAI Gym
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym 简介
- en: In the previous chapter, we provided a practical overview of what you can expect
    in RL when applied to robotics. In this chapter, we will provide a general view
    in which you will discover how RL is used to train smart *agents*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们提供了将 RL 应用到机器人时可以期待的内容的实用概述。在本章中，我们将提供一个一般性的视角，你将发现 RL 是如何用于训练智能 **代理**
    的。
- en: First, we will need to install OpenAI Gym and OpenAI ROS on our laptop in preparation
    for the practical examples. Then, we will explain its concepts.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在我们的笔记本电脑上安装 OpenAI Gym 和 OpenAI ROS，为实际示例做准备。然后，我们将解释其概念。
- en: Installing OpenAI Gym
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 OpenAI Gym
- en: 'As we did in the previous chapter, we are going to create a virtual environment
    for the Python setup of this chapter, which we will call `gym`. The following
    two commands allow for the creation and then the activation of `gym`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中所做的那样，我们将为本章的 Python 设置创建一个虚拟环境，我们将称之为 `gym`。以下两个命令允许创建并激活 `gym`：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Following this, install the specific Python packages that we are going to need
    for the examples:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，安装我们将需要的特定 Python 包：
- en: The Keras package ([https://keras.io/](https://keras.io/)), which is a high-level
    neural network API that it used within OpenAI Gym. Remember that it was also used
    in the previous chapter, but we need to install it again because this is a new `gym` environment.
    Keras will let us train an agent using the **DQN** (short for **Deep Q-Network**)
    algorithm, which is deep learning-based.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 包 ([https://keras.io/](https://keras.io/))，这是一个高级神经网络 API，在 OpenAI Gym
    中使用。记住，它也在上一章中使用过，但我们需要再次安装它，因为这是一个新的 `gym` 环境。Keras 将使我们能够使用 **DQN**（即 **深度 Q
    网络**）算法训练代理，这是一个基于深度学习的算法。
- en: You also need TensorFlow since it is used as the backend for Keras.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还需要 TensorFlow，因为它被用作 Keras 的后端。
- en: Finally, you will need the Gym package ([https://github.com/openai/gym](https://github.com/openai/gym)),
    which is the implementation in Python of OpenAI Gym.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，你需要 Gym 包 ([https://github.com/openai/gym](https://github.com/openai/gym))，这是
    OpenAI Gym 的 Python 实现。
- en: 'You can install three of the packages in a row, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以连续安装三个包，如下所示：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now check the version of `gym`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，检查 `gym` 的版本：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output should be as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该是这样的：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In addition to this, install the Jupyter notebooks, since some of the Python
    examples are explained in this friendly format:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还需要安装 Jupyter 笔记本，因为一些 Python 示例是以这种友好的格式解释的：
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, install an optional library called `pybox2d`. This is a 2D physics
    engine for games and simple simulations, used by some of the premade environments
    that ship with Gym:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，安装一个名为 `pybox2d` 的可选库。这是一个用于游戏和简单模拟的 2D 物理引擎，由 Gym 的一些预装环境使用：
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The technical requirements end here. The following subsections are optional
    and are intended to increase your background of different ways of managing Python
    and Anaconda. We will show you how to install OpenAI Gym from the source and host
    the package in your working folder, which is a typical way of using a Python package
    in development mode.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 技术要求到此结束。以下小节是可选的，旨在增加你对管理 Python 和 Anaconda 的不同方式的了解。我们将向你展示如何从源代码安装 OpenAI
    Gym 并在您的工作文件夹中托管该包，这是一种在开发模式下使用 Python 包的典型方式。
- en: Without Anaconda (optional)
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无 Anaconda（可选）
- en: 'If you do not want to use Anaconda but instead keep working in the Python environment
    that ships with Ubuntu, you can install an in-home user directory by adding the
    `--user` flag:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想使用 Anaconda，而是继续在 Ubuntu 随附的 Python 环境中工作，你可以通过添加 `--user` 标志在本地用户目录中安装：
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This places the necessary packages in the `~/.local/lib/python2.7/site-packages` folder.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这将必要的包放置在 `~/.local/lib/python2.7/site-packages` 文件夹中。
- en: Installing gym in development mode (optional)
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以开发模式安装 gym（可选）
- en: 'This allows you to have the source code of OpenAI Gym in your working directory,
    change the files of the package, and see their effects instantly, without needing
    to reinstall the `gym` module:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许你在工作目录中拥有 OpenAI Gym 的源代码，更改包中的文件，并立即看到它们的效果，而无需重新安装 `gym` 模块：
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `-e` option allows this kind of installation, and it is suitable to be used
    as developer mode. The `--user` option performs the installation locally to the
    user at the `~/.local/lib/python2.7/site-packages` location.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`-e` 选项允许这种安装方式，并且适合用作开发者模式。`--user` 选项将安装本地化到用户的 `~/.local/lib/python2.7/site-packages`
    位置。'
- en: 'To keep the environment clean, remove the package installation, keeping only
    the `gym` Python package in the Gym virtual environment:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持环境整洁，移除包安装，只保留 Gym 虚拟环境中的 `gym` Python 包：
- en: '[PRE17]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This snippet lets you know how to manually remove a Python package from the
    system.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码片段会告诉你如何手动从系统中移除一个 Python 包。
- en: To reproduce the examples in this chapter, we will be following the former approach,
    that is, installing Gym as a system package within the Conda environment.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了复现本章中的示例，我们将遵循之前的方法，即在 Conda 环境中将 Gym 安装为系统包。
- en: Installing OpenAI ROS
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 OpenAI ROS
- en: 'So that the code can run inside ROS, you have to install OpenAI ROS, which
    is built on top of OpenAI Gym. Execute the following command to clone the contributed
    ROS package and start the setup for ROS:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让代码能在 ROS 中运行，你必须安装 OpenAI ROS，它建立在 OpenAI Gym 之上。执行以下命令以克隆贡献的 ROS 包并开始 ROS
    的设置：
- en: '[PRE18]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Be aware that we had to install the ROS package from the source because the
    compiled binary is not available in Ubuntu. In particular, it is worth noting
    that the `rosdep install openai_ros` command does the equivalent of the general `sudo
    apt install <package>` command of Ubuntu; that is, every time you install a new
    component, it automatically includes the required dependencies. Remember that,
    for an ROS package, the dependencies are declared in the `package.xml` file located
    in the root folder of its source code.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不得不从源代码安装 ROS 包，因为编译的二进制文件在 Ubuntu 中不可用。特别是，值得注意的是，`rosdep install openai_ros`
    命令相当于 Ubuntu 的通用 `sudo apt install <package>` 命令；也就是说，每次安装新组件时，它都会自动包含所需的依赖项。记住，对于一个
    ROS 包，其依赖项在源代码根目录下的 `package.xml` 文件中声明。
- en: Once the installation of OpenAI Gym is complete, we can go on to explain its
    concepts.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 OpenAI Gym 的安装完成，我们就可以继续解释其概念。
- en: Agents, artificial intelligence, and machine learning
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理、人工智能和机器学习
- en: 'The concept of an agent comes from the artificial intelligence field and is
    used to designate anything that makes decisions. Well, that is, it is what a typical
    computer program does with the conditional instructions of the `if ... then ...
    else ...` type. Put simply, an *agent* is a program that can take more elaborated
    decisions rather than using pure conditionals. For example, consider a video game:
    when you play against a machine, your opponent observes your actions and decides
    what to do next to win the game. What powers the opponent''s decisions is an agent.
    Generalizing this idea, an agent can be used to solve many kinds of problems;
    for example, when to stop and start a heater to keep a room warm at a set temperature
    point.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的概念来源于人工智能领域，用于指代任何做出决策的事物。换句话说，这就是典型的计算机程序使用 `if ... then ... else ...` 类型的条件指令所做的事情。简单来说，一个
    *代理* 是一个可以做出比纯条件更复杂决策的程序。例如，考虑一个视频游戏：当你与机器对战时，你的对手会观察你的行动并决定下一步做什么以赢得游戏。驱动对手决策的是代理。推广这个想法，代理可以用来解决许多类型的问题；例如，何时停止和启动加热器以在设定温度点保持房间温暖。
- en: When instead of using analytical formulas—as in the case of using a **PID**
    controller (acronym of *Proportional–Integral–Derivative*) to address the problem
    of temperature regulation mentioned previously—you use empirical data to tell
    the *agent* what to do in *hundreds or thousands of particular situations* (the
    outside temperature, the room temperature, the number of people in the room, the
    time of the day, and so on), you are training it so that it is able to generalize
    and respond correctly when faced with a broad range of conditions. And this training
    process is what we call machine learning in general, and RL for the particular
    scope of these last two chapters of the book.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用经验数据而不是解析公式——就像使用**PID**控制器（比例-积分-微分）来解决之前提到的温度调节问题一样——告诉**智能体**在数百或数千种特定情况下应该做什么（室外温度、室内温度、室内人数、一天中的时间等），你正在训练它，使其能够概括并正确应对广泛的条件。而这个训练过程就是我们通常所说的机器学习，以及本书最后两章特定范围的强化学习。
- en: At this point, you should also be aware that an *agent* that uses machine learning
    can make good decisions when the input conditions are in the range for which it
    has been trained. You cannot expect good decisions if one or more relevant conditions
    are outside of the training range. Hence the importance of the preceding paragraph
    in providing the empirical data of *hundreds or thousands of particular situations* for
    the training process.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你也应该意识到，如果一个使用机器学习的**智能体**在它被训练的输入条件范围内做出好的决策。如果你或多个相关条件超出了训练范围，你不能期望它做出好的决策。因此，前面段落提供实验数据的重要性，这些数据是数百或数千种特定情况，用于训练过程。
- en: OpenAI Gym is a structured framework to train agents based on RL techniques.
    Once this agent is trained, it can be reused in similar problems to power the
    decision capability. To illustrate these concepts, we are going to use a simple
    mechanism, the cart pole, which is also known as the inverted pendulum.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym是一个基于强化学习技术的智能体训练的框架。一旦这个智能体被训练，它可以在类似的问题中重复使用，以增强决策能力。为了说明这些概念，我们将使用一个简单的机制，即车杆，也称为倒立摆。
- en: The cart pole example
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 车杆示例
- en: 'This is the classical control problem of the inverted pendulum that experiments
    with an unstable equilibrium (take a look at the following diagram). By applying
    lateral forces, *F*, you may compensate its tendency to fall down and get it to
    stay up (that is, when angle θ is close to zero):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是倒立摆的经典控制问题，它实验了一个不稳定的平衡（看看下面的图）。通过施加横向力*F*，你可以补偿它的倾向向下坠落，并使其保持竖立（即当角度θ接近零时）：
- en: '![](img/0a9bbe2f-0f25-4bbc-a1b5-56a0c9901cf1.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0a9bbe2f-0f25-4bbc-a1b5-56a0c9901cf1.png)'
- en: 'Source: https://de.wikipedia.org/wiki/Datei:Cart-pendulum.svg'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：https://de.wikipedia.org/wiki/Datei:Cart-pendulum.svg
- en: We are going to solve this problem with OpenAI Gym, and the approach consists
    of starting with an *agent* that has no knowledge of the physics of the problem,
    that is, it does not have any idea of what lateral forces to apply so that the
    cart pole stays up. Following a trial-and-error strategy, the agent will learn
    which force directions and values are adequate for each angle of the pendulum.
    It is a quick problem to solve because you have only one degree of freedom—the angle θ—and
    one independent variable—the force, *F*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用OpenAI Gym来解决这个问题，方法是从一个没有任何关于问题物理知识的**智能体**开始，也就是说，它没有任何关于应该施加什么横向力的想法，以便车杆保持竖立。通过试错策略，智能体将学会哪些力的方向和值对于摆的每个角度是合适的。这是一个快速解决的问题，因为你只有一个自由度——角度θ——和一个独立变量——力，*F*。
- en: 'This example is included with the code provided in the book repository, and
    we will take it as the base to explain the common concepts of the OpenAI Gym framework:
    environments, observations, and spaces.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子包含在本书的代码库中，我们将以此为基础来解释OpenAI Gym框架的常见概念：环境、观察和空间。
- en: Environments
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境
- en: 'An environment is a scenario that models a problem (such as keeping a cart
    pole standing up) with a minimal interface that an agent can interact with. You
    can see the cart pole environment in action by running this snippet of code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 环境是一个场景，它通过一个最小化的接口来模拟一个问题（例如保持车杆竖立），这个接口允许一个智能体与之交互。你可以通过运行以下代码片段来看到车杆环境在动作中的样子：
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You should see the cart pole moving and rotating randomly, as shown in the
    following screenshot:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到车杆在随机移动和旋转，如下面的截图所示：
- en: '![](img/2f8cc531-5306-4eb0-8198-51889cf73587.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2f8cc531-5306-4eb0-8198-51889cf73587.png)'
- en: 'The content of the script is quite simple:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的内容相当简单：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: After importing the `gym` module, we set up the `env` variable to the predefined
    `CartPole-v0` environment. Then, in the next line, the `.reset()` method is applied
    to `env` so that the environment is initialized.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入`gym`模块后，我们将`env`变量设置为预定义的`CartPole-v0`环境。然后，在下一行，我们对`env`应用`.reset()`方法，以便初始化环境。
- en: 'The body of the script is the `for` loop, which we set to 1,000 iterations.
    In each of these iterations, the script does two things:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的主体是一个`for`循环，我们将其设置为1,000次迭代。在这些迭代中的每一次，脚本都会做两件事：
- en: It renders the state of the cart pole with `env.render()`.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`env.render()`渲染小车和杆的状态。
- en: It takes a random action to execute a step of the simulation, something that
    is done with the line `env.step(env.action_space.sample())`. The `.sample()` method
    provides a random force, *F*, to act on the base of the cart pole.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行模拟的一步需要随机动作，这是通过`env.step(env.action_space.sample())`这一行实现的。`.sample()`方法提供了一个随机力，*F*，作用于小车和杆的底部。
- en: 'By chaining several steps together and letting the system evolve, the agent
    completes an episode, the end of which is defined by one of three possibilities:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将几个步骤连接起来并让系统演化，代理完成一个剧集，其结束由以下三种可能性之一定义：
- en: The pole angle is greater than ±12°.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆的角度大于±12°。
- en: The position of the cart with respect to the center of the track is more than
    ±2.4 units.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小车相对于轨道中心的位移超过±2.4单位。
- en: The episode length exceeds 200 steps.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集剧长度超过200步。
- en: This definition is part of the environment specification that can be found at [https://github.com/openai/gym/wiki/CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0).
    Let's now review the definitions of observations and spaces.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义是环境规范的一部分，可以在[https://github.com/openai/gym/wiki/CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0)找到。现在让我们回顾一下观察和空间定义。
- en: Spaces
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 空间
- en: 'Spaces describe valid actions (*F* forces) and observations (cart pole angle θ).
    This concept of observations will be covered in detail in the next subsection.
    Every environment has two spaces attached:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 空间描述了有效的动作（*F* 力）和观察（小车和杆的角度θ）。这个观察的概念将在下一小节中详细讨论。每个环境都有两个空间与之相关联：
- en: '**Action space**: This is characterized by a set of state variables under the `env.action_space` *object. *This
    space defines the possible actions an agent is allowed to take. For the case of
    the cart pole, there is only one variable: to apply a lateral force, *F*.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作空间**：这由`env.action_space`对象下的状态变量集合定义。这个空间定义了代理可以采取的可能动作。对于小车和杆的情况，只有一个变量：施加一个侧向力，*F*。'
- en: '**Observation space**: This describes the physical state of the agent, that
    is, the angular position of the cart pole, θ. Operationally, it is a set of state
    variables under the `env.observation_space` object.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察空间**：这描述了代理的物理状态，即小车和杆的角位置θ。从操作上讲，它是`env.observation_space`对象下的状态变量集合。'
- en: Let's now describe the concept of observations in order to get a full understanding
    of how they support the learning process.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们描述观察的概念，以便全面理解它们如何支持学习过程。
- en: Observations
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察
- en: 'Given an environment, an observation consists of a set of values that define
    a given state of the environment, the angle θ. It is like taking a snapshot of
    a scene. The environment''s step function, `env.step`, returns the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个环境，一个观察由一组定义环境给定状态的值组成，即角度θ。这就像是对场景进行快照。环境步骤函数`env.step`返回以下内容：
- en: The current state; that is to say, it sets the current value of the state variable, **θ**.
    Operationally, it is an object type variable called `observation`.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前状态；也就是说，它设置状态变量当前值，**θ**。从操作上讲，它是一个名为`observation`的对象类型变量。
- en: The reward that the agent has obtained from the last action, force *F*, as was
    previously mentioned when describing the action space. The reward is like points
    in a game—a quantitative value that accumulates all the rewards (points) obtained
    from the actions performed in the current episode from the beginning, that is,
    the current score in the game analogy. If the applied force contributes to getting
    the cart pole to stay up, the reward is positive; if not, a negative reward (or
    penalty) is given. This variable is of the float type and is called `reward`.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体从最后一步获得的奖励，即力 *F*，正如在描述动作空间时之前提到的。奖励就像游戏中的分数——一个累积所有奖励（分数）的定量值，即从当前剧集开始执行的动作中获得的所有奖励（分数）。如果施加的力有助于使购物车杆保持竖立，则奖励为正；如果不这样做，则给予负奖励（或惩罚）。这个变量是浮点类型，称为
    `reward`。
- en: Whether the current episode has finished, with a Boolean variable called `done`.
    When it finishes, the `env.reset` function is called to restart the environment,
    getting ready for the next episode.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前剧集是否结束，通过一个名为 `done` 的布尔变量。当它结束时，调用 `env.reset` 函数来重新启动环境，为下一剧集做准备。
- en: Diagnostic information under the form of a Python dictionary object called `info`.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诊断信息以名为 `info` 的 Python 字典对象的形式呈现。
- en: Hence, the agent will try to maximize its score, which is calculated as the
    cumulative sum of rewards it receives for every force it applies. This maximization
    algorithm will make it learn to keep the cart pole up.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，智能体将尝试最大化其得分，该得分是它为每次施加的力所获得的奖励的累积总和。这个最大化算法将教会它保持购物车杆竖立。
- en: The preceding explanations should allow you to understand how the scripts work.
    Now we will run a training session of the cart pole so that you are able to see
    how the *good* actions are positively rewarded, encouraging the agent to build
    an effective strategy.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的解释应该能让你理解脚本是如何工作的。现在我们将运行一个购物车杆的训练会话，以便你能看到如何对**良好**的行为进行正面奖励，鼓励智能体构建有效的策略。
- en: Running the full cart pole example
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行完整的购物车杆示例
- en: 'The first script that we ran, `cart-pole_env.py`, was intended to show a sequence
    of 1,000 random steps. The new script will provide feedback for every action that
    is taken by giving rewards for good actions:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行的第一脚本 `cart-pole_env.py` 的目的是展示 1,000 个随机步骤的序列。新的脚本将为每个采取的动作提供反馈，通过给予良好行为的奖励：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The iterative block in the script includes the following lines:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本中的迭代块包括以下行：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Following the order of the lines, this is what is done in each step:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 按照行顺序，以下是每个步骤所执行的操作：
- en: Render the environment. This is the window that shows you what is happening.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 渲染环境。这是显示正在发生什么的窗口。
- en: Choose the next action. This is where the accumulated experience is used to
    decide what to do, taking into account the current state.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择下一个动作。这是使用累积经验来决定做什么的地方，同时考虑到当前状态。
- en: Run the new step; that is to say, perform the selected action and observe what
    happens. The observation returns the new state of the agent, `next_state`, the
    reward the agent obtains, and the Boolean variable, `done`, telling you if the
    episode has finished.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行新的步骤；也就是说，执行所选动作并观察发生了什么。观察返回智能体的新状态 `next_state`、智能体获得的奖励以及一个布尔变量 `done`，告诉你剧集是否结束。
- en: The reward is added to the `score` variable, which accumulates all the rewards
    obtained from the beginning of the episode.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励被添加到 `score` 变量中，该变量累积了从剧集开始以来获得的所有奖励。
- en: Next, the set (state, action, and reward) is stored in memory—`agent.remember`—so
    that the agent can take advantage of their past experience, promoting the actions
    in a given state that gave them more rewards.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将（状态、动作和奖励）集存储在记忆中——`agent.remember`——这样智能体就可以利用它们过去的经验，促进在给定状态下给予它们更多奖励的动作。
- en: Finally, we update the current `state` variable with the output of *step 3*,
    which is the value of `next_state`.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将当前 `state` 变量更新为 *步骤 3* 的输出，即 `next_state` 的值。
- en: 'When the training finishes, a curve representing the evolution of the score
    as a function of the episode is depicted:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练完成时，一个表示得分随剧集演变的曲线被描绘出来：
- en: '![](img/9122e7ca-773d-4b4c-9158-0fe32700455b.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9122e7ca-773d-4b4c-9158-0fe32700455b.png)'
- en: You can see how after 40 episodes the agent starts getting good scores of around
    200\. What this means is that the agent has learned to keep the pole in equilibrium
    by applying the force in the direction that prevents it from falling down. This
    simple example takes a few minutes to achieve the target of getting 200 points
    in every new episode, so let's quickly understand how RL works.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，在40个回合之后，代理开始获得大约200分的良好成绩。这意味着代理已经学会了通过施加防止杆子掉落的力的方向来保持杆子的平衡。这个简单的例子需要几分钟才能在每一新回合中达到200分的目标，所以让我们快速了解强化学习是如何工作的。
- en: Be aware that the cart pole problem is not representative of real scenarios.
    In actual cases, the state is defined by a set of many variables, and the possible
    actions an agent may take are also many. Real RL problems are very CPU-intensive
    and they take thousands and even millions of episodes to get a reasonably good
    performance.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，小车杆问题并不代表真实场景。在实际情况下，状态由一组许多变量定义，代理可能采取的可能动作也有很多。真实的强化学习问题非常CPU密集，它们需要成千上万甚至数百万个回合才能获得相当好的性能。
- en: In the next section, we will describe a more realistic problem where there are
    500 states and six possible actions. The goal of this second example is to understand
    the score maximization algorithm in its most basic version, that is, through the
    Q-learning algorithm.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将描述一个更现实的问题，其中存在500个状态和六个可能的行为。第二个示例的目标是理解分数最大化算法在其最基本版本中的工作原理，即通过Q学习算法。
- en: Q-learning explained – the self-driving cab example
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习解释——自动驾驶出租车示例
- en: 'The problem we are going to solve consists of a self-driving cab that has to
    pick up passengers and drop them off to the right location. It must do so as quickly
    as possible while respecting the traffic rules. The graphics are based on ASCII
    characters and we are going to use real images to explain the goal. We will also
    follow a time-ordered sequence of frames:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要解决的问题包括一辆自动驾驶出租车，它必须将乘客接送到正确的位置。它必须尽可能快地这样做，同时遵守交通规则。图形基于ASCII字符，我们将使用真实图像来解释目标。我们还将遵循按时间顺序的帧序列：
- en: 'The first sequence represents the cab—the yellow square—in the start position.
    It can move up, down, left, or right except when it finds a vertical bar; that
    motion is not allowed. There are four possible taxi stands where the cab can pick
    up or drop off a passenger. They are marked with the letters **R**, **G**, **B**,
    and **Y**. The blue letter (**R**, in this case) is the pick-up location, and
    the purple letter (**G**) is the destination of where to transport the passenger
    to:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一序列表示出租车——黄色方块——在起始位置。它可以向上、向下、向左或向右移动，除非它遇到垂直杆；这种移动是不允许的。有四个可能的出租车停靠站，出租车可以在那里接客或送客。它们用字母**R**、**G**、**B**和**Y**标记。蓝色字母（在这种情况下是**R**）是接客地点，紫色字母（在这种情况下是**G**）是乘客需要被运送到的目的地：
- en: '![](img/07e86d35-1770-41cf-9c05-029469eb4236.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/07e86d35-1770-41cf-9c05-029469eb4236.png)'
- en: 'This second sequence in the next diagram shows a step of the simulation where
    the passenger is inside the cab. They were picked up at location **R** and are
    being transported to the destination. This state is visually recognizable because
    the square representing the taxi is filled in green (when it does not carry any
    passenger, the color remains yellow):'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个图中的第二个序列显示了模拟的一个步骤，其中乘客在车内。他们是在位置**R**被接走的，正在被运送到目的地。这种状态在视觉上是可以识别的，因为代表出租车的方块被填充为绿色（当它不载有乘客时，颜色保持为黄色）：
- en: '![](img/6676cc83-c73e-4394-b489-560cdbb91b22.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6676cc83-c73e-4394-b489-560cdbb91b22.png)'
- en: 'The final sequence corresponds to the scenario in which the taxi leaves the
    passenger at their destination; this is represented by the letter **G**, in this
    case. When this happens, the cab color changes to yellow again:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个序列对应于出租车将乘客送到目的地的场景；在这种情况下，用字母**G**表示。当这种情况发生时，出租车颜色再次变为黄色：
- en: '![](img/707db954-8f85-47b9-a85e-1453b7da5c5b.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/707db954-8f85-47b9-a85e-1453b7da5c5b.png)'
- en: The goal of this problem is to train an RL agent to learn to drive the taxi
    following the shortest path for every trip. The shortest path is operationally
    implemented by a rewards policy, which gives the agent a predefined reward for
    every action it takes depending on its utility. Hence, the RL agent will try to
    maximize its total reward by associating states with *useful* actions. In this
    way, it will gradually discover a transport policy that will let it minimize (in
    global average) the time from traveling between pick-up locations to drop-off
    stands. The rewards policy will be detailed later in this section.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的目标是训练一个强化学习智能体，使其学会在每个行程中按照最短路径驾驶出租车。最短路径通过奖励策略来操作实现，该策略为智能体在每次采取的动作中根据其效用给予预定义的奖励。因此，强化学习智能体将尝试通过将状态与*有用*的动作关联起来，来最大化其总奖励。这样，它将逐渐发现一个运输策略，使其在全局平均意义上最小化从接客地点到下客站点的旅行时间。奖励策略将在本节后面详细说明。
- en: 'The advantage of using a simple example like this to explain Q-learning is
    that it allows you to apply a **model-free RL algorithm**. This is because it
    does not include any mathematical model to predict which will be the next state
    of the system based on the current state. To understand what the difference would
    be if there was a model, let''s take the example of a robot at position *x* moving
    at the speed of *v*. After a time, *t*, the new position is expected to be as
    follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这样简单的例子来解释Q学习的优势在于，它允许你应用**无模型强化学习算法**。这是因为它不包含任何数学模型来预测基于当前状态，系统的下一个状态将是什么。为了理解如果存在模型会有什么不同，让我们以一个位于位置*x*、以速度*v*移动的机器人为例。经过时间*t*后，新的位置预计如下：
- en: '![](img/66864c44-4bcf-481c-86e2-3420294c55b2.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/66864c44-4bcf-481c-86e2-3420294c55b2.png)'
- en: Given that the state of the robot is represented by its position, *y*, the next
    state, *y'*, shall be a function of the speed, *v*, that is, *f(v)*. If the applied
    speed is doubled—*2v*—the robot will reach a different state, *x''*, because it
    will travel double the distance for the same time step, *t*. In this case, the
    set of speed values constitutes the action space. Based on this prediction, the
    robot is able to anticipate what reward it will obtain before executing the action.
    On the other hand, for the model-free case, it is not possible to anticipate the
    reward. The only thing the robot can do is to execute the action and see what
    happens.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器人的状态由其位置*y*表示，下一个状态*y'*将是速度*v*的函数，即*f(v)*。如果应用的速度加倍—*2v*—机器人将达到不同的状态，*x''*，因为它将在相同的时间步长*t*内行驶双倍的距离。在这种情况下，速度值的集合构成了动作空间。基于这个预测，机器人能够预测在执行动作之前将获得什么奖励。另一方面，对于无模型的情况，无法预测奖励。机器人唯一能做的就是执行动作并看看会发生什么。
- en: Having this perspective, you are aware of the didactic reason to explain Q-learning
    using a model-free RL algorithm. The agent simply learns to select the most rewarded
    action in every state—it does not need to make any prediction in advance. And
    after many attempts, it will learn an optimal transport strategy.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这种视角，你将意识到使用无模型强化学习算法解释Q学习的教学理由。智能体只需学会在每个状态下选择最有奖励的动作——它不需要提前做出任何预测。经过多次尝试，它将学会一个最优的运输策略。
- en: How to run the code for the self-driving cab
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何运行自动驾驶出租车的代码
- en: The code is a Python file located in the `Chapter12_OpenAI_Gym/taxi` folder
    of the repository. As for the cart pole, the program is written in Python and
    the filename is `Taxi-v3.ipynb`. The `.ipynb` extension is the known Jupyter notebook
    extension. We have chosen this way of coding in Python so that the example can
    be understood by just following the notebook, because you have the code and the
    explanations in one place.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 代码是一个位于仓库`Chapter12_OpenAI_Gym/taxi`文件夹中的Python文件。至于小车杆，程序是用Python编写的，文件名为`Taxi-v3.ipynb`。`.ipynb`扩展名是已知的Jupyter笔记本扩展。我们选择这种方式在Python中编码，以便通过仅仅跟随笔记本就能理解示例，因为你在同一个地方既有代码又有解释。
- en: Jupyter notebooks were introduced in [Chapter 2](7a2b1b82-c666-42df-9f10-9777eabe82df.xhtml),
    *Unit Testing of GoPiGo3*. There, we have covered the practical explanations of
    the sensor and actuator using Python code in the notebook environment.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本在[第2章](7a2b1b82-c666-42df-9f10-9777eabe82df.xhtml)中介绍，*GoPiGo3的单元测试*。在那里，我们使用笔记本环境中的Python代码对传感器和执行器进行了实际解释。
- en: 'We suggest that you open the notebook, read it from the beginning until the
    end, and then come back here to complete your comprehension of the topic. To do
    so, follow these instructions:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Activate the `gym` environment:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Move to the location of the example:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Launch the notebook server. This command will open a file explorer in a window
    of your default browser:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Click on the `Taxi-v3.ipynb` file and another window of the browser will open
    showing the notebook content.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After having read it, we are ready to return to the spaces (action and state)
    and reward concepts that we introduced in the previous section, covering them
    in detail for the current example.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Reward table
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This table specifies what reward the agent gets for every action it takes.
    A well-designed policy incentivizes the most desired actions with greater rewards.
    For the case of the cab example, the reward table is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The agent receives +20 points for a successful drop-off.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It loses 1 point for every time step. This way, we encourage it to solve the
    environment as quickly as possible: all the time it is on the road it is consuming
    resources, such as fuel, so this negative reward can be understood as the fuel
    expense.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is given a 10-point penalty for every illegal action it performs (during
    the pick-up or drop-off actions).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we proceed to describe the action and state spaces that the cab has to
    comply with to evolve in the environment.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Action space
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The action space consists of the possible actions the agent can perform. For
    the case of the taxi example, these are as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The four possible moves: move (*S*) south, (*N*) north, (*E*) east, or (*W*) west
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Picking up a passenger (*P*)
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropoff (*D*)
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, there are six possible actions in total, and we will call them **action
    variables**.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: State space
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The state space is composed of all the possible combinations of values of the
    **state variables** that define our problem. For the case of the taxi example,
    these variables are as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: The current position is defined on the basis of rows and column numbers. These
    account for 5 rows x 5 columns = 25 cells (positions).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Four destinations: Marked with **R** (red; the color it shows in the accompanying
    Jupyter notebook), **B **(blue), **Y**(yellow) and **G** (green).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Five possible passenger locations with regard to the taxi:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pickup/drop-off in any of the four locations
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Plus one for the passenger inside in any of the remaining cells (+1)
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hence, we have a total of 25 x 4 x 5 = 500 possible states. The following represents
    one of them:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The movement of hitting a wall is known to the encoded environment thanks to
    the `|` character. If the wall is to be crossed, when the environment tries to
    update the state, the cab could not move and will remain in the same cell. This
    is accomplished by keeping the state unchanged. Otherwise, the `:` character lets
    the cab move to the new cell. Bear in mind that there is no additional penalty
    for hitting a wall, just the -1 of the time step.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: If you introduce this new rule, the training should be somewhat faster since
    the agent will implicitly learn where there is a wall and will not insist on moving
    in that direction after hitting it several times.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你引入这个新规则，训练应该会更快一些，因为智能体将隐式地学习墙的位置，并且在被撞到几次后不会坚持朝那个方向移动。
- en: Self-driving cab example using the RL algorithm
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RL算法的自动驾驶出租车示例
- en: 'As stated previously, we will explain the learning process using Q-learning
    because of its simplicity and physical sense. Bear in mind that the Q-learning
    algorithm lets the agent keep track of its rewards to learn the best action for
    every single state:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用Q-learning来解释学习过程，因为它简单且具有物理意义。请注意，Q-learning算法让智能体跟踪其奖励，以学习每个状态的最好动作：
- en: Each time an action is taken from a given state, a reward is obtained according
    to P.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次从给定状态采取一个动作时，根据P获得一个奖励。
- en: The reward associated with each pair (state, action) creates a q-table that
    is a 500 x 6 matrix.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与每个对（状态，动作）相关的奖励创建了一个500 x 6的q表。
- en: 'The q-value for a concrete pair state-action stands for the *quality* of that
    action in that state. Hence, for a completely trained model, we will have a 500
    x 6 matrix, that is, 3,000 q-values:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个具体的状态-动作对，q值代表在该状态下该动作的*质量*。因此，对于一个完全训练好的模型，我们将有一个500 x 6的矩阵，即3,000个q值：
- en: Every row represents a state.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一行代表一个状态。
- en: The maximum q-value in each row lets the agent know what action is the best
    to take in that state.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每行的最大q值让智能体知道在该状态下应该采取什么动作。
- en: 'In the first step, q-values are arbitrary. Then, when the agent receives rewards
    as it interacts with the environment, the q-value for every pair (**state, action**)
    is updated according to the following equation:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，q值是任意的。然后，当智能体在与环境交互时获得奖励，每个状态-动作对的q值将根据以下方程更新：
- en: '[PRE27]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding equation is described as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程描述如下：
- en: α, or alpha, is the learning rate (0<α≤1) and is applied to the new information
    the agent discovers, that is, the first part of the second term of the sum in
    the formula, `α * reward`.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α，或alpha，是学习率（0<α≤1），并应用于智能体发现的新信息，即公式中求和的第二部分的第一个部分，`α * reward`。
- en: γ, or gamma, is the discount factor (0≤γ≤1) and determines how much importance
    we want to give to future rewards. If this factor is 0, it makes the agent consider
    only the immediate reward, making it behave in a greedy manner. Hence, this parameter
    ponders the utility of future actions. It applies to the second part of the second
    term of the sum: `γ * maxQ[next_state, all actions]`.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: γ，或伽马，是折扣因子（0≤γ≤1），它决定了我们想要给予未来奖励的重要性程度。如果这个因子为0，它会使智能体只考虑即时奖励，使其表现出贪婪的行为。因此，这个参数权衡了未来行动的效用。它适用于求和公式的第二部分的第二部分：`γ
    * maxQ[next_state, all actions]`。
- en: 'Finally, we should also consider the trade-off between **exploration** and **exploitation**. Let''s
    explain what these concepts mean:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还应该考虑**探索**和**利用**之间的权衡。让我们解释一下这些概念的含义：
- en: Exploration refers to the behavior where the robot executes an action from a
    given state for the first time. This will let it discover whether that state-action
    pair has a high reward or not. Operationally, it consists of taking a random action.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索指的是机器人从给定状态执行动作的行为，这是第一次。这将让它发现该状态-动作对是否有高奖励。在操作上，它包括采取随机动作。
- en: On the other hand, exploitation refers to the behavior of executing an action
    from a given state that was the more rewarded in the past. Operationally, it consists
    of taking the action with the maximum q-value for that state.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，利用指的是从给定状态执行过去奖励最多的动作的行为。在操作上，它包括采取该状态下具有最大q值的动作。
- en: We need to balance these two behaviors, and, for that, we introduce the ϵ (epsilon)
    parameter, which represents the percentage of actions that should be of the exploration type.
    This prevents the agent from following a single route, which may not necessarily
    be the best.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要平衡这两种行为，为此，我们引入了ϵ（epsilon）参数，它代表应该采取探索类型动作的百分比。这防止智能体遵循单一路线，这条路线可能并不一定是最好的。
- en: Evaluating the agent
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估智能体
- en: 'The script runs 100,000 episodes in less than one minute. Then, we evaluate
    the agent with 100 episodes and obtain these average values:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本在一分钟内运行了100,000个回合。然后，我们用100个回合评估智能体，并得到以下平均值：
- en: 'Steps per episode: 12'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个回合的步骤：12
- en: 'Penalties per episode: 0'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个回合的惩罚：0
- en: In front of the brute-force approach (which you can find in the Jupyter notebook),
    you obtain routes that vary from hundreds to thousands of steps and more than
    1,000 penalties (remember that a penalty of 1 was given for each illegal action
    incurred).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 与暴力方法（你可以在Jupyter笔记本中找到）相比，你获得的路由从数百到数千步，以及超过1,000次惩罚（记住，每次非法动作都会给出1次惩罚）。
- en: Hyperparameters and optimization
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数和优化
- en: 'So, how can you choose the values of alpha, gamma, and epsilon? Well, this
    strategy has to be based on both intuition and trial and error. In any case, the
    three of them should decrease over time as the agent learns the best actions:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你如何选择alpha、gamma和epsilon的值呢？嗯，这个策略必须基于直觉和试错。无论如何，随着智能体学习最佳动作，这三个值都应该随时间减少：
- en: Decreasing the need for learning, alpha, as the agent knows more about the environment
    and may trust in the acquired experience
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着智能体对环境的了解越来越多，并且可能信任所获得的经验，降低学习需求alpha。
- en: Also decreasing the discount factor, gamma, as an agent develops an end-to-end
    strategy and not only focuses on the immediate reward
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着智能体发展出端到端策略，并不仅仅关注即时奖励，同时降低折扣因子gamma。
- en: And, finally, decreasing the exploitation rate, epsilon, because the exploration
    gains lose priority as the environment is well known
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，降低利用率epsilon，因为随着环境变得熟悉，探索的收益会失去优先级。
- en: At this point, you should be ready to enter OpenAI ROS to train agents that
    power robots in Gazebo.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，你应该准备好进入OpenAI ROS来训练在Gazebo中为机器人提供动力的智能体。
- en: Running an environment
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行环境
- en: 'The goal of the rest of the chapter is to apply what you have learned about
    RL in general problems to a specific domain such as robotics. To easily transfer
    that knowledge, we will reproduce the simple cart pole example, modeling it as
    a robot in Gazebo. The code samples are in the `cart-pole_ROS` folder of the code
    repository of this chapter. Move to that location on your laptop:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 本章剩余部分的目标是将你在RL（强化学习）一般问题中学到的知识应用到特定领域，如机器人学。为了方便知识迁移，我们将重现简单的小车杆示例，将其建模为Gazebo中的机器人。代码示例位于本章代码库的`cart-pole_ROS`文件夹中。请在您的笔记本电脑上移动到该位置：
- en: '[PRE28]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Inside, you will find two ROS packages, each one giving its name to the folder:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，你可以找到两个ROS包，每个包都以其名称命名文件夹：
- en: '`cartpole_description` contains the Gazebo simulation framework for the cart
    pole using ROS. The structure of this package is very similar to the one described
    in [Chapter 5](74284adc-e0d7-4e40-a54b-e2e447b8e2fe.xhtml), *Simulating Robot
    Behavior with Gazebo*. Hence, it is not necessary to dive into its details.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cartpole_description` 包含了使用ROS的Gazebo仿真框架，用于小车杆。这个包的结构与第5章中描述的非常相似，即*使用Gazebo模拟机器人行为*。因此，没有必要深入研究其细节。'
- en: '`cartpole_dqn` contains the OpenAI Gym environment for the preceding Gazebo
    simulation. This is where the RL algorithm is introduced, and we will focus on
    this in the coming paragraphs.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cartpole_dqn` 包含了先前的Gazebo模拟的OpenAI Gym环境。这就是引入RL算法的地方，我们将在接下来的段落中关注这一点。'
- en: 'The package is quite similar. Let''s enter through the launch file, `start_training.launch`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 该包相当相似。让我们通过启动文件`start_training.launch`进入：
- en: '[PRE29]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The line with the `<rosparam>` tag is the one that loads the configuration of
    the training process. We will explain this in the next section. This file is `cartpole_dqn_params.yaml`
    and it is hosted inside the `config` folder.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 带有`<rosparam>`标签的行是加载训练过程配置的行。我们将在下一节中解释这一点。此文件是`cartpole_dqn_params.yaml`，它位于`config`文件夹中。
- en: 'The other line, tagged with `<node>`, launches the single ROS `cartpole_dqn` node
    that implements the training process for the cart pole under the Python script, `cartpole_dqn.py`.
    What this code performs is briefly described in the following ordered points:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 另一行，带有`<node>`标签，启动单个ROS `cartpole_dqn`节点，该节点在Python脚本`cartpole_dqn.py`下实现小车杆的训练过程。以下按顺序简要描述了此代码执行的操作：
- en: It creates the Gym environment for the cart pole.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它为小车杆创建Gym环境。
- en: It loads the configuration from the ROS parameter server (this point is detailed
    in the following subsection).
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它从ROS参数服务器加载配置（这一点将在以下子节中详细说明）。
- en: Then, it initializes the learning algorithm with the loaded parameters.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它使用加载的参数初始化学习算法。
- en: Finally, it loops over the predefined number of episodes, each one composed
    of a fixed number of steps (both values are also part of the configuration file).
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，它循环预定义的剧集数量，每个剧集由固定数量的步骤组成（这两个值也是配置文件的一部分）。
- en: 'For every episode, it performs the following:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the environment and get the first state of the robot.
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For every step in the current episode, the agent chooses an action to run that
    will be one of random versus best action selection (depending on the epsilon exploration
    parameter):'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This is the key line of the loop, and you can see, it is the same that is used
    for the cart pole in pure Python and the taxi examples in the preceding section.
    Hence, the output of the steps are as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '`observation`: The new state of the environment resulting from applying the
    action'
  id: totrans-227
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward`: The value that indicates how effective the action taken is'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`done`: The Boolean variable telling you if the goal has been achieved'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we let the algorithm learn from the result by following two subsequent
    steps:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remember the running step: `self.remember(state, action, reward, next_state,
    done)`'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Replay to optimize the action selection: `self.replay(self.batch_size)`
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this case, the algorithm that we use is somewhat different from the Q-learning
    we described in the *self-driving* *cab* example. It is called DQN and makes use
    of deep learning to select the best action for a given state. This is the algorithm
    that is more extensively used in RL problems, and if you want to deepen its formulation,
    you can do so by following the last reference in the *Further reading* section
    at the end of the chapter. In brief, this is what it performs:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: The remember process in every step of an episode saves what it is running and
    acts as the memory of the agent.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, the replay process takes a mini-batch of the last steps of the episode
    and applies for the improvement of the neural network. Such a network provides
    the agent with the *best* action to be carried out at every given state.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conceptually, what the agent does is to use its memory from previous experiences
    to guess which might be the most convenient action to maximize its total reward
    in the episode.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: In the remaining sections, we will focus on the specific training and evaluation
    inside ROS with Gazebo.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the environment file
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *step 2* of the preceding algorithmic description of the `start_training.py` script,
    ROS parameters are loaded into the model. Their definitions come from this line
    of the `start_training.launch` file:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'When executing this part, the parameters in the `cartpole_dqn_params.yaml` file are
    loaded into memory and are available to the `cartpole_dqn.py` script. The more
    relevant are the following:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`cartpole_v0` is the namespace that is declared before the definitions in the `yaml` file. The
    meaning of every parameter was covered in the *Self driving cab example using
    RL algorithm* subsection. Although the DQN algorithm is more sophisticated than
    Q-learning, the conceptual meaning of *alpha*, *gamma*, and *epsilon* is equivalent
    to both. You can remember them by reviewing the preceding Q-learning algorithm
    section.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Running the simulation and plotting the results
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run this simulation scenario, we follow the standard approach of first launching
    a Gazebo environment—part of the `cartpole_description` package with the model
    of the robot—and, afterward, we will start the training process:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The result in the Gazebo window should be similar to the following screenshot.
    Although this is a 3D environment, the model itself behaves like a 2D model, since
    the cart pole can only slide along the direction of the guide:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d13622f9-529f-4d6a-900b-41045b1d5660.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: 'For the training process, we have the launch file in the other ROS package,
    that is, `cartpole_v0_training`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Be aware that before running the launch file, you have to activate the `gym` Python
    environment, which is where you installed OpenAI Gym.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see a live plot of the evolution of the training process that shows,
    in real time, the reward obtained in each episode. After the training concludes,
    you should obtain a graph similar to this one:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5549d733-7555-4df1-999f-36ba7da50417.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: 'Bear in mind that for every tick of the cart pole, a reward of +1 is given.
    Hence, the graph also represents the total number of rewards per episode. To have
    a measurement of the convergence it is more useful to plot—for every episode—the
    average number of ticks (= reward) over the last 100 episodes. For example, for
    episode 1,000 this means to take the reward (number of ticks) of episodes 901
    to 1,000 and calculate the average of these 100 values. This result is the one
    plotted for episode 1,000 in the following graph:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e251153c-6d2b-4f07-a5ff-24721c0a5b48.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: In fact, the criteria for the convergence is to obtain an average reward greater
    than 800 over the last 100 episodes. You may check that the curve experiments
    with a boosting after 2,600 episodes and quickly reaches the criteria.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of this section, we will present a friendly way to access
    the ROS console log to follow the training process in detail.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Checking your progress with the logger
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you may have observed, the log output is huge and runs at a very high speed.
    ROS provides another `rqt_tool` to easily follow the log of the session. To access
    it, launch it from a Terminal:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This should display a window that is similar to the following screenshot:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e81757f-4a25-442e-a8b5-15193f94b142.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: 'In the two boxes below the message feed, you can exclude or include messages
    based on your own criteria. If you want to change the log level of the node, run
    the `rqt_logger_level` utility:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following screenshot shows the log level of the node:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a04bd37-53d1-4dba-ad5d-31d89896e698.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: The `rqt_console` tool allows you to both follow the log in real time and save
    it to a file for offline analysis.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided you with the theoretical background that you need to apply
    RL to real robots. By dissecting the simple example of the cart pole, you should
    now understand what happens under the hood in classical RL tasks.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, by doing this first with the native OpenAI Gym framework in Python,
    and afterward, inside ROS, you should have acquired the basic skills to perform
    an RL process with a real robot, our GoPiGo3\. This is what you will learn to
    do in the final chapter of the book.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does an agent learn following the RL approach?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Via the experience that it gets from the reward it receives each time it
    executes an action.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: B) By randomly exploring the environment and discovering the best strategy by
    trial and error.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: C) Via a neural network that gives as output a q-value as a function of the
    state of the system.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Does an agent trained with RL have to make predictions of the expected outcome
    of an action?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Yes; this is a characteristic called model-free RL.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: B) Only if it does not take the model-free RL approach.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: C) No; by definition, RL methods only need to be aware of rewards and penalties
    to ensure the learning process.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: If you run the Q-learning algorithm with a learning rate, alpha, of 0.7, what
    does this mean from the point of view of the learning process?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) That you keep the top 30% of the pair state-actions that provide the higher
    rewards.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: B) That you keep the 30% of the values of all the elements of the Q-matrix,
    and take the remaining 70% from the result of the new action.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: C) That you keep the top 70% of the acquired knowledge from every iteration
    step to the next one.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: If you run the Q-learning algorithm with a discount factor, gamma, of 1, what
    does this mean from the point of view of the learning process?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) That the agent will only be interested in the immediate reward.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: B) That the agent will only be interested in the goal of the task.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: C) That the agent will only be interested in achieving the goal once.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: If you run the Q-learning algorithm with an exploration rate, epsilon, of 0.5,
    what does this mean from the point of view of the learning process?
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) That the behavior of the agent will be similar to that of an agent that selects
    random actions.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: B) That the agent will choose a random action in 50% of the steps of the episode.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: C) That the agent will choose random actions in 50% of all the episodes.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To delve deeper into the concepts explained in this chapter, you can refer
    to the following sources:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '*Reinforcement Learning: An Introduction*, Sutto R., Barto A. (2018), The MIT
    Press, licensed under the Creative Commons Attribution-NonCommercial-NoDeriv 2.0
    Generic License ([http://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf](http://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf))'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine Learning Projects*, Chapter: *Bias-Variance for Deep Reinforcement
    Learning: How To Build a Bot for Atari with OpenAI Gym* ([https://assets.digitalocean.com/books/python/machine-learning-projects-python.pdf](https://assets.digitalocean.com/books/python/machine-learning-projects-python.pdf))'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning with ROS and Gazebo ([https://ai-mrkogao.github.io/reinforcement%20learning/ROSRL](https://ai-mrkogao.github.io/reinforcement%20learning/ROSRL))
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ROS和Gazebo进行强化学习 ([https://ai-mrkogao.github.io/reinforcement%20learning/ROSRL](https://ai-mrkogao.github.io/reinforcement%20learning/ROSRL))
- en: Testing different OpenAI RL algorithms with ROS And Gazebo ([https://www.theconstructsim.com/testing-different-openai-rl-algorithms-with-ros-and-gazebo/](https://www.theconstructsim.com/testing-different-openai-rl-algorithms-with-ros-and-gazebo/))
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ROS和Gazebo测试不同的OpenAI RL算法 ([https://www.theconstructsim.com/testing-different-openai-rl-algorithms-with-ros-and-gazebo/](https://www.theconstructsim.com/testing-different-openai-rl-algorithms-with-ros-and-gazebo/))
- en: '*Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning
    using ROS and Gazebo*, Zamora I., González N., Maoral V., Hernández A. (2016),
    arXiv:1608.05742 [cs.RO]'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*扩展OpenAI Gym用于机器人：使用ROS和Gazebo的强化学习工具包*，Zamora I.，González N.，Maoral V.，Hernández
    A. (2016)，arXiv:1608.05742 [cs.RO]'
- en: '*Deep Q-Learning with Keras and Gym* ([https://keon.github.io/deep-q-learning](https://keon.github.io/deep-q-learning))'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用Keras和Gym进行深度Q学习* ([https://keon.github.io/deep-q-learning](https://keon.github.io/deep-q-learning))'
