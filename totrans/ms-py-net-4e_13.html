<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer265">
    <h1 class="chapterNumber">13</h1>
    <h1 class="chapterTitle" id="_idParaDest-278">Network Data Analysis with Elastic Stack</h1>
    <p class="normal">In <em class="chapterRef">Chapter 7</em>, <em class="italic">Network Monitoring with Python – Part 1</em>, and <em class="chapterRef">Chapter 8</em>, <em class="italic">Network Monitoring with Python – Part 2</em>, we discussed the various ways to monitor a network. In the two chapters, we looked at two different approaches for network data collection: we can either retrieve data from network devices such as SNMP, or we can listen for the data sent by network devices using flow-based exports. After the data is collected, we will need to store the data in a database, then analyze the data to gain insights to decide what the data means. Most of the time, the analyzed results are displayed in a graph, whether a line graph, bar graph, or pie chart. We can use individual tools such as PySNMP, Matplotlib, and Pygal for each step, or we can leverage all-in-one tools such as Cacti or ntop for monitoring. The tools introduced in those two chapters gave us basic monitoring and understanding of the network.</p>
    <p class="normal">We then moved on to <em class="chapterRef">Chapter 9</em>, <em class="italic">Building Network Web Services with Python</em>, to build API services to abstract our network from higher-level tools. In <em class="chapterRef">Chapter 11</em>, <em class="italic">AWS Cloud Networking</em>, and <em class="chapterRef">Chapter 12</em>, <em class="italic">Azure Cloud Networking</em>, we extended our on-premises network to the cloud using AWS and Azure. We have covered much ground in these chapters and have a solid set of tools to help us make our network programmable.</p>
    <p class="normal">Starting with this chapter, we will build on our toolsets from previous chapters and look at other tools and projects that I have found useful in my journey once I was comfortable with the tools covered in previous chapters. In this chapter, we will take a look at an open source project, Elastic Stack (<a href="https://www.elastic.co"><span class="url">https://www.elastic.co</span></a>), that can help us with analyzing and monitoring our network beyond what we have seen before.</p>
    <p class="normal">In this chapter, we will look at the following topics:</p>
    <ul>
      <li class="bulletList">What is the Elastic (or ELK) Stack?</li>
      <li class="bulletList">Elastic Stack installation</li>
      <li class="bulletList">Data ingestion with Logstash</li>
      <li class="bulletList">Data ingestion with Beats</li>
      <li class="bulletList">Search with Elasticsearch</li>
      <li class="bulletList">Data visualization with Kibana</li>
    </ul>
    <p class="normal">Let’s begin by answering the question: what exactly is the Elastic Stack?</p>
    <h1 class="heading-1" id="_idParaDest-279">What is the Elastic Stack?</h1>
    <p class="normal">The Elastic Stack <a id="_idIndexMarker1010"/>is also known as the “ELK” Stack. So, what is it? Let’s see what the developers <a id="_idIndexMarker1011"/>have to say in their own words (<a href="https://www.elastic.co/what-is/elk-stack"><span class="url">https://www.elastic.co/what-is/elk-stack</span></a>):</p>
    <blockquote class="packt_quote">
      <p class="quote">”ELK” is the acronym for three open source projects: Elasticsearch, Logstash, and Kibana. Elasticsearch is a search and analytics engine. Logstash is a serverside data processing pipeline that ingests data from multiple sources simultaneously, transforms it, and then sends it to a “stash” like Elasticsearch. Kibana lets users visualize data with charts and graphs in Elasticsearch. The Elastic Stack is the next evolution of the ELK Stack.</p>
    </blockquote>
    <figure class="mediaobject"><img alt="A picture containing graphical user interface  Description automatically generated" src="../Images/B18403_13_01.png"/></figure>
    <p class="packt_figref">Figure 13.1: Elastic Stack (source: https://www.elastic.co/what-is/elk-stack)</p>
    <p class="normal">As we can see from the statement, the Elastic Stack is a collection of different projects working together to cover the whole spectrum of data collection, storage, retrieval, analytics, and visualization. What is nice about the stack is that it is tightly integrated, but each <a id="_idIndexMarker1012"/>component can also be used separately. If we dislike Kibana for visualization, we can easily plug in Grafana for the graphs. What if we have other data ingestion tools that we want to use? No problem, we can use the RESTful API to post our data to Elasticsearch. At the center of the stack is Elasticsearch, an open source, distributed search engine. The other projects were created to enhance and support the search function. This might sound a bit confusing at first, but as we look deeper at the components of the project, it will become clearer.</p>
    <p class="normal">Why did they change the name of ELK Stack to Elastic Stack? In 2015, Elastic introduced a family of lightweight, single-purpose data shippers called Beats. They were an instant hit and continue to be very popular, but the creators could not come up with a good acronym for the “B” and decided to just rename the whole stack to Elastic Stack.</p>
    <p class="normal">We will focus on the network monitoring and data analysis aspects of the Elastic Stack. Still, the stack has many use cases, including risk management, e-commerce personalization, security analysis, fraud detection, and more. It is being used by various organizations, from web companies such as Cisco, Box, and Adobe, to government agencies such as NASA JPL, the United States Census Bureau, and others (<a href="https://www.elastic.co/customers/"><span class="url">https://www.elastic.co/customers/</span></a>).</p>
    <p class="normal">When we talk <a id="_idIndexMarker1013"/>about Elastic, we are referring to the company behind the Elastic Stack. The tools are open source and the company makes money by selling support, hosted solutions, and consulting around open source projects. The company stock is publicly traded on the New York Stock Exchange with the ESTC symbol.</p>
    <p class="normal">Now that we have a better idea of what the ELK Stack is, let’s take a look at the lab topology for this chapter.</p>
    <h1 class="heading-1" id="_idParaDest-280">Lab topology</h1>
    <p class="normal">For the network lab, we will reuse the network topology we used in <em class="chapterRef">Chapter 8</em>, <em class="italic">Network Monitoring with Python – Part 2. </em>The network gear will have the management interfaces in the <code class="inlineCode">192.168.2.0/24</code> management network with the interconnections in the <code class="inlineCode">10.0.0.0/8</code> network and the subnets in <code class="inlineCode">/30s</code>.</p>
    <p class="normal">Where can <a id="_idIndexMarker1014"/>we install the ELK Stack in the lab? In production, we should run the ELK Stack in a dedicated cluster. In our lab, however, we can quickly spin up a testing instance via Docker containers. If a refresher of Docker is needed, please refer to <em class="chapterRef">Chapter 5</em>, <em class="italic">Docker Containers for Network Engineers</em>. </p>
    <p class="normal">Following is a graphical representation of our network lab topology:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B18403_13_02.png"/></figure>
    <p class="packt_figref">Figure 13.2: Lab Topology</p>
    <table class="table-container" id="table001-2">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Device</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Management IP</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Loopback IP</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">r1</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">192.168.2.218</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">192.168.0.1</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">r2</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">192.168.2.219</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">192.168.0.2</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">r3</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">192.168.2.220</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">192.168.0.3</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">r5</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">192.168.2.221</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">192.168.0.4</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">r6</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">192.168.2.222</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">192.168.0.5</code></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">The Ubuntu <a id="_idIndexMarker1015"/>hosts information is as follows:</p>
    <table class="table-container" id="table002-2">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Device Name</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">External Link Eth0</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Internal IP Eth1</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Client</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">192.168.2.211</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">10.0.0.9</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Server</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">192.168.2.212</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">10.0.0.5</code></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">To run multiple containers, we should allocate at least 4 GB RAM or more to the host. Let’s start Docker Engine, if not done already, then pull the image from Docker Hub: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>sudo service docker start
<span class="hljs-con-meta">$ </span>docker network create elastic
<span class="hljs-con-meta">$ </span>docker pull docker.elastic.co/elasticsearch/elasticsearch:8.4.2
<span class="hljs-con-meta">$ </span>docker run --name elasticsearch –-<span class="hljs-con-built_in">rm</span> -it --network elastic -p 9200:9200 -p 9300:9300 -e <span class="hljs-con-string">"discovery.type=single-node"</span> -t docker.elastic.co/elasticsearch/elasticsearch:8.4.2
</code></pre>
    <p class="normal">When the <a id="_idIndexMarker1016"/>Docker container is run, the generated default Elastic user password and Kibana enrollment token are output to the terminal; please take a note of them as we will need them later. You might need to scroll up the screen a bit to find them: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">-&gt; </span> Password <span class="hljs-con-keyword">for</span> the elastic user (reset with 'bin/elasticsearch-reset-password -u elastic'):
  &lt;password&gt;
<span class="hljs-con-meta">-&gt; </span> Configure Kibana to use this cluster:
* Run Kibana and click the configuration link in the terminal when Kibana starts.
* Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):
  &lt;token&gt;
</code></pre>
    <p class="normal">Once the Elasticsearch container runs, we can test out the instance by browsing to <code class="inlineCode">https://&lt;your ip&gt;:9200</code>:</p>
    <figure class="mediaobject"><img alt="Text  Description automatically generated" src="../Images/B18403_13_03.png"/></figure>
    <p class="packt_figref">Figure 13.3: Elasticsearch Initial Result</p>
    <p class="normal">We can then pull and run the Kibana container image from a separate terminal: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker pull docker.elastic.co/kibana/kibana:8.4.2
<span class="hljs-con-meta">$</span> docker run --name kibana –-rm -it --network elastic -p 5601:5601 docker.elastic.co/kibana/kibana:8.4.2
</code></pre>
    <p class="normal">Once Kibana <a id="_idIndexMarker1017"/>boots up, we can access it via port 5601:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application, chat or text message  Description automatically generated" src="../Images/B18403_13_04.png"/></figure>
    <p class="packt_figref">Figure 13.4: Kibana Start Page</p>
    <p class="normal">Notice it is asking for the enrolment token we jotted down before. We can paste that in and click on <strong class="screenText">Configure Elastic</strong>. It will prompt us for a token, which is now displayed on the Kibana <a id="_idIndexMarker1018"/>terminal. Once that is authenticated, Kibana will start to configure Elastic: </p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application  Description automatically generated" src="../Images/B18403_13_05.png"/></figure>
    <p class="packt_figref">Figure 13.5: Configurating Elastic </p>
    <p class="normal">Finally, we should be able to access the Kibana interface at <code class="inlineCode">http://&lt;ip&gt;:5601</code>. We do not need any integration at this point; we will pick <strong class="screenText">Explore on my own</strong>: </p>
    <figure class="mediaobject"><img alt="" src="../Images/B18403_13_06.png"/></figure>
    <p class="packt_figref">Figure 13.6: </p>
    <p class="normal">We will <a id="_idIndexMarker1019"/>be presented with an option to load some sample data. This is a great way to get our feet wet with the tool, so let’s import this data:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, application, website  Description automatically generated" src="../Images/B18403_13_07.png"/></figure>
    <p class="packt_figref">Figure 13.7: Kibana Home Page</p>
    <p class="normal">We will <a id="_idIndexMarker1020"/>choose <strong class="screenText">Try sample data</strong> and add the sample eCommerce orders, sample flight data, and sample web logs: </p>
    <figure class="mediaobject"><img alt="" src="../Images/B18403_13_08.png"/></figure>
    <p class="packt_figref">Figure 13.8: Adding Sample Data</p>
    <p class="normal">To summarize, we now have Elasticsearch and Kibana running as containers with forwarded <a id="_idIndexMarker1021"/>ports on the management host: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker ps
CONTAINER ID   IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                                                                  NAMES
f7d6d8842060   docker.elastic.co/kibana/kibana:8.4.2                 "/bin/tini -- /usr/l…"   42 minutes ago       Up 42 minutes       0.0.0.0:5601-&gt;5601/tcp, :::5601-&gt;5601/tcp                                              kibana
dc2a1fa15e3b   docker.elastic.co/elasticsearch/elasticsearch:8.4.2   "/bin/tini -- /usr/l…"   46 minutes ago       Up 46 minutes       0.0.0.0:9200-&gt;9200/tcp, :::9200-&gt;9200/tcp, 0.0.0.0:9300-&gt;9300/tcp, :::9300-&gt;9300/tcp   elasticsearch
</code></pre>
    <p class="normal">Great! We are almost done. The last piece of the puzzle is Logstash. Since we will be working with different Logstash configuration files, modules, and plugins, we will install it on the management host with a package instead of a Docker container. Logstash requires Java to run: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>sudo apt install openjdk-11-jre-headless
<span class="hljs-con-meta">$ </span>java --version
openjdk 11.0.16 2022-07-19
OpenJDK Runtime Environment (build 11.0.16+8-post-Ubuntu-0ubuntu122.04)
OpenJDK 64-Bit Server VM (build 11.0.16+8-post-Ubuntu-0ubuntu122.04, mixed mode, sharing)
</code></pre>
    <p class="normal">We can <a id="_idIndexMarker1022"/>download the Logstash bundled package: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>wget https://artifacts.elastic.co/downloads/logstash/logstash-8.4.2-linux-x86_64.tar.gz
<span class="hljs-con-meta">$ </span>tar -xvzf logstash-8.4.2-linux-x86_64.tar.gz
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> logstash-8.4.2/
</code></pre>
    <p class="normal">We will modify a few fields in the Logstash configuration file:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>vim config/logstash.yml 
<span class="hljs-con-meta"># </span>change the following fields
node.name: mastering-python-networking
api.http.host: &lt;your host ip&gt;
api.http.port: 9600-9700
</code></pre>
    <p class="normal">We will not start Logstash just yet. We will wait until we have installed the network-related plugins and created the necessary configuration file later in the chapter to start the Logstash process.</p>
    <p class="normal">Let’s take a moment to look at deploying the ELK Stack as a hosted service in the next section.</p>
    <h1 class="heading-1" id="_idParaDest-281">Elastic Stack as a service</h1>
    <p class="normal">Elasticsearch <a id="_idIndexMarker1023"/>is a popular service available as a hosted option by both <a id="_idIndexMarker1024"/>Elastic.co and other cloud providers. Elastic Cloud (<a href="https://www.elastic.co/cloud/"><span class="url">https://www.elastic.co/cloud/</span></a>) does not have an infrastructure of its own, but it offers the option to spin up deployments on AWS, Google Cloud Platform, or Azure. Because Elastic Cloud is built on other public cloud VM offerings, the cost will be a bit more than getting it directly from a cloud provider, such as AWS:</p>
    <figure class="mediaobject"><img alt="Timeline  Description automatically generated" src="../Images/B18403_13_09.png"/></figure>
    <p class="packt_figref">Figure 13.9: Elastic Cloud Offerings</p>
    <p class="normal">AWS offers <a id="_idIndexMarker1025"/>the hosted OpenSearch product (<a href="https://aws.amazon.com/opensearch-service/"><span class="url">https://aws.amazon.com/opensearch-service/</span></a>) tightly integrated with the existing AWS offerings. For example, AWS CloudWatch Logs can be streamed directly to the AWS <a id="_idIndexMarker1026"/>OpenSearch instance (<a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html"><span class="url">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html</span></a> ).</p>
    <p class="normal">From my <a id="_idIndexMarker1027"/>own experience, as attractive as the Elastic Stack is for its advantages, it is a project that I feel is easy to get started but hard to scale without a steep learning curve. The learning curve is even steeper when we do not deal with Elasticsearch on a daily basis. If you, like me, want to take advantage of the features Elastic Stack offers but do not want to become a full-time Elastic engineer, I would highly recommend using one of the hosted options for production.</p>
    <p class="normal">Which hosted provider to choose depends on your preference of cloud provider lockdown and if you want to use the latest features. Since Elastic Cloud is built by the folks behind the Elastic Stack project, they tend to offer the latest features faster than AWS. On the <a id="_idIndexMarker1028"/>other hand, if your infrastructure is fully built in the AWS cloud, having a tightly integrated OpenSearch instance saves you the time and effort required to maintain a separate cluster.</p>
    <p class="normal">Let’s look at an end-to-end example from data ingestion to visualization in the next section.</p>
    <h1 class="heading-1" id="_idParaDest-282">First End-to-End example</h1>
    <p class="normal">One of the most common pieces of feedback from people new to Elastic Stack is the amount of <a id="_idIndexMarker1029"/>detail you need to understand to get started. To get the first usable record in the Elastic Stack, the user needs to build a cluster, allocate master and data nodes, ingest the data, create the index, and manage it via the web or command-line interface. Over the years, Elastic Stack has simplified the installation process, improved its documentation, and created sample datasets for new users to get familiar with the tools before using the stack in production.</p>
    <div class="packt_tip">
      <p class="normal">Running the components in Docker containers helps with some of the pain of installation but increases the complexity of maintenance. It is a balancing act to choose between running them in a virtual machine vs. containers.</p>
    </div>
    <p class="normal">Before we dig deeper into the different components of the Elastic Stack, it is helpful to look at an example that spans Logstash, Elasticsearch, and Kibana. By going over this end-to-end example, we will become familiar with the function that each component provides. When we look at each component in more detail later in the chapter, we can compartmentalize where the particular component fits into the overall picture.</p>
    <p class="normal">Let’s start by putting our log data into Logstash. We will configure each of the routers to export the log data to the Logstash server:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">r[1-6]#</span>sh run | i logging
logging host &lt;logstash ip&gt; vrf Mgmt-intf transport udp port 5144
</code></pre>
    <p class="normal">On our Elastic Stack host, with all of the components installed, we will create a simple Logstash configuration that listens on UDP port <code class="inlineCode">5144</code> and outputs the data to the console in JSON format as well as the Elasticsearch host:</p>
    <pre class="programlisting con"><code class="hljs-con">echou@elk-stack-mpn:~$ cd logstash-8.4.2/
echou@elk-stack-mpn:~/logstash-8.4.2$ mkdir network_configs
echou@elk-stack-mpn:~/logstash-8.4.2$ touch network_configs/simple_config.cfg
echou@elk-stack-mpn:~/logstash-8.4.2$ cat network_configs/simple_config.conf 
input {
  udp {
    port =&gt; 5144
    type =&gt; "syslog-ios"
  }
}
output {
  stdout { codec =&gt; json }
  elasticsearch {
    hosts =&gt; ["https://&lt;elasticsearch ip&gt;:9200"]
    ssl =&gt; true
    ssl_certificate_verification =&gt; false
    user =&gt; "elastic"
    password =&gt; "&lt;password&gt;"
    index =&gt; "cisco-syslog-%{+YYYY.MM.dd}"
  }
} 
</code></pre>
    <p class="normal">The configuration <a id="_idIndexMarker1030"/>file consists of only an input section and an output section without modifying the data. The type, <code class="inlineCode">syslog-ios</code>, is a name we picked to identify this index. In the <code class="inlineCode">output</code> section, we configure the index name with variables representing today’s date. We can run the Logstash process directly from the binary directory in the foreground:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>./bin/logstash -f network_configs/simple_config.conf 
Using bundled JDK: /home/echou/Mastering_Python_Networking_Fourth_Edition/logstash-8.4.2/jdk
[2022-09-23T13:46:25,876][INFO ][logstash.inputs.udp      ][main][516c12046954cb8353b87ba93e5238d7964349b0fa7fa80339b72c6baca637bb] UDP listener started {:address=&gt;"0.0.0.0:5144", :receive_buffer_bytes=&gt;"106496", :queue_size=&gt;"2000"} 
&lt;skip&gt;
</code></pre>
    <p class="normal">By default, Elasticsearch allows automatic index generation when data is sent to it. We can generate some log data on the router by resetting the interface, reloading BGP, or simply going into the configuration mode and exiting out. Once there are some new logs generated, we will see the <code class="inlineCode">cisco-syslog-&lt;date&gt;</code> index being created:</p>
    <pre class="programlisting con"><code class="hljs-con">{"@timestamp":"2022-09-23T20:48:31.354Z", "log.level": "INFO", "message":"[cisco-syslog-2022.09.23/B7PH3hxNSHqAegikXyp9kg] create_mapping", "ecs.version": "1.2.0","service.name":"ES_ECS","event.dataset":"elasticsearch.server","process.thread.name":"elasticsearch[24808013b64b][masterService#updateTask][T#1]","log.logger":"org.elasticsearch.cluster.metadata.MetadataMappingService","elasticsearch.cluster.uuid":"c-j9Dg8YTh2PstO3JFP9AA","elasticsearch.node.id":"Pa4x3YJ-TrmFn5Pb2tObVw","elasticsearch.node.name":"24808013b64b","elasticsearch.cluster.name":"docker-cluster"}
</code></pre>
    <p class="normal">At this point, we can <a id="_idIndexMarker1031"/>do a quick <code class="inlineCode">curl</code> to see the index created on Elasticsearch. The <code class="inlineCode">curl</code> command use the <code class="inlineCode">insecure</code> flag to accommodate the self-signed certificate. The URL is in the “<code class="inlineCode">https://&lt;username&gt;:&lt;password&gt;@&lt;ip&gt;&lt;port&gt;/&lt;path</code>&gt;” format. <code class="inlineCode">"_cat/indices/cisco*"</code> shows the category of indices, then match the indices name:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>curl -X GET --insecure <span class="hljs-con-string">"https://elastic:-Rel0twWMUk8L-ZtZr=I@192.168.2.126:9200/_cat/indices/cisco*"</span>
yellow open cisco-syslog-2022.09.23 B7PH3hxNSHqAegikXyp9kg 1 1 9 0 21kb 21kb
</code></pre>
    <p class="normal">We can now use Kibana to create the index by going to <strong class="screenText">Menu -&gt; Management -&gt; Stack Management</strong>:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, application, Teams  Description automatically generated" src="../Images/B18403_13_10.png"/></figure>
    <p class="packt_figref">Figure 13.10: Stack Management</p>
    <p class="normal">Under <strong class="screenText">Data -&gt; Index Management</strong>, we can <a id="_idIndexMarker1032"/>see the newly created <strong class="screenText">cisco-syslog</strong> index: </p>
    <figure class="mediaobject"><img alt="Graphical user interface, application, Teams  Description automatically generated" src="../Images/B18403_13_11.png"/></figure>
    <p class="packt_figref">Figure 13.11: Index Management</p>
    <p class="normal">We can <a id="_idIndexMarker1033"/>now move to <strong class="screenText">Stack Management -&gt; Kibana -&gt; Data Views</strong> to create a data view. </p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application, email  Description automatically generated" src="../Images/B18403_13_12.png"/></figure>
    <p class="packt_figref">Figure 13.12: Create New Data Views Step 1</p>
    <p class="normal">Since the index is already in Elasticsearch, we will only need to match the index name. Remember that our index name is a variable based on time; we can use a star wildcard (<code class="inlineCode">*</code>) to match all the current and future indices starting with <strong class="keyWord">cisco-syslog</strong>:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application, email  Description automatically generated" src="../Images/B18403_13_13.png"/></figure>
    <p class="packt_figref">Figure 13.13: Create New Data Views Step 2</p>
    <p class="normal">Our index <a id="_idIndexMarker1034"/>is time-based, that is, we have a field that can be used as a timestamp, and we can search based on time. We should specify the field that we designated as the timestamp. In our case, Elasticsearch was already smart enough to pick a field from our syslog for the timestamp; we just need to choose it in the second step from the drop-down menu. </p>
    <p class="normal">After the index pattern is created, we can use the <strong class="screenText">Menu -&gt; Discover</strong> (under <strong class="screenText">Analytics</strong>) tab to look at the entries. Make sure you pick the right indices and the time range:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application  Description automatically generated" src="../Images/B18403_13_14.png"/></figure>
    <p class="packt_figref">Figure 13.14: Elasticsearch Index Document Discovery</p>
    <p class="normal">After we have <a id="_idIndexMarker1035"/>collected some more log information, we can stop the Logstash process by using <em class="keystroke">Ctrl + C</em> on the Logstash process. This first example shows how we can leverage the Elastic Stack pipeline from data ingestion, storage, and visualization. The data ingestion used in Logstash (or Beats) is a continuous data stream that automatically flows into Elasticsearch. The Kibana visualization tool provides a way for us to analyze the data in Elasticsearch in a more intuitive way, then create a permanent visualization once we are happy with the result. There are more visualization graphs we can create with Kibana, which we will see more examples of later in the chapter.</p>
    <p class="normal">Even with just one example, we can see that the most important part of the workflow is Elasticsearch. It is the simple RESTful interface, storage scalability, automatic indexing, and quick search result that gives the stack the power to adapt to our network analysis needs.</p>
    <p class="normal">In the next section, we will look at how we can use Python to interact with Elasticsearch.</p>
    <h1 class="heading-1" id="_idParaDest-283">Elasticsearch with a Python client</h1>
    <p class="normal">We can interact with Elasticsearch via its HTTP RESTful API using a Python library. For instance, in the <a id="_idIndexMarker1036"/>following example, we will use the <code class="inlineCode">requests</code> library to perform a <code class="inlineCode">GET</code> operation to retrieve information from the Elasticsearch host. For example, we know that <code class="inlineCode">HTTP GET</code> for the following URL endpoint can retrieve the current indices starting with <code class="inlineCode">kibana</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>curl -X GET --insecure <span class="hljs-con-string">"https://elastic:-Rel0twWMUk8L-ZtZr=I@192.168.2.126:9200/_cat/indices/kibana*"</span>
green open kibana_sample_data_ecommerce QcLgMu7CTEKNjeJeBxaD3w 1 0  4675 0 4.2mb 4.2mb
green open kibana_sample_data_logs      KPcJfMoSSaSs-kyqkuspKg 1 0 14074 0 8.1mb 8.1mb
green open kibana_sample_data_flights   q8MkYKooT8C5CQzbMMNTpg 1 0 13059 0 5.8mb 5.8mb
</code></pre>
    <p class="normal">We can use the <code class="inlineCode">requests</code> library to make a similar function in a Python script, <code class="inlineCode">Chapter13_1.py</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#!/usr/bin/env python3</span>
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> requests.packages.urllib3.exceptions <span class="hljs-keyword">import</span> InsecureRequestWarning
<span class="hljs-comment"># disable https verification check warning</span>
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
<span class="hljs-keyword">def</span> <span class="hljs-title">current_indices_list</span>(<span class="hljs-params">es_host, index_prefix</span>):
    current_indices = []
    http_header = {<span class="hljs-string">'content-type'</span>: <span class="hljs-string">'application/json'</span>}
    response = requests.get(es_host + <span class="hljs-string">"/_cat/indices/"</span> + index_prefix + <span class="hljs-string">"*"</span>, headers=http_header, verify=<span class="hljs-literal">False</span>)
    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> response.text.split(<span class="hljs-string">'\n'</span>):
        <span class="hljs-keyword">if</span> line:
            current_indices.append(line.split()[<span class="hljs-number">2</span>])
    <span class="hljs-keyword">return</span> current_indices
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    username = <span class="hljs-string">'elastic'</span>
    password = <span class="hljs-string">'-Rel0twWMUk8L-ZtZr=I'</span>
    es_host = <span class="hljs-string">'https://'</span>+username+<span class="hljs-string">':'</span>+password+<span class="hljs-string">'@192.168.2.126:9200'</span>
    indices_list = current_indices_list(es_host, <span class="hljs-string">'kibana'</span>)
    <span class="hljs-built_in">print</span>(indices_list)
</code></pre>
    <p class="normal">Executing the script will give us a list of indices starting with <code class="inlineCode">kibana</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python Chapter13_1.py 
['kibana_sample_data_ecommerce', 'kibana_sample_data_logs', 'kibana_sample_data_flights']
</code></pre>
    <p class="normal">We can <a id="_idIndexMarker1037"/>also use the Python Elasticsearch client, <a href="https://elasticsearch-py.readthedocs.io/en/master/"><span class="url">https://elasticsearch-py.readthedocs.io/en/master/</span></a>. It is designed as a thin wrapper around Elasticsearch’s RESTful API to allow for maximum flexibility. Let’s install it and run a simple example:</p>
    <pre class="programlisting con"><code class="hljs-con">(venv) $ pip install elasticsearch
</code></pre>
    <p class="normal">The example, <code class="inlineCode">Chapter13_2</code>, simply connects to the Elasticsearch cluster and does a search <a id="_idIndexMarker1038"/>for anything that matches the indices that start with <code class="inlineCode">kibana</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#!/usr/bin/env python3</span>
<span class="hljs-keyword">from</span> elasticsearch <span class="hljs-keyword">import</span> Elasticsearch
es_host = Elasticsearch([<span class="hljs-string">"https://elastic:-Rel0twWMUk8L-ZtZr=I@192.168.2.126:9200/"</span>],
                        ca_certs=<span class="hljs-literal">False</span>, verify_certs=<span class="hljs-literal">False</span>)
res = es_host.search(index=<span class="hljs-string">"kibana*"</span>, body={<span class="hljs-string">"query"</span>: {<span class="hljs-string">"match_all"</span>: {}}})
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Hits Total: "</span> + <span class="hljs-built_in">str</span>(res[<span class="hljs-string">'hits'</span>][<span class="hljs-string">'total'</span>][<span class="hljs-string">'value'</span>]))
</code></pre>
    <p class="normal">By default, the result will return the first 10,000 entries:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python Chapter13_2.py
Hits Total: 10000
</code></pre>
    <p class="normal">Using the simple script, the advantage of the client library is not obvious. However, the client library is very helpful when we create a more complex search operation, such as a scroll where we need to use the returned token per query to continue executing the subsequent queries until all the results are returned. The client can also help with more complicated administrative tasks, such as when we need to re-index an existing index. We will see more examples using the client library in the remainder of the chapter.</p>
    <p class="normal">In the next section, we will look at more data ingestion examples from our Cisco device syslogs.</p>
    <h1 class="heading-1" id="_idParaDest-284">Data ingestion with Logstash</h1>
    <p class="normal">In the last example, we used Logstash to ingest log data from network devices. Let’s build <a id="_idIndexMarker1039"/>on that example and add a few more configuration changes in <code class="inlineCode">network_config/config_2.cfg</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">input</span> {
  udp {
    <span class="hljs-attr">port</span> =&gt; <span class="hljs-number">5144</span>
    <span class="hljs-attr">type</span> =&gt; <span class="hljs-string">"syslog-core"</span>
  }
  <span class="hljs-keyword">udp</span> {
    <span class="hljs-attr">port</span> =&gt; <span class="hljs-number">5145</span>
    <span class="hljs-attr">type</span> =&gt; <span class="hljs-string">"syslog-edge"</span>
  }
}
<span class="hljs-keyword">filter</span> {
 <span class="hljs-keyword">if</span> [<span class="hljs-built_in">type</span>] == <span class="hljs-string">"syslog-edge"</span> {
    grok {
      <span class="hljs-attr">match</span> =&gt; { <span class="hljs-string">"message"</span> =&gt; <span class="hljs-string">".*"</span> }
      <span class="hljs-keyword">add_field</span> =&gt; [ <span class="hljs-string">"received_at"</span>, <span class="hljs-string">"%{@timestamp}"</span> ]
    }
  } 
}
<span class="hljs-keyword">output</span> {
  stdout { <span class="hljs-attr">codec</span> =&gt; json }
  <span class="hljs-keyword">elasticsearch</span> {
    <span class="hljs-attr">hosts</span> =&gt; [<span class="hljs-string">"https://192.168.2.126:9200"</span>]
    &lt;skip&gt;
  }
}
</code></pre>
    <p class="normal">In the input section, we will listen on two UDP ports, <code class="inlineCode">5144</code> and <code class="inlineCode">5145</code>. When the logs are received, we will tag the log entries with either <code class="inlineCode">syslog-core</code> or <code class="inlineCode">syslog-edge</code>. We will also add a filter section to the configuration to specifically match the <code class="inlineCode">syslog-edge</code> type and apply a regular expression section, <code class="inlineCode">Grok</code>, for the message section. In this case, we will match everything and add an extra field, <code class="inlineCode">received_at</code>, with the value of the timestamp.</p>
    <p class="normal">For more <a id="_idIndexMarker1040"/>information on Grok, take a look at the following documentation: <a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html"><span class="url">https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html</span></a>.</p>
    <p class="normal">We will change <code class="inlineCode">r5</code> and <code class="inlineCode">r6</code> to send syslog information to UDP port <code class="inlineCode">5145</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">r[5-6]#</span>sh run | i logging
logging host 192.168.2.126 vrf Mgmt-intf transport udp port 5145
</code></pre>
    <p class="normal">When we <a id="_idIndexMarker1041"/>start the Logstash server, we will see that both ports are now listening:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>./bin/logstash -f network_configs/config_2.conf
&lt;skip&gt;
[2022-09-23T14:50:42,097][INFO ][logstash.inputs.udp      ][main][212f078853a453d3d8a5d8c1df268fd628577245cd1b66acb06b9e1cb1ff8a10] UDP listener started {:address=&gt;"0.0.0.0:5144", :receive_buffer_bytes=&gt;"106496", :queue_size=&gt;"2000"}
[2022-09-23T14:50:42,106][INFO ][logstash.inputs.udp      ][main][6c3825527b168b167846f4ca7dea5ef55e1437753219866bdcc2eb51aee53c84] UDP listener started {:address=&gt;"0.0.0.0:5145", :receive_buffer_bytes=&gt;"106496", :queue_size=&gt;"2000"}
</code></pre>
    <p class="normal">By separating out the entries using different types, we can specifically search for the types in the Kibana <strong class="screenText">Discover</strong> dashboard:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application, email  Description automatically generated" src="../Images/B18403_13_15.png"/></figure>
    <p class="packt_figref">Figure 13.15: Syslog Index</p>
    <p class="normal">If we expand on the entry with the <code class="inlineCode">syslog-edge</code> type, we can see the new field that we added:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application  Description automatically generated" src="../Images/B18403_13_16.png"/></figure>
    <p class="packt_figref">Figure 13.16: Syslog Timestamp</p>
    <p class="normal">The Logstash <a id="_idIndexMarker1042"/>configuration file provides many options in the input, filter, and output. In particular, the <strong class="screenText">Filter</strong> section provides ways for us to enhance the data by selectively matching the data and further processing it before outputting it to Elasticsearch. Logstash can be extended with modules; each module provides a quick end-to-end solution for ingesting data and visualizations with purpose-built dashboards.</p>
    <p class="normal">For more <a id="_idIndexMarker1043"/>information on the Logstash modules, take a look at the following document: <a href="https://www.elastic.co/guide/en/logstash/8.4/logstash-modules.html"><span class="url">https://www.elastic.co/guide/en/logstash/8.4/logstash-modules.html</span></a> .</p>
    <p class="normal">Elastic Beats are similar to Logstash modules. They are single-purpose data shippers, usually installed as an agent, that collect data on the host and send the output data either directly to Elasticsearch or Logstash for further processing.</p>
    <p class="normal">There are hundreds of different downloadable Beats, such as Filebeat, Metricbeat, Packetbeat, Heartbeat, and so on. In the next section, we will see how we can use Filebeat to ingest Syslog data into Elasticsearch.</p>
    <h1 class="heading-1" id="_idParaDest-285">Data ingestion with Beats</h1>
    <p class="normal">As good as Logstash is, the data ingestion process can get complicated and hard to scale. If we expand on our network log example, we can see that even with just network logs, it can get <a id="_idIndexMarker1044"/>complicated trying to parse different log formats from IOS routers, NXOS routers, ASA firewalls, Meraki wireless controllers, and more. What if we need to ingest log data from Apache web logs, server host health, and security information? What about data formats such as NetFlow, SNMP, and counters? The more data we need to aggregate, the more complicated it can get.</p>
    <p class="normal">While we cannot completely get away from aggregation and the complexity of data ingestion, the current trend is to move toward a more lightweight, single-purpose agent that sits as close to the data source as possible. For example, we can have a data collection agent installed directly on our Apache server specialized in collecting web log data; or we can have a host that only collects, aggregates, and organizes Cisco IOS logs. Elastic Stack <a id="_idIndexMarker1045"/>collectively calls these lightweight data shippers Beats: <a href="https://www.elastic.co/products/beats"><span class="url">https://www.elastic.co/products/beats</span></a>.</p>
    <p class="normal">Filebeat is a <a id="_idIndexMarker1046"/>version of Elastic Beats software intended for forwarding and centralizing log data. It looks for the log file we specified in the configuration to be harvested; once it has finished processing, it will send the new log data to an underlying process that aggregates the events and outputs to Elasticsearch. In this section, we will look at using Filebeat with the Cisco modules to collect network log data.</p>
    <p class="normal">Let’s install Filebeat and set up the Elasticsearch host with the bundled visualization template and index:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>$ curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-8.4.2-amd64.deb
<span class="hljs-con-meta">$ </span>sudo dpkg -i filebeat-8.4.2-amd64.deb
</code></pre>
    <p class="normal">The directory layout can be confusing because they are installed in various <code class="inlineCode">/usr</code>, <code class="inlineCode">/etc/</code>, and <code class="inlineCode">/var</code> locations:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" src="../Images/B18403_13_17.png"/></figure>
    <p class="packt_figref">Figure 13.17: Elastic Filebeat File Locations (source: https://www.elastic.co/guide/en/beats/filebeat/8.4/directory-layout.html)</p>
    <p class="normal">We will <a id="_idIndexMarker1047"/>make a few changes to the configuration file, <code class="inlineCode">/etc/filebeat/filebeat.yml</code>, for the location of Elasticsearch and Kibana:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">output.elasticsearch:</span>
  <span class="hljs-comment"># Array of hosts to connect to.</span>
  <span class="hljs-attr">hosts:</span> [<span class="hljs-string">"192.168.2.126:9200"</span>]
  <span class="hljs-comment"># Protocol - either 'http' (default) or 'https'.</span>
  <span class="hljs-attr">protocol:</span> <span class="hljs-string">"https"</span>
  <span class="hljs-comment"># Authentication credentials - either API key or username/password.</span>
  <span class="hljs-attr">username:</span> <span class="hljs-string">"</span><span class="hljs-string">elastic"</span>
  <span class="hljs-attr">password:</span> <span class="hljs-string">"changeme"</span>
  <span class="hljs-attr">ssl.verification_mode:</span> <span class="hljs-string">none</span>
<span class="hljs-attr">setup.kibana:</span>
  <span class="hljs-attr">host:</span> <span class="hljs-string">"192.168.2.126:5601"</span>
</code></pre>
    <p class="normal">Filebeat can be used to set up the index templates and example Kibana dashboards:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>sudo filebeat setup --index-management -E output.logstash.enabled=<span class="hljs-con-literal">false</span> -E <span class="hljs-con-string">'output.elasticsearch.hots=["https://elastic:-Rel0twWMUk8L-ZtZr=I@192.168.2.126:9200/"]'</span>
<span class="hljs-con-meta">$ </span>sudo filebeat setup –dashboards
</code></pre>
    <p class="normal">Let’s enable the <code class="inlineCode">cisco</code> module for Filebeat:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>sudo filebeat modules <span class="hljs-con-built_in">enable</span> cisco
Enabled cisco
</code></pre>
    <p class="normal">Let’s configure the <code class="inlineCode">cisco</code> module for <code class="inlineCode">syslog</code> first. The file is located under <code class="inlineCode">/etc/filebeat/modules.d/cisco.yml</code>. In our case, I am also specifying a custom log file location:</p>
    <pre class="programlisting code"><code class="hljs-code">- module: cisco
  ios:
    enabled: true
    <span class="hljs-selector-tag">var</span><span class="hljs-selector-class">.input</span>: syslog
    <span class="hljs-selector-tag">var</span><span class="hljs-selector-class">.syslog_host</span>: <span class="hljs-number">0.0</span>.<span class="hljs-number">0.0</span>
    <span class="hljs-selector-tag">var</span><span class="hljs-selector-class">.syslog_port</span>: <span class="hljs-number">514</span>
    <span class="hljs-selector-tag">var</span><span class="hljs-selector-class">.paths</span>: <span class="hljs-selector-attr">[</span><span class="hljs-string">'/home/echou/syslog/my_log.log'</span><span class="hljs-selector-attr">]</span>
</code></pre>
    <p class="normal">We can <a id="_idIndexMarker1048"/>start, stop, and check the status of the Filebeat service using the common Ubuntu Linux command <code class="inlineCode">service Filebeat</code> [<code class="inlineCode">start</code>|<code class="inlineCode">stop</code>|<code class="inlineCode">status</code>]:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>sudo service filebeat start
<span class="hljs-con-meta">$ </span>sudo service filebeat status
● filebeat.service - Filebeat sends log files to Logstash or directly to Elasticsearch.
     Loaded: loaded (/lib/systemd/system/filebeat.service; disabled; vendor preset: enabled)
     Active: active (running) since Fri 2022-09-23 16:06:09 PDT; 3s ago
&lt;skip&gt;
</code></pre>
    <p class="normal">Modify or add UDP port <code class="inlineCode">514</code> for syslog on our devices. We should be able to see the syslog information under the <strong class="keyWord">filebeat-*</strong> index search:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application, email  Description automatically generated" src="../Images/B18403_13_18.png"/></figure>
    <p class="packt_figref">Figure 13.18: Elastic Filebeat Index</p>
    <p class="normal">If we <a id="_idIndexMarker1049"/>compare that to the previous syslog example, we can see that there are a lot more fields and meta information associated with each record, such as <code class="inlineCode">agent.version</code>, <code class="inlineCode">event.code</code>, and <code class="inlineCode">event.severity</code>:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" src="../Images/B18403_13_19.png"/></figure>
    <p class="packt_figref">Figure 13.19: Elastic Filebeat Cisco Log</p>
    <p class="normal">Why do <a id="_idIndexMarker1050"/>the extra fields matter? Among other advantages, the fields make search aggregation easier, and this, in turn, allows us to graph the results better. We will see graphing examples in the upcoming section where we discuss Kibana.</p>
    <p class="normal">Besides the <code class="inlineCode">cisco</code> module, there are modules for Palo Alto Networks, AWS, Google Cloud, MongoDB, and <a id="_idIndexMarker1051"/>many more. The most up-to-date module list can be viewed at <a href="https://www.elastic.co/guide/en/beats/filebeat/8.4/filebeat-modules.html"><span class="url">https://www.elastic.co/guide/en/beats/filebeat/8.4/filebeat-modules.html</span></a>. </p>
    <p class="normal">What if we want to monitor NetFlow data? No problem, there is a module for that! We will run through the same process with the Cisco module by enabling the module and setting up the dashboard:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>sudo filebeat modules <span class="hljs-con-built_in">enable</span> netflow
<span class="hljs-con-meta">$ </span>sudo filebeat setup -e
</code></pre>
    <p class="normal">Then, configure <a id="_idIndexMarker1052"/>the module configuration file, <code class="inlineCode">/etc/filebeat/modules.d/netflow.yml</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-bullet">-</span> <span class="hljs-attr">module:</span> <span class="hljs-string">netflow</span>
  <span class="hljs-attr">log:</span>
    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">var:</span>
      <span class="hljs-attr">netflow_host:</span> <span class="hljs-number">0.0.0.0</span>
      <span class="hljs-attr">netflow_port:</span> <span class="hljs-number">2055</span>
</code></pre>
    <p class="normal">We will configure the devices to send the NetFlow data to port <code class="inlineCode">2055</code>. If you need a refresher, please read the relevant configuration in <em class="chapterRef">Chapter 8</em>, <em class="italic">Network Monitoring with Python – Part 2</em>. We should be able to see the new <code class="inlineCode">netflow</code> data input type:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, website  Description automatically generated" src="../Images/B18403_13_20.png"/></figure>
    <p class="packt_figref">Figure 13.20: Elastic NetFlow Input</p>
    <p class="normal">Remember that each module came pre-bundled with visualization templates? Not to jump ahead <a id="_idIndexMarker1053"/>too much into visualization, but if we click on the <strong class="keyWord">visualization</strong> tab on the left panel, then search for <strong class="keyWord">netflow</strong>, we can see a few visualizations that were created for us:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B18403_13_21.png"/></figure>
    <p class="packt_figref">Figure 13.21: Kibana Visualization</p>
    <p class="normal">Click on the <strong class="screenText">Conversation Partners [Filebeat Netflow]</strong> option, which will give us a nice table of the top talkers that we can reorder by each of the fields:</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B18403_13_22.png"/></figure>
    <p class="packt_figref">Figure 13.22: Kibana Table</p>
    <p class="normal">In the <a id="_idIndexMarker1054"/>next section, we will focus on the Elasticsearch part of the ELK Stack.</p>
    <h1 class="heading-1" id="_idParaDest-286">Search with Elasticsearch</h1>
    <p class="normal">We need more data in Elasticsearch to make the search and graph more interesting. I would <a id="_idIndexMarker1055"/>recommend reloading a few of the lab devices to have the log entries for interface resets, BGP and OSPF establishments, as well as device boot-up messages. Otherwise, feel free to use the sample data we imported at the beginning of this chapter for this section.</p>
    <p class="normal">If we look back at the <code class="inlineCode">Chapter13_2.py</code> script example, when we searched, there were two pieces of information that could potentially change from each query: the index and query body. What I typically like to do is to break that information into input variables that I can dynamically change at runtime to separate the logic of the search and the script itself. Let’s make a file called <code class="inlineCode">query_body_1.json</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">"query"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">"match_all"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{}</span>
  <span class="hljs-punctuation">}</span>
<span class="hljs-punctuation">}</span>
</code></pre>
    <p class="normal">We will create a script, <code class="inlineCode">Chapter13_3.py</code>, that uses <code class="inlineCode">argparse</code> to take the user input at the command line:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> argparse
parser = argparse.ArgumentParser(description=<span class="hljs-string">'Elasticsearch Query Options'</span>)
parser.add_argument(<span class="hljs-string">"-i"</span>, <span class="hljs-string">"--index"</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">"index to query"</span>)
parser.add_argument(<span class="hljs-string">"-q"</span>, <span class="hljs-string">"--query"</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">"query file"</span>)
args = parser.parse_args()
</code></pre>
    <p class="normal">We can then use the two input values to construct the search the same way we have done before:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># load elastic index and query body information</span>
query_file = args.query
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(query_file) <span class="hljs-keyword">as</span> f:
    query_body = json.loads(f.read())
<span class="hljs-comment"># Elasticsearch instance</span>
es_host = Elasticsearch([<span class="hljs-string">"https://elastic:&lt;pass&gt; @192.168.2.126:9200/"</span>],
              ca_certs=<span class="hljs-literal">False</span>, verify_certs=<span class="hljs-literal">False</span>) 
<span class="hljs-comment"># Query both index and put into dictionary</span>
index = args.index
res = es.search(index=index, body=query_body)
<span class="hljs-built_in">print</span>(res[<span class="hljs-string">'hits'</span>][<span class="hljs-string">'total'</span>][<span class="hljs-string">'value'</span>])
</code></pre>
    <p class="normal">We can <a id="_idIndexMarker1056"/>use the <code class="inlineCode">help</code> option to see what arguments should be supplied with the script. Here are the results when we use the same query against the two different indices we created:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python Chapter13_3.py --<span class="hljs-con-built_in">help</span>
usage: Chapter12_3.py [-h] [-i INDEX] [-q QUERY]
Elasticsearch Query Options
optional arguments:
  -h, --help            show this help message and exit
  -i INDEX, --index INDEX
                        index to query
  -q QUERY, --query QUERY
                        query file
<span class="hljs-con-meta">$ </span>python3 Chapter13_3.py -q query_body_1.json -i <span class="hljs-con-string">"cisco*"</span>
50
<span class="hljs-con-meta">$ </span>python3 Chapter13_3.py -q query_body_1.json -i <span class="hljs-con-string">"filebeat*"</span>
10000
</code></pre>
    <p class="normal">When developing our search, it usually takes a few tries before we get the result we are looking for. One of the tools Kibana provides is a developer console that allows us to play around with the search criteria and view the search results on the same page. The tool is under the menu section <em class="italic">Management for Dev Tools</em>.</p>
    <p class="normal">For example, in the following figure, we execute the same search we have done now and we’re able to see the returned JSON result. This is one of my favorite tools on the Kibana interface:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, text  Description automatically generated" src="../Images/B18403_13_23.png"/></figure>
    <p class="packt_figref">Figure 13.23: Kibana Dev Tools</p>
    <p class="normal">Much of the <a id="_idIndexMarker1057"/>network data is based on time, such as the log and NetFlow data we have collected. The values are taken at a snapshot in time, and we will likely group the value in a time scope. For example, we might want to know, “who are the NetFlow top talkers in the last 7 days?” or “which device has the most BGP reset messages in the last hour?” Most of these questions have to do with aggregation and time scope. Let’s look at a query that limits the time range, <code class="inlineCode">query_body_2.json</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">{
  <span class="hljs-string">"query"</span>: {
    <span class="hljs-string">"bool"</span>: {
    <span class="hljs-string">"filter"</span>: [
      {
        <span class="hljs-string">"range"</span>: {
          <span class="hljs-string">"@timestamp"</span>: {
            <span class="hljs-string">"gte"</span>: <span class="hljs-string">"now-10m"</span>
          }
        }
      }
    ]
    }
  }
}
</code></pre>
    <p class="normal">This is a Boolean query, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html"><span class="url">https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html</span></a>, which means it can take a combination of other queries. In our query, we use the <a id="_idIndexMarker1058"/>filter to limit the time range to be the last 10 minutes. We copy the <code class="inlineCode">Chapter13_3.py</code> script to <code class="inlineCode">Chapter13_4.py</code> and modify the output to grab the number of hits as well as a loop over the actual returned results list:</p>
    <pre class="programlisting code"><code class="hljs-code">&lt;skip&gt;
res = es.search(index=index, body=query_body)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Total hits: "</span> + <span class="hljs-built_in">str</span>(res[<span class="hljs-string">'hits'</span>][<span class="hljs-string">'total'</span>][<span class="hljs-string">'value'</span>]))
<span class="hljs-keyword">for</span> hit <span class="hljs-keyword">in</span> res[<span class="hljs-string">'hits'</span>][<span class="hljs-string">'hits'</span>]:
    pprint(hit)
</code></pre>
    <p class="normal">Executing the script will show that we only have <code class="inlineCode">23</code> hits in the last 10 minutes:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python Chapter13_4.py -i <span class="hljs-con-string">"filebeat*"</span> -q query_body_2.json
Total hits: 23
</code></pre>
    <p class="normal">We can add another filter option in the query to limit the source IP via <code class="inlineCode">query_body_3.json</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">{
  <span class="hljs-string">"query"</span>: {
    <span class="hljs-string">"bool"</span>: {
      <span class="hljs-string">"must"</span>: {
        <span class="hljs-string">"term"</span>: {
          <span class="hljs-string">"source.ip"</span>: <span class="hljs-string">"192.168.0.1"</span>
        }
      },
<span class="hljs-variable">&lt;skip&gt;</span>
</code></pre>
    <p class="normal">The result will be limited by both the source IP of r1’s loopback IP in the last 10 minutes:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python Chapter12_4.py -i <span class="hljs-con-string">"filebeat*"</span> -q query_body_3.json
Total hits: 18
</code></pre>
    <p class="normal">Let’s modify the search body one more time to add an aggregation, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket.html"><span class="url">https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket.html</span></a>, that takes a sum of all the network bytes from our previous search:</p>
    <pre class="programlisting code"><code class="hljs-code">{
  <span class="hljs-string">"aggs"</span>: {
        <span class="hljs-string">"network_bytes_sum"</span>: {
          <span class="hljs-string">"sum"</span>: {
            <span class="hljs-string">"field"</span>: <span class="hljs-string">"network.bytes"</span>
        }
    }
  },
  <span class="hljs-variable">&lt;skip&gt;</span>
}
</code></pre>
    <p class="normal">The result <a id="_idIndexMarker1059"/>will be different every time we run the script <code class="inlineCode">Chapter13_5.py</code>. The current result is about 1 MB for me when I run the script consecutively:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python Chapter13_5.py -i <span class="hljs-con-string">"filebeat*"</span> -q query_body_4.json
1089.0
<span class="hljs-con-meta">$ </span>python Chapter13_5.py -i <span class="hljs-con-string">"filebeat*"</span> -q query_body_4.json
990.0
</code></pre>
    <p class="normal">As you can see, building a search query is an iterative process; you typically start with a wide net and gradually narrow the criteria to fine-tune the results. In the beginning, you will probably spend a lot of time reading the documentation and searching for the exact syntax and filters. As you gain more experience under your belt, the search syntax will become easier. Going back to the previous visualization we saw from the <code class="inlineCode">netflow</code> module setup for the NetFlow top talker, we can use the inspection tool to see the <strong class="keyWord">Request</strong> body:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application  Description automatically generated" src="../Images/B18403_13_24.png"/></figure>
    <p class="packt_figref">Figure 13.24: Kibana Request</p>
    <p class="normal">We can <a id="_idIndexMarker1060"/>put that into a query JSON file, <code class="inlineCode">query_body_5.json</code>, and execute it with the <code class="inlineCode">Chapter13_6.py</code> file. We will receive the raw data that the graph was based on:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python Chapter13_6.py -i <span class="hljs-con-string">"filebeat*"</span> -q query_body_5.json
{'1': {'value': 8156040.0}, 'doc_count': 8256, 'key': '10.0.0.5'}
{'1': {'value': 4747596.0}, 'doc_count': 103, 'key': '172.16.1.124'}
{'1': {'value': 3290688.0}, 'doc_count': 8256, 'key': '10.0.0.9'}
{'1': {'value': 576446.0}, 'doc_count': 8302, 'key': '192.168.0.2'}
{'1': {'value': 576213.0}, 'doc_count': 8197, 'key': '192.168.0.1'}
{'1': {'value': 575332.0}, 'doc_count': 8216, 'key': '192.168.0.3'}
{'1': {'value': 433260.0}, 'doc_count': 6547, 'key': '192.168.0.5'}
{'1': {'value': 431820.0}, 'doc_count': 6436, 'key': '192.168.0.4'}
</code></pre>
    <p class="normal">In the next section, let’s take a deeper look at the visualization part of the Elastic Stack: Kibana.</p>
    <h1 class="heading-1" id="_idParaDest-287">Data visualization with Kibana</h1>
    <p class="normal">So far, we have used Kibana to discover data, manage indices in Elasticsearch, use developer <a id="_idIndexMarker1061"/>tools to develop queries, and use a few other features. We also saw the pre-populated visualization charts from NetFlow, which gave us <a id="_idIndexMarker1062"/>the top talker pair from our data. In this section, we will walk through the steps of creating our own graphs. We will start by creating a pie chart.</p>
    <p class="normal">A pie chart is great at visualizing a portion of a component in relation to the whole. Let’s create a pie chart based on the Filebeat index that graphs the top 10 source IP addresses based on the number of record counts. We will select<strong class="screenText"> Dashboard -&gt; Create dashboard -&gt; Create visualization -&gt; Pie</strong>:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" src="../Images/B18403_13_25.png"/></figure>
    <p class="packt_figref">Figure 13.25: Kibana Pie Chart</p>
    <p class="normal">Then we will type <strong class="screenText">netflow</strong> in the search bar to pick our <strong class="screenText">[Filebeat NetFlow]</strong> indices:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, application, Teams  Description automatically generated" src="../Images/B18403_13_26.png"/></figure>
    <p class="packt_figref">Figure 13.26: Kibana Pie Chart Source</p>
    <p class="normal">By default, we are <a id="_idIndexMarker1063"/>given the total count of all the records <a id="_idIndexMarker1064"/>in the default time range. The time range can be dynamically changed:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B18403_13_27.png"/></figure>
    <p class="packt_figref">Figure 13.27: Kibana Time Range</p>
    <p class="normal">We can assign a custom label for the graph:</p>
    <figure class="mediaobject"><img alt="Icon  Description automatically generated" src="../Images/B18403_13_28.png"/></figure>
    <p class="packt_figref">Figure 13.28: Kibana Chart Label</p>
    <p class="normal">Let’s click on the <strong class="screenText">Add</strong> option to add more buckets. We will choose to split the slices, pick the terms for aggregation, and select the <strong class="screenText">source.ip</strong> field from the drop-down menu. We will leave the <strong class="screenText">order</strong> <strong class="screenText">Descending</strong> but increase S<strong class="screenText">ize</strong> to <strong class="screenText">10</strong>.</p>
    <p class="normal">The change <a id="_idIndexMarker1065"/>will only be applied when you click the <strong class="screenText">Apply</strong> button at <a id="_idIndexMarker1066"/>the top. It is a common mistake to expect the change to happen in real time when using a modern website and not by clicking on the <strong class="screenText">Apply</strong> button:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" src="../Images/B18403_13_29.png"/></figure>
    <p class="packt_figref">Figure 13.29: Kibana Play Button</p>
    <p class="normal">We can click on <strong class="screenText">Options</strong> at the top to turn off <strong class="screenText">Donut</strong> and turn on <strong class="screenText">Show labels</strong>:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, application, Teams  Description automatically generated" src="../Images/B18403_13_30.png"/></figure>
    <p class="packt_figref">Figure 13.30: Kibana Chart Options</p>
    <p class="normal">The final <a id="_idIndexMarker1067"/>graph is a nice pie chart showing the top IP <a id="_idIndexMarker1068"/>sources based on the number of document counts:</p>
    <figure class="mediaobject"><img alt="Chart, pie chart  Description automatically generated" src="../Images/B18403_13_31.png"/></figure>
    <p class="packt_figref">Figure 13.31: Kibana Pie Chart </p>
    <p class="normal">As with Elasticsearch, the Kibana graph is also an iterative process that typically takes a few tries to get right. What if we split the result into different charts instead of slices on the same chart? Yeah, that is not very visually appealing:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, application, Teams  Description automatically generated" src="../Images/B18403_13_32.png"/></figure>
    <p class="packt_figref">Figure 13.32: Kibana Split Chart </p>
    <p class="normal">Let’s stick <a id="_idIndexMarker1069"/>to splitting things into slices on the same <a id="_idIndexMarker1070"/>pie chart and change the time range to <strong class="screenText">Last 1 hour</strong>, then save the chart so that we can come back to it later.</p>
    <p class="normal">Note that we can also share the graph either in an embedded URL (if Kibana is accessible from a shared location) or a snapshot:</p>
    <figure class="mediaobject"><img alt="Chart, pie chart  Description automatically generated" src="../Images/B18403_13_33.png"/></figure>
    <p class="packt_figref">Figure 13.33: Kibana Save Chart </p>
    <p class="normal">We can <a id="_idIndexMarker1071"/>also do more with the metrics operations. For example, we can <a id="_idIndexMarker1072"/>pick the data table chart type and repeat our previous bucket breakdown with the source IP. But we can also add a second metric by adding up the total number of network bytes per bucket:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" src="../Images/B18403_13_34.png"/></figure>
    <p class="packt_figref">Figure 13.34: Kibana Metrics</p>
    <p class="normal">The result <a id="_idIndexMarker1073"/>is a table showing both the number of document <a id="_idIndexMarker1074"/>counts as well as the sum of the network bytes. This can be downloaded in CSV format for local storage:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, table  Description automatically generated with medium confidence" src="../Images/B18403_13_35.png"/></figure>
    <p class="packt_figref">Figure 13.35: Kibana Tables</p>
    <p class="normal">Kibana is a very powerful visualization tool in the Elastic Stack. We are just scratching the surface of its visualization capabilities. Besides many other graph options to better tell the story of your data, we can also group multiple visualizations onto a dashboard to be displayed. We can <a id="_idIndexMarker1075"/>also use Timelion (<a href="https://www.elastic.co/guide/en/kibana/8.4/timelion.html"><span class="url">https://www.elastic.co/guide/en/kibana/8.4/timelion.html</span></a>) to group independent <a id="_idIndexMarker1076"/>data sources for a single visualization or use Canvas (<a href="https://www.elastic.co/guide/en/kibana/current/canvas.html"><span class="url">https://www.elastic.co/guide/en/kibana/current/canvas.html</span></a>) as a presentation tool based on data in Elasticsearch.</p>
    <p class="normal">Kibana is <a id="_idIndexMarker1077"/>typically used at the end of the workflow to present <a id="_idIndexMarker1078"/>our data meaningfully. We have covered the basic workflow from data ingestion to storage, retrieval, and visualization in the span of a chapter. It still amazes me that we can accomplish so much in a short period with the aid of an integrated, open source stack such as Elastic Stack.</p>
    <h1 class="heading-1" id="_idParaDest-288">Summary</h1>
    <p class="normal">In this chapter, we used the Elastic Stack to ingest, analyze, and visualize network data. We used Logstash and Beats to ingest the network syslog and NetFlow data. Then we used Elasticsearch to index and categorize the data for easier retrieval. Finally, we used Kibana to visualize the data. We used Python to interact with the stack and help us gain more insights into our data. Together, Logstash, Beats, Elasticsearch, and Kibana present a powerful all-in-one project that can help us understand our data better.</p>
    <p class="normal">In the next chapter, we will look at using Git for network development with Python.</p>
    <h1 class="heading-1">Join our book community</h1>
    <p class="normal">To join our community for this book – where you can share feedback, ask questions to the author, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="https://packt.link/networkautomationcommunity"><span class="url">https://packt.link/networkautomationcommunity</span></a></p>
    <p class="normal"><img alt="" src="../Images/QR_Code2903617220506617062.png"/></p>
  </div>
</body></html>