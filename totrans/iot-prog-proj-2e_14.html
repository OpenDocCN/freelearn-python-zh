<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer301">
<h1 class="chapter-number" id="_idParaDest-226"><a id="_idTextAnchor228"/>14</h1>
<h1 id="_idParaDest-227"><a id="_idTextAnchor229"/>Adding Computer Vision to A.R.E.S.</h1>
<p>In this final chapter, we will be adding computer vision to A.R.E.S. This will give A.R.E.S. the ability to recognize objects as well as alert us via text message to the presence of these objects. For our example, we will be recognizing dogs, although we could just as easily set up our object recognition code to recognize <span class="No-Break">other objects.</span></p>
<p>We will start our journey by exploring <strong class="bold">computer vision</strong> and what it is before downloading and installing the <strong class="bold">Open Source Computer Vision</strong> (<strong class="bold">OpenCV</strong>) library and the <strong class="bold">You Only Look Once</strong> (<strong class="bold">YOLO</strong>) object detection system. After setting up these tools, we will explore <span class="No-Break">hands-on examples.</span></p>
<p>By the end of this chapter, you will have built a smart video streaming application that utilizes the video stream from the camera <span class="No-Break">on A.R.E.S.</span></p>
<p>We will cover the following topics in <span class="No-Break">this chapter:</span></p>
<ul>
<li>Exploring <span class="No-Break">computer vision</span></li>
<li>Adding computer vision <span class="No-Break">to A.R.E.S.</span></li>
<li>Sending out a <span class="No-Break">text alert</span></li>
</ul>
<p><span class="No-Break">Let’s begin!</span></p>
<h1 id="_idParaDest-228"><a id="_idTextAnchor230"/>Technical requirements</h1>
<p>You will need the following to complete <span class="No-Break">this chapter:</span></p>
<ul>
<li>Intermediate knowledge of <span class="No-Break">Python programming</span></li>
<li>The A.R.E.S. robot from <a href="B21282_13.xhtml#_idTextAnchor209"><span class="No-Break"><em class="italic">Chapter 13</em></span></a></li>
<li>A computer with a GUI-style operating system, such as the Raspberry Pi 5, macOS, <span class="No-Break">or Windows</span></li>
</ul>
<p>The code for this chapter can be found <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/-Internet-of-Things-Programming-Projects-2nd-Edition/tree/main/Chapter14"><span class="No-Break">https://github.com/PacktPublishing/-Internet-of-Things-Programming-Projects-2nd-Edition/tree/main/Chapter14</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-229"><a id="_idTextAnchor231"/>Exploring computer vision</h1>
<p>Computer vision<a id="_idIndexMarker1076"/> began in the 1950s, evolving significantly with key milestones such as image processing algorithms in the 1960s and the introduction of GPUs in the 1990s. These advancements improved processing speeds and complex computations and enabled real-time image analysis and sophisticated models. Modern computer vision technology is a result of these developments. The following diagram shows where computer vision sits in terms of <span class="No-Break">artificial intelligence:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer283">
<img alt="Figure 14.1 – Artificial intelligence" height="828" src="image/B21282_14_1.jpg" width="1400"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1 – Artificial intelligence</p>
<p><span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.1</em> shows a set of concentric circles representing the relationship between different fields of artificial intelligence. At the core is computer vision, surrounded by <strong class="bold">object detection</strong>, which<a id="_idIndexMarker1077"/> is a subset of deep learning, something that’s nested within machine learning. All are encompassed by the broader field of <span class="No-Break">artificial intelligence.</span></p>
<p>Not all computer vision techniques involve machine learning or deep learning, but object detection, a part of computer vision, <span class="No-Break">often does.</span></p>
<p class="callout-heading">What is the difference between image recognition, object recognition, object detection, and image classification?</p>
<p class="callout">In computer <a id="_idIndexMarker1078"/>vision, terms such as <strong class="bold">image recognition</strong>, <strong class="bold">object recognition</strong>, <strong class="bold">object detection</strong>, and <strong class="bold">image classification</strong> describe specific processes. Image <a id="_idIndexMarker1079"/>recognition<a id="_idIndexMarker1080"/> detects features or<a id="_idIndexMarker1081"/> patterns within an image. Object recognition moves beyond this to identify distinct objects within an image or video, although it does not specify their precise locations, concentrating on identifying <em class="italic">what</em> objects are present rather than <em class="italic">where</em> they are. Object detection, conversely, not only identifies objects but also locates them spatially, often using bounding boxes. Meanwhile, image classification involves analyzing an entire image to assign it to a specific category, such as determining whether an image shows a dog, cat, or car. For A.R.E.S., we want a video feed that creates a bounding box around an object that’s been detected. So, we will use object detection in <span class="No-Break">our application.</span></p>
<p>In this chapter, we will integrate OpenCV and the YOLO deep learning algorithm into A.R.E.S. so that we<a id="_idIndexMarker1082"/> can use object detection to detect the presence of <span class="No-Break">a dog.</span></p>
<p>We will start our foray into computer vision by familiarizing ourselves with the <span class="No-Break">OpenCV library.</span></p>
<h2 id="_idParaDest-230"><a id="_idTextAnchor232"/>Introducing OpenCV</h2>
<p>OpenCV is a<a id="_idIndexMarker1083"/> foundational<a id="_idIndexMarker1084"/> tool in the field of computer vision that offers a vast range of capabilities for real-time image processing. OpenCV supports a multitude of applications, from simple image transformations to complex machine <span class="No-Break">learning algorithms.</span></p>
<p>OpenCV not only allows for rapid prototyping but also supports full-scale application development across various operating systems, making it an excellent choice for hobbyists, educators, and commercial <span class="No-Break">developers alike.</span></p>
<p>In this section, we will explore the core functionalities of OpenCV. We will start by creating a Python virtual environment and a <span class="No-Break">project folder.</span></p>
<h3>Viewing an image using OpenCV</h3>
<p>Getting started <a id="_idIndexMarker1085"/>with OpenCV can be as simple as displaying an image in a window. This basic exercise introduces us to key functions for image loading, handling, and window operations in OpenCV. Follow <span class="No-Break">these steps:</span></p>
<ol>
<li>Start by opening a terminal window. We can use the Raspberry Pi operating system on our Raspberry Pi 5 or another operating system of <span class="No-Break">our choice.</span></li>
<li>To store our project files, we must create a new directory and subdirectory (for images) with the following command (Linux commands are being <span class="No-Break">used here):</span><pre class="source-code">
<strong class="bold">mkdir -p Chapter14/images</strong></pre></li> <li>Then, we navigate to the <span class="No-Break">new directory:</span><pre class="source-code">
<strong class="bold">cd Chapter14</strong></pre></li> <li>We will need an image file to test OpenCV with. To download one directly from this chapter’s GitHub repository to our new <strong class="source-inline">images</strong> subdirectory, we can run the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">curl -o images/toronto.png https://raw.githubusercontent.com/PacktPublishing/Internet-of-Things-Programming-Projects-2nd-Edition/main/Chapter14/images/toronto.png</strong></pre></li> <li>Next, we need to create a new Python virtual environment for our project by running the following command (we may need to install the Python <strong class="source-inline">venv</strong> library if it is not <span class="No-Break">already installed):</span><pre class="source-code">
<strong class="bold">python -m venv ch14-env --system-site-packages</strong></pre></li> <li>With our new Python virtual environment created, we can source into it with the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">source ch14-env/bin/activate</strong></pre></li> <li>For our project, we need the <strong class="source-inline">opencv-python</strong> library. We can install this library with the following <span class="No-Break">Terminal command:</span><pre class="source-code">
<strong class="bold">pip install opencv-python</strong></pre></li> <li>We close the terminal by running the <span class="No-Break">following comma<a id="_idTextAnchor233"/>nd:</span><pre class="source-code">
<strong class="bold">exit</strong></pre></li> <li>Next, we launch Thonny and source our newly created Python <span class="No-Break">virtual environment:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer284">
<img alt="Figure 14.2 – Sourcing our Python virtual environment" height="183" src="image/B21282_14_2.jpg" width="561"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2 – Sourcing our Python virtual environment</p>
<ol>
<li value="10">Next, we’ll create <a id="_idIndexMarker1086"/>a new tab. We can do this by selecting <strong class="bold">File</strong> and then <strong class="bold">New</strong> or by hitting <em class="italic">Ctrl</em> + <em class="italic">N</em> on <span class="No-Break">our keyboard.</span></li>
<li>Enter the following code in <span class="No-Break">the editor:</span><pre class="source-code">
import cv2 as cv
img = cv.imread('images/Toronto.png')
cv.imshow('Downtown Toronto', img)
cv.waitKey(0)
cv.destroyAllWindows()</pre><p class="list-inset">Let’s take a closer look at <span class="No-Break">this code:</span></p><ol><li class="upper-roman">First, we import the <strong class="source-inline">OpenCV</strong> library and give it an alias of <strong class="source-inline">cv</strong> for easier reference in <span class="No-Break">the code.</span></li><li class="upper-roman">Our code then reads the <strong class="source-inline">Toronto.png</strong> image file into the <strong class="source-inline">img</strong> variable from the <span class="No-Break"><strong class="source-inline">images</strong></span><span class="No-Break"> folder.</span></li><li class="upper-roman">Next, we create a window named <strong class="source-inline">Downtown Toronto</strong> and display <strong class="source-inline">img</strong> within <span class="No-Break">this window.</span></li><li class="upper-roman">Then, our code waits indefinitely for a key event before moving on to the next line of code. The <strong class="source-inline">0</strong> value means it will wait until a key <span class="No-Break">is pressed.</span></li><li class="upper-roman">Finally, we destroy all the windows that have been created during the session and ensure no window from the OpenCV UI remains open after the script is run. This could potentially cause a <span class="No-Break">memory leak.</span></li></ol></li> <li>We save the code with a descriptive name such as <strong class="source-inline">Toronto.py</strong> in our <strong class="source-inline">Chapter14</strong> <span class="No-Break">project folder.</span></li>
<li>We run the<a id="_idIndexMarker1087"/> code by clicking on the green run button, hitting <em class="italic">F5</em> on our keyboard, or clicking on the <strong class="bold">Run</strong> menu option at the top and then <strong class="bold">Run </strong><span class="No-Break"><strong class="bold">current script</strong></span><span class="No-Break">.</span><p class="list-inset">We should see a window appear that contains an image <span class="No-Break">of Toronto:</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer285">
<img alt="Figure 14.3 – OpenCV window popup showing downtown Toronto (image: Maximillian Dow)" height="663" src="image/B21282_14_3.jpg" width="473"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.3 – OpenCV window popup showing downtown Toronto (image: Maximillian Dow)</p>
<ol>
<li value="14">To close the pop-up window, we hit any key on <span class="No-Break">our keyboard.</span></li>
</ol>
<p>Although very<a id="_idIndexMarker1088"/> simple, this exercise lays the groundwork for more complex computer vision projects. Before we move on to using artificial intelligence, we will investigate using OpenCV to view the camera feed coming <span class="No-Break">from A.R.E.S.</span></p>
<h3>Streaming video using OpenCV</h3>
<p>In <a href="B21282_13.xhtml#_idTextAnchor209"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, we <a id="_idIndexMarker1089"/>streamed a video from A.R.E.S. using the VLC media player. To utilize the video coming from A.R.E.S., we can use OpenCV for real-time image and <span class="No-Break">video analysis.</span></p>
<p>To view our video feed using OpenCV, we must do <span class="No-Break">the following:</span></p>
<ol>
<li>We launch Thonny and source the <strong class="source-inline">ch14-env</strong> Python <span class="No-Break">virtual environment.</span></li>
<li>We create a new tab by selecting <strong class="bold">File</strong> and then <strong class="bold">New</strong> or by hitting <em class="italic">Ctrl</em> + <em class="italic">N</em> on <span class="No-Break">our keyboard.</span></li>
<li>We enter the following code in <span class="No-Break">the editor:</span><pre class="source-code">
import cv2
stream_url = '&lt;&lt;rtsp address&gt;&gt;'
cap = cv2.VideoCapture(stream_url)
if not cap.isOpened():
    print("Error: Could not open stream")
    exit()
while True:
    ret, frame = cap.read()
    if not ret:
        print("Error: Can't receive frame. Exiting ...")
        break
    cv2.imshow('A.R.E.S. Stream', frame)
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()</pre><p class="list-inset">Let’s take a closer look at <span class="No-Break">this code:</span></p><ol><li class="upper-roman">We start by importing the <span class="No-Break">OpenCV library.</span></li><li class="upper-roman">Then, we define the RTSP URL as a string for the video <span class="No-Break">stream source.</span></li><li class="upper-roman">Our code creates a <strong class="source-inline">VideoCapture</strong> object that attempts to open the video stream from the specified RTSP URL. If the stream can’t be opened, an error message <span class="No-Break">is printed.</span></li><li class="upper-roman">Then, we start <a id="_idIndexMarker1090"/>an infinite loop to continuously fetch frames from <span class="No-Break">the stream.</span></li><li class="upper-roman">After, we attempt to read the next frame from <span class="No-Break">the stream.</span></li><li class="upper-roman">We print an error message if a frame can’t be received and exit <span class="No-Break">the loop.</span></li><li class="upper-roman">We display the current frame in a window titled <span class="No-Break"><strong class="source-inline">A.R.E.S. Stream</strong></span><span class="No-Break">.</span></li><li class="upper-roman">Then, we allow the user to close the stream window manually if they press <strong class="source-inline">q</strong> on <span class="No-Break">their keyboard.</span></li><li class="upper-roman">Next, release the video capture object, freeing up resources and closing the video file or <span class="No-Break">capturing device.</span></li><li class="upper-roman">Finally, we close all OpenCV windows, cleaning up any remaining resources associated with the <span class="No-Break">window displays.</span></li></ol></li> <li>We save the code with a descriptive name such as <strong class="source-inline">video-feed.py</strong> in our <strong class="source-inline">Chapter14</strong> <span class="No-Break">project folder.</span></li>
<li>We run the code by clicking on the green run button, hitting <em class="italic">F5</em> on our keyboard, or clicking on the <strong class="bold">Run</strong> menu option at the top and then <strong class="bold">Run </strong><span class="No-Break"><strong class="bold">current script</strong></span><span class="No-Break">.</span></li>
<li>We should see a window appear, displaying the feed from the camera <span class="No-Break">on A.R.E.S:</span></li>
</ol>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer286">
<img alt="Figure 14.4 – Video feed from the camera on A.R.E.S." height="335" src="image/B21282_14_4.jpg" width="396"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.4 – Video feed from the camera on A.R.E.S.</p>
<ol>
<li value="7">To close the pop-up window, we hit <strong class="source-inline">q</strong> on <span class="No-Break">our keyboard.</span></li>
</ol>
<p>Now that we have some experience using OpenCV to view images and videos, let’s take this a step <a id="_idIndexMarker1091"/>further and have it identify objects, specifically dogs, as we continue our computer <span class="No-Break">vision journey.</span></p>
<p>We will start by looking at neural networks and how they are used to <span class="No-Break">identify objects.</span></p>
<h2 id="_idParaDest-231"><a id="_idTextAnchor234"/>Understanding YOLO and neural networks</h2>
<p>In this section, we’ll<a id="_idIndexMarker1092"/> focus on YOLO and the <a id="_idIndexMarker1093"/>various <a id="_idIndexMarker1094"/>layers of neural networks so that we can construct object detection code that can identify dogs. Turning our attention to <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.1</em>, we can see that object detection is a part of computer vision, where both deep learning and machine learning techniques <span class="No-Break">are used.</span></p>
<p class="callout-heading">Machine learning versus deep learning</p>
<p class="callout">Machine <a id="_idIndexMarker1095"/>learning is a subset of artificial intelligence<a id="_idIndexMarker1096"/> where algorithms use statistical methods to enable machines to improve with experience, typically requiring manual feature selection. In contrast, deep learning, a specialized subset of machine learning, operates with neural networks, which automatically extract and learn features from large volumes of data. This is ideal for complex tasks such as image and speech recognition. While machine learning works with less data and provides more model transparency, deep learning requires substantial data and computational power, often acting as a <em class="italic">black box</em> with <span class="No-Break">less interpretability.</span></p>
<p>To represent <a id="_idIndexMarker1097"/>deep learning, YOLO uses a <a id="_idIndexMarker1098"/>sophisticated <a id="_idIndexMarker1099"/>neural network that assesses images in a <span class="No-Break">single sweep.</span></p>
<h2 id="_idParaDest-232"><a id="_idTextAnchor235"/>Exploring object detection</h2>
<p>As mentioned<a id="_idIndexMarker1100"/> previously, object detection is the<a id="_idIndexMarker1101"/> process of finding objects in an image or video feed. The following figure illustrates the sequential stages of an object detection algorithm, using the example of an image of <span class="No-Break">a dog:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer287">
<img alt="" height="568" role="presentation" src="image/B21282_14_5.jpg" width="1635"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.5 – Stages of object detection in computer vision</p>
<p>First, we have the original image as the input. We proceed by decomposing it into input pixels and then identify the edges, corners, and contours for structural interpretation. Our algorithm proceeds to recognize individual object parts and places bounding boxes around these components. This leads to the final output, which highlights the detected object within <span class="No-Break">the image.</span></p>
<p>Now that we <a id="_idIndexMarker1102"/>understand how object detection works when<a id="_idIndexMarker1103"/> used with neural networks, let’s consider an example. In the next subsection, we will use YOLO to identify a dog in <span class="No-Break">a picture.</span></p>
<h3>Using YOLO to identify a dog in a picture</h3>
<p>In this section, we <a id="_idIndexMarker1104"/>will write a program using<a id="_idIndexMarker1105"/> OpenCV and the YOLO deep learning algorithm to detect a dog in <span class="No-Break">a picture.</span></p>
<p>To get started, we need to download the YOLO configuration files, the pre-trained weights, and the <strong class="source-inline">coco.names</strong> file, which contains the list of classes recognized by the model. These files are typically available on the official YOLO website or reputable GitHub repositories dedicated to YOLO. The configuration file (<strong class="source-inline">yolov4.cfg</strong>) outlines the network architecture, the weights file (<strong class="source-inline">yolov4.weights</strong>) contains the trained model parameters, and the class names file lists the object categories the YOLO model can detect, all of which are crucial for the object detection task <span class="No-Break">at hand.</span></p>
<p>To make this easier, we have included all the files you’ll need for this exercise in this chapter’s <span class="No-Break">GitHub repository.</span></p>
<p class="callout-heading">What is the yolo4.weights file?</p>
<p class="callout">The <strong class="source-inline">yolov4.weights</strong> file contains <a id="_idIndexMarker1106"/>pre-trained weights for the YOLOv4 object detection model, enabling it to accurately detect and locate objects in images and videos. Since this file is too large to be included in this chapter’s GitHub repository, you’ll need to download it from the official YOLO website or GitHub <span class="No-Break">repository (</span><a href="https://github.com/AlexeyAB/darknet/releases"><span class="No-Break">https://github.com/AlexeyAB/darknet/releases</span></a><span class="No-Break">).</span></p>
<p>To create our<a id="_idIndexMarker1107"/> object detection code, follow <a id="_idIndexMarker1108"/><span class="No-Break">these steps:</span></p>
<ol>
<li>We start by opening a terminal window. We can use the Raspberry Pi operating system on our Raspberry Pi 5 or another operating system of <span class="No-Break">our choice.</span></li>
<li>We navigate to our <strong class="source-inline">Chapter14</strong> project directory with the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">cd Chapter14</strong></pre></li> <li>To store our YOLO files, create a new directory with the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">mkdir YOLO</strong></pre></li> <li>We copy the <strong class="source-inline">coco.names</strong>, <strong class="source-inline">yolov4.cfg</strong>, and <strong class="source-inline">yolov4.weights</strong> files from the YOLO directory of this chapter’s GitHub repository to our local YOLO directory using whichever method <span class="No-Break">suits us.</span></li>
<li>For our test image, we copy the <strong class="source-inline">dog.png</strong> file from the images directory of this chapter’s GitHub repository to our project folder’s <span class="No-Break">images directory.</span></li>
<li>We launch Thonny and source the <strong class="source-inline">ch14-env</strong> Python <span class="No-Break">virtual environment.</span></li>
<li>We create a new tab by selecting <strong class="bold">File</strong> and then <strong class="bold">New</strong> or by hitting <em class="italic">Ctrl</em> + <em class="italic">N</em> on <span class="No-Break">our keyboard.</span></li>
<li>In the editor, we add the necessary imports for our code. Here, we’ll need OpenCV as our library and NumPy for its <span class="No-Break">mathematical functions:</span><pre class="source-code">
import cv2
import numpy as np</pre></li> <li>Now, we load the YOLO algorithm by running the following lines <span class="No-Break">of code:</span><pre class="source-code">
net = cv2.dnn.readNet("YOLO/yolov4.weights", "YOLO/yolov4.cfg")
classes = []
layer_names = net.getLayerNames()</pre><p class="list-inset">Let’s take a closer look at <span class="No-Break">this code:</span></p><ol><li class="upper-roman">First, we initialize the YOLO network by loading the pre-trained weights (<strong class="source-inline">yolov4.weights</strong>) and configuration (<strong class="source-inline">yolov4.cfg</strong>). This creates a neural network ready for <span class="No-Break">object detection.</span></li><li class="upper-roman">Then, we <a id="_idIndexMarker1109"/>create an<a id="_idIndexMarker1110"/> empty list intended to store the class names (for example, dog and cat) that YOLO can detect, once they are read from <span class="No-Break">a file.</span></li><li class="upper-roman">Our code then retrieves the names of all the layers in the YOLO network. These layer names are used to identify output layers. This is crucial for obtaining <span class="No-Break">detection results.</span></li></ol></li> <li>We enter the following code to fetch the indices of the final layers in the YOLO neural network. These directly output detection results using the OpenCV <span class="No-Break"><strong class="source-inline">getUnconnectedOutLayers()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
output_layer_indices = net.getUnconnectedOutLayers()</pre></li> <li>Next, create a list of the names of the output layers of the YOLO neural network by indexing into the list of all layer names using the indices provided by <strong class="source-inline">output_layer_indices</strong>, adjusted for zero-based indexing. This corresponds to the <em class="italic">Bounding Boxes</em> stage of the algorithm, as outlined in <span class="No-Break"><em class="italic">Figure 14</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span><pre class="source-code">
output_layers = [layer_names[i - 1] for i in output_layer_indices]</pre></li> <li>Next, we read the <strong class="source-inline">coco.names</strong> file, which contains the list of object classes that the YOLO model can identify, and creates a list of class names by removing any leading or trailing whitespace from each line. The following code finds and stores the index of the <strong class="source-inline">"dog"</strong> class within that list, effectively preparing the program to specifically recognize and identify dogs in the images processed by the <span class="No-Break">YOLO model:</span><pre class="source-code">
with open("YOLO/coco.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]
dog_class_id = classes.index("dog")</pre></li> <li>The following code reads the <strong class="source-inline">dog.png</strong> image from the <strong class="source-inline">images</strong> directory, scales it down to 40% of its original size to reduce computational load, and extracts its dimensions and color channel count. The resizing step is crucial because YOLO<a id="_idIndexMarker1111"/> models typically <a id="_idIndexMarker1112"/>expect a fixed input size, and resizing helps to match that requirement while also accelerating the detection process due to the smaller <span class="No-Break">image size:</span><pre class="source-code">
img = cv2.imread('images/dog.png')
img = cv2.resize(img, None, fx=0.4, fy=0.4)
height, width, channels = img.shape</pre></li> <li>Next, we must convert the resized image<a id="_idIndexMarker1113"/> into a <strong class="bold">blob</strong> – a preprocessed image that’s compatible with the neural network – by normalizing pixel values and setting the size to 416x416 pixels, a standard input size for YOLO models. Then, we must set this blob as the input to the neural network. Finally, we must perform a forward pass through the network using the specified output layers to obtain the detection predictions. This includes class labels, confidences, and bounding box coordinates. The following snippet corresponds to the action that takes place between the <em class="italic">Input Pixels</em> and the <em class="italic">Bounding Boxes</em> stages shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.5</em>. It processes the image through various layers to detect objects, with the final line in the snippet producing the detections that lead to the <span class="No-Break"><em class="italic">Output</em></span><span class="No-Break"> stage:</span><pre class="source-code">
blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0),
True, crop=False)
net.setInput(blob)
outs = net.forward(output_layers)</pre></li> <li>The following code analyzes the results from the neural network’s forward pass, filtering and processing detected objects for the <strong class="source-inline">dog</strong> class with a confidence level above 50%. It calculates the bounding box coordinates based on the object’s center, width, and height, then stores these coordinates along with the detection confidence and class ID in corresponding lists. This aligns with the <em class="italic">Bounding Boxes</em> stage shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.5</em>, where the processed outputs are used to specifically locate and classify the detected objects within the image. This <a id="_idIndexMarker1114"/>sets the stage for the <a id="_idIndexMarker1115"/>final visual representation in the <em class="italic">Output</em> phase, where these bounding boxes are drawn to indicate where dogs are located within <span class="No-Break">the image:</span><pre class="source-code">
class_ids = []
confidences = []
boxes = []
for out in outs:
    for detect in out:
        scores = detect[5:]
        class_id = np.argmax(scores)
        confidence = scores[class_id]
        if confidence &gt; 0.5 and class_id == dog_class_id:
            center_x = int(detection[0] * width)
            center_y = int(detection[1] * height)
            w = int(detection[2] * width)
            h = int(detection[3] * height)
            x = int(center_x - w / 2)
            y = int(center_y - h / 2)
            boxes.append([x, y, w, h])
            confidences.append(float(confidence))
            class_ids.append(class_id)</pre></li> <li>The following code <a id="_idIndexMarker1116"/>uses <strong class="bold">non-maximum suppression</strong> (<strong class="bold">NMS</strong>) via the <strong class="source-inline">NMSBoxes</strong> function from OpenCV to refine the detection results by reducing overlap among bounding boxes, ensuring that each detected object is represented only once. After determining the best bounding boxes based on their confidence and overlap, it iterates through these optimized boxes to visually annotate the image. It does this by drawing a rectangle for each box and labeling it with<a id="_idIndexMarker1117"/> the corresponding<a id="_idIndexMarker1118"/> class name. This final step marks and identifies the detected objects (<strong class="source-inline">dogs</strong>) in the image, aligning with the <em class="italic">Output</em> stage shown in <span class="No-Break"><em class="italic">Figure 14</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span><pre class="source-code">
indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5,
0.4)
for i in indexes.flatten():
    x, y, w, h = boxes[i]
    label = str(classes[class_ids[i]])
cv2.rectangle(img, (x, y), (x + w, y + h), (0,
255, 0), 2)
    cv2.putText(img, label, (x, y + 30),
cv2.FONT_HERSHEY_PLAIN, 3,
(0, 255, 0), 3)</pre></li> <li>In the final section of our code, we display the processed image with detected objects marked by bounding boxes. The <strong class="source-inline">cv2.imshow("Image", img)</strong> function displays the image in a window titled <strong class="source-inline">"Image"</strong>. The <strong class="source-inline">cv2.waitKey(0)</strong> function pauses the execution of the script, waiting indefinitely for a key press to proceed, allowing the user to view the image for as long as needed. Finally, <strong class="source-inline">cv2.destroyAllWindows()</strong> closes all OpenCV windows opened by the script, ensuring a clean exit without leaving any GUI <span class="No-Break">windows open:</span><pre class="source-code">
cv2.imshow("Image", img)
cv2.waitKey(0)
cv2.destroyAllWindows()</pre></li> <li>We save the <a id="_idIndexMarker1119"/>code with a descriptive<a id="_idIndexMarker1120"/> name, such as <strong class="source-inline">recognize-dog.py</strong>, in our <strong class="source-inline">Chapter14</strong> <span class="No-Break">project folder.</span></li>
<li>We run the code by clicking on the green run button, hitting <em class="italic">F5</em> on our keyboard, or clicking on the <strong class="bold">Run</strong> menu option at the top and then <strong class="bold">Run </strong><span class="No-Break"><strong class="bold">current script</strong></span><span class="No-Break">.</span></li>
<li>We should observe a pop-up window appear with a bounding box around the face of <span class="No-Break">the dog:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer288">
<img alt="Figure 14.6 – YOLO library used to recognize a dog" height="905" src="image/B21282_14_6.jpg" width="1027"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.6 – YOLO library used to recognize a dog</p>
<ol>
<li value="21">We press any key on our keyboard to close the <span class="No-Break">pop-up window.</span></li>
</ol>
<p>As we can see, our<a id="_idIndexMarker1121"/> program can identify a<a id="_idIndexMarker1122"/> dog from a picture. If we were to provide a picture of any object identified in the <strong class="source-inline">coco.names</strong> file (a person, for example), our program should be able to <span class="No-Break">identify that.</span></p>
<p>Now that we have a little exposure to YOLO, neural networks, and object detection, let’s add this functionality to A.R.E.S. We will program our application to send a text message whenever A.R.E.S. detects <span class="No-Break">a dog.</span></p>
<h1 id="_idParaDest-233"><a id="_idTextAnchor236"/>Adding computer vision to A.R.E.S.</h1>
<p>In the previous <a id="_idIndexMarker1123"/>section, we explored OpenCV and YOLO, using<a id="_idIndexMarker1124"/> OpenCV to view images and video feeds, and YOLO to identify a dog in a picture. In this section, we’ll apply what we’ve learned to create a smart video streaming application that represents the eyes of A.R.E.S. We’ll only use dogs as an example, but we could easily adapt this application for tracking <span class="No-Break">other objects.</span></p>
<p>We will start by encapsulating our YOLO code into a class called <strong class="source-inline">DogTracker</strong> before creating a video streaming application using this class <span class="No-Break">with OpenCV.</span></p>
<h2 id="_idParaDest-234"><a id="_idTextAnchor237"/>Creating the DogTracker class</h2>
<p>The <strong class="source-inline">DogTracker</strong> class<a id="_idIndexMarker1125"/> embodies the <a id="_idIndexMarker1126"/>artificial intelligence component of A.R.E.S. Although it could be installed directly on the Raspberry Pi 3B+ within A.R.E.S. and accessed remotely via the streaming window application, we will install it on a computer alongside our streaming application for simplicity and improved performance. In our example, we will utilize a <span class="No-Break">Windows PC.</span></p>
<p>To create the <strong class="source-inline">DogTracker</strong> class, we must do <span class="No-Break">the following:</span></p>
<ol>
<li>We launch Thonny and source our <strong class="source-inline">ch14-env</strong> Python <span class="No-Break">virtual environment.</span></li>
<li>We create a new tab by selecting <strong class="bold">File</strong> and then <strong class="bold">New</strong> or by hitting <em class="italic">Ctrl</em> + <em class="italic">N</em> on <span class="No-Break">our keyboard.</span></li>
<li>Next, we add the <span class="No-Break">necessary imports:</span><pre class="source-code">
import cv2
import numpy as np</pre></li> <li>Then, we define our class and <span class="No-Break">initialization method:</span><pre class="source-code">
class DogDetector:
    def __init__(self, model_weights, model_cfg, class_file):
        self.net = cv2.dnn.readNet(model_weights, model_cfg)
        self.layer_names = self.net.getLayerNames()
        output_layer_indices = self.net.getUnconnectedOutLayers()
        if output_layer_indices.ndim == 1:
            self.output_layers = [self.layer_names[
i - 1] for i in output_layer_indices]
        else:
            self.output_layers = [self.layer_names[
i[0] - 1] for i in output_layer_indices]
        with open(class_file, "r") as f:
            self.classes = [line.strip() for line in f.readlines()]
        self.dog_class_id = self.classes.index("dog")</pre></li> <li>Now, we must define our only method: <strong class="source-inline">detect_dogs()</strong>. This method processes video frames to detect dogs using a YOLO neural network model. It begins by<a id="_idIndexMarker1127"/> resizing <a id="_idIndexMarker1128"/>the input frame for optimal processing and creates a blob from the resized image, which is then fed into the neural network. The network outputs detection results, which include bounding boxes, confidences, and class IDs for detected objects. The method checks each detection to see if it meets the confidence threshold and corresponds to the class ID for dogs. If such detections are found, it calculates and stores their bounding box coordinates. NMS is then applied to refine these bounding boxes by reducing overlaps. If any boxes remain after this process, it confirms the presence of dogs, draws these boxes on the frame, and labels them. Finally, the method returns the processed frame, along with a Boolean indicating whether any dogs <span class="No-Break">were detected:</span><pre class="source-code">
   def detect_dogs(self, frame):
        img_resized = cv2.resize(frame, None, fx=0.4, fy=0.4)
        height, width, channels = img_resized.shape
        dog_detected = False
         blob = cv2.dnn.blobFromImage(img_resized, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
        self.net.setInput(blob)
        outs = self.net.forward(self.output_layers)
        class_ids = []
        confidences = []
        boxes = []
        for out in outs:
            for detection in out:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]
                if confidence &gt; 0.5 and class_id ==
self.dog_class_id:
                    center_x = int(detection[0] * width)
                    center_y = int(detection[1] * height)
                    w = int(detection[2] * width)
                    h = int(detection[3] * height)
                    x = int(center_x - w / 2)
                    y = int(center_y - h / 2)
                    boxes.append([x, y, w, h])
                    confidences.append(float(confidence))
                    class_ids.append(class_id)
        indexes = cv2.dnn.NMSBoxes(boxes, confidences,
0.5, 0.4)
        if indexes is not None and len(indexes) &gt; 0:
            dog_detected = True
            indexes = indexes.flatten()
            for i in indexes:
                x, y, w, h = boxes[i]
                label = str(self.classes[class_ids[i]])
                cv2.rectangle(img_resized, (x, y), (x + w, y + h), (0, 255, 0), 2)
                cv2.putText(img_resized, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        return img_resized, dog_detected</pre></li> <li>We save the code with a descriptive name, such as <strong class="source-inline">DogDetector.py</strong>, in our <strong class="source-inline">Chapter14</strong> <span class="No-Break">project folder.</span></li>
</ol>
<p>Here, we reorganized the <strong class="source-inline">recognize-dog.py</strong> code from the previous section into a class that we will<a id="_idIndexMarker1129"/> use for our smart video <a id="_idIndexMarker1130"/>streamer. With this class in place, it’s time to create our streaming application. We will use OpenCV <span class="No-Break">for this.</span></p>
<h2 id="_idParaDest-235"><a id="_idTextAnchor238"/>Building a smart video streamer</h2>
<p>We will use <a id="_idIndexMarker1131"/>the <strong class="source-inline">detect_dogs()</strong> method <a id="_idIndexMarker1132"/>inside the <strong class="source-inline">DogDetector</strong> class to identify dogs from the video stream coming from A.R.E.S. As mentioned previously, we could easily change our code so that we can use YOLO to identify other objects. Recognizing dogs presents a fun way for those of us with dogs to program A.R.E.S. as a sort of pet detection robot. We will install our smart video streamer onto the same computer as our <span class="No-Break"><strong class="source-inline">DogDetector</strong></span><span class="No-Break"> class.</span></p>
<p>To create the smart video streamer, follow <span class="No-Break">these steps:</span></p>
<ol>
<li>We launch Thonny and source our <strong class="source-inline">ch14-env</strong> Python <span class="No-Break">virtual environment.</span></li>
<li>We create a new tab by selecting <strong class="bold">File</strong> and then <strong class="bold">New</strong> or by hitting <em class="italic">Ctrl</em> + <em class="italic">N</em> on <span class="No-Break">our keyboard.</span></li>
<li>We start by adding <span class="No-Break">our imports:</span><pre class="source-code">
import cv2
from DogDetector import DogDetector
import time</pre></li> <li>Then, we define our <span class="No-Break">variable declarations:</span><pre class="source-code">
detector = DogDetector("YOLO/yolov4.weights", "YOLO/yolov4.cfg", "YOLO/coco.names")
stream_url = '&lt;&lt;rtsp address&gt;&gt;'
cap = cv2.VideoCapture(stream_url)
cv2.namedWindow("Dog Detector", cv2.WINDOW_NORMAL)
cv2.resizeWindow("Dog Detector", 800, 600)
last_time = time.time()</pre></li> <li>The bulk of our code sits inside an infinite loop. This code continuously captures frames from a video source, checking if each frame is successfully retrieved. If a second passes since the last processed frame, it detects dogs in the current frame using the <strong class="source-inline">detect_dogs()</strong> method, updates the time marker, and displays the result; if the <strong class="source-inline">q</strong> key is pressed, the loop breaks, and the video capture and<a id="_idIndexMarker1133"/> any OpenCV <a id="_idIndexMarker1134"/>windows are cleanly released <span class="No-Break">and closed:</span><pre class="source-code">
try:
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        current_time = time.time()
        if current_time - last_time &gt;= 1.0:  # 1.0 seconds
            result_frame, dog_detected = detector.detect_dogs(frame)
            last_time = current_time
            cv2.imshow("Dog Detector", result_frame)
            if dog_detected:
                print('Dog detected!')
        if cv2.waitKey(1) &amp; 0xFF == ord('q'):
            break
finally:
    cap.release()
    cv2.destroyAllWindows()</pre></li> <li>We save the code with a descriptive name, such as <strong class="source-inline">smart-video-feed.py</strong>, in our <strong class="source-inline">Chapter14</strong> <span class="No-Break">project folder.</span></li>
<li>We run the <a id="_idIndexMarker1135"/>code by clicking <a id="_idIndexMarker1136"/>on the green run button, hitting <em class="italic">F5</em> on our keyboard, or clicking on the <strong class="bold">Run</strong> menu option at the top and then <strong class="bold">Run </strong><span class="No-Break"><strong class="bold">current script</strong></span><span class="No-Break">.</span><p class="list-inset">We should observe a pop-up window appear with a video feed from A.R.E.S. Our application should detect the presence of <span class="No-Break">a dog:</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer289">
<img alt="Figure 14.7 – Detecting a dog using our smart video streamer" height="790" src="image/B21282_14_7.jpg" width="998"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.7 – Detecting a dog using our smart video streamer</p>
<p>With this, we have successfully added artificial intelligence in the form of object detection to A.R.E.S. We may adjust the size, font, and color of the frame in our <strong class="source-inline">DogDetector</strong> class. As impressive as getting object detection to work with A.R.E.S. is, we will take this<a id="_idIndexMarker1137"/> a step further and<a id="_idIndexMarker1138"/> introduce text notification, turning the smart video streaming functionality into a true <span class="No-Break">IoT application.</span></p>
<h1 id="_idParaDest-236"><a id="_idTextAnchor239"/>Sending out a text alert</h1>
<p>To make A.R.E.S. a<a id="_idIndexMarker1139"/> true IoT device, we will add text functionality. This will give A.R.E.S. the ability to send out text alerts when an object of interest – in our case, a dog – is detected. We will use the Twilio service <span class="No-Break">for this.</span></p>
<p>We will start by setting up our Twilio account and testing the number we are assigned before we integrate text messaging functionality into A.R.E.S. We must ensure that we follow the upcoming<a id="_idIndexMarker1140"/> steps carefully so that we can set up our Twilio <span class="No-Break">account successfully.</span></p>
<h2 id="_idParaDest-237"><a id="_idTextAnchor240"/>Setting up our Twilio account</h2>
<p>Setting up a Twilio<a id="_idIndexMarker1141"/> account involves registering on their website, where <a id="_idIndexMarker1142"/>we’ll be provided with an account SID and an auth token to authenticate API requests. Once registered, we can also obtain a Twilio phone number, which is necessary for sending SMS messages and making calls through their service. In this section, we will set up our Twilio account and send a test <span class="No-Break">SMS message.</span></p>
<p>To set up our Twilio account, follow <span class="No-Break">these steps:</span></p>
<ol>
<li>Using a web browser, we navigate to <a href="https://www.twilio.com">www.twilio.com</a> and click the blue <strong class="bold">Start for </strong><span class="No-Break"><strong class="bold">free</strong></span><span class="No-Break"> button:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer290">
<img alt="Figure 14.8 – The Twilio website" height="111" src="image/B21282_14_8.jpg" width="350"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.8 – The Twilio website</p>
<ol>
<li value="2">This will take us to the <strong class="bold">Sign up</strong> page. Here, we can create a Twilio account or use a <span class="No-Break">Google account:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer291">
<img alt="Figure 14.9 – Twilio’s Sign up page" height="804" src="image/B21282_14_9.jpg" width="416"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.9 – Twilio’s Sign up page</p>
<ol>
<li value="3">To verify our<a id="_idIndexMarker1143"/> new account, we type in a phone number and <a id="_idIndexMarker1144"/>click on the blue <strong class="bold">Send code via </strong><span class="No-Break"><strong class="bold">SMS</strong></span><span class="No-Break"> button:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer292">
<img alt="Figure 14.10 – Verify page" height="400" src="image/B21282_14_10.jpg" width="812"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.10 – Verify page</p>
<ol>
<li value="4">We should receive a text message containing a verification code. We enter the number and click on the blue <span class="No-Break"><strong class="bold">Verify</strong></span><span class="No-Break"> button:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer293">
<img alt="Figure 14.11 – Verification code step" height="538" src="image/B21282_14_11.jpg" width="1010"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.11 – Verification code step</p>
<ol>
<li value="5">This will <a id="_idIndexMarker1145"/>take<a id="_idIndexMarker1146"/> us to the <strong class="bold">You’re all verified!</strong> page, which will provide us with a <strong class="bold">Recovery code</strong> value. We click on the blue <strong class="bold">Continue</strong> button to go to the <span class="No-Break">next page:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer294">
<img alt="Figure 14.12 – Recovery code" height="481" src="image/B21282_14_12.jpg" width="1109"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.12 – Recovery code</p>
<ol>
<li value="6">The next page <a id="_idIndexMarker1147"/>allows us to customize our <span class="No-Break">Twilio experience:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer295">
<img alt="Figure 14.13 – Customizing Twilio" height="990" src="image/B21282_14_13.jpg" width="659"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.13 – Customizing Twilio</p>
<ol>
<li value="7">The dashboard <a id="_idIndexMarker1148"/>screen allows us to get a Twilio phone number. We <a id="_idIndexMarker1149"/>click on the blue <strong class="bold">Get phone number</strong> button <span class="No-Break">to continue.</span></li>
<li>The next page provides us with a Twilio phone number we can use. We click on the blue <strong class="bold">Next</strong> button <span class="No-Break">to proceed.</span></li>
<li>On the next screen, we can test out our new Twilio number. The <strong class="bold">To phone number</strong> field and the <strong class="bold">From phone number</strong> field should be prepopulated with the number <a id="_idIndexMarker1150"/>we provided and our new Tilio number, respectively. To <a id="_idIndexMarker1151"/>test out our new number, type a message in the <strong class="bold">Body</strong> field, such as <strong class="source-inline">IoT test</strong>, and click on the blue <strong class="bold">Send test </strong><span class="No-Break"><strong class="bold">SMS</strong></span><span class="No-Break"> button:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer296">
<img alt="Figure 14.14 – Testing our new Twilio phone number" height="380" src="image/B21282_14_14.jpg" width="756"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.14 – Testing our new Twilio phone number</p>
<ol>
<li value="10">We should receive a text message with our test message, <strong class="bold">IoT test</strong>, on the phone we provided the number to <span class="No-Break">Twilio on:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer297">
<img alt="Figure 14.15 – Test message successfully received, as seen on a cell phone" height="984" src="image/B21282_14_15.jpg" width="642"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.15 – Test message successfully received, as seen on a cell phone</p>
<p>With our Twilio <a id="_idIndexMarker1152"/>account set up and our phone number tested, we are ready <a id="_idIndexMarker1153"/>to incorporate text messaging <span class="No-Break">into A.R.E.S.</span></p>
<h2 id="_idParaDest-238"><a id="_idTextAnchor241"/>Adding text message functionality to A.R.E.S.</h2>
<p>To integrate<a id="_idIndexMarker1154"/> text messaging<a id="_idIndexMarker1155"/> functionality into A.R.E.S., we will develop a new class named <strong class="source-inline">TwilioMessage</strong> and a new version of the <span class="No-Break"><strong class="source-inline">smart-video-feed.py</strong></span><span class="No-Break"> script:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer298">
<img alt="Figure 14.16 – Adding text functionality to A.R.E.S." height="419" src="image/B21282_14_16.jpg" width="1408"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.16 – Adding text functionality to A.R.E.S.</p>
<p>The <strong class="source-inline">TwilioMessage</strong> class will encapsulate communication to the Twilio server. As shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.16</em>, our new <strong class="source-inline">TwilioMessage</strong> class is called from our smart video streamer and <a id="_idIndexMarker1156"/>sends <a id="_idIndexMarker1157"/>out <span class="No-Break">text messages.</span></p>
<p>We will start by creating <span class="No-Break">this class.</span></p>
<h3>Creating the TwilioMessage class</h3>
<p>To create<a id="_idIndexMarker1158"/> the <strong class="source-inline">TwilioMessage</strong> class, we<a id="_idIndexMarker1159"/> must do <span class="No-Break">the following:</span></p>
<ol>
<li>We launch Thonny and source our <strong class="source-inline">ch14-env</strong> Python <span class="No-Break">virtual environment.</span></li>
<li>As we require a library from Twilio to make our code work, we will install it in our Python virtual environment. To do so, open the system shell by clicking on <strong class="bold">Tools</strong> | <strong class="bold">Open system shell…</strong> <span class="No-Break">in Thonny:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer299">
<img alt="Figure 14.17 – Open system shell…" height="223" src="image/B21282_14_17.jpg" width="675"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.17 – Open system shell…</p>
<ol>
<li value="3">At the command prompt, we execute the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">pip install twilio</strong></pre></li> <li>Once the library has been installed, we close <span class="No-Break">the terminal.</span></li>
<li>We create a new tab by selecting <strong class="bold">File</strong> and then <strong class="bold">New</strong> or by hitting <em class="italic">Ctrl</em> + <em class="italic">N</em> on <span class="No-Break">our keyboard.</span></li>
<li>We add the<a id="_idIndexMarker1160"/> following<a id="_idIndexMarker1161"/> code to <span class="No-Break">the editor:</span><pre class="source-code">
from twilio.rest import Client
class TwilioMessage:
    def __init__(self, account_sid, auth_token, from_number):
        self.client = Client(account_sid, auth_token)
        self.from_number = from_number
    def send_sms(self, to_number, message):
        sms = self.client.messages.create(
            body=message,
            from_=self.from_number,
            to=to_number
        )
        print(f"Message sent with SID: {sms.sid}")
if __name__ == "__main__":
    twilio_message = TwilioMessage(
' account_sid', ' auth_token', '+twilio_number')
    twilio_message.send_sms('+our_number', 'Hello from A.R.E.S.')</pre><p class="list-inset">Let’s take a closer look at <span class="No-Break">our code:</span></p><ol><li class="upper-roman">First, we import the Twilio client and define the <span class="No-Break"><strong class="source-inline">TwilioMessage</strong></span><span class="No-Break"> class.</span></li><li class="upper-roman">Then, we initialize our class with Twilio credentials (account SID, auth token, and Twilio <span class="No-Break">phone number).</span></li><li class="upper-roman">The <strong class="source-inline">send_sms()</strong> method sends an SMS to a specified number and prints the message SID <span class="No-Break">after sending.</span></li><li class="upper-roman">In the main<a id="_idIndexMarker1162"/> execution<a id="_idIndexMarker1163"/> block, an instance of <strong class="source-inline">TwilioMessage</strong> is created with Twilio credentials, and a test SMS is sent to <span class="No-Break">our number.</span></li></ol></li> <li>We save the code with a descriptive name, such as <strong class="source-inline">TwilioMessage.py</strong>, in our <strong class="source-inline">Chapter14</strong> <span class="No-Break">project folder.</span></li>
<li>We run the code by clicking on the green run button, hitting <em class="italic">F5</em> on our keyboard, or clicking on the <strong class="bold">Run</strong> menu option at the top and then <strong class="bold">Run </strong><span class="No-Break"><strong class="bold">current script</strong></span><span class="No-Break">.</span></li>
<li>We should receive a text message saying, <strong class="source-inline">Hello from A.R.E.S.</strong> on our <span class="No-Break">cell phone.</span></li>
</ol>
<p>With the <strong class="source-inline">TwilioMessage</strong> class created, it is time to modify the smart video streamer code so that text<a id="_idIndexMarker1164"/> messages <a id="_idIndexMarker1165"/>will be sent when a dog or dogs are detected. We will do that in the <span class="No-Break">next section.</span></p>
<h3>Modifying the smart video streamer</h3>
<p>The final step in<a id="_idIndexMarker1166"/> providing <a id="_idIndexMarker1167"/>text message functionality in A.R.E.S. is to create a new smart video streamer script to send a text message when a dog is detected. To limit the number of messages sent, we will increase the time between frames to <span class="No-Break">5 seconds.</span></p>
<p>To modify our smart video streamer, follow <span class="No-Break">these steps:</span></p>
<ol>
<li>In Thonny, we create a new tab by selecting <strong class="bold">File</strong> and then <strong class="bold">New</strong> or by hitting <em class="italic">Ctrl</em> + <em class="italic">N</em> on <span class="No-Break">our keyboard.</span></li>
<li>We add the following code to <span class="No-Break">the editor:</span><pre class="source-code">
import cv2
from DogDetector import DogDetector
from TwilioMessage import TwilioMessage
import time
detector = DogDetector("YOLO/yolov4.weights", "YOLO/yolov4.cfg", "YOLO/coco.names")
twilio_message = TwilioMessage('account_sid', 'auth_token', '+twilio_number')
stream_url = '&lt;&lt;rtsp address&gt;&gt;'
cap = cv2.VideoCapture(stream_url)
cv2.namedWindow("Dog Detector", cv2.WINDOW_NORMAL)
cv2.resizeWindow("Dog Detector", 800, 600)
last_time = time.time()
try:
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        # Check if 1 second has passed
        current_time = time.time()
        if current_time - last_time &gt;= 5.0:
            result_frame, dog_detected = detector.detect_dogs(frame)
            last_time = current_time
            cv2.imshow("Dog Detector", result_frame)
            if dog_detected:
                twilio_message.send_sms(' +phone_num', 'Dog(s) detected!')
        if cv2.waitKey(1) &amp; 0xFF == ord('q'):
            break
finally:
    cap.release()
    cv2.destroyAllWindows()</pre></li> <li>We save the code with a descriptive name, such as <strong class="source-inline">smart-video-sms.py</strong>, in our <strong class="source-inline">Chapter14</strong> <span class="No-Break">project folder.</span></li>
<li>We run the <a id="_idIndexMarker1168"/>code by <a id="_idIndexMarker1169"/>clicking on the green run button, hitting <em class="italic">F5</em> on n our keyboard, or clicking on the <strong class="bold">Run</strong> menu option at the top and then <strong class="bold">Run </strong><span class="No-Break"><strong class="bold">current script</strong></span><span class="No-Break">.</span></li>
<li>We should observe a window appear with a video feed coming from A.R.E.S. that provides object detection functionality when a dog <span class="No-Break">is present.</span></li>
<li>We should receive a text message once a dog has <span class="No-Break">been detected:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer300">
<img alt="Figure 14.18 – A text message indicating that a dog or dogs were detected" height="1142" src="image/B21282_14_18.jpg" width="761"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.18 – A text message indicating that a dog or dogs were detected</p>
<p>With that, we <a id="_idIndexMarker1170"/>have <a id="_idIndexMarker1171"/>successfully added text message functionality to A.R.E.S. to alert us when an object we are interested in – in this case, a dog – <span class="No-Break">is detected.</span></p>
<h1 id="_idParaDest-239"><a id="_idTextAnchor242"/>Summary</h1>
<p>In this chapter, we explored the field of computer vision and successfully integrated it into our A.R.E.S. robot car. By incorporating this technology, A.R.E.S. can now process and interpret visual data. We also added text messaging functionality to A.R.E.S., turning our robot car into a true <span class="No-Break">IoT device.</span></p>
<p>Although not implemented, we could easily imagine how we would take A.R.E.S. to the next level by incorporating obstacle avoidance based on the data we get back from YOLO. We may also imagine how we could make A.R.E.S. follow a certain object <span class="No-Break">if desired.</span></p>
<p>This chapter marks the end of our IoT journey together. Throughout, we have explored the world of IoT while utilizing and building our programming skills – from using the Sense HAT to serve up web services data to utilizing LoRa for long-range communication to implementing advanced features such as computer vision in a robot car we control using the internet. This adventure has not only taught us technical know-how but also showed us the potential of IoT to innovate and solve any real-world challenges in <span class="No-Break">the future.</span></p>
<p>It’s been <span class="No-Break">a pleasure.</span></p>
</div>
</div></body></html>