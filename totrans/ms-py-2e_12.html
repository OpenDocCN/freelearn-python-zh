<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer226" class="Basic-Text-Frame">&#13;
    <h1 class="chapterNumber">12</h1>&#13;
    <h1 id="_idParaDest-324" class="chapterTitle">Performance – Tracking and Reducing Your Memory and CPU Usage</h1>&#13;
    <p class="normal">Before we talk about performance, there is a quote by <em class="italic">Donald Knuth</em> you need to consider first:</p>&#13;
    <blockquote class="packt_quote">&#13;
      <p class="quote"> “The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming”.</p>&#13;
    </blockquote>&#13;
    <div class="note">&#13;
      <p class="normal">Donald Knuth is often called the father of algorithm analysis. His book series, <em class="italic">The Art of Computer Programming</em>, can be considered the Bible of all fundamental algorithms.</p>&#13;
    </div>&#13;
    <p class="normal">As long as you pick the correct data structures with the right algorithms, performance should not be something to worry about. That does not mean you should ignore performance entirely, but just make sure you pick the right battles and optimize only when it is actually needed. Micro/premature optimizations can definitely be fun, but are only very rarely useful.</p>&#13;
    <p class="normal">We have seen the performance characteristics of many data structures in <em class="chapterRef">Chapter 2</em>, <em class="italic">Pythonic Syntax and Common Pitfalls</em>, already, so we won’t discuss that, but we will show you how performance can be measured and how problems can be detected. There are cases where micro optimizations make a difference, but you won’t know until you measure the performance.</p>&#13;
    <p class="normal">Within this chapter, we will cover:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Profiling CPU usage</li>&#13;
      <li class="bulletList">Profiling memory usage</li>&#13;
      <li class="bulletList">Learning how to correctly compare performance metrics</li>&#13;
      <li class="bulletList">Optimizing performance</li>&#13;
      <li class="bulletList">Finding and fixing memory leaks</li>&#13;
    </ul>&#13;
    <p class="normal">Globally, the chapter is split between CPU usage and/or CPU time, and memory usage. The first half of the chapter mainly concerns CPU/time; the second half covers memory usage.</p>&#13;
    <h1 id="_idParaDest-325" class="heading-1">What is performance?</h1>&#13;
    <p class="normal">Performance <a id="_idIndexMarker930"/>is a very broad term. It has many different meanings and, in many cases, it is defined incorrectly. Within this chapter, we will attempt to measure and improve performance in terms of CPU usage/time and memory usage. Many of the examples here are a trade-off between execution time and memory usage. Note that a fast algorithm that can only use a single CPU core can be outperformed in terms of execution time by a slower algorithm that is easily parallelizable given enough CPU cores.</p>&#13;
    <p class="normal">When it comes to incorrect statements about performance, you have probably heard statements similar to “Language X is faster than Python.” That statement is inherently wrong. Python is neither fast nor slow; Python is a programming language, and a language has no performance metrics whatsoever. If you were to say that the CPython interpreter is faster or slower than interpreter Y for language X, that would be possible. The performance characteristics of code can vary greatly between different interpreters. Just take a look at this small test (which uses ZSH shell script):</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> <span class="hljs-con-built_in">export</span> SCRIPT=<span class="hljs-con-string">'"".join(str(i) for i in range(10000))'</span>&#13;
&#13;
<span class="hljs-con-meta">$</span> <span class="hljs-con-keyword">for</span> p <span class="hljs-con-keyword">in</span> pypy3 pyston python3.{8..10}; <span class="hljs-con-keyword">do</span> <span class="hljs-con-built_in">echo</span> -n <span class="hljs-con-string">"</span><span class="hljs-con-variable">$p</span><span class="hljs-con-string">: "</span>; <span class="hljs-con-variable">$p</span> -m timeit <span class="hljs-con-string">"</span><span class="hljs-con-variable">$SCRIPT</span><span class="hljs-con-string">"</span>; <span class="hljs-con-keyword">done</span>&#13;
pypy3: ... 2000 loops, average of 7: 179 +- 6.05 usec per loop ...&#13;
pyston: 500 loops, best of 5: 817 usec per loop&#13;
python3.8: 200 loops, best of 5: 1.21 msec per loop&#13;
python3.9: 200 loops, best of 5: 1.64 msec per loop&#13;
python3.10: 200 loops, best of 5: 1.14 msec per loop&#13;
</code></pre>&#13;
    <p class="normal">Five different Python interpreters, each with a different performance! All are Python, but the interpreters obviously vary.</p>&#13;
    <div class="packt_tip">&#13;
      <p class="normal">You might not have heard of the PyPy3 and Pyston interpreters yet.</p>&#13;
      <p class="normal">The PyPy3 interpreter is an alternative Python interpreter that uses JIT (Just-In-Time) compiling to perform much better than CPython in many, but certainly not all, cases. The big caveat of PyPy3 is that code that has speedups in C and depends on CPython extensions (which is a large portion of performance-critical libraries) either does not support PyPy3 or suffers a performance hit.</p>&#13;
      <p class="normal">Pyston attempts to be a drop-in replacement for CPython with JIT compiling added to it. While JIT compiling might be added to CPython pretty soon, as of Python 3.10, that is not the case yet. This is why Pyston can offer a great performance benefit over CPython. The downside is that it is currently only supported on Unix/Linux systems.</p>&#13;
    </div>&#13;
    <p class="normal">Looking<a id="_idIndexMarker931"/> at this benchmark, you might be tempted to drop the CPython interpreter completely and only use PyPy3. The danger with benchmarks such as these is that they rarely offer any meaningful results. For this limited example, the Pypy interpreter was about 200 times faster than the CPython3.10 interpreter, but that has very little relevance for the general case. The only conclusion that can safely be drawn here is that this specific version of the PyPy3 interpreter is much faster than this specific version of CPython3 <strong class="keyWord">for this exact test</strong>. For any other test and interpreter version, the results could be vastly different.</p>&#13;
    <h1 id="_idParaDest-326" class="heading-1">Measuring CPU performance and execution time</h1>&#13;
    <p class="normal">When talking <a id="_idIndexMarker932"/>about performance you can measure a great number of things. When it comes to CPU performance, we can measure:</p>&#13;
    <ul>&#13;
      <li class="bulletList">The “wall time” (the absolute time on the clock).</li>&#13;
      <li class="bulletList">Relative time (when comparing multiple runs or multiple functions)</li>&#13;
      <li class="bulletList">Used CPU time. Due to multithreading, multiprocessing, or asynchronous processing, this can be vastly different from the wall time.</li>&#13;
      <li class="bulletList">When inspecting really low-level performance, measuring the number of CPU cycles and loop counts.</li>&#13;
    </ul>&#13;
    <p class="normal">In addition<a id="_idIndexMarker933"/> to all these different measurement options, you should also consider the observer effect. Simply put, measuring takes time, and depending on how you are measuring the performance, the impact can be huge.</p>&#13;
    <p class="normal">Within this section, we will be exploring several methods to inspect the CPU performance and execution time of your code. Tricks to improve your performance after measuring will come later in the chapter.</p>&#13;
    <h2 id="_idParaDest-327" class="heading-2">Timeit – comparing code snippet performance</h2>&#13;
    <p class="normal">Before we <a id="_idIndexMarker934"/>can start improving execution/CPU times, we need a reliable method to measure them. Python has a really nice module (<code class="inlineCode">timeit</code>) with the specific purpose <a id="_idIndexMarker935"/>of measuring the execution times of bits of code. It executes a bit of code many times to make sure there is as little variation as possible and to make the measurement fairly clean. It’s very useful if you want to compare a few code snippets. Following are some example executions:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 -m timeit <span class="hljs-con-string">'x=[]; [x.insert(0, i) for i in range(10000)]'</span>&#13;
10 loops, best of 3: 30.2 msec per loop&#13;
<span class="hljs-con-meta">$</span> python3 -m timeit <span class="hljs-con-string">'x=[]; [x.append(i) for i in range(10000)]'</span>&#13;
1000 loops, best of 3: 1.01 msec per loop&#13;
<span class="hljs-con-meta">$</span> python3 -m timeit <span class="hljs-con-string">'x=[i for i in range(10000)]'</span>&#13;
1000 loops, best of 3: 381 usec per loop&#13;
<span class="hljs-con-meta">$</span> python3 -m timeit <span class="hljs-con-string">'x=list(range(10000))'</span>&#13;
10000 loops, best of 3: 212 usec per loop&#13;
</code></pre>&#13;
    <p class="normal">These few examples demonstrate the performance difference between <code class="inlineCode">list.insert</code>, <code class="inlineCode">list.append</code>, a list comprehension, and the <code class="inlineCode">list</code> function. As we have seen in <em class="chapterRef">Chapter 4</em>, doing <code class="inlineCode">list.insert</code> is very inefficient and that quickly shows here, in this case being 30 times slower than <code class="inlineCode">list.append</code>.</p>&#13;
    <p class="normal">More importantly, however, the code demonstrates how we can use the <code class="inlineCode">timeit</code> module and how it works. As you can see in the output, the <code class="inlineCode">list.append</code> variant was executed only <code class="inlineCode">10</code> times, whereas the <code class="inlineCode">list</code> call was executed <code class="inlineCode">10000</code> times. That is one of the most convenient features of the <code class="inlineCode">timeit</code> module: it automatically figures out some useful parameters for you, and it shows the “best of 3” to try and reduce the amount of variance in your tests.</p>&#13;
    <div class="note">&#13;
      <p class="normal">The <code class="inlineCode">timeit</code> module is great at comparing the performance of similar bits of code within a code base. Comparing the execution time between different Python interpreters using <code class="inlineCode">timeit</code> is generally useless because it is rarely representative of the performance of your whole application.</p>&#13;
    </div>&#13;
    <p class="normal">Naturally, the <a id="_idIndexMarker936"/>command can be used with regular scripts as well, but that won’t automatically determine the <a id="_idIndexMarker937"/>number of repetitions like the command-line interface does. So we will have to do that ourselves:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> timeit&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">test_list</span><span class="hljs-function">():</span>&#13;
    <span class="hljs-keyword">return</span> <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>))&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">test_list_comprehension</span><span class="hljs-function">():</span>&#13;
    <span class="hljs-keyword">return</span> [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>)]&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">test_append</span><span class="hljs-function">():</span>&#13;
    x = []&#13;
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>):&#13;
        x.append(i)&#13;
&#13;
    <span class="hljs-keyword">return</span> x&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">test_insert</span><span class="hljs-function">():</span>&#13;
    x = []&#13;
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>):&#13;
        x.insert(<span class="hljs-number">0</span>, i)&#13;
&#13;
    <span class="hljs-keyword">return</span> x&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">benchmark</span><span class="hljs-function">(</span><span class="hljs-params">function, number=</span><span class="hljs-number">100</span><span class="hljs-params">, repeat=</span><span class="hljs-number">10</span><span class="hljs-function">):</span>&#13;
    <span class="hljs-comment"># Measure the execution times. Passing the globals() is an</span>&#13;
    <span class="hljs-comment"># easy way to make the functions available.</span>&#13;
    times = timeit.repeat(function, number=number,&#13;
                          <span class="hljs-built_in">globals</span>=<span class="hljs-built_in">globals</span>())&#13;
    <span class="hljs-comment"># The repeat function gives 'repeat' results so we take the</span>&#13;
    <span class="hljs-comment"># min() and divide it by the number of runs</span>&#13;
    time = <span class="hljs-built_in">min</span>(times) / number&#13;
    print(<span class="hljs-string">f'</span><span class="hljs-subst">{number}</span><span class="hljs-string"> loops, best of </span><span class="hljs-subst">{repeat}</span><span class="hljs-string">: </span><span class="hljs-subst">{time:</span><span class="hljs-number">9.6</span><span class="hljs-subst">f}</span><span class="hljs-string">s :: '</span>,&#13;
          function.__name__)&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    benchmark(test_list)&#13;
    benchmark(test_list_comprehension)&#13;
    benchmark(test_append)&#13;
    benchmark(test_insert) &#13;
</code></pre>&#13;
    <p class="normal">When executing this, you will get something along the following lines:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_00_timeit.py&#13;
100 loops, best of 10:  0.000168s ::  test_list&#13;
100 loops, best of 10:  0.000322s ::  test_list_comprehension&#13;
100 loops, best of 10:  0.000573s ::  test_append&#13;
100 loops, best of 10:  0.027552s ::  test_insert&#13;
</code></pre>&#13;
    <p class="normal">As you <a id="_idIndexMarker938"/>may have noticed, this script is still a bit basic. While the command-line version of <code class="inlineCode">timeit</code> keeps trying until it <a id="_idIndexMarker939"/>reaches 0.2 seconds or more, this script just has a fixed number of executions. Since Python 3.6, we do have the option of using <code class="inlineCode">timeit.Timer.autorange</code> to replicate this behavior, but it is a bit less convenient to use and would produce a lot more output in our current case. Depending on your use case, however, it could be useful to try this benchmark code instead:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">autorange_benchmark</span><span class="hljs-function">(</span><span class="hljs-params">function</span><span class="hljs-function">):</span>&#13;
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">print_result</span><span class="hljs-function">(</span><span class="hljs-params">number, time_taken</span><span class="hljs-function">):</span>&#13;
        <span class="hljs-comment"># The autorange function keeps trying until the total</span>&#13;
        <span class="hljs-comment"># runtime (time_taken) reaches 0.2 seconds. To get the</span>&#13;
        <span class="hljs-comment"># time per run we need to divide it by the number of runs</span>&#13;
        time = time_taken / number&#13;
        name = function.__name__&#13;
        print(<span class="hljs-string">f'</span><span class="hljs-subst">{number}</span><span class="hljs-string"> loops, average: </span><span class="hljs-subst">{time:</span><span class="hljs-number">9.6</span><span class="hljs-subst">f}</span><span class="hljs-string">s :: </span><span class="hljs-subst">{name}</span><span class="hljs-string">'</span>)&#13;
&#13;
    <span class="hljs-comment"># Measure the execution times. Passing the globals() is an</span>&#13;
    <span class="hljs-comment"># easy way to make the functions available.</span>&#13;
    timer = timeit.Timer(function, <span class="hljs-built_in">globals</span>=<span class="hljs-built_in">globals</span>())&#13;
    timer.autorange(print_result)&#13;
</code></pre>&#13;
    <p class="normal">If you want to use <code class="inlineCode">timeit</code> interactively, I would recommend using IPython, since it has a magic <code class="inlineCode">%timeit</code> command that shows even more useful output:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> ipython&#13;
In [1]: %timeit x=[]; [x.insert(0, i) for i in range(100000)]&#13;
2.5 s ± 112 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)&#13;
&#13;
In [2]: %timeit x=[]; [x.append(i) for i in range(100000)]&#13;
6.67 ms ± 252 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)&#13;
</code></pre>&#13;
    <p class="normal">In this <a id="_idIndexMarker940"/>case, IPython <a id="_idIndexMarker941"/>automatically takes care of the string wrapping and passing of <code class="inlineCode">globals()</code>. Still, this is all very limited and useful only for comparing multiple methods of doing the same thing. When it comes to full Python applications, there are more methods available, as we will see later in this chapter.</p>&#13;
    <div class="packt_tip">&#13;
      <p class="normal">To view the source of both IPython functions and regular modules, entering <code class="inlineCode">object??</code> in the IPython shell returns the source. In this case, just enter <code class="inlineCode">timeit??</code> to view the <code class="inlineCode">timeit</code> IPython function definition.</p>&#13;
    </div>&#13;
    <p class="normal">The easiest way you can implement a function similar to the <code class="inlineCode">%timeit</code> function is to call <code class="inlineCode">timeit.main</code>:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> timeit&#13;
&#13;
timeit.main(args=[<span class="hljs-string">'[x for x in range(1000000)]'</span>])&#13;
</code></pre>&#13;
    <p class="normal">This effectively does the same as:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 -m timeit <span class="hljs-con-string">'[x for x in range(1000000)]'</span>&#13;
</code></pre>&#13;
    <p class="normal">The internals of the <code class="inlineCode">timeit</code> module are nothing too special, but take care to minimize a few sources of inaccuracy, such as the setup and the teardown code. Additionally, the module reports the fastest run because other processes on your system can interfere with the measurement.</p>&#13;
    <p class="normal">A basic version can be implemented with a few calls to <code class="inlineCode">time.perf_counter</code> (the highest resolution timer available in Python), which is also used by <code class="inlineCode">timeit</code> internally. The <code class="inlineCode">timeit.default_timer</code> function is simply a reference to <code class="inlineCode">time.perf_counter</code>. This basic implementation of the <code class="inlineCode">timeit</code> function<a id="_idIndexMarker942"/> is comparable<a id="_idIndexMarker943"/> to the internals of the <code class="inlineCode">timeit</code> module:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gc&#13;
<span class="hljs-keyword">import</span> time&#13;
<span class="hljs-keyword">import</span> functools&#13;
&#13;
<span class="hljs-keyword">assert</span> time&#13;
&#13;
TIMEIT_TEMPLATE = <span class="hljs-string">'''</span>&#13;
<span class="hljs-string">def run(number):</span>&#13;
<span class="hljs-string">    {setup}</span>&#13;
<span class="hljs-string">    start = time.perf_counter()</span>&#13;
<span class="hljs-string">    for i in range(number):</span>&#13;
<span class="hljs-string">        {statement}</span>&#13;
<span class="hljs-string">    stop = time.perf_counter()</span>&#13;
<span class="hljs-string">    return stop - start</span>&#13;
<span class="hljs-string">'''</span>&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">timeit</span><span class="hljs-function">(</span><span class="hljs-params">statement, setup=</span><span class="hljs-string">''</span><span class="hljs-params">, number=</span><span class="hljs-number">1000000</span><span class="hljs-params">, globals_=</span><span class="hljs-literal">None</span><span class="hljs-function">):</span>&#13;
    <span class="hljs-comment"># Get or create globals</span>&#13;
    globals_ = <span class="hljs-built_in">globals</span>() <span class="hljs-keyword">if</span> globals_ <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> globals_&#13;
&#13;
    <span class="hljs-comment"># Create the test code so we can separate the namespace</span>&#13;
    src = TIMEIT_TEMPLATE.<span class="hljs-built_in">format</span>(&#13;
        statement=statement,&#13;
        setup=setup,&#13;
        number=number,&#13;
    )&#13;
    <span class="hljs-comment"># Compile the source</span>&#13;
    code = <span class="hljs-built_in">compile</span>(src, <span class="hljs-string">'&lt;source&gt;'</span>, <span class="hljs-string">'exec'</span>)&#13;
&#13;
    <span class="hljs-comment"># Define locals for the benchmarked code</span>&#13;
    locals_ = {}&#13;
&#13;
    <span class="hljs-comment"># Execute the code so we can get the benchmark fuction</span>&#13;
    exec(code, globals_, locals_)&#13;
&#13;
    <span class="hljs-comment"># Get the run function from locals() which was added by 'exec'</span>&#13;
    run = functools.partial(locals_[<span class="hljs-string">'run'</span>], number=number)&#13;
&#13;
    <span class="hljs-comment"># Disable garbage collection to prevent skewing results</span>&#13;
    gc.disable()&#13;
    <span class="hljs-keyword">try</span>:&#13;
        result = run()&#13;
    <span class="hljs-keyword">finally</span>:&#13;
        gc.enable()&#13;
&#13;
    <span class="hljs-keyword">return</span> result&#13;
</code></pre>&#13;
    <p class="normal">The actual <code class="inlineCode">timeit</code> code is a bit more advanced in terms of checking the input, but this example <a id="_idIndexMarker944"/>roughly shows how the <code class="inlineCode">timeit.timeit</code> function can be implemented, including several of the features added<a id="_idIndexMarker945"/> for more precision:</p>&#13;
    <ul>&#13;
      <li class="bulletList">First, we can see that the code has a <code class="inlineCode">number</code> parameter that defaults to 1 million. This has been done to reduce the result variance a little, as we will see when running the code.</li>&#13;
      <li class="bulletList">Second, the code disables the Python garbage collector so we don’t get any slowdowns from Python deciding to clean up its memory.</li>&#13;
    </ul>&#13;
    <p class="normal">When we actually call this code, we will see why a high value for <code class="inlineCode">number</code> can be important:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span> T_02_custom_timeit <span class="hljs-con-keyword">import</span> timeit&#13;
&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> statement = <span class="hljs-con-string">'[x for x in range(100)]'</span>&#13;
&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> print(<span class="hljs-con-string">'{:.7f}'</span>.<span class="hljs-con-built_in">format</span>(timeit(statement, number=<span class="hljs-con-number">1</span>)))&#13;
0.0000064&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> print(<span class="hljs-con-string">'{:.7f}'</span>.<span class="hljs-con-built_in">format</span>(timeit(statement) / <span class="hljs-con-number">1000000</span>))&#13;
0.0000029&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> print(<span class="hljs-con-string">'{:.7f}'</span>.<span class="hljs-con-built_in">format</span>(timeit(statement, number=<span class="hljs-con-number">1</span>)))&#13;
0.0000287&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> print(<span class="hljs-con-string">'{:.7f}'</span>.<span class="hljs-con-built_in">format</span>(timeit(statement) / <span class="hljs-con-number">1000000</span>))&#13;
0.0000029&#13;
</code></pre>&#13;
    <p class="normal">Even though we called the exact same code each time, the single repetition took more than two times as long in the first run and more than 10 times as long in the second run compared to the 1 million repetitions version. To make your results more consistent and reliable between runs, it is always good to repeat your tests several times and <code class="inlineCode">timeit</code> can certainly help with that.</p>&#13;
    <p class="normal">The <code class="inlineCode">timeit.repeat</code> function simply calls the <code class="inlineCode">timeit.timeit</code> function several times and can be emulated using a list comprehension:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">[timeit(statement) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(repeat)]&#13;
</code></pre>&#13;
    <p class="normal">Now that we know how to test simple code statements, let’s look at how to find slow statements in our code.</p>&#13;
    <h2 id="_idParaDest-328" class="heading-2">cProfile – Finding the slowest components</h2>&#13;
    <p class="normal">The <code class="inlineCode">profile</code> and <code class="inlineCode">cProfile</code> modules <a id="_idIndexMarker946"/>make it easily possible to analyze the relative CPU cycles used in a script/application. Be very careful not to compare these with the results from the <code class="inlineCode">timeit</code> module. The <code class="inlineCode">timeit</code> module tries as best as possible to give an accurate benchmark of the <strong class="keyWord">absolute</strong> amount of time it takes to execute a code snippet; the <code class="inlineCode">profile</code>/<code class="inlineCode">cProfile</code> modules are only useful for <strong class="keyWord">relative</strong> results because profiling increases the runtime. There are ways to make the results more accurate, but more about that later.</p>&#13;
    <div class="note">&#13;
      <p class="normal">The <code class="inlineCode">profile</code> and <code class="inlineCode">cProfile</code> modules offer the exact same interface, but the latter is written in C and is much faster. I would recommend using <code class="inlineCode">cProfile</code> if it is available on your system. If not, you can safely replace any occurrence of <code class="inlineCode">cProfile</code> with <code class="inlineCode">profile</code> in the following examples.</p>&#13;
    </div>&#13;
    <h3 id="_idParaDest-329" class="heading-3">First profiling run</h3>&#13;
    <p class="normal">Let’s profile our <a id="_idIndexMarker947"/>Fibonacci function from <em class="chapterRef">Chapter 6</em>, <em class="italic">Decorators – Enabling Code Reuse by Decorating</em>, both with and without the cache function. First, the code:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> sys&#13;
<span class="hljs-keyword">import</span> functools&#13;
&#13;
<span class="hljs-meta">@functools.lru_cache()</span>&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">fibonacci_cached</span><span class="hljs-function">(</span><span class="hljs-params">n</span><span class="hljs-function">):</span>&#13;
    <span class="hljs-keyword">if</span> n &lt; <span class="hljs-number">2</span>:&#13;
        <span class="hljs-keyword">return</span> n&#13;
    <span class="hljs-keyword">else</span>:&#13;
        <span class="hljs-keyword">return</span> fibonacci_cached(n - <span class="hljs-number">1</span>) + fibonacci_cached(n - <span class="hljs-number">2</span>)&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">fibonacci</span><span class="hljs-function">(</span><span class="hljs-params">n</span><span class="hljs-function">):</span>&#13;
    <span class="hljs-keyword">if</span> n &lt; <span class="hljs-number">2</span>:&#13;
        <span class="hljs-keyword">return</span> n&#13;
    <span class="hljs-keyword">else</span>:&#13;
        <span class="hljs-keyword">return</span> fibonacci(n - <span class="hljs-number">1</span>) + fibonacci(n - <span class="hljs-number">2</span>)&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    n = <span class="hljs-number">30</span>&#13;
    <span class="hljs-keyword">if</span> sys.argv[-<span class="hljs-number">1</span>] == <span class="hljs-string">'cache'</span>:&#13;
        fibonacci_cached(n)&#13;
    <span class="hljs-keyword">else</span>:&#13;
        fibonacci(n)&#13;
</code></pre>&#13;
    <div class="note">&#13;
      <p class="normal">For the sake of readability, all <code class="inlineCode">cProfile</code> statistics will be stripped of the <code class="inlineCode">percall</code> columns in all <code class="inlineCode">cProfile</code> outputs. These columns contain the duration per function call, which is irrelevant for these examples since they will be either 0 or identical to the <code class="inlineCode">cumtime</code> (cumulative time) column in nearly all cases.</p>&#13;
    </div>&#13;
    <p class="normal">First, we’ll <a id="_idIndexMarker948"/>execute the function without cache:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 -m cProfile T_03_profile_fibonacci.py no_cache&#13;
   2692557 function calls (21 primitive calls) in 0.596 seconds&#13;
&#13;
   Ordered by: standard name&#13;
&#13;
   ncalls tottime cumtime filename:lineno(function)&#13;
        1   0.000   0.596 T_03_profile_fibonacci.py:1(&lt;module&gt;)&#13;
2692537/1   0.596   0.596 T_03_profile_fibonacci.py:13(fibonacci)&#13;
        1   0.000   0.000 functools.py:35(update_wrapper)&#13;
        1   0.000   0.000 functools.py:479(lru_cache)&#13;
        1   0.000   0.000 functools.py:518(decorating_function)&#13;
        1   0.000   0.596 {built-in method builtins.exec}&#13;
        7   0.000   0.000 {built-in method builtins.getattr}&#13;
        1   0.000   0.000 {built-in method builtins.isinstance}&#13;
        5   0.000   0.000 {built-in method builtins.setattr}&#13;
        1   0.000   0.000 {method 'disable' of '_lsprof.Profile...&#13;
        1   0.000   0.000 {method 'update' of 'dict' objects}&#13;
</code></pre>&#13;
    <p class="normal">We see <code class="inlineCode">2692557</code> calls in total, which is quite a lot of calls. We called the <code class="inlineCode">test_fibonacci</code> function nearly 3 million times. That is where the profiling modules provide a lot of insight. Let’s analyze the metrics a bit further, in the order they appear:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><code class="inlineCode">ncalls</code>: The number of calls that were made to the function.</li>&#13;
      <li class="bulletList"><code class="inlineCode">tottime</code>: The total time spent in this function, <strong class="keyWord">excluding</strong> the sub-functions.</li>&#13;
      <li class="bulletList"><code class="inlineCode">percall</code>: The time per call without sub-functions: <code class="inlineCode">tottime / ncalls</code>.</li>&#13;
      <li class="bulletList"><code class="inlineCode">cumtime</code>: The total time spent in this function, <strong class="keyWord">including</strong> sub-functions.</li>&#13;
      <li class="bulletList"><code class="inlineCode">percall</code>: The time per call including sub-functions: <code class="inlineCode">cumtime / ncalls</code>. This is distinct from the <code class="inlineCode">percall</code> metric above, despite having the same name.</li>&#13;
    </ul>&#13;
    <p class="normal">Which is the <a id="_idIndexMarker949"/>most useful depends on your use case. It’s quite simple to change the sort order using the <code class="inlineCode">-s</code> parameter within the default output. But now let’s see what the result is with the cached version. Once again, with stripped output:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 -m cProfile T_03_profile_fibonacci.py cache&#13;
         51 function calls (21 primitive calls) in 0.000 seconds&#13;
&#13;
   Ordered by: standard name&#13;
&#13;
ncalls tottime cumtime filename:lineno(function)&#13;
     1  0.000  0.000 T_03_profile_fibonacci.py:1(&lt;module&gt;)&#13;
  31/1  0.000  0.000 T_03_profile_fibonacci.py:5(fibonacci_cached)&#13;
     1  0.000  0.000 functools.py:35(update_wrapper)&#13;
     1  0.000  0.000 functools.py:479(lru_cache)&#13;
     1  0.000  0.000 functools.py:518(decorating_function)&#13;
     1  0.000  0.000 {built-in method builtins.exec}&#13;
     7  0.000  0.000 {built-in method builtins.getattr}&#13;
     1  0.000  0.000 {built-in method builtins.isinstance}&#13;
     5  0.000  0.000 {built-in method builtins.setattr}&#13;
     1  0.000  0.000 {method 'disable' of '_lsprof.Profiler' ...}&#13;
     1  0.000  0.000 {method 'update' of 'dict' objects}&#13;
</code></pre>&#13;
    <p class="normal">This time, we see a <code class="inlineCode">tottime</code> of <code class="inlineCode">0.000</code> because it’s just too fast to measure. But also, while the <code class="inlineCode">fibonacci_cached</code> function is still the most executed function, it’s only being executed 31 times instead of 3 million times.</p>&#13;
    <h3 id="_idParaDest-330" class="heading-3">Calibrating your profiler</h3>&#13;
    <p class="normal">To illustrate<a id="_idIndexMarker950"/> the difference between <code class="inlineCode">profile</code> and <code class="inlineCode">cProfile</code>, let’s try the uncached run again with the <code class="inlineCode">profile</code> module instead. Just a heads up: this is much slower, so don’t be surprised if it stalls a little:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 -m profile T_03_profile_fibonacci.py no_cache&#13;
         2692558 function calls (22 primitive calls) in 4.541 seconds&#13;
&#13;
   Ordered by: standard name&#13;
&#13;
   ncalls  tottime cumtime filename:lineno(function)&#13;
        1    0.000   4.530 :0(exec)&#13;
        7    0.000   0.000 :0(getattr)&#13;
        1    0.000   0.000 :0(isinstance)&#13;
        5    0.000   0.000 :0(setattr)&#13;
        1    0.010   0.010 :0(setprofile)&#13;
        1    0.000   0.000 :0(update)&#13;
        1    0.000   4.530 T_03_profile_fibonacci.py:1(&lt;module&gt;)&#13;
2692537/1    4.530   4.530 T_03_profile_fibonacci.py:13(fibonacci)&#13;
        1    0.000   0.000 functools.py:35(update_wrapper)&#13;
        1    0.000   0.000 functools.py:479(lru_cache)&#13;
        1    0.000   0.000 functools.py:518(decorating_function)&#13;
        1    0.000   4.541 profile:0(&lt;code object &lt;module&gt; at ...&#13;
        0    0.000   0.000 profile:0(profiler)&#13;
</code></pre>&#13;
    <p class="normal">The code now runs nearly 10 times more slowly, and the only difference is using the pure Python <code class="inlineCode">profile</code> module instead of the <code class="inlineCode">cProfile</code> module. This does indicate a big problem<a id="_idIndexMarker951"/> with the <code class="inlineCode">profile</code> module. The overhead from the module itself is great enough to skew the results, which means we should account for that offset. </p>&#13;
    <p class="normal">That’s what the <code class="inlineCode">Profile.calibrate()</code> function takes care of, as it calculates the performance bias incurred by the profile module. To calculate the bias, we can use the following script:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> profile&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    profiler = profile.Profile()&#13;
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):&#13;
        print(profiler.calibrate(<span class="hljs-number">100000</span>))&#13;
</code></pre>&#13;
    <p class="normal">The numbers will vary slightly, but you should be able to get a fair estimate of the performance bias that the <code class="inlineCode">profile</code> module introduces to your code. It effectively runs a bit of code both with and without profiling enabled and calculates a multiplier to apply to all results so they are closer to the actual duration.</p>&#13;
    <div class="packt_tip">&#13;
      <p class="normal">If the numbers still vary a lot, you can increase the trials from <code class="inlineCode">100000</code> to something even larger.</p>&#13;
      <p class="normal">Note that with many modern processors, the burst CPU performance (the first few seconds) can vary greatly from the sustained CPU performance (2 minutes or more).</p>&#13;
      <p class="normal">The CPU performance is also highly temperature-dependent, so if your system has a large CPU cooler or is water-cooled, it can take up to 20 minutes at 100% CPU load before the CPU performance becomes consistent. The bias after that 20 minutes would be completely unusable as a bias for a cold CPU.</p>&#13;
    </div>&#13;
    <p class="normal">This <a id="_idIndexMarker952"/>type of calibration only works for the <code class="inlineCode">profile</code> module and should help a lot in achieving more accurate results. The bias can be set globally for all newly created profilers:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> profile&#13;
&#13;
<span class="hljs-comment"># The number here is bias calculated earlier</span>&#13;
profile.Profile.bias = <span class="hljs-number">9.809351906482531e-07</span>&#13;
</code></pre>&#13;
    <p class="normal">Or for a specific <code class="inlineCode">Profile</code> instance:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> profile&#13;
&#13;
profiler = profile.Profile(bias=<span class="hljs-number">9.809351906482531e-07</span>)&#13;
</code></pre>&#13;
    <p class="normal">Note that in general, a smaller bias is better to use than a large one because a large bias could cause very strange results. If the bias is large enough, you will even get negative timings. Let’s give it a try for our Fibonacci code:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> sys&#13;
<span class="hljs-keyword">import</span> pstats&#13;
<span class="hljs-keyword">import</span> profile&#13;
&#13;
...&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    profiler = profile.Profile(bias=<span class="hljs-number">9.809351906482531e-07</span>)&#13;
    n = <span class="hljs-number">30</span>&#13;
&#13;
    <span class="hljs-keyword">if</span> sys.argv[-<span class="hljs-number">1</span>] == <span class="hljs-string">'cache'</span>:&#13;
        profiler.runcall(fibonacci_cached, n)&#13;
    <span class="hljs-keyword">else</span>:&#13;
        profiler.runcall(fibonacci, n)&#13;
&#13;
    stats = pstats.Stats(profiler).sort_stats(<span class="hljs-string">'calls'</span>)&#13;
    stats.print_stats()&#13;
</code></pre>&#13;
    <p class="normal">While running it, it indeed appears that we’ve used a bias that’s too large:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_05_profiler_large_bias.py&#13;
      2692539 function calls (3 primitive calls) in -0.746 seconds&#13;
&#13;
   Ordered by: call count&#13;
&#13;
   ncalls tottime cumtime filename:lineno(function)&#13;
2692537/1  -0.747  -0.747 T_05_profiler..._bias.py:15(fibonacci)&#13;
        1   0.000  -0.746 profile:0(&lt;function fibonacci at ...&gt;)&#13;
        1   0.000   0.000 :0(setprofile)&#13;
        0   0.000   0.000 profile:0(profiler)&#13;
</code></pre>&#13;
    <p class="normal">Still, it shows how the code can be used properly. You can even incorporate the bias calculation within the script using a snippet like this:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> profile&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    profiler = profile.Profile()&#13;
    profiler.bias = profiler.calibrate(<span class="hljs-number">100000</span>)&#13;
</code></pre>&#13;
    <p class="normal">It is not a <a id="_idIndexMarker953"/>bad idea to always have a snippet like this enabled when using the <code class="inlineCode">profile</code> module. The only cost is the duration of the <code class="inlineCode">calibrate()</code> run, and with a small number of trials (say, <code class="inlineCode">10000</code>), it only takes about 0.2 seconds on my current system while still greatly increasing the accuracy of the results. Because of this properly calculated bias, the results can actually be more accurate than the <code class="inlineCode">cProfile</code> module.</p>&#13;
    <h3 id="_idParaDest-331" class="heading-3">Selective profiling using decorators</h3>&#13;
    <p class="normal">Calculating<a id="_idIndexMarker954"/> simple timings is easy enough using decorators, but profiling can show a lot more and can also be applied selectively using decorators or context wrappers. Let’s look at a <code class="inlineCode">timer</code> and a <code class="inlineCode">profiler</code> decorator:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> cProfile&#13;
<span class="hljs-keyword">import</span> datetime&#13;
<span class="hljs-keyword">import</span> functools&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">timer</span><span class="hljs-function">(</span><span class="hljs-params">function</span><span class="hljs-function">):</span>&#13;
<span class="hljs-meta">    @functools.wraps(</span><span class="hljs-params">function</span><span class="hljs-meta">)</span>&#13;
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">_timer</span><span class="hljs-function">(</span><span class="hljs-params">*args, **kwargs</span><span class="hljs-function">):</span>&#13;
        start = datetime.datetime.now()&#13;
        <span class="hljs-keyword">try</span>:&#13;
            <span class="hljs-keyword">return</span> function(*args, **kwargs)&#13;
        <span class="hljs-keyword">finally</span>:&#13;
            end = datetime.datetime.now()&#13;
            print(<span class="hljs-string">f'</span><span class="hljs-subst">{function.__name__}</span><span class="hljs-string">: </span><span class="hljs-subst">{end - start}</span><span class="hljs-string">'</span>)&#13;
&#13;
    <span class="hljs-keyword">return</span> _timer&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">profiler</span><span class="hljs-function">(</span><span class="hljs-params">function</span><span class="hljs-function">):</span>&#13;
<span class="hljs-meta">    @functools.wraps(</span><span class="hljs-params">function</span><span class="hljs-meta">)</span>&#13;
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">_profiler</span><span class="hljs-function">(</span><span class="hljs-params">*args, **kwargs</span><span class="hljs-function">):</span>&#13;
        profiler = cProfile.Profile()&#13;
        <span class="hljs-keyword">try</span>:&#13;
            profiler.enable()&#13;
            <span class="hljs-keyword">return</span> function(*args, **kwargs)&#13;
        <span class="hljs-keyword">finally</span>:&#13;
            profiler.disable()&#13;
            profiler.print_stats()&#13;
&#13;
    <span class="hljs-keyword">return</span> _profiler&#13;
</code></pre>&#13;
    <p class="normal">Now that<a id="_idIndexMarker955"/> we have created the decorators, we can profile and time our functions with them:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@profiler</span>&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">profiled_fibonacci</span><span class="hljs-function">(</span><span class="hljs-params">n</span><span class="hljs-function">):</span>&#13;
    <span class="hljs-keyword">return</span> fibonacci(n)&#13;
&#13;
<span class="hljs-meta">@timer</span>&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">timed_fibonacci</span><span class="hljs-function">(</span><span class="hljs-params">n</span><span class="hljs-function">):</span>&#13;
    <span class="hljs-keyword">return</span> fibonacci(n)&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">fibonacci</span><span class="hljs-function">(</span><span class="hljs-params">n</span><span class="hljs-function">):</span>&#13;
    <span class="hljs-keyword">if</span> n &lt; <span class="hljs-number">2</span>:&#13;
        <span class="hljs-keyword">return</span> n&#13;
    <span class="hljs-keyword">else</span>:&#13;
        <span class="hljs-keyword">return</span> fibonacci(n - <span class="hljs-number">1</span>) + fibonacci(n - <span class="hljs-number">2</span>)&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    timed_fibonacci(<span class="hljs-number">32</span>)&#13;
    profiled_fibonacci(<span class="hljs-number">32</span>)&#13;
</code></pre>&#13;
    <p class="normal">The code is simple enough: just a basic <code class="inlineCode">timer</code> and <code class="inlineCode">profiler</code> decorator printing some default statistics. Which functions best for you depends on your use case, of course. The <code class="inlineCode">timer()</code> decorator is very useful for quick performance tracking and/or a sanity check while developing. The <code class="inlineCode">profiler()</code> decorator is great while you are actively working on the performance characteristics of a function.</p>&#13;
    <p class="normal">The added<a id="_idIndexMarker956"/> advantage of this selective profiling is that the output is more limited, which helps with readability, albeit still much more verbose than the <code class="inlineCode">timer()</code> decorator:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_06_selective_profiling.py&#13;
timed_fibonacci: 0:00:00.744912&#13;
         7049157 function calls (3 primitive calls) in 1.675 seconds&#13;
&#13;
   Ordered by: standard name&#13;
&#13;
   ncalls  tottime cumtime filename:lineno(function)&#13;
        1    0.000   1.675 T_06_select...py:31(profiled_fibonacci)&#13;
7049155/1    1.675   1.675 T_06_selec...profiling.py:41(fibonacci)&#13;
        1    0.000   0.000 {method 'disable' of '_lsprof.Profil...&#13;
</code></pre>&#13;
    <p class="normal">As you can see, the profiler still makes the code about twice as slow, but it’s definitely usable.</p>&#13;
    <h3 id="_idParaDest-332" class="heading-3">Using profile statistics</h3>&#13;
    <p class="normal">To get slightly more <a id="_idIndexMarker957"/>interesting profiling results, we will profile using the <code class="inlineCode">pyperformance.benchmarks.bm_float</code> script.</p>&#13;
    <div class="note">&#13;
      <p class="normal">The <code class="inlineCode">pyperformance</code> library is the official Python benchmarks library optimized for the CPython interpreter. It contains a large (ever-growing) list of benchmarks to monitor the performance of the CPython interpreter under many scenarios.</p>&#13;
    </div>&#13;
    <p class="normal">It can be installed through <code class="inlineCode">pip</code>:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> pip3 install pyperformance&#13;
</code></pre>&#13;
    <p class="normal">First, let’s create the statistics using this script:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> sys&#13;
<span class="hljs-keyword">import</span> pathlib&#13;
<span class="hljs-keyword">import</span> pstats&#13;
<span class="hljs-keyword">import</span> cProfile&#13;
&#13;
<span class="hljs-keyword">import</span> pyperformance&#13;
&#13;
<span class="hljs-comment"># pyperformance doesn't expose the benchmarks anymore so we need</span>&#13;
<span class="hljs-comment"># to manually add the path</span>&#13;
pyperformance_path = pathlib.Path(pyperformance.__file__).parent&#13;
sys.path.append(str(pyperformance_path / 'data-files'))&#13;
&#13;
<span class="hljs-comment"># Now we can import the benchmark</span>&#13;
from benchmarks.bm_float import run_benchmark as bm_float  <span class="hljs-comment"># noqa</span>&#13;
&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">benchmark</span>():&#13;
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):&#13;
        bm_float.benchmark(bm_float.POINTS)&#13;
&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    profiler = cProfile.Profile()&#13;
    profiler.runcall(benchmark)&#13;
    profiler.dump_stats(<span class="hljs-string">'bm_float.profile'</span>)&#13;
    stats = pstats.Stats(<span class="hljs-string">'bm_float.profile'</span>)&#13;
    stats.strip_dirs()&#13;
    stats.sort_stats(<span class="hljs-string">'calls'</span>, <span class="hljs-string">'cumtime'</span>)&#13;
    stats.print_stats(<span class="hljs-number">10</span>)&#13;
</code></pre>&#13;
    <p class="normal">When executing<a id="_idIndexMarker958"/> the script, you should get something like this:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_07_profile_statistics.py&#13;
Sun May  1 06:14:26 2022    bm_float.profile&#13;
&#13;
         6000012 function calls in 2.501 seconds&#13;
&#13;
   Ordered by: call count, cumulative time&#13;
&#13;
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)&#13;
  1000000    0.446    0.000    0.682    0.000 run_benchmark.py:15(__init__)&#13;
  1000000    0.525    0.000    0.599    0.000 run_benchmark.py:23(normalize)&#13;
  1000000    0.120    0.000    0.120    0.000 {built-in method math.cos}&#13;
  1000000    0.116    0.000    0.116    0.000 {built-in method math.sin}&#13;
  1000000    0.073    0.000    0.073    0.000 {built-in method math.sqrt}&#13;
   999990    0.375    0.000    0.375    0.000 run_benchmark.py:32(maximize)&#13;
       10    0.625    0.063    2.446    0.245 run_benchmark.py:46(benchmark)&#13;
       10    0.165    0.017    0.540    0.054 run_benchmark.py:39(maximize)&#13;
        1    0.055    0.055    2.501    2.501 T_07_profile_statistics.py:17(benchmark)&#13;
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}&#13;
</code></pre>&#13;
    <p class="normal">After running the script, you should have a <code class="inlineCode">bm_float.profile</code> file containing the profiling results. As we can see in the script, these statistics can be viewed through the <code class="inlineCode">pstats</code> module.</p>&#13;
    <p class="normal">In some cases, it can be interesting to combine the results from multiple measurements. That is possible by specifying multiple files or by using <code class="inlineCode">stats.add(*filenames)</code>.</p>&#13;
    <p class="normal">The main advantage of saving these profile results to files is that several applications support this output and can visualize it in a clearer way. One option is SnakeViz, which uses your <a id="_idIndexMarker959"/>web browser to render the profile results interactively. Also, we have QCacheGrind, a very nice visualizer for profile statistics, but which requires some manual compiling to get running or some searching for binaries of course.</p>&#13;
    <p class="normal">Let’s look at the output from QCacheGrind. In the case of Windows, the QCacheGrindWin package provides a binary, whereas within Linux it is most likely available through your package manager, and with OS X you can try <code class="inlineCode">brew install qcachegrind</code>.</p>&#13;
    <p class="normal">However, there is one more package you will require: the <code class="inlineCode">pyprof2calltree</code> package. It transforms the <code class="inlineCode">profile</code> output into a format that QCacheGrind understands. So, after a simple <code class="inlineCode">pip install pyprof2calltree</code>, we can now convert the <code class="inlineCode">profile</code> file into a <code class="inlineCode">callgrind</code> file:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> pyprof2calltree -i bm_float.profile -o bm_float.callgrind&#13;
writing converted data to: bm_float.callgrind&#13;
<span class="hljs-con-meta">$</span> qcachegrind bm_float.callgrind&#13;
</code></pre>&#13;
    <p class="normal">This results in the running of the <code class="inlineCode">QCacheGrind</code> application. After switching to the appropriate tabs, you should see something like the following screenshot:</p>&#13;
    <figure class="mediaobject"><img src="Images/B15882_12_01.png" alt="" width="867" height="587"/></figure>&#13;
    <p class="packt_figref">Figure 12.1: QCacheGrind</p>&#13;
    <p class="normal">For a simple<a id="_idIndexMarker960"/> script such as this, pretty much all output works. However, with full applications, a tool such as QCacheGrind is invaluable. Looking at the output generated by QCacheGrind, it is immediately obvious which process took the most time. The structure at the top right shows bigger rectangles if the amount of time taken was greater, which is a very useful visualization of the chunks of CPU time that were used. The list at the left is very similar to <code class="inlineCode">cProfile</code> and therefore nothing new. The tree at the bottom right can be very valuable or very useless, as it is in this case. It shows you the percentage of CPU time taken in a function and, more importantly, the relationship of that function with the other functions.</p>&#13;
    <p class="normal">Because these tools scale depending on the input, the results are useful for just about any application. Whether a function takes 100 milliseconds or 100 minutes makes no difference – the output will show a clear overview of the slow parts, which is what we will try to fix.</p>&#13;
    <h2 id="_idParaDest-333" class="heading-2">Line profiler – Tracking performance per line</h2>&#13;
    <p class="normal"><code class="inlineCode">line_profiler</code> is actually <a id="_idIndexMarker961"/>not a package that’s bundled with Python, but it’s far too useful to ignore. While the regular <code class="inlineCode">profile</code> module<a id="_idIndexMarker962"/> profiles all (sub)functions within a certain block, <code class="inlineCode">line_profiler</code> allows for profiling line <em class="italic">per line</em> within a function. The Fibonacci function is not best suited here, but we can use a prime number generator instead. But first, install <code class="inlineCode">line_profiler</code>:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> pip3 install line_profiler&#13;
</code></pre>&#13;
    <p class="normal">Now that we have installed the <code class="inlineCode">line_profiler</code> module (and with that the <code class="inlineCode">kernprof</code> command), let’s test <code class="inlineCode">line_profiler</code>:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> itertools&#13;
&#13;
<span class="hljs-meta">@profile</span>&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">primes</span><span class="hljs-function">():</span>&#13;
    n = <span class="hljs-number">2</span>&#13;
    primes = <span class="hljs-built_in">set</span>()&#13;
    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:&#13;
        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> primes:&#13;
            <span class="hljs-keyword">if</span> n % p == <span class="hljs-number">0</span>:&#13;
                <span class="hljs-keyword">break</span>&#13;
        <span class="hljs-keyword">else</span>:&#13;
            primes.add(n)&#13;
            <span class="hljs-keyword">yield</span> n&#13;
        n += <span class="hljs-number">1</span>&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    total = <span class="hljs-number">0</span>&#13;
    n = <span class="hljs-number">2000</span>&#13;
    <span class="hljs-keyword">for</span> prime <span class="hljs-keyword">in</span> itertools.islice(primes(), n):&#13;
        total += prime&#13;
&#13;
    print(<span class="hljs-string">'The sum of the first %d primes is %d'</span> % (n, total))&#13;
</code></pre>&#13;
    <p class="normal">You might be wondering where the <code class="inlineCode">profile</code> decorator is coming from. It originates from the <code class="inlineCode">line_profiler</code> module, which<a id="_idIndexMarker963"/> is why we have to run the script with the <code class="inlineCode">kernprof</code> command:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> kernprof --line-by-line T_08_line_profiler.py&#13;
The sum of the first 2000 primes is 16274627&#13;
Wrote profile results to T_08_line_profiler.py.lprof&#13;
</code></pre>&#13;
    <p class="normal">As the <a id="_idIndexMarker964"/>command says, the results have been written to the <code class="inlineCode">T_08_line_profiler.py.lprof</code> file, so we can now look at the output of that file. For readability, we’ve skipped the <code class="inlineCode">Line #</code> column:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 -m line_profiler T_08_line_profiler.py.lprof&#13;
Timer unit: 1e-06 s&#13;
&#13;
Total time: 1.34623 s&#13;
File: T_08_line_profiler.py&#13;
Function: primes at line 4&#13;
   Hits         Time  Per Hit   % Time  Line Contents&#13;
=====================================================&#13;
                                        @profile&#13;
                                        def primes():&#13;
      1          3.0      3.0      0.0      n = 2&#13;
      1          1.0      1.0      0.0      primes = set()&#13;
                                            while True:&#13;
2055131     625266.0      0.3     46.4          for p in primes:&#13;
2053131     707403.0      0.3     52.5              if n % p == 0:&#13;
  15388       4893.0      0.3      0.4                  break&#13;
                                                else:&#13;
   2000       1519.0      0.8      0.1              primes.add(n)&#13;
   2000        636.0      0.3      0.0              yield n&#13;
  17387       6510.0      0.4      0.5          n += 1&#13;
</code></pre>&#13;
    <p class="normal">Wonderful output, isn’t it? It makes it trivial to find the slow part within a bit of code. Within this code, the slowness is obviously originating from the loop, but within other code it might not be that clear.</p>&#13;
    <div class="note">&#13;
      <p class="normal">This module can be added as an IPython extension as well, which enables the <code class="inlineCode">%lprun</code> command within IPython. To load the extension, the <code class="inlineCode">load_ext</code> command can be used from the IPython shell, <code class="inlineCode">%load_ext line_profiler</code>.</p>&#13;
    </div>&#13;
    <p class="normal">We have seen several methods of measuring CPU performance and execution time. Now it’s time<a id="_idIndexMarker965"/> to look at how to improve performance. Since this largely applies to CPU performance and not memory performance, we will <a id="_idIndexMarker966"/>cover that first. Later in this chapter, we will take a look at memory usage and leaks.</p>&#13;
    <h1 id="_idParaDest-334" class="heading-1">Improving execution time</h1>&#13;
    <p class="normal">Much can be said about <a id="_idIndexMarker967"/>performance optimization, but truthfully, if you have read the entire book up to this point, you know most of the Python-specific techniques for writing fast code. The most important factor in overall application performance will always be the choice of algorithms and, by extension, the data structures. Searching for an item within a <code class="inlineCode">list (O(n))</code> is almost always a worse idea than searching for an item in a <code class="inlineCode">dict</code> or <code class="inlineCode">set (O(1))</code>, as we have seen in <em class="chapterRef">Chapter 4</em>.</p>&#13;
    <p class="normal">Naturally, there are more factors and tricks that can help make your application faster. The extremely abbreviated version of all performance tips is quite simple, however: do as little as possible. No matter how fast you make your calculations and operations, doing nothing at all will always be faster. The following sections cover some of the most common performance bottlenecks in Python and test a few common assumptions about performance, such as the performance of <code class="inlineCode">try</code>/<code class="inlineCode">except</code> blocks versus <code class="inlineCode">if</code> statements, which can have a huge impact in many languages.</p>&#13;
    <p class="normal">Some of the tricks in this section will be a trade-off between memory and execution time; others will trade readability with performance. When in doubt, go for readability by default and only improve performance if you have to.</p>&#13;
    <h2 id="_idParaDest-335" class="heading-2">Using the right algorithm</h2>&#13;
    <p class="normal">Within any <a id="_idIndexMarker968"/>application, the right choice of algorithm is by far the most important performance characteristic, which is why I am repeating it to illustrate the results of a bad choice. Consider the following:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">In [1]: a = list(range(1000000))&#13;
&#13;
In [2]: b = dict.fromkeys(range(1000000))&#13;
&#13;
In [3]: %timeit 'x' in a&#13;
12.2 ms ± 245 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)&#13;
&#13;
In [4]: %timeit 'x' in b&#13;
40.1 ns ± 0.446 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)&#13;
</code></pre>&#13;
    <p class="normal">Checking <a id="_idIndexMarker969"/>whether an item is within a <code class="inlineCode">list</code> is an <code class="inlineCode">O(n)</code> operation, and checking whether an item is within a <code class="inlineCode">dict</code> is an <code class="inlineCode">O(1)</code> operation. This makes a huge difference when <code class="inlineCode">n=1000000</code>; in this simple test, we can see that for 1 million items, it’s 300,000 times faster.</p>&#13;
    <div class="note">&#13;
      <p class="normal">The big-O notation ( <code class="inlineCode">O(...)</code>) is covered in more detail in <em class="chapterRef">Chapter 4</em>, but we can provide a quick recap.</p>&#13;
      <p class="normal"><code class="inlineCode">O(n)</code> means that for a <code class="inlineCode">list</code> with <code class="inlineCode">len(some_list) = n</code>, it will take <code class="inlineCode">n</code> steps to perform the operation. Consequently, <code class="inlineCode">O(1)</code> means that it takes a constant amount of time regardless of the size of the collection.</p>&#13;
    </div>&#13;
    <p class="normal">All other performance tips combined might make your code twice as fast, but using the right algorithm for the job can cause a much greater improvement. Using an algorithm that takes <code class="inlineCode">O(n)</code> time instead of <code class="inlineCode">O(n</code><sup class="superscript">2</sup><code class="inlineCode">)</code> time will make your code <code class="inlineCode">1000</code> times faster for <code class="inlineCode">n=1000</code>, and with a larger <code class="inlineCode">n</code>, the difference only grows further.</p>&#13;
    <h2 id="_idParaDest-336" class="heading-2">Global interpreter lock</h2>&#13;
    <p class="normal">One of the most <a id="_idIndexMarker970"/>obscure components of the CPython interpreter is the <strong class="keyWord">global interpreter lock</strong> (<strong class="keyWord">GIL</strong>), a <strong class="keyWord">mutual exclusion lock</strong> (<strong class="keyWord">mutex</strong>) required<a id="_idIndexMarker971"/> to prevent memory corruption. The Python memory manager is not thread-safe, which is why the GIL is needed. Without the GIL, multiple threads might alter memory at the same time, causing all sorts of unexpected and potentially dangerous results. The GIL is covered in much more detail in <em class="chapterRef">Chapter 14</em>.</p>&#13;
    <p class="normal">What is the impact of the GIL in a real-life application? Within single-threaded applications, it makes no difference whatsoever and is actually an extremely fast method for memory consistency. </p>&#13;
    <p class="normal">Within multithreaded applications, however, it can slow your application down a bit because only a single thread can access the GIL at a time. If your code has to access the GIL a lot, it might benefit from some restructuring.</p>&#13;
    <p class="normal">Luckily, Python offers a few other options for parallel processing. The <code class="inlineCode">asyncio</code> module, which we will see in <em class="chapterRef">Chapter 13</em>, can help a lot by switching tasks whenever you are waiting for a slow operation. In <em class="chapterRef">Chapter 14</em>, we will see the <code class="inlineCode">multiprocessing</code> library, which allows us to use multiple processors simultaneously.</p>&#13;
    <h2 id="_idParaDest-337" class="heading-2">try versus if</h2>&#13;
    <p class="normal">In many <a id="_idIndexMarker972"/>languages, a <code class="inlineCode">try/except</code> type of block incurs quite a performance hit, but within Python, this is <em class="italic">not</em> the case as long as you don’t hit the <code class="inlineCode">except</code> block. If you do hit an <code class="inlineCode">except</code>, it will be slightly heavier than an <code class="inlineCode">if</code> statement, but not enough to be noticeable in most cases.</p>&#13;
    <p class="normal">It’s not that an <code class="inlineCode">if</code> statement is heavy, but if you expect your <code class="inlineCode">try/except</code> to succeed most of the time and only fail in rare cases, it is definitely a valid alternative. As always though, focus on readability and conveying the purpose of the code. If the intention of the code is clearer using an <code class="inlineCode">if</code> statement, use the <code class="inlineCode">if</code> statement. If <code class="inlineCode">try</code>/<code class="inlineCode">except</code> conveys the intention in a better way, use that.</p>&#13;
    <p class="normal">Most programming languages depend on the use <a id="_idIndexMarker973"/>of the <strong class="keyWord">Look Before You Leap </strong>(<strong class="keyWord">LBYL</strong>) ideology. This means that you always check before you try, so if you are getting <code class="inlineCode">some_key</code> from a <code class="inlineCode">dict</code>, you use:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> some_key <span class="hljs-keyword">in</span> some_dict:&#13;
    process_value(some_dict[some_key])&#13;
</code></pre>&#13;
    <p class="normal">Because you are always doing the <code class="inlineCode">if</code>, it hints that <code class="inlineCode">some_key</code> is usually not part of <code class="inlineCode">some_dict</code>.</p>&#13;
    <p class="normal">Within Python, it is common to use the <strong class="keyWord">Easier to Ask for Forgiveness than Permission</strong> (<strong class="keyWord">EAFP</strong>) ideology <a id="_idIndexMarker974"/>when applicable. This means that the code assumes everything will work, but still catches errors:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">try</span>:&#13;
    process_value(some_dict[some_key])&#13;
<span class="hljs-keyword">except</span> KeyError:&#13;
    <span class="hljs-keyword">pass</span>&#13;
</code></pre>&#13;
    <p class="normal">These two examples function mostly the same, but the latter gives the idea that you expect the key to be available and will catch errors if needed. This is one of the cases where the Zen of Python (explicit is better than implicit) applies.</p>&#13;
    <p class="normal">The only caveat of the code above is that you might accidentally catch a <code class="inlineCode">KeyError</code> from <code class="inlineCode">process_value()</code>, so if you want to avoid that you should use the following code instead:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">try</span>:&#13;
    value = some_dict[some_key]&#13;
<span class="hljs-keyword">except</span> KeyError:&#13;
    <span class="hljs-keyword">pass</span>&#13;
<span class="hljs-keyword">else</span>:&#13;
    process_value(value)&#13;
</code></pre>&#13;
    <p class="normal">Which <a id="_idIndexMarker975"/>one you use comes mostly down to personal preference, but the takeaway should be that, with Python, both options are perfectly valid and will perform similarly.</p>&#13;
    <h2 id="_idParaDest-338" class="heading-2">Lists versus generators</h2>&#13;
    <p class="normal">Evaluating <a id="_idIndexMarker976"/>code lazily using generators is almost always a better idea than calculating the entire dataset. The most important rule of performance optimization is probably that you shouldn’t calculate anything you’re not going to use. If you’re not sure that you are going to need it, don’t calculate it.</p>&#13;
    <p class="normal">Don’t forget that you can easily chain multiple generators, so everything is calculated only when it’s actually needed. Do be careful that this won’t result in recalculation though; <code class="inlineCode">itertools.tee()</code> is generally a better idea than recalculating your results completely.</p>&#13;
    <p class="normal">To recap <code class="inlineCode">itertools.tee()</code> from <em class="chapterRef">Chapter 7</em>, a regular generator can only be consumed once, so if you need to process the results two or more times, you can use <code class="inlineCode">itertools.tee()</code> to store the intermediate results:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span> itertools&#13;
&#13;
<span class="hljs-con-comment"># Without itertools.tee:</span>&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> generator = itertools.count()&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">list</span>(itertools.islice(generator, <span class="hljs-con-number">5</span>))&#13;
[0, 1, 2, 3, 4]&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">list</span>(itertools.islice(generator, <span class="hljs-con-number">5</span>))&#13;
[5, 6, 7, 8, 9]&#13;
&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> generator_a, generator_b = itertools.tee(itertools.count())&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">list</span>(itertools.islice(generator_a, <span class="hljs-con-number">5</span>))&#13;
[0, 1, 2, 3, 4]&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">list</span>(itertools.islice(generator_b, <span class="hljs-con-number">5</span>))&#13;
[0, 1, 2, 3, 4]&#13;
</code></pre>&#13;
    <p class="normal">As you can see, if you forget to use <code class="inlineCode">itertools.tee()</code> here, you would only process the results once, and <a id="_idIndexMarker977"/>both would process different values. The alternative fix is to use <code class="inlineCode">list()</code> and store the intermediate results, but this can cost much more memory, and you are required to pre-calculate all items without knowing whether you actually need them all.</p>&#13;
    <h2 id="_idParaDest-339" class="heading-2">String concatenation</h2>&#13;
    <p class="normal">You<a id="_idIndexMarker978"/> might have seen benchmarks saying that using <code class="inlineCode">+=</code> is much slower than joining strings because the <code class="inlineCode">str</code> object (as is the case with <code class="inlineCode">bytes</code>) is immutable. The result is that every time you do <code class="inlineCode">+=</code> on a string, it will have to create a new object. At one point, this made quite a lot of difference indeed. With Python 3, however, most of the differences have vanished:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">In [1]: %%timeit&#13;
   ...: s = ''&#13;
   ...: for i in range(1000000):&#13;
   ...:     s += str(i)&#13;
   ...:&#13;
1 loops, best of 3: 362 ms per loop&#13;
&#13;
In [2]: %%timeit&#13;
   ...: ss = []&#13;
   ...: for i in range(1000000):&#13;
   ...:     ss.append(str(i))&#13;
   ...: s = ''.join(ss)&#13;
   ...:&#13;
1 loops, best of 3: 332 ms per loop&#13;
&#13;
In [3]: %timeit ''.join(str(i) for i in range(1000000))&#13;
1 loops, best of 3: 324 ms per loop&#13;
&#13;
In [4]: %timeit ''.join([str(i) for i in range(1000000)])&#13;
1 loops, best of 3: 294 ms per loop&#13;
</code></pre>&#13;
    <p class="normal">There are still some differences, of course, but they are so small that I recommend you simply ignore them and choose the most readable option instead.</p>&#13;
    <h2 id="_idParaDest-340" class="heading-2">Addition versus generators</h2>&#13;
    <p class="normal">As is the<a id="_idIndexMarker979"/> case with string concatenation, addition from a loop was significantly slower with older Python versions, but the difference is now too small to consider:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">In [1]: %%timeit&#13;
   ...: x = 0&#13;
   ...: for i in range(1000000):&#13;
   ...:     x += i&#13;
   ...:&#13;
10 loops, best of 3: 73.2 ms per loop&#13;
&#13;
In [2]: %timeit x = sum(i for i in range(1000000))&#13;
10 loops, best of 3: 75.3 ms per loop&#13;
&#13;
In [3]: %timeit x = sum([i for i in range(1000000)])&#13;
10 loops, best of 3: 71.2 ms per loop&#13;
&#13;
In [4]: %timeit x = sum(range(1000000))&#13;
10 loops, best of 3: 25.6 ms per loop&#13;
</code></pre>&#13;
    <p class="normal">What does help, though, is letting Python handle everything internally using native functions, as can be seen in the last example.</p>&#13;
    <h2 id="_idParaDest-341" class="heading-2">Map versus generators and list comprehensions</h2>&#13;
    <p class="normal">Once again, readability generally counts more than performance, so only rewrite for performance if it <a id="_idIndexMarker980"/>really makes a difference. There<a id="_idIndexMarker981"/> are a few cases where <code class="inlineCode">map()</code> is faster than list comprehensions and generators, but only if the <code class="inlineCode">map()</code> function can use a predefined function. As soon as you need to whip out <code class="inlineCode">lambda</code>, it’s actually slower. Not that it matters much, since readability should be key anyhow. If <code class="inlineCode">map()</code> makes your code more readable than a generator or list comprehension, feel free to use it. Otherwise, I would not recommend it:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">In [1]: %timeit list(map(lambda x: x/2, range(1000000)))&#13;
10 loops, best of 3: 182 ms per loop&#13;
&#13;
In [2]: %timeit list(x/2 for x in range(1000000))&#13;
10 loops, best of 3: 122 ms per loop&#13;
&#13;
In [3]: %timeit [x/2 for x in range(1000000)]&#13;
10 loops, best of 3: 84.7 ms per loop&#13;
</code></pre>&#13;
    <p class="normal">As you can see, the list comprehension<a id="_idIndexMarker982"/> is quite a bit<a id="_idIndexMarker983"/> faster than the generator. In many cases, I would still recommend the generator over the list comprehension, though, if only because of the memory usage and the potential laziness. </p>&#13;
    <p class="normal">If, for some reason, you are only going to use the first 10 items when generating 1,000 items, you’re still wasting a lot of resources by calculating the full list of items.</p>&#13;
    <h2 id="_idParaDest-342" class="heading-2">Caching</h2>&#13;
    <p class="normal">We have <a id="_idIndexMarker984"/>already covered the <code class="inlineCode">functools.lru_cache</code> decorator in <em class="chapterRef">Chapter 6</em>, <em class="italic">Decorators – Enabling Code Reuse by Decorating</em>, but its importance should not be underestimated. Regardless of how fast and smart your code is, not having to calculate results is always better and that’s what caching does. Depending on your use case, there are many options available. Within a simple script, <code class="inlineCode">functools.lru_cache</code> is a very good contender, but between multiple executions of an application, the <code class="inlineCode">cPickle</code> module can be a lifesaver as well.</p>&#13;
    <div class="note">&#13;
      <p class="normal">We have already seen the effects of this with the <code class="inlineCode">fibonacci_cached</code> function in the <code class="inlineCode">cProfile</code> section of this chapter, which uses <code class="inlineCode">functools.lru_cache()</code>.</p>&#13;
    </div>&#13;
    <p class="normal">There are several scenarios where you need a more powerful solution, however:</p>&#13;
    <ul>&#13;
      <li class="bulletList">If you need caching between multiple executions of a script</li>&#13;
      <li class="bulletList">If you need caching shared across multiple processes</li>&#13;
      <li class="bulletList">If you need caching shared across multiple servers</li>&#13;
    </ul>&#13;
    <p class="normal">At least for the first two scenarios, you could write the cache to a local pickle/CSV/JSON/YAML/DBM/etc. file. This is a perfectly valid solution that I use often.</p>&#13;
    <p class="normal">If you need a more powerful solution, however, I can highly recommend taking a look at <strong class="keyWord">Redis</strong>. The Redis server is a fully in-memory server that is extremely fast and has many useful data structures available. If you see articles or tutorials about improving performance using Memcached, simply replace Memcached with Redis everywhere. Redis is superior to Memcached in every way and, in its most basic form, the API is compatible.</p>&#13;
    <h2 id="_idParaDest-343" class="heading-2">Lazy imports</h2>&#13;
    <p class="normal">A <a id="_idIndexMarker985"/>common problem in application load times is that everything is loaded immediately at the start of the program while, with many applications, this is actually not needed and certain parts of the application only require loading when they are actually used. To facilitate this, you can occasionally move the imports inside of functions so they can be loaded on demand.</p>&#13;
    <p class="normal">While it’s a valid strategy in some cases, I don’t generally recommend it for two reasons:</p>&#13;
    <ul>&#13;
      <li class="bulletList">It makes your code less clear; having all imports in the same style at the top of the file improves readability.</li>&#13;
      <li class="bulletList">It doesn’t make the code faster as it just moves the load time to a different part.</li>&#13;
    </ul>&#13;
    <h2 id="_idParaDest-344" class="heading-2">Using slots</h2>&#13;
    <p class="normal">The <code class="inlineCode">__slots__</code> feature <a id="_idIndexMarker986"/>was written by Guido van Rossum to enhance Python performance. Effectively what the <code class="inlineCode">__slots__</code> feature does is specify a fixed list of attributes for a class. When <code class="inlineCode">__slots__</code> are used, several changes are made to a class and several (side-)effects must be considered:</p>&#13;
    <ul>&#13;
      <li class="bulletList">All attributes must be explicitly named in the <code class="inlineCode">__slots__</code>. It is not possible to do <code class="inlineCode">some_instance.some_variable = 123</code> if <code class="inlineCode">some_variable</code> is not in <code class="inlineCode">__slots__</code>.</li>&#13;
      <li class="bulletList">Because the list of attributes is fixed in <code class="inlineCode">__slots__</code>, there is no longer any need for a <code class="inlineCode">__dict__</code> attribute, which saves memory.</li>&#13;
      <li class="bulletList">Attribute access is faster because there is no intermediate lookup through <code class="inlineCode">__dict__</code>.</li>&#13;
      <li class="bulletList">It is not possible to use multiple inheritance if both parents have defined <code class="inlineCode">__slots__</code>.</li>&#13;
    </ul>&#13;
    <p class="normal">So, how much performance benefit can <code class="inlineCode">__slots__</code> give us? Well, let’s give it a test:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> timeit&#13;
<span class="hljs-keyword">import</span> functools&#13;
&#13;
<span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">WithSlots</span><span class="hljs-class">:</span>&#13;
    __slots__ = <span class="hljs-string">'eggs'</span>,&#13;
&#13;
<span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">WithoutSlots</span><span class="hljs-class">:</span>&#13;
    <span class="hljs-keyword">pass</span>&#13;
&#13;
with_slots = WithSlots()&#13;
no_slots = WithoutSlots()&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">test_set</span><span class="hljs-function">(</span><span class="hljs-params">obj</span><span class="hljs-function">):</span>&#13;
    obj.eggs = <span class="hljs-number">5</span>&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">test_get</span><span class="hljs-function">(</span><span class="hljs-params">obj</span><span class="hljs-function">):</span>&#13;
    <span class="hljs-keyword">return</span> obj.eggs&#13;
&#13;
timer = functools.partial(&#13;
    timeit.timeit,&#13;
    number=<span class="hljs-number">20000000</span>,&#13;
    setup=<span class="hljs-string">'</span><span class="hljs-string">\n'</span>.join((&#13;
        <span class="hljs-string">f'from </span><span class="hljs-subst">{__name__}</span><span class="hljs-string"> import with_slots, no_slots'</span>,&#13;
        <span class="hljs-string">f'from </span><span class="hljs-subst">{__name__}</span><span class="hljs-string"> import test_get, test_set'</span>,&#13;
    )),&#13;
)&#13;
<span class="hljs-keyword">for</span> function <span class="hljs-keyword">in</span> <span class="hljs-string">'test_set'</span>, <span class="hljs-string">'test_get'</span>:&#13;
    print(function)&#13;
    print(<span class="hljs-string">'with slots'</span>, timer(<span class="hljs-string">f'</span><span class="hljs-subst">{function}</span><span class="hljs-string">(with_slots)'</span>))&#13;
    print(<span class="hljs-string">'with slots'</span>, timer(<span class="hljs-string">f'</span><span class="hljs-subst">{function}</span><span class="hljs-string">(no_slots)'</span>))&#13;
</code></pre>&#13;
    <p class="normal">When we actually<a id="_idIndexMarker987"/> run this code, we can definitely see some improvements from using <code class="inlineCode">__slots__</code>:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_10_slots_performance.py&#13;
test_set&#13;
with slots 1.748628467&#13;
with slots 2.0184642979999996&#13;
test_get&#13;
with slots 1.5832197570000002&#13;
with slots 1.6575410809999997&#13;
</code></pre>&#13;
    <p class="normal">In most cases, I would argue that the 5-15% difference in performance isn’t going to help you that much. However, if it’s applied to a bit of code that is near the core of your application and executed very often, it can help.</p>&#13;
    <p class="normal">Don’t expect miracles from this method, but use it when you need it. </p>&#13;
    <h2 id="_idParaDest-345" class="heading-2">Using optimized libraries</h2>&#13;
    <p class="normal">This is actually<a id="_idIndexMarker988"/> a very broad tip, but useful nonetheless. If there’s a highly optimized library that suits your purpose, you most likely won’t be able to beat its performance without a significant amount of effort. Libraries such as <code class="inlineCode">numpy</code>, <code class="inlineCode">pandas</code>, <code class="inlineCode">scipy</code>, and <code class="inlineCode">sklearn</code> are highly optimized for performance and their native operations can be incredibly fast. If they suit your purpose, be sure to give them a try.</p>&#13;
    <div class="note">&#13;
      <p class="normal">Before you can use <code class="inlineCode">numpy</code>, you need to install it: <code class="inlineCode">pip3 install numpy.</code></p>&#13;
    </div>&#13;
    <p class="normal">Just to illustrate how fast <code class="inlineCode">numpy</code> can be compared to plain Python, refer to the following:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">In [1]: import numpy&#13;
&#13;
In [2]: a = list(range(1000000))&#13;
&#13;
In [3]: b = numpy.arange(1000000)&#13;
&#13;
In [4]: %timeit c = [x for x in a if x &gt; 500000]&#13;
10 loops, best of 3: 44 ms per loop&#13;
&#13;
In [5]: %timeit d = b[b &gt; 500000]&#13;
1000 loops, best of 3: 1.61 ms per loop&#13;
</code></pre>&#13;
    <p class="normal">The <code class="inlineCode">numpy</code> code does exactly the same as the Python code, except that it uses <code class="inlineCode">numpy</code> arrays instead of Python lists. This little difference has made the code more than 25 times faster.</p>&#13;
    <h2 id="_idParaDest-346" class="heading-2">Just-in-time compiling</h2>&#13;
    <p class="normal"><strong class="keyWord">Just-in-time</strong> (<strong class="keyWord">JIT</strong>) compiling<a id="_idIndexMarker989"/> is a method of dynamically compiling (parts of) an application during runtime. Because there is much more information available at runtime, this can have a huge effect and make your application much faster.</p>&#13;
    <p class="normal">When it comes to JIT compiling, you currently have three options:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><strong class="keyWord">Pyston</strong>: An<a id="_idIndexMarker990"/> alternative, currently Linux only, CPython-compatible Python interpreter.</li>&#13;
      <li class="bulletList"><strong class="keyWord">Pypy</strong>: A <a id="_idIndexMarker991"/>really fast alternative Python interpreter without full CPython compatibility.</li>&#13;
      <li class="bulletList"><strong class="keyWord">Numba</strong>: A package<a id="_idIndexMarker992"/> that allows for JIT compiling per function and execution on either the CPU or the GPU.</li>&#13;
      <li class="bulletList"><strong class="keyWord">CPython 3.12 and 3.13</strong>? At<a id="_idIndexMarker993"/> the time of writing, there is little concrete data about the upcoming Python releases, but there are <a id="_idIndexMarker994"/>plans to greatly increase the CPython interpreter performance. How much will be achieved and how well it will work is currently unknown, but the ambitious plan is to make CPython 5x faster over the next 5 releases (with 3.10 being the first in the series). The expectation is to add JIT compiling in CPython 3.12 and extend that further in 3.13. </li>&#13;
    </ul>&#13;
    <p class="normal">If you are <a id="_idIndexMarker995"/>looking for global JIT compiling in existing projects, I can currently recommend trying Pyston. It is a CPython fork that promises about a 30% performance increase without having to change any code. In addition, because it is CPython-compatible, you can still use regular CPython modules. </p>&#13;
    <p class="normal">The downside is that it currently only supports Linux systems and, as will always be the case with forks, it’s behind the current Python version. At the time of writing, CPython is at Python 3.10.1, whereas Pyston is at Python 3.8.</p>&#13;
    <p class="normal">If compatibility with all CPython modules is not a requirement for you and you don’t require Python features that are too recent, PyPy3 can also offer amazing performance in many cases. They are up to Python 3.7, whereas the main Python release is at 3.10.1 at the time of writing. That makes PyPy roughly 2-3 years behind CPython in terms of features, but I doubt this is a big issue. The differences between Python 3.7, 3.8, 3.9, and 3.10 are largely incremental and Python 3.7 is already a very well-rounded Python version.</p>&#13;
    <p class="normal">The <code class="inlineCode">numba</code> package provides selective JIT compiling for you, allowing you to mark the functions that are JIT compiler-compatible. Essentially, if your functions follow the functional programming paradigm of basing the calculations only on the input, then it will most likely work with the JIT compiler.</p>&#13;
    <p class="normal">Here is a basic example of how the <a id="_idIndexMarker996"/><code class="inlineCode">numba</code> JIT compiler can be used:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numba&#13;
&#13;
<span class="hljs-meta">@numba.jit</span>&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">sum</span><span class="hljs-function">(</span><span class="hljs-params">array</span><span class="hljs-function">):</span>&#13;
    total = <span class="hljs-number">0.0</span>&#13;
    <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> array:&#13;
        total += value&#13;
    <span class="hljs-keyword">return</span> value&#13;
</code></pre>&#13;
    <p class="normal">If you are using <code class="inlineCode">numpy</code> or <code class="inlineCode">pandas</code>, you will most likely benefit from looking at <code class="inlineCode">numba</code>.</p>&#13;
    <p class="normal">Another<a id="_idIndexMarker997"/> very interesting fact to note is that <code class="inlineCode">numba</code> supports not only CPU-optimized execution, but GPU as well. This means that for certain operations you can use the fast processor in your video card to process the results.</p>&#13;
    <h2 id="_idParaDest-347" class="heading-2">Converting parts of your code to C</h2>&#13;
    <p class="normal">We will <a id="_idIndexMarker998"/>see more about this in <em class="chapterRef">Chapter 17</em>, <em class="italic">Extensions in C/C++, System Calls, and C/C++ Libraries</em>, but if high performance is really required, then a native C function can help quite a lot. This doesn’t even have to be that difficult; the Cython module makes it trivial to write parts of your code with performance very close to native C code.</p>&#13;
    <p class="normal">The following is an example from the Cython manual to approximate the value of pi:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">cdef inline double recip_square(<span class="hljs-built_in">int</span> i):&#13;
    <span class="hljs-keyword">return</span> <span class="hljs-number">1.</span>/(i*i)&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">approx_pi</span><span class="hljs-function">(</span><span class="hljs-built_in">int</span><span class="hljs-params"> n=</span><span class="hljs-number">10000000</span><span class="hljs-function">):</span>&#13;
    cdef double val = <span class="hljs-number">0.</span>&#13;
    cdef <span class="hljs-built_in">int</span> k&#13;
    <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,n+<span class="hljs-number">1</span>):&#13;
        val += recip_square(k)&#13;
    <span class="hljs-keyword">return</span> (<span class="hljs-number">6</span> * val)**<span class="hljs-number">.5</span>&#13;
</code></pre>&#13;
    <p class="normal">While there are some small differences, such as <code class="inlineCode">cdef</code> instead of <code class="inlineCode">def</code> and type definitions such as <code class="inlineCode">int i</code> instead of just <code class="inlineCode">i</code> for the values and parameters, the code is largely the same as regular Python would be, but certainly much faster.</p>&#13;
    <h1 id="_idParaDest-348" class="heading-1">Memory usage</h1>&#13;
    <p class="normal">So far, we have simply looked at the execution times and largely ignored the memory usage of the scripts. In many cases, the execution times are the most important, but memory usage<a id="_idIndexMarker999"/> should not be ignored. In almost all cases, CPU and memory are traded; an algorithm either uses a lot of CPU time or a lot of memory, which means that both do matter a lot.</p>&#13;
    <p class="normal">Within this section, we are going to look at:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Analyzing memory usage</li>&#13;
      <li class="bulletList">When Python leaks memory and how to avoid these scenarios </li>&#13;
      <li class="bulletList">How to reduce memory usage</li>&#13;
    </ul>&#13;
    <h2 id="_idParaDest-349" class="heading-2">tracemalloc</h2>&#13;
    <p class="normal">Monitoring memory usage <a id="_idIndexMarker1000"/>used to be something that was only possible through external Python modules such as <strong class="keyWord">Dowser</strong> or <strong class="keyWord">Heapy</strong>. While<a id="_idIndexMarker1001"/> those <a id="_idIndexMarker1002"/>modules <a id="_idIndexMarker1003"/>still work, they are partially obsolete now because of the <code class="inlineCode">tracemalloc</code> module. Let’s give the <code class="inlineCode">tracemalloc</code> module a try to see how easy memory usage monitoring is nowadays:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tracemalloc&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    tracemalloc.start()&#13;
&#13;
    <span class="hljs-comment"># Reserve some memory</span>&#13;
    x = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000000</span>))&#13;
&#13;
    <span class="hljs-comment"># Import some modules</span>&#13;
    <span class="hljs-keyword">import</span> os&#13;
    <span class="hljs-keyword">import</span> sys&#13;
    <span class="hljs-keyword">import</span> asyncio&#13;
&#13;
    <span class="hljs-comment"># Take a snapshot to calculate the memory usage</span>&#13;
    snapshot = tracemalloc.take_snapshot()&#13;
    <span class="hljs-keyword">for</span> statistic <span class="hljs-keyword">in</span> snapshot.statistics(<span class="hljs-string">'lineno'</span>)[:<span class="hljs-number">10</span>]:&#13;
        print(statistic)&#13;
</code></pre>&#13;
    <p class="normal">This results in:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_11_tracemalloc.py&#13;
T_11_tracemalloc.py:8: size=34.3 MiB, count=999746, average=36 B&#13;
&lt;frozen importlib._bootstrap_external&gt;:587: size=1978 KiB, coun...&#13;
&lt;frozen importlib._bootstrap&gt;:228: size=607 KiB, count=5433, av...&#13;
abc.py:85: size=32.6 KiB, count=155, average=215 B&#13;
enum.py:172: size=26.2 KiB, count=134, average=200 B&#13;
collections/__init__.py:496: size=24.1 KiB, count=117, average=...&#13;
enum.py:225: size=23.3 KiB, count=451, average=53 B&#13;
enum.py:391: size=15.0 KiB, count=21, average=729 B&#13;
&lt;frozen importlib._bootstrap_external&gt;:64: size=14.3 KiB, count...&#13;
enum.py:220: size=12.2 KiB, count=223, average=56 B&#13;
</code></pre>&#13;
    <p class="normal">You can<a id="_idIndexMarker1004"/> easily see how every part of the code allocated memory <a id="_idIndexMarker1005"/>and where it might be wasted. While it might still be unclear which part was actually causing the memory usage, there are options for that as well, as we will see in the following sections.</p>&#13;
    <h2 id="_idParaDest-350" class="heading-2">Memory Profiler</h2>&#13;
    <p class="normal">The <code class="inlineCode">memory_profiler</code> module<a id="_idIndexMarker1006"/> is very similar to <code class="inlineCode">line_profiler</code> discussed earlier, but for memory usage instead. Installing it is as easy as <code class="inlineCode">pip install memory_profiler</code>, but the optional <code class="inlineCode">pip install psutil</code> is also highly recommended (and required in the case of Windows) as it increases your performance by a large amount. To test <code class="inlineCode">memory_profiler</code>, we will use the following script:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> memory_profiler&#13;
&#13;
<span class="hljs-meta">@memory_profiler.profile</span>&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">main</span><span class="hljs-function">():</span>&#13;
    n = <span class="hljs-number">100000</span>&#13;
    a = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]&#13;
    b = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]&#13;
    c = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(n))&#13;
    d = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(n))&#13;
    e = <span class="hljs-built_in">dict</span>.fromkeys(a, b)&#13;
    f = <span class="hljs-built_in">dict</span>.fromkeys(c, d)&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    main()&#13;
</code></pre>&#13;
    <p class="normal">Note that we actually import <code class="inlineCode">memory_profiler</code> here although that is not strictly required. It can also be executed through <code class="inlineCode">python3 -m memory_profiler your_scripts.py</code>:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">Filename: CH_12_performance/T_12_memory_profiler.py&#13;
&#13;
Mem usage  Increment  Occurrences  Line Contents&#13;
===============================================&#13;
 14.7 MiB   14.7 MiB           1  @memory_profiler.profile&#13;
                                  def main():&#13;
 14.7 MiB    0.0 MiB           1      n = 100000&#13;
 18.5 MiB    3.8 MiB      100003      a = [i for i in range(n)]&#13;
 22.4 MiB    3.9 MiB      100003      b = [i for i in range(n)]&#13;
 26.3 MiB    3.9 MiB           1      c = list(range(n))&#13;
 30.2 MiB    3.9 MiB           1      d = list(range(n))&#13;
 39.9 MiB    9.8 MiB           1      e = dict.fromkeys(a, b)&#13;
 44.9 MiB    5.0 MiB           1      f = dict.fromkeys(c, d)&#13;
 44.9 MiB    0.0 MiB           1      assert e&#13;
 44.9 MiB    0.0 MiB           1      assert f<span class="code-highlight"><strong class="hljs-slc"> </strong></span>&#13;
</code></pre>&#13;
    <p class="normal">Even<a id="_idIndexMarker1007"/> though everything runs as expected, you might be wondering about the varying amounts of memory used by the lines of code here. </p>&#13;
    <p class="normal">Why does <code class="inlineCode">e</code> take <code class="inlineCode">9.8 MiB</code> and <code class="inlineCode">f</code> <code class="inlineCode">5.0 MiB</code>? This is caused by the Python memory allocation code; it reserves memory in larger blocks, which is subdivided and reused internally. Another problem is that <code class="inlineCode">memory_profiler</code> takes snapshots internally, which results in memory being attributed to the wrong variables in some cases. The variations should be small enough not to make a large difference in the end, but some changes are to be expected.</p>&#13;
    <div class="note">&#13;
      <p class="normal">This module can be added as an IPython extension as well, which enables the <code class="inlineCode">%mprun</code> command within IPython. To load the extension, the <code class="inlineCode">load_ext</code> command can be used from the IPython shell: <code class="inlineCode">%load_ext memory_profiler</code>. Another very useful command is <code class="inlineCode">%memit</code>, which is the memory equivalent of the <code class="inlineCode">%timeit</code> command.</p>&#13;
    </div>&#13;
    <h2 id="_idParaDest-351" class="heading-2">Memory leaks</h2>&#13;
    <p class="normal">The usage of these <a id="_idIndexMarker1008"/>modules will generally be limited to the search for memory leaks. In particular, the <code class="inlineCode">tracemalloc</code> module has a few features to make that fairly easy. The Python memory management system is fairly straightforward; it has a simple reference counter to see whether an object is (still) used. While this works great in most cases, it can easily introduce memory leaks when circular references are involved. The basic premise of a memory leak with leak detection code looks like this:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-number">1</span> <span class="hljs-keyword">import</span> tracemalloc&#13;
  <span class="hljs-number">2</span>&#13;
  <span class="hljs-number">3</span>&#13;
  <span class="hljs-number">4</span> <span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">SomeClass</span><span class="hljs-class">:</span>&#13;
  <span class="hljs-number">5</span>     <span class="hljs-keyword">pass</span>&#13;
  <span class="hljs-number">6</span>&#13;
  <span class="hljs-number">7</span>&#13;
  <span class="hljs-number">8</span> <span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'</span><span class="hljs-string">__main__'</span>:&#13;
  <span class="hljs-number">9</span>     <span class="hljs-comment"># Initialize some variables to ignore them from the leak</span>&#13;
 <span class="hljs-number">10</span>     <span class="hljs-comment"># detection</span>&#13;
 <span class="hljs-number">11</span>     n = <span class="hljs-number">100000</span>&#13;
 <span class="hljs-number">12</span>&#13;
 <span class="hljs-number">13</span>     tracemalloc.start()&#13;
 <span class="hljs-number">14</span>     <span class="hljs-comment"># Your application should initialize here</span>&#13;
 <span class="hljs-number">15</span>&#13;
 <span class="hljs-number">16</span>     snapshot_a = tracemalloc.take_snapshot()&#13;
 <span class="hljs-number">17</span>     instances = []&#13;
 <span class="hljs-number">18</span>&#13;
 <span class="hljs-number">19</span>     <span class="hljs-comment"># This code should be the memory leaking part</span>&#13;
 <span class="hljs-number">20</span>     <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):&#13;
 <span class="hljs-number">21</span>         a = SomeClass()&#13;
 <span class="hljs-number">22</span>         b = SomeClass()&#13;
 <span class="hljs-number">23</span>         <span class="hljs-comment"># Circular reference. a references b, b references a</span>&#13;
 <span class="hljs-number">24</span>         a.b = b&#13;
 <span class="hljs-number">25</span>         b.a = a&#13;
 <span class="hljs-number">26</span>         <span class="hljs-comment"># Force Python to keep the object in memory for now</span>&#13;
 <span class="hljs-number">27</span>         instances.append(a)&#13;
 <span class="hljs-number">28</span>&#13;
 <span class="hljs-number">29</span>     <span class="hljs-comment"># Clear the list of items again. Now all memory should be</span>&#13;
 <span class="hljs-number">30</span>     <span class="hljs-comment"># released, right?</span>&#13;
 <span class="hljs-number">31</span>     <span class="hljs-keyword">del</span> instances&#13;
 <span class="hljs-number">32</span>     snapshot_b = tracemalloc.take_snapshot()&#13;
 <span class="hljs-number">33</span>&#13;
 <span class="hljs-number">34</span>     statistics = snapshot_b.compare_to(snapshot_a, <span class="hljs-string">'</span><span class="hljs-string">lineno'</span>)&#13;
 <span class="hljs-number">35</span>     <span class="hljs-keyword">for</span> statistic <span class="hljs-keyword">in</span> statistics[:<span class="hljs-number">10</span>]:&#13;
 <span class="hljs-number">36</span>         print(statistic)&#13;
</code></pre>&#13;
    <div class="note">&#13;
      <p class="normal">The line numbers in the code above are provided as a reference for the <code class="inlineCode">tracemalloc</code> output and are not functionally part of the code.</p>&#13;
    </div>&#13;
    <p class="normal">The big problem<a id="_idIndexMarker1009"/> in this code is that we have two objects that are referencing each other. As we can see, <code class="inlineCode">a.b</code> is referencing <code class="inlineCode">b</code>, and <code class="inlineCode">b.a</code> is referencing <code class="inlineCode">a</code>. This loop makes it so that Python doesn’t immediately understand that the objects can be safely deleted from memory.</p>&#13;
    <p class="normal">Let’s see how badly this code is actually leaking:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_12_memory_leaks.py&#13;
T_12_memory_leaks.py:25: size=22.1 MiB (+22.1 MiB), count=199992 (+199992), average=116 B&#13;
T_12_memory_leaks.py:24: size=22.1 MiB (+22.1 MiB), count=199992 (+199992), average=116 B&#13;
T_12_memory_leaks.py:22: size=4688 KiB (+4688 KiB), count=100000 (+100000), average=48 B&#13;
T_12_memory_leaks.py:21: size=4688 KiB (+4688 KiB), count=100000 (+100000), average=48 B&#13;
tracemalloc.py:423: size=88 B (+88 B), count=2 (+2), average=44 B&#13;
tracemalloc.py:560: size=48 B (+48 B), count=1 (+1), average=48 B&#13;
tracemalloc.py:315: size=40 B (+40 B), count=1 (+1), average=40 B&#13;
T_12_memory_leaks.py:20: size=28 B (+28 B), count=1 (+1), average=28 B&#13;
</code></pre>&#13;
    <p class="normal">This example<a id="_idIndexMarker1010"/> shows a leak of 22.1 megabytes due to the nearly 200,000 instances of <code class="inlineCode">SomeClass</code>. Python correctly lets us know that this memory was allocated at lines 24 and 25, which can really help when trying to ascertain what is causing the memory usage in your application.</p>&#13;
    <p class="normal">The Python garbage collector (<code class="inlineCode">gc</code>) is smart enough to clean circular references like these eventually, but it won’t clean them until a certain limit is reached. More about that soon.</p>&#13;
    <h3 id="_idParaDest-352" class="heading-3">Circular references</h3>&#13;
    <p class="normal">Whenever<a id="_idIndexMarker1011"/> you want to have a circular reference that does not cause memory leaks, the <code class="inlineCode">weakref</code> module is available. It creates references that don’t count toward the object reference count. Before we look at the <code class="inlineCode">weakref</code> module, let’s take a look at the object references themselves through the eyes of the Python garbage collector (<code class="inlineCode">gc</code>):</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gc&#13;
&#13;
<span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">SomeClass</span><span class="hljs-class">(</span><span class="hljs-built_in">object</span><span class="hljs-class">):</span>&#13;
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, name</span><span class="hljs-function">):</span>&#13;
        self.name = name&#13;
&#13;
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__repr__</span><span class="hljs-function">(</span><span class="hljs-params">self</span><span class="hljs-function">):</span>&#13;
        <span class="hljs-keyword">return</span> <span class="hljs-string">f'&lt;</span><span class="hljs-subst">{self.__class__.__name__}</span><span class="hljs-string">: </span><span class="hljs-subst">{self.name}</span><span class="hljs-string">'</span>&#13;
&#13;
<span class="hljs-comment"># Create the objects</span>&#13;
a = SomeClass(<span class="hljs-string">'a'</span>)&#13;
b = SomeClass(<span class="hljs-string">'b'</span>)&#13;
<span class="hljs-comment"># Add some circular references</span>&#13;
a.b = a&#13;
b.a = b&#13;
&#13;
<span class="hljs-comment"># Remove the objects</span>&#13;
<span class="hljs-keyword">del</span> a&#13;
<span class="hljs-keyword">del</span> b&#13;
&#13;
<span class="hljs-comment"># See if the objects are still there</span>&#13;
print(<span class="hljs-string">'Before manual collection:'</span>)&#13;
<span class="hljs-keyword">for</span> object_ <span class="hljs-keyword">in</span> gc.get_objects():&#13;
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(object_, SomeClass):&#13;
        print(<span class="hljs-string">'\t'</span>, object_, gc.get_referents(object_))&#13;
&#13;
print(<span class="hljs-string">'After manual collection:'</span>)&#13;
gc.collect()&#13;
<span class="hljs-keyword">for</span> object_ <span class="hljs-keyword">in</span> gc.get_objects():&#13;
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(object_, SomeClass):&#13;
        print(<span class="hljs-string">'\t'</span>, object_, gc.get_referents(object_))&#13;
&#13;
print(<span class="hljs-string">'Thresholds:'</span>, gc.get_threshold())&#13;
</code></pre>&#13;
    <p class="normal">First, we<a id="_idIndexMarker1012"/> create two instances of <code class="inlineCode">SomeClass</code> and add some circular references between them. Once that is done, we delete them from memory, except that they are not actually deleted until the garbage collector runs.</p>&#13;
    <p class="normal">To verify this, we inspect the objects in memory through <code class="inlineCode">gc.get_objects()</code>, and until we tell the garbage collector to manually collect, they stay in memory.</p>&#13;
    <p class="normal">Once we do run <code class="inlineCode">gc.collect()</code> to manually call the garbage collector, the objects are gone from memory:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_14_garbage_collection.py&#13;
Before manual collection:&#13;
         &lt;SomeClass: a&gt; [{'name': 'a', 'b': &lt;SomeClass: a&gt;}, &lt;class '__main__.SomeClass'&gt;]&#13;
         &lt;SomeClass: b&gt; [{'name': 'b', 'a': &lt;SomeClass: b&gt;}, &lt;class '__main__.SomeClass'&gt;]&#13;
After manual collection:&#13;
Thresholds: (700, 10, 10)&#13;
</code></pre>&#13;
    <p class="normal">Now, you might wonder, are you always required to manually call <code class="inlineCode">gc.collect()</code> to remove these references? No, that is not needed, as the Python garbage collector will automatically collect once thresholds have been reached.</p>&#13;
    <p class="normal">By default, the <a id="_idIndexMarker1013"/>thresholds for the Python garbage collector are set to <code class="inlineCode">700, 10, 10</code> for the three generations of collected objects. The collector keeps track of all the memory allocations and deallocations in Python, and as soon as the number of allocations minus the number of deallocations reaches <code class="inlineCode">700</code>, the object is either removed if it’s not referenced anymore, or it is moved to the next generation if it still has a reference. The same is repeated for generations 2 and 3, albeit with the lower thresholds of 10.</p>&#13;
    <p class="normal">This begs the question: where and when is it useful to manually call the garbage collector? Since the Python memory allocator reuses blocks of memory and only rarely releases it, for long-running scripts the garbage collector can be very useful. That’s exactly where I recommend its usage: long-running scripts in memory-strapped environments and, specifically, right before you <strong class="keyWord">allocate</strong> a large amount of memory. If you call the garbage collector before doing a memory-intensive operation, you can maximize the amount of reuse of the memory that Python has previously reserved.</p>&#13;
    <h3 id="_idParaDest-353" class="heading-3">Analyzing memory usage using the garbage collector</h3>&#13;
    <p class="normal">The <code class="inlineCode">gc</code> module <a id="_idIndexMarker1014"/>can help you a lot when looking for memory leaks as well. The <code class="inlineCode">tracemalloc</code> module can show you the parts that <a id="_idIndexMarker1015"/>take the most memory in bytes, but the <code class="inlineCode">gc</code> module can help you find the most commonly occurring object types (for example, <code class="inlineCode">SomeClass</code>, <code class="inlineCode">int</code>, and <code class="inlineCode">list</code>). Just be careful when setting the garbage collector debug settings such as <code class="inlineCode">gc.set_debug(gc.DEBUG_LEAK)</code>; this returns a large amount of output even if you don’t reserve any memory yourself. Let’s see the output for one of the most basic scripts you can get:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gc&#13;
<span class="hljs-keyword">import</span> collections&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    objects = collections.Counter()&#13;
    <span class="hljs-keyword">for</span> object_ <span class="hljs-keyword">in</span> gc.get_objects():&#13;
        objects[<span class="hljs-built_in">type</span>(object_)] += <span class="hljs-number">1</span>&#13;
&#13;
    print(<span class="hljs-string">f'Different object count: </span><span class="hljs-subst">{</span><span class="hljs-built_in">len</span><span class="hljs-subst">(objects)}</span><span class="hljs-string">'</span>)&#13;
    <span class="hljs-keyword">for</span> object_, count <span class="hljs-keyword">in</span> objects.most_common(<span class="hljs-number">10</span>):&#13;
        print(<span class="hljs-string">f'</span><span class="hljs-subst">{count}</span><span class="hljs-string">: </span><span class="hljs-subst">{object_}</span><span class="hljs-string">'</span>)&#13;
</code></pre>&#13;
    <p class="normal">Now, when<a id="_idIndexMarker1016"/> we run the code, you can see a<a id="_idIndexMarker1017"/> bit of what has been added to our memory with such a simple script:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_15_garbage_collection_viewing.py&#13;
Different object count: 42&#13;
1058: &lt;class 'wrapper_descriptor'&gt;&#13;
887: &lt;class 'function'&gt;&#13;
677: &lt;class 'method_descriptor'&gt;&#13;
652: &lt;class 'builtin_function_or_method'&gt;&#13;
545: &lt;class 'dict'&gt;&#13;
484: &lt;class 'tuple'&gt;&#13;
431: &lt;class 'weakref'&gt;&#13;
251: &lt;class 'member_descriptor'&gt;&#13;
238: &lt;class 'getset_descriptor'&gt;&#13;
76: &lt;class 'type'&gt;&#13;
</code></pre>&#13;
    <p class="normal">As you can see, there are actually 42 different types of objects that should have been shown here, but even without that, the number of different objects in memory is impressive, if you ask me. With just a little bit of extra code, the output can quickly explode and become unusable without significant filtering.</p>&#13;
    <h3 id="_idParaDest-354" class="heading-3">Weak references</h3>&#13;
    <p class="normal">An easy method to <a id="_idIndexMarker1018"/>make the work easier for the garbage collector is to use <strong class="keyWord">weak references</strong>. These are references to variables that are not included when counting the references to a variable. Since the garbage collector removes an object from memory when its reference count gets to zero, this can help a lot with memory leaks.</p>&#13;
    <p class="normal">In the earlier example, we saw that the objects weren’t removed until we manually called <code class="inlineCode">gc.collect()</code>. Now we will see what happens if we use the <code class="inlineCode">weakref</code> module instead:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gc&#13;
<span class="hljs-keyword">import</span> weakref&#13;
<span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">SomeClass</span><span class="hljs-class">(</span><span class="hljs-built_in">object</span><span class="hljs-class">):</span>&#13;
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, name</span><span class="hljs-function">):</span>&#13;
        self.name = name&#13;
&#13;
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__repr__</span><span class="hljs-function">(</span><span class="hljs-params">self</span><span class="hljs-function">):</span>&#13;
        <span class="hljs-keyword">return</span> <span class="hljs-string">'&lt;%s: %s&gt;'</span> % (self.__class__.__name__, self.name)&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">print_mem</span><span class="hljs-function">(</span><span class="hljs-params">message</span><span class="hljs-function">):</span>&#13;
    print(message)&#13;
    <span class="hljs-keyword">for</span> object_ <span class="hljs-keyword">in</span> gc.get_objects():&#13;
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(object_, SomeClass):&#13;
            print(<span class="hljs-string">'</span><span class="hljs-string">\t'</span>, object_, gc.get_referents(object_))&#13;
&#13;
<span class="hljs-comment"># Create the objects</span>&#13;
a = SomeClass(<span class="hljs-string">'a'</span>)&#13;
b = SomeClass(<span class="hljs-string">'b'</span>)&#13;
&#13;
<span class="hljs-comment"># Add some weak circular references</span>&#13;
a.b = weakref.ref(a)&#13;
b.a = weakref.ref(b)&#13;
&#13;
print_mem(<span class="hljs-string">'Objects in memory before del:'</span>)&#13;
&#13;
<span class="hljs-comment"># Remove the objects</span>&#13;
<span class="hljs-keyword">del</span> a&#13;
<span class="hljs-keyword">del</span> b&#13;
&#13;
<span class="hljs-comment"># See if the objects are still there</span>&#13;
print_mem(<span class="hljs-string">'</span><span class="hljs-string">Objects in memory after del:'</span>)&#13;
</code></pre>&#13;
    <p class="normal">Now let’s <a id="_idIndexMarker1019"/>see what remained this time:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_16_weak_references.py&#13;
Objects in memory before del:&#13;
         &lt;SomeClass: a&gt; [{'name': 'a', 'b': ...}, ...]&#13;
         &lt;SomeClass: b&gt; [{'name': 'b', 'a': ...}, ...]&#13;
Objects in memory after del:&#13;
</code></pre>&#13;
    <p class="normal">Perfect – no instances of <code class="inlineCode">SomeClass</code> exist in memory after <code class="inlineCode">del</code>, which is exactly what we had hoped for.</p>&#13;
    <h3 id="_idParaDest-355" class="heading-3">Weakref limitations and pitfalls</h3>&#13;
    <p class="normal">You might be <a id="_idIndexMarker1020"/>wondering what happens when you still try to reference a since-removed <code class="inlineCode">weakref</code>. As you would expect, the object is gone now, so you can no longer use it. What is more, not all objects can be used through weak references directly:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span> weakref&#13;
&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> weakref.ref(<span class="hljs-con-built_in">dict</span>(a=<span class="hljs-con-number">123</span>))&#13;
Traceback (most recent call last):&#13;
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;&#13;
TypeError: cannot create weak reference to 'dict' object&#13;
&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> weakref.ref([<span class="hljs-con-number">1</span>, <span class="hljs-con-number">2</span>, <span class="hljs-con-number">3</span>])&#13;
Traceback (most recent call last):&#13;
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;&#13;
TypeError: cannot create weak reference to 'list' object&#13;
&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> weakref.ref(<span class="hljs-con-string">'test'</span>)&#13;
Traceback (most recent call last):&#13;
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;&#13;
TypeError: cannot create weak reference to 'str' object&#13;
&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> weakref.ref(<span class="hljs-con-string">b'test'</span>)&#13;
Traceback (most recent call last):&#13;
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;&#13;
TypeError: cannot create weak reference to 'bytes' object&#13;
&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> a = weakref.WeakValueDictionary(a=<span class="hljs-con-number">123</span>)&#13;
Traceback (most recent call last):&#13;
    ...&#13;
TypeError: cannot create weak reference to 'int' object&#13;
</code></pre>&#13;
    <p class="normal">We can use <code class="inlineCode">weakref</code> for custom classes though, so we can subclass the types before we create the <code class="inlineCode">weakref</code>:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">class</span><span class="hljs-con-class"> </span><span class="hljs-con-title">CustomDict</span><span class="hljs-con-class">(</span><span class="hljs-con-built_in">dict</span><span class="hljs-con-class">):</span>&#13;
<span class="hljs-con-meta">...</span>     <span class="hljs-con-keyword">pass</span>&#13;
&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> weakref.ref(CustomDict())&#13;
&lt;weakref at 0x...; dead&gt;&#13;
</code></pre>&#13;
    <p class="normal">For <code class="inlineCode">dict</code> and <code class="inlineCode">set</code> instances, the <code class="inlineCode">weakref</code> library also has the <code class="inlineCode">weakref.WeakKeyDictionary</code>, <code class="inlineCode">weakref.WeakValueDictionary</code>, and <code class="inlineCode">weakref.WeakSet</code> classes. These behave similarly to the regular instances of <code class="inlineCode">dict</code> and <code class="inlineCode">set</code>, but remove the values based on the key or value.</p>&#13;
    <p class="normal">We need to <a id="_idIndexMarker1021"/>be careful when using a <code class="inlineCode">weakref</code>, of course. As soon as all regular references are deleted, the object will be inaccessible:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">class</span><span class="hljs-con-class"> </span><span class="hljs-con-title">SomeClass</span><span class="hljs-con-class">:</span>&#13;
<span class="hljs-con-meta">...</span>     <span class="hljs-con-keyword">def</span><span class="hljs-con-function"> </span><span class="hljs-con-title">__init__</span><span class="hljs-con-function">(</span><span class="hljs-con-params">self, name</span><span class="hljs-con-function">):</span>&#13;
<span class="hljs-con-meta">...</span>         self.name = name&#13;
&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> a = SomeClass(<span class="hljs-con-string">'a'</span>)&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> b = weakref.proxy(a)&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> b.name&#13;
'a'&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">del</span> a&#13;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> b.name&#13;
Traceback (most recent call last):&#13;
    ...&#13;
ReferenceError: weakly-referenced object no longer exists&#13;
</code></pre>&#13;
    <p class="normal">After deleting <code class="inlineCode">a</code>, which is the only real reference to the <code class="inlineCode">SomeClass</code> instance, we cannot use the instance anymore. While this is to be expected, you should be wary of this problem if your main reference has a chance to disappear.</p>&#13;
    <p class="normal">Whenever you are working with large self-referencing data structures, it can be a good idea to use the <code class="inlineCode">weakref</code> module. However, don’t forget to check if your instance still exists before using it.</p>&#13;
    <h2 id="_idParaDest-356" class="heading-2">Reducing memory usage</h2>&#13;
    <p class="normal">In general, memory usage <a id="_idIndexMarker1022"/>probably won’t be your biggest problem in Python, but it can still be useful to know what you can do to reduce it. When trying to reduce memory usage, it’s important to understand how Python allocates memory.</p>&#13;
    <p class="normal">There are four concepts that you need to know about within the Python memory manager:</p>&#13;
    <ul>&#13;
      <li class="bulletList">First, we have the <strong class="keyWord">heap</strong>. The <a id="_idIndexMarker1023"/>heap is the collection of all Python-managed memory. Note that this is separate from the regular heap and mixing the two could result in corrupt memory and crashes.</li>&#13;
      <li class="bulletList">Second are the <strong class="keyWord">arenas</strong>. These<a id="_idIndexMarker1024"/> are the chunks that Python requests from the system. These chunks have a fixed size of 256 KiB each and they are the objects that make up the heap.</li>&#13;
      <li class="bulletList">Third we have the <strong class="keyWord">pools</strong>. These<a id="_idIndexMarker1025"/> are the chunks of memory that make up the arenas. These chunks are 4 KiB each. Since the pools and arenas have fixed sizes, they are simple arrays.</li>&#13;
      <li class="bulletList">Fourth and last, we have the <strong class="keyWord">blocks</strong>. The <a id="_idIndexMarker1026"/>Python objects get stored within these and every block has a specific format depending on the data type. Since an integer takes up more space than a character, for efficiency, a different block size is used.</li>&#13;
    </ul>&#13;
    <p class="normal">Now that we<a id="_idIndexMarker1027"/> know how the memory is allocated, we can also understand how it can be returned to the operating system and why this is often very hard to do.</p>&#13;
    <p class="normal">Releasing a block back to the pool is easy enough: a simple <code class="inlineCode">del some_variable</code> followed by a <code class="inlineCode">gc.collect()</code> should do the trick. The problem is that this is no guarantee that the memory will be released back to the operating system yet.</p>&#13;
    <p class="normal">To illustrate what needs to happen in order for the memory to release to the operating system:</p>&#13;
    <ul>&#13;
      <li class="bulletList">All blocks in a pool need to be released before the pool can be released</li>&#13;
      <li class="bulletList">All pools in an arena need to be released before the arena can be released</li>&#13;
      <li class="bulletList">Once the arena has been released to the heap, memory <em class="italic">might</em> be released to the operating system, but that depends on the C runtime and/or operating system</li>&#13;
    </ul>&#13;
    <p class="normal">That is why I would always recommend running <code class="inlineCode">gc.collect()</code> in long-running scripts right before you start allocating large blocks of memory.</p>&#13;
    <div class="note">&#13;
      <p class="normal">It is a common and incorrect misconception that Python never releases any memory to the system. Before Python 2.5, this was indeed the case because arenas were never freed to the heap.</p>&#13;
    </div>&#13;
    <p class="normal">Let’s illustrate the effects of allocating and releasing memory by allocating and releasing twice:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> os&#13;
<span class="hljs-keyword">import</span> psutil&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">print_usage</span><span class="hljs-function">(</span><span class="hljs-params">message</span><span class="hljs-function">):</span>&#13;
    process = psutil.Process(os.getpid())&#13;
    usage = process.memory_info().rss / (<span class="hljs-number">1</span> &lt;&lt; <span class="hljs-number">20</span>)&#13;
    print(<span class="hljs-string">f'Memory usage </span><span class="hljs-subst">{message}</span><span class="hljs-string">: </span><span class="hljs-subst">{usage:</span><span class="hljs-number">.1</span><span class="hljs-subst">f}</span><span class="hljs-string"> MiB'</span>)&#13;
&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">allocate_and_release</span><span class="hljs-function">():</span>&#13;
    <span class="hljs-comment"># Allocate large block of memory</span>&#13;
    large_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000000</span>))&#13;
    print_usage(<span class="hljs-string">'after allocation'</span>)&#13;
&#13;
    <span class="hljs-keyword">del</span> large_list&#13;
    print_usage(<span class="hljs-string">'after releasing'</span>)&#13;
&#13;
print_usage(<span class="hljs-string">'initial'</span>)&#13;
allocate_and_release()&#13;
allocate_and_release()&#13;
</code></pre>&#13;
    <p class="normal">You might <a id="_idIndexMarker1028"/>expect that the memory usage after the second block has been released will be near identical to after the first block has been released, or even back to the original state. Let’s see what actually happens:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_18_freeing_memory.py&#13;
Memory usage initial: 9.4 MiB&#13;
Memory usage after allocation: 48.1 MiB&#13;
Memory usage after releasing: 17.3 MiB&#13;
Memory usage after allocation: 55.7 MiB&#13;
Memory usage after releasing: 25.0 MiB&#13;
</code></pre>&#13;
    <p class="normal">That’s odd, isn’t it? The memory usage has grown between the two allocations. The truth is that I cherry-picked the result somewhat and that the output changes between each run, because releasing memory back to the operating system is not a guarantee that the operating system will immediately handle it. In some other cases, the memory had properly returned to 17 MiB.</p>&#13;
    <p class="normal">The astute among you might wonder if the results are skewed because I forgot the <code class="inlineCode">gc.collect()</code>. In this case, the answer is no because the memory allocation is large enough to immediately trigger the garbage collector by itself and the difference is negligible.</p>&#13;
    <p class="normal">This is roughly the best case, however – just a few contiguous blocks of memory. The real challenge is when you have many variables so only parts of the pools/arenas are used. Python uses some heuristics to find space in an empty arena so it doesn’t have to allocate new arenas when you are storing new variables, but that does not always succeed, of course. This is a case where running <code class="inlineCode">gc.collect()</code> before allocation can help because<a id="_idIndexMarker1029"/> it can tell Python which pools are now free.</p>&#13;
    <div class="note">&#13;
      <p class="normal">It is important to note that the regular heap and Python heap are maintained separately, as mixing them can result in corruption and/or the crashing of applications. Unless you write your own Python extensions in C/C++, you will probably never have to worry about manual memory allocation though.</p>&#13;
    </div>&#13;
    <h3 id="_idParaDest-357" class="heading-3">Generators versus lists</h3>&#13;
    <p class="normal">The most<a id="_idIndexMarker1030"/> important tip is to use generators whenever possible. Python 3 has come a long way in replacing lists with generators already, but it really pays to keep that in mind as it saves not only memory, but CPU as well, when not all of that memory needs to be kept at the same time.</p>&#13;
    <p class="normal">To illustrate the difference:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">Line #    Mem usage    Increment   Line Contents&#13;
================================================&#13;
     4     11.0 MiB      0.0 MiB   @memory_profiler.profile&#13;
     5                             def main():&#13;
     6     11.0 MiB      0.0 MiB    a = range(1000000)&#13;
     7     49.7 MiB     38.6 MiB    b = list(range(1000000))&#13;
</code></pre>&#13;
    <p class="normal">The <code class="inlineCode">range()</code> generator takes such little memory that it doesn’t even register, whereas the list of numbers takes <code class="inlineCode">38.6 MiB</code>.</p>&#13;
    <h3 id="_idParaDest-358" class="heading-3">Recreating collections versus removing items</h3>&#13;
    <p class="normal">One<a id="_idIndexMarker1031"/> very important detail about collections in Python is that many of them can only grow; they won’t just shrink by themselves. To illustrate:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">Mem usage    Increment   Line Contents&#13;
======================================&#13;
 11.5 MiB      0.0 MiB   @memory_profiler.profile&#13;
                         def main():&#13;
                         # Generate a huge dict&#13;
 26.3 MiB     14.8 MiB   a = dict.fromkeys(range(100000))&#13;
&#13;
                         # Remove all items&#13;
 26.3 MiB      0.0 MiB   for k in list(a.keys()):&#13;
 26.3 MiB      0.0 MiB   del a[k]&#13;
&#13;
                         # Recreate the dict&#13;
 23.6 MiB     -2.8 MiB   a = dict((k, v) for k, v in a.items())&#13;
</code></pre>&#13;
    <p class="normal">Even after <a id="_idIndexMarker1032"/>removing all items from the <code class="inlineCode">dict</code>, the memory usage remains the same. This is one of the most common memory usage mistakes made with lists and dictionaries. The only way to reclaim the memory is by recreating the object. Or, never allocate the memory at all by using generators.</p>&#13;
    <h3 id="_idParaDest-359" class="heading-3">Using slots</h3>&#13;
    <p class="normal">In addition to the<a id="_idIndexMarker1033"/> performance benefits of using <code class="inlineCode">__slots__</code> that we saw earlier in this chapter, <code class="inlineCode">__slots__</code> can also help to reduce memory usage. As a recap, <code class="inlineCode">__slots__</code> allows you to specify which fields you want to store in a class and it skips all the others by not implementing <code class="inlineCode">instance.__dict__</code>. </p>&#13;
    <p class="normal">While this method does save a little bit of memory in your class definitions, the effect is often limited. For a nearly empty class with just a single/tiny attribute such as a <code class="inlineCode">bool</code> or <code class="inlineCode">byte</code>, this can make quite a bit of difference. For classes that actually store a bit of data, the effect can diminish quickly.</p>&#13;
    <p class="normal">The biggest caveat of <code class="inlineCode">__slots__</code> is that multiple inheritance is impossible if both parent classes have <code class="inlineCode">__slots__</code> defined. Beyond that, it can be used in nearly all cases.</p>&#13;
    <p class="normal">You might wonder if <code class="inlineCode">__slots__</code> will limit dynamic attribute assignments, effectively blocking you from doing <code class="inlineCode">Spam.eggs = 123</code> if <code class="inlineCode">eggs</code> was not part of <code class="inlineCode">__slots__</code>. And you are right – partially, at least. With a standard fixed list of attributes in <code class="inlineCode">__slots__</code>, you cannot dynamically add new attributes – but you can if you add <code class="inlineCode">__dict__</code> to <code class="inlineCode">__slots__</code>. </p>&#13;
    <p class="normal">I’m embarrassed to say that it took me about 15 years before I found out about this feature, but knowing about this feature makes <code class="inlineCode">__slots__</code> so much more versatile that I really feel like I should mention it.</p>&#13;
    <p class="normal">Let’s now illustrate<a id="_idIndexMarker1034"/> the difference in memory usage:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> memory_profiler&#13;
&#13;
<span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Slots</span><span class="hljs-class">(</span><span class="hljs-built_in">object</span><span class="hljs-class">):</span>&#13;
    __slots__ = <span class="hljs-string">'index'</span>, <span class="hljs-string">'name'</span>, <span class="hljs-string">'description'</span>&#13;
&#13;
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, index</span><span class="hljs-function">):</span>&#13;
        self.index = index&#13;
        self.name = <span class="hljs-string">'slot %d'</span> % index&#13;
        self.description = <span class="hljs-string">'some slot with index %d'</span> % index&#13;
&#13;
<span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">NoSlots</span><span class="hljs-class">(</span><span class="hljs-built_in">object</span><span class="hljs-class">):</span>&#13;
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, index</span><span class="hljs-function">):</span>&#13;
        self.index = index&#13;
        self.name = <span class="hljs-string">'slot %d'</span> % index&#13;
        self.description = <span class="hljs-string">'some slot with index %d'</span> % index&#13;
&#13;
<span class="hljs-meta">@memory_profiler.profile</span>&#13;
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">main</span><span class="hljs-function">():</span>&#13;
    slots = [Slots(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">25000</span>)]&#13;
    no_slots = [NoSlots(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">25000</span>)]&#13;
    <span class="hljs-keyword">return</span> slots, no_slots&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    main()&#13;
</code></pre>&#13;
    <p class="normal">And the memory usage:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">Mem usage Increment Occurrences Line Contents&#13;
============================================&#13;
38.4 MiB  38.4 MiB          1 @memory_profiler.profile&#13;
                              def main():&#13;
44.3 MiB   5.9 MiB      25003     slots = [Slots(i) for i in range(25000)]&#13;
52.4 MiB   8.1 MiB      25003     no_slots = [NoSlots(i) for i in range(25000)]&#13;
52.4 MiB   0.0 MiB          1     return slots, no_slots&#13;
</code></pre>&#13;
    <p class="normal">You might argue that this is not a fair comparison since they both store a lot of data, which skews the results. And you would indeed be right because the “bare” comparison, storing only <code class="inlineCode">index</code> and nothing else, gives <code class="inlineCode">2 MiB</code> versus <code class="inlineCode">4.5 MiB</code>. But, let’s be honest, if you’re not going <a id="_idIndexMarker1035"/>to store data, then what’s the point in creating class instances? I’m not saying that <code class="inlineCode">__slots__</code> have no purpose, but don’t go overboard because the advantages are generally limited.</p>&#13;
    <p class="normal">There is one more structure that’s even more memory-efficient: the <code class="inlineCode">array</code> module. It stores the data in pretty much the same way a bare memory array in C would do. Note that this is generally slower than lists and much less convenient to use. If you need to store large amounts of numbers, I would suggest looking at <code class="inlineCode">numpy.array</code> or <code class="inlineCode">scipy.sparse</code> instead.</p>&#13;
    <h1 id="_idParaDest-360" class="heading-1">Performance monitoring</h1>&#13;
    <p class="normal">So far, we have <a id="_idIndexMarker1036"/>seen how to measure and improve both CPU and memory performance, but there is one part we have completely skipped over. Performance changes due to external factors such as growing amounts of data are very hard to predict. In real-life applications, bottlenecks aren’t constant. They change all the time and code that was once extremely fast might bog down as soon as more load is applied.</p>&#13;
    <p class="normal">Because of that, I recommend implementing a monitoring solution that tracks the performance of anything and everything over time. The big problem with performance monitoring is that you can’t know what will slow down in the future and what the cause is going to be. I’ve even had websites slow down because of Memcached and Redis calls. These are memory-only caching servers that respond well within a millisecond, which makes slowdowns highly unlikely, until you do over 100 cache calls and the latency toward the cache server increases from 0.1 milliseconds to 2 milliseconds, and all of a sudden those 100 calls take 200 milliseconds instead of 10 milliseconds. Even though 200 milliseconds still sounds like very little, if your total page load time is generally below 100 milliseconds, that is, all of a sudden, an enormous increase and definitely noticeable.</p>&#13;
    <p class="normal">To monitor performance and to be able to track changes over time and find the responsible components, I can personally recommend several systems for monitoring performance:</p>&#13;
    <ul>&#13;
      <li class="bulletList">For simple short-term (up to a few weeks) application performance tracking, the <strong class="keyWord">Prometheus</strong> monitoring<a id="_idIndexMarker1037"/> system is very easy to set up and when paired <a id="_idIndexMarker1038"/>with <strong class="keyWord">Grafana</strong>, you can create the prettiest charts to monitor your performance.</li>&#13;
      <li class="bulletList">If you want a more long-term performance tracking solution that scales well to large numbers of variables, you might be<a id="_idIndexMarker1039"/> interested in <strong class="keyWord">InfluxDB</strong> instead. It can also be paired with Grafana for really useful interactive charting:</li>&#13;
    </ul>&#13;
    <figure class="mediaobject"><img src="Images/B15882_12_02.png" alt="" width="882" height="207"/></figure>&#13;
    <p class="packt_figref">Figure 12.2: Grafana heatmap of response times</p>&#13;
    <figure class="mediaobject"><img src="Images/B15882_12_03.png" alt="" width="881" height="184"/></figure>&#13;
    <p class="packt_figref">Figure 12.3: Grafana chart of request latency</p>&#13;
    <p class="normal">To enter <a id="_idIndexMarker1040"/>data into systems like these, you have several options. You can use the native APIs, but you can also use an intermediate system such as <strong class="keyWord">StatsD</strong>. The StatsD <a id="_idIndexMarker1041"/>system doesn’t store data itself, but it makes it really easy to fire and forget performance metrics from your system without having to worry whether the monitoring system is still up and running. Because the system commonly uses UDP to send the information, even if the monitoring server is completely down and unreachable, your application won’t notice the difference.</p>&#13;
    <p class="normal">To be able to use these, you will have to send the metrics from your application to the StatsD server. To do just that, I have written<a id="_idIndexMarker1042"/> the Python-StatsD (<a href="Chapter_12.xhtml"><span class="url">https://pypi.org/project/python-statsd/</span></a>) and Django-StatsD (<a href="Chapter_12.xhtml"><span class="url">https://pypi.org/project/django-statsd/</span></a>) packages. These packages allow you to monitor your<a id="_idIndexMarker1043"/> application from beginning to end and, in the case of Django, you will be able to monitor your performance per application or view, and within those see all of the components, such as the database, template, and caching layers. This way, you know exactly what is causing the slowdowns in your website (or application). And best of all, it is in (near) real time.</p>&#13;
    <h1 id="_idParaDest-361" class="heading-1">Exercises</h1>&#13;
    <p class="normal">Now that you have learned about many of the available tools for performance measuring and optimization, try and create a few useful decorators or context wrappers that will help you prevent issues:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Try to create a decorator that monitors each run of a function and warns you if the memory usage grows each run.</li>&#13;
      <li class="bulletList">Try to create a decorator that monitors the runtime of a function and warns you if it deviates too much from the previous run. Optionally, you could make the function generate a (running) average runtime as well.</li>&#13;
      <li class="bulletList">Try to create a memory manager for your classes that warns you when more than a configured number of instances remain in memory. If you never expect more than 5 instances of a certain class, you can warn the user when that number is exceeded.</li>&#13;
      </ul>&#13;
        <div class="note">&#13;
          <p class="normal">Example answers for these exercises can be found on GitHub: <a href="Chapter_12.xhtml"><span class="url">https://github.com/mastering-python/exercises</span></a>. You are encouraged to submit your own solutions and learn about alternative solutions from others.</p>&#13;
        </div>&#13;
     &#13;
    &#13;
    <h1 id="_idParaDest-362" class="heading-1">Summary</h1>&#13;
    <p class="normal">When it comes to performance, there is no holy grail, no single thing you can do to ensure peak performance in all cases. This shouldn’t worry you, however, as in most cases, you will never need to tune the performance and, if you do, a single tweak could probably fix your problem. You should be able to find performance problems and memory leaks in your code now, which is what matters most, so just try to contain yourself and only tweak when it’s actually needed.</p>&#13;
    <p class="normal">Here is a quick recap of the tools in this chapter:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Measuring CPU performance: <code class="inlineCode">timeit</code>, <code class="inlineCode">profile</code>/<code class="inlineCode">cProfile</code>, and <code class="inlineCode">line_profiler</code></li>&#13;
      <li class="bulletList">Analyzing profiling results: SnakeViz, <code class="inlineCode">pyprof2calltree</code>, and QCacheGrind</li>&#13;
      <li class="bulletList">Measuring memory usage: <code class="inlineCode">tracemalloc</code>, <code class="inlineCode">memory_profiler</code></li>&#13;
      <li class="bulletList">Reducing memory usage and leaks: <code class="inlineCode">weakref</code> and <code class="inlineCode">gc</code> (garbage collector)</li>&#13;
    </ul>&#13;
    <p class="normal">If you know how to use these tools, you should be able to track down and fix most performance issues in your code.</p>&#13;
    <p class="normal">The most important takeaways from this chapter are:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Test before you invest any effort. Making some functions faster seems like a great achievement, but it is only rarely needed.</li>&#13;
      <li class="bulletList">Choosing the correct data structure/algorithm is much more effective than any other performance optimization.</li>&#13;
      <li class="bulletList">Circular references drain the memory until the garbage collector starts cleaning.</li>&#13;
      <li class="bulletList">Slots come with several caveats, so I would recommend limited usage.</li>&#13;
    </ul>&#13;
    <p class="normal">The next chapter will properly introduce us to working asynchronously using the <code class="inlineCode">asyncio</code> module. This module makes it possible to “background” the waiting for external I/O. Instead of keeping your foreground thread busy, it can switch to a different thread when your code is waiting for endpoints such as TCP, UDP, files, and processes.</p>&#13;
    <h1 class="heading-1">Join our community on Discord</h1>&#13;
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers: <a href="Chapter_12.xhtml"><span class="url">https://discord.gg/QMzJenHuJf</span></a></p>&#13;
    <p class="normal"><img src="Images/QR_Code156081100001293319171.png" alt="" width="177" height="177"/></p>&#13;
  </div>&#13;
</div></body></html>