- en: Concurrent Downloading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, our crawlers downloaded web pages sequentially, waiting
    for each download to complete before starting the next one. Sequential downloading
    is fine for the relatively small example website but quickly becomes impractical
    for larger crawls. To crawl a large website of one million web pages at an average
    of one web page per second would take over 11 days of continuous downloading.
    This time can be significantly improved by downloading multiple web pages simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will cover downloading web pages with multiple threads and processes
    and comparing the performance with sequential downloading.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: One million web pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential crawler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threaded crawler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiprocessing crawler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One million web pages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To test the performance of concurrent downloading, it would be preferable to
    have a larger target website. For this reason, we will use the Alexa list, which
    tracks the top one million most popular websites according to users who have installed
    the Alexa Toolbar. Only a small percentage of people use this browser plugin,
    so the data is not authoritative, but it's fine for our purposes and gives us
    a larger list to crawl.
  prefs: []
  type: TYPE_NORMAL
- en: These top one million web pages can be browsed on the Alexa website at [http://www.alexa.com/topsites](http://www.alexa.com/topsites).
    Additionally, a compressed spreadsheet of this list is available at [http://s3.amazonaws.com/alexa-static/top-1m.csv.zip](http://s3.amazonaws.com/alexa-static/top-1m.csv.zip),
    so scraping Alexa is not necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing the Alexa list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Alexa list is provided in a spreadsheet with columns for the rank and domain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4364OS_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Extracting this data requires a number of steps, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the `.zip` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the CSV file from the `.zip` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate each row of the CSV file to extract the domain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is an implementation to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You may have noticed that the downloaded zipped data is wrapped with the `BytesIO` class
    and passed to `ZipFile`. This is necessary because `ZipFile` expects a file-like
    interface rather than a raw byte object. We also utilize `stream=True`, which
    helps speed up the request. Next, the CSV filename is extracted from the filename
    list. The `.zip` file only contains a single file, so the first filename is selected.
    Then, the CSV file is read using a `TextIOWrapper` to help handle encoding and
    read issues. This file is then iterated, and the domain in the second column is
    added to the URL list. The `http://` protocol is prepended to each domain to make
    them valid URLs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reuse this function with the crawlers developed earlier, it needs to be
    modified to an easily callable class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A new input argument was added here, called `max_urls`, which sets the number
    of URLs to extract from the Alexa file. By default, this is set to 500 URLs because
    downloading a million web pages takes a long time (as mentioned in the chapter
    introduction, more than 11 days when downloaded sequentially).
  prefs: []
  type: TYPE_NORMAL
- en: Sequential crawler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now use `AlexaCallback` with a slightly modified version of the link
    crawler we developed earlier to download the top 500 Alexa URLs sequentially.
    To update the link crawler, it will now take either a start URL or a list of start
    URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to update the way the `robots.txt` is handled for each site. We
    use a simple dictionary to store the parsers per domain (see: [https://github.com/kjam/wswp/blob/master/code/chp4/advanced_link_crawler.py#L53-L72](https://github.com/kjam/wswp/blob/master/code/chp4/advanced_link_crawler.py#L53-L72)).
    We also need to handle the fact that not every URL we encounter will be relative,
    and some of them aren''t even URLs we can visit, such as e-mail addresses with `mailto:`
    or `javascript:` event commands. Additionally, due to some sites not having the `robots.txt`
    files and other poorly formed URLs, there are some additional error-handling sections
    added and a new `no_robots` variable, which allows us to continue crawling if
    we cannot, in good faith, find a `robots.txt` file. Finally, we added a `socket.setdefaulttimeout(60)`
    to handle timeouts for the `robotparser` and some additional `timeout` arguments
    for the `Downloader` class in [Chapter 3](py-web-scrp-2e_ch03.html), *Caching
    Downloads*,.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary code to handle these cases is available at [https://github.com/kjam/wswp/blob/master/code/chp4/advanced_link_crawler.py](https://github.com/kjam/wswp/blob/master/code/chp4/advanced_link_crawler.py).
    The new crawler can then be used directly with the `AlexaCallback` and run from
    the command line as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Taking a look at the code that runs in the `__main__` section of the file,
    we use `''$^''` as our pattern to avoid collecting links from each page. You can
    also try to crawl all links on every page using `''.''` to match everything. (Warning:
    This will take a long time, potentially days!)'
  prefs: []
  type: TYPE_NORMAL
- en: The time for only crawling the first page is as expected for sequential downloading,
    with an average of ~2.7 seconds per URL (this includes the time to test the `robots.txt`
    file). Depending on your ISP speeds, and if you run the script on a server in
    the cloud, you might see much faster results.
  prefs: []
  type: TYPE_NORMAL
- en: Threaded crawler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will extend the sequential crawler to download the web pages in parallel.
    Note that, if misused, a threaded crawler could request content too quickly and
    overload a web server or cause your IP address to be blocked.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this, our crawlers will have a `delay` flag to set the minimum number
    of seconds between requests to the same domain.
  prefs: []
  type: TYPE_NORMAL
- en: The Alexa list example used in this chapter covers one million separate domains,
    so this particular problem does not apply here. However, a delay of at least one
    second between downloads should be considered when crawling many web pages from
    a single domain in the future.
  prefs: []
  type: TYPE_NORMAL
- en: How threads and processes work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a diagram of a process containing multiple threads of execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4364OS_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: When a Python script or any other computer program is run, a process is created,
    containing the code and state, as well as the stack. These processes are executed
    by the CPU cores of a computer. However, each core can only execute a single thread at
    a time and will quickly switch between them to give the impression that multiple
    programs are running simultaneously. Similarly, within a process, the program
    execution can switch between multiple threads, with each thread executing different
    parts of the program.
  prefs: []
  type: TYPE_NORMAL
- en: This means that when one thread is waiting for a web page to download, the process
    can switch and execute another thread to avoid wasting CPU cycles. So, using all
    the compute resources on our computer to download data as fast as possible requires
    distributing our downloads across multiple threads and processes.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a multithreaded crawler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fortunately, Python makes threading relatively straightforward. This means
    we can keep a similar queuing structure to the link crawler developed in [Chapter
    1](py-web-scrp-2e_ch01.html), *Introduction to Web Scraping*, but start the crawl
    loop in multiple threads to download these links in parallel. Here is a modified
    version of the start of the link crawler with the `crawl` loop moved into a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the remainder of the `threaded_crawler` function to start `process_queue`
    in multiple threads and wait until they have completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The loop in the preceding code will keep creating threads while there are URLs
    to crawl until it reaches the maximum number of threads set. During the crawl,
    threads may also prematurely shut down when there are currently no more URLs in
    the queue. For example, consider a situation when there are two threads and two
    URLs to download. When the first thread finishes its download, the crawl queue
    is empty so this thread exits. However, the second thread may then complete its
    download and discover additional URLs to download. The `thread` loop will then
    notice that there are still more URLs to download, and the maximum number of threads
    has not been reached, so it will create a new download thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'We might also want to add parsing to this threaded crawler later. To do so,
    we can add a section for a function callback using the returned HTML. We likely
    want to return even more links from this logic or extraction, so we need to also
    expand the links we parse in the later `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The fully updated code can be viewed at [https://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler.py.](https://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler.py)
    To have a fair test, you will also need to flush your `RedisCache` or use a different
    default database. If you have the `redis-cli` installed, you can do so easily
    from your command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To exit, use your normal program exit (usually *Ctrl* + *C* or c*md* + *C*).
    Now, let''s test the performance of this multi-threaded version of the link crawler
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you take a look at the `__main__` section of this crawler, you will note
    that you can easily pass arguments to this script including `max_threads` and `url_pattern`.
    In the previous example, we are using the defaults of `max_threads=5` and `url_pattern='$^'`.
  prefs: []
  type: TYPE_NORMAL
- en: Since there are five threads, downloading is nearly four times faster! Again,
    your results might vary depending on your ISP or if you run the script from a
    server. Further analysis of thread performance will be covered in the *Performance*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing crawler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To improve the performance further, the threaded example can be extended to
    support multiple processes. Currently, the crawl queue is held in local memory,
    which means other processes cannot contribute to the same crawl. To address this,
    the crawl queue will be transferred to Redis. Storing the queue independently
    means that even crawlers on separate servers could collaborate on the same crawl.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more robust queuing, a dedicated distributed task tool, such as Celery,
    should be considered; however, Redis will be reused here to minimize the number
    of technologies and dependencies introduced. Here is an implementation of the
    new Redis-backed queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can see in the preceding `RedisQueue` class that we are maintaining a few
    different data types. First, we have the expected Redis list type, which is handled
    via the `lpush` and `rpop` commands, and the name of the queue is stored in the `self.name`
    attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Next we have a Redis set, which functions similarly to a Python set with a unique
    membership. The set name is stored in `self.seen_set` and is managed via the `sadd`
    and `sismember` methods (to add new keys and test membership).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have moved the depth functionality to the `set_depth` and `get_depth` methods,
    which use a normal Redis hash table with the name stored in `self.depth` and each
    URL as the key with the depth as the value. One useful addition to the code would
    be to set the last time a domain was accessed so we can make a more efficient `delay`
    functionality for our `Downloader` class. This is left as an exercise for the
    reader.
  prefs: []
  type: TYPE_NORMAL
- en: If you want a queue with more functionality but with the same availability as
    Redis, I recommend looking at `python-rq`  ([http://python-rq.org/](http://python-rq.org/)),
    which is an easy-to-use-and-install Python job queue similar to Celery but with
    less functionality and dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing with our current `RedisQueue` implementation, we need to make a
    few updates to the threaded crawler to support the new queue type, which are highlighted
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The first change is replacing our Python list with the new Redis-based queue,
    named `Redis``Queue`. This queue handles duplicate URLs internally, so the `seen`
    variable is no longer required. Finally, the `RedisQueue` `len` method is called
    to determine if there are still URLs in the queue. Further logic changes to handle
    the depth and seen functionality are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The full code can be seen at [http://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler_with_queue.py](http://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler_with_queue.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'This updated version of the threaded crawler can then be started using multiple
    processes with this snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This structure might look familiar because the multiprocessing module follows
    a similar interface to the threading module used earlier in the chapter. This
    code either utilizes the number of CPUs available (eight on my machine) or the
     `num_procs` as passed via arguments when starting the script. Then, each process starts
    the threaded crawler and waits for all the processes to complete execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s test the performance of this multiprocess version of the link crawler
    using the following command. The code for `mp_threaded``_crawler` is available
    at [http://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler_with_queue.py](http://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler_with_queue.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As detected by the script, my machine has eight CPUs (four physical cores and
    four virtual cores), and the default setting for threads is five.  To use a different
    combination, you can see the arguments expected by using the `-h` command, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `-h` command is also available for testing different values in the `threaded_crawler.py`
    script.
  prefs: []
  type: TYPE_NORMAL
- en: For the default options with eight processes and five threads per process, the
    running time is ~1.8X faster than that of the previous threaded crawler using
    a single process. In the next section, we will further investigate the relative
    performances of these three approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To further understand how increasing the number of threads and processes affects
    the time required when downloading, here is a table of results for crawling 500
    web pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Script** | **Number of threads** | **Number of processes** | **Time** |
    **Comparison with sequential** | **Errors Seen?** |'
  prefs: []
  type: TYPE_TB
- en: '| Sequential | 1 | 1 | 1349.798s | 1 | N |'
  prefs: []
  type: TYPE_TB
- en: '| Threaded | 5 | 1 | 361.504s | 3.73 | N |'
  prefs: []
  type: TYPE_TB
- en: '| Threaded | 10 | 1 | 275.492s | 4.9 | N |'
  prefs: []
  type: TYPE_TB
- en: '| Threaded | 20 | 1 | 298.168s | 4.53 | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Processes | 2 | 2 | 726.899s | 1.86 | N |'
  prefs: []
  type: TYPE_TB
- en: '| Processes | 2 | 4 | 559.93s | 2.41 | N |'
  prefs: []
  type: TYPE_TB
- en: '| Processes | 2 | 8 | 451.772s | 2.99 | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Processes | 5 | 2 | 383.438s | 3.52 | N |'
  prefs: []
  type: TYPE_TB
- en: '| Processes | 5 | 4 | 156.389s | 8.63 | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Processes | 5 | 8 | 296.610s | 4.55 | Y |'
  prefs: []
  type: TYPE_TB
- en: The fifth column shows the proportion of time in comparison to the base case
    of sequential downloading. We can see that the increase in performance is not
    linearly proportional to the number of threads and processes but appears logarithmic,
    that is, until adding more threads actually decreases performance. For example,
    one process and five threads lead to 4X better performance, but 10 threads only
    leads to 5X better performance, and using 20 threads actually decreases performance.
    Depending on your system, these performance gains and losses may vary; however,
    it's well known that each extra thread helps expedite execution but is less effective
    than the previously added thread (that is, it is not a linear speedup). This is
    to be expected, considering the process has to switch between more threads and
    can devote less time to each.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the amount of bandwidth available for downloading is limited,
    so eventually adding additional threads will not lead to a faster download speed.
    If you run these yourself, you may notice errors, such as `urlopen error [Errno
    101] Network is unreachable`, sprinkled throughout your testing, particularly
    when using high numbers of threads or processes. This is obviously suboptimal
    and leads to more frequent downloading errors than you would experience when choosing
    a lower number of threads. Of course, network constraints will be different if
    you are running this on a more distributed setup or in a cloud server environment.
    The final column in the preceding table tracks the errors experienced in these
    trials from my single laptop using a normal ISP cable connection.
  prefs: []
  type: TYPE_NORMAL
- en: Your results may vary, and this chart was built using a laptop rather than a
    server (which would have better bandwidth and fewer background processes); so,
    I challenge you to build a similar chart for your computer and/or servers. Once
    you discover the bounds of your machine(s), achieving greater performance would
    require distributing the crawl across multiple servers, all pointing to the same
    Redis instance.
  prefs: []
  type: TYPE_NORMAL
- en: Python multiprocessing and the GIL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a longer performance review of Python's threads and processes, one must
    first understand the **Global Interpreter Lock** (**GIL**). The GIL is a mechanism
    used by the Python interpreter to execute code using only one thread at a time,
    meaning Python code will only execute linearly (even when using multiprocessing
    and multiple cores). This design decision was made so Python could run quickly
    but still be thread-safe.
  prefs: []
  type: TYPE_NORMAL
- en: If you haven't already seen it, I recommend watching David Beazley's  Understanding
    the GIL talk from
  prefs: []
  type: TYPE_NORMAL
- en: PyCon 2010 ([https://www.youtube.com/watch?v=Obt-vMVdM8s](https://www.youtube.com/watch?v=Obt-vMVdM8s)).
    Beazley also has numerous write-ups on his blog and some interesting talks on
    the GILectomy (attempting to remove the GIL from Python for speedier multiprocessing).
  prefs: []
  type: TYPE_NORMAL
- en: The GIL puts an extra performance burden on high I/O operations, like what we
    are doing with our web scraper. There are also ways to utilize Python's multiprocessing
    library for better shared data across processes and threads.
  prefs: []
  type: TYPE_NORMAL
- en: We could have written our scraper as a map with a worker pool or queue to compare
    Python's own multiprocessing internals with our Redis-based system. We could also
    use asynchronous programming to better thread performance and higher network utilization.
    Asynchronous libraries, such as async, tornado, or even NodeJS, allow rograms
    to execute in a non-blocking manner, meaning processes can switch to a different
    thread when waiting for responses from the web server. It is likely some of these implementations
    might be faster for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we can use projects such as PyPy ([https://pypy.org/](https://pypy.org/))
    to help increase threading and multiprocessing speed. That said, measure your
    performance and evaluate your needs before implementing optimizations (don't optimize
    prematurely). It is a good rule to ask just how important speed is over clarity
    and how correct intuition is over actual observation. Remember the Zen of Python
    and proceed accordingly!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered why sequential downloading creates performance bottlenecks.
    We looked at how to download large numbers of web pages efficiently across multiple
    threads and processes and compared when optimizations or increasing threads and
    processes might be useful and when they could be harmful. We also implemented
    a new Redis queue which we can use across several machines or processes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover how to scrape content from web pages which load
    their content dynamically using JavaScript.
  prefs: []
  type: TYPE_NORMAL
