- en: Distributed Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we introduced the concept of parallel processing and learned
    how to leverage multicore processors and GPUs. Now, we can step up our game a
    bit and turn our attention on distributed processing, which involves executing
    tasks across multiple machines to solve a certain problem.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will illustrate the challenges, use cases, and examples
    of how to run code on a cluster of computers. Python offers easy-to-use and reliable
    packages for distribute processing, which will allow us to implement scalable
    and fault-tolerant code with relative ease.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of topics for this chapter is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed computing and the MapReduce model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directed Acyclic Graphs with Dask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing parallel code with Dask's `array`, `Bag`, and `DataFrame` data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributing parallel algorithms with Dask Distributed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark's Resilient Distributed Datasets and DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scientific computing with `mpi4py`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to distributed computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today's world, computers, smartphones, and other devices have become an integral
    part of our lives. Every day, massive quantities of data is produced. Billions
    of people access services on the Internet, and companies are constantly collecting
    data to learn about their users to better target products and improve user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Handling this ever increasing amount of data presents substantial challenges.
    Large companies and organizations often build clusters of machines designed to
    store, process, and analyze large and complex datasets. Similar datasets are also
    produced in data-intensive fields such as environmental sciences and health care.
    These large-scale datasets have been recently called **big data**. The analysis
    techniques applied to big data usually involve a combination of machine learning,
    information retrieval, and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Computing clusters have been used for decades in scientific computing, where
    the study of complex problems requires the use of parallel algorithms executed
    on high-performance distributed systems. For such applications, universities and
    other organizations provide and manage supercomputers for research and engineering
    purposes. Applications that run on supercomputers are generally focused on highly
    numerical workloads, such as protein and molecular simulations, quantum mechanical
    calculations, climate models, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: The challenges of programming for distributed systems are apparent if we think
    back on how the cost of communication increases as we distribute data and computational
    tasks across a local network. Network transfers are extremely slow compared to
    the processor speed, and when using distributed processing, it is even more important
    to keep network communications as limited as possible. This can be achieved using
    a few different strategies that favor local data processing and resort to data
    transfers only when strictly necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Other challenges of distributed processing involve the general unreliability
    of computer networks. When you think that in a computing cluster there may be
    thousands of machines, it becomes clear that (probabilistically speaking) faulty
    nodes become very common. For this reason, distributed systems need to be able
    to handle node failures gracefully and without disrupting the ongoing work. Luckily,
    companies have invested a great deal of resources in developing fault-tolerant
    distributed engines that take care of these aspects automatically.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**MapReduce** is a programming model that allows you to express algorithms
    for efficient execution on a distributed system. The MapReduce model was first
    introduced by Google in 2004 ([https://research.google.com/archive/mapreduce.html](https://research.google.com/archive/mapreduce.html)),
    as a way to automatically partition datasets over different machines and for automatic
    local processing and the communication between *cluster nodes*.'
  prefs: []
  type: TYPE_NORMAL
- en: The MapReduce framework was used in cooperation with a distributed filesystem,
    the **Google File System** (GFS or GoogleFS), which was designed to partition
    and replicate data across the computing cluster. Partitioning was useful for storing
    and processing datasets that wouldn't fit on a single node while replication ensured
    that the system was able to handle failures gracefully. MapReduce was used by
    Google, in conjunction with GFS, for indexing of their web pages. Later on, the
    MapReduce and GFS concepts were implemented by Doug Cutting (at the time, an employee
    at Yahoo!), resulting in the first versions of the **Hadoop Distributed File System**
    (**HDFS**) and Hadoop MapReduce.
  prefs: []
  type: TYPE_NORMAL
- en: 'The programming model exposed by MapReduce is actually quite simple. The idea
    is to express the computation as a combination of two, fairly generic, steps:
    *Map* and *Reduce*. Some readers will probably be familiar with Python''s `map`
    and `reduce` functions; however, in the context of MapReduce, the Map and Reduce
    steps are capable of representing a broader class of operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Map takes a collection of data as input and produces a *transformation* on this
    data. What is generally emitted by Map is a series of key value pairs that can
    be passed to a Reduce step. The Reduce step will aggregate items with the same
    key and apply a function to the collection to form a usually smaller collection
    of values.
  prefs: []
  type: TYPE_NORMAL
- en: The estimation of *pi*, which was shown in the last chapter, can be trivially
    converted using a series of Map and Reduce steps. In that case, the input was
    a collection of pairs of random numbers. The transformation (Map step) was the
    hit test, and the Reduce step was counting the number of times the hit test was
    True.
  prefs: []
  type: TYPE_NORMAL
- en: The prototypical example of the MapReduce model is the implementation of a word
    count; the program takes a series of documents as input, and returns, for each
    word, the total number of occurrences in the document collection. The following
    figure illustrates the Map and Reduce steps of the word count program. On the
    left, we have the input documents. The Map operation will produce a (key, value)
    entry where the first element is the word and the second element is **1** (that's
    because every word contributes **1** to the final count).
  prefs: []
  type: TYPE_NORMAL
- en: 'We then perform the reduce operation to aggregate all the elements of the same
    key and produce the global count for each of the words. In the figure, we can
    see how all values of the items with key **the** are summed to produce the final
    entry (**the, 4**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_08CHPNO_01.png)'
  prefs: []
  type: TYPE_IMG
- en: If we implement our algorithm using the Map and Reduce operation, the framework
    implementation will ensure that data production and aggregation is done efficiently,
    by limiting the communication between nodes through clever algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: However, how does MapReduce manage to keep communication to a minimum? Let's
    go through the journey of a MapReduce task. Imagine that you have a cluster with
    two nodes, and a partition of the data (this is usually found locally in each
    node) is loaded in each node from disk and is ready for processing. A mapper process
    is created in each node and processes the data to produce the intermediate results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, it is necessary to send the data to the reducer for further processing.
    In order to do this, however, it is necessary that all the items that possess
    the same key are shipped to the same reducer. This operation is called **shuffling**
    and is the principal communication task in the MapReduce model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_08CHPNO_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, before the data exchange happens, it is necessary to assign a subset
    of keys to each reducer; this step is called **partitioning**.  Once a reducer
    receives its own partition of keys, it is free to process data and write the resulting
    output on disk.
  prefs: []
  type: TYPE_NORMAL
- en: The MapReduce framework (through the Apache Hadoop project) has been extensively
    used in its original form by many companies and organizations. More recently,
    new frameworks that extend the ideas introduced by MapReduce have been developed
    to create systems able to express more complex workflows, to use memory more efficiently
    and to support a lean and efficient execution of distributed tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will describe two of the most used libraries in
    the Python distributed landscape: Dask and PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Dask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Dask** is a project of Continuum Analytics (the same company that''s responsible
    for Numba and the `conda` package manager) and a pure Python library for parallel
    and distributed computation. It excels at performing data analysis tasks and is
    very well integrated in the Python ecosystem.'
  prefs: []
  type: TYPE_NORMAL
- en: Dask was initially conceived as a package for bigger-than-memory calculations
    on a single machine. Recently, with the Dask Distributed project, its code has
    been adapted to execute tasks on a cluster with excellent performance and fault-tolerance
    capabilities. It supports MapReduce-style tasks as well as complex numerical algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Directed Acyclic Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea behind Dask is quite similar to what we already saw in the last chapter
    with Theano and Tensorflow. We can use a familiar Pythonic API to build an execution
    plan, and the framework will automatically split the workflow into tasks that
    will be shipped and executed on multiple processes or computers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dask expresses its variables and operations as a **Directed Acyclic Graph**
    (**DAG**) that can be represented through a simple Python dictionary. To briefly
    illustrate how this works, we will implement the sum of two numbers with Dask.
    We will define our computational graph by storing the values of our input variables
    in the dictionary. The `a` and `b` input variables will be given a value of `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Each variable represents a node in the DAG. The next step necessary to build
    our DAG is the execution of operations on the nodes we just defined. In Dask,
    a task can be defined by placing a tuple containing a Python function and its
    positional arguments in the `dsk` dictionary. To implement a `sum`, we can add
    a new node, named `result`, (the actual name is completely arbitrary) with a tuple
    containing the function we intend to execute, followed by its arguments. This
    is illustrated in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For better style and clarity, we can calculate the sum by replacing the `lambda`
    statement with the standard `operator.add` library function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It's important to note that the arguments we intend to pass to the function
    are the `"a"` and `"b"` strings, which refer to the `a` and `b` nodes in the graph.
    Note that we didn't use any Dask-specific functions to define the DAG; this is
    the first indication of how the framework is flexible and lean since all manipulations
    are performed on simple and familiar Python dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The execution of tasks is performed by a scheduler, which is a function that
    takes a DAG and the task or tasks we''d like to perform and returns the computed
    value. The default Dask scheduler is the `dask.get` function, which can be used
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: All the complexity is hidden behind the scheduler, which will take care of distributing
    the tasks across threads, processes, or even different machines. The `dask.get`
    scheduler is a synchronous and serial implementation that is useful for testing
    and debugging purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Defining graphs using a simple dictionary is useful to understand how Dask does
    its magic and for debugging purposes. Raw dictionaries can also be used to implement
    more complex algorithms not covered by the Dask API. Now, we will learn how Dask
    is capable of generating tasks automatically through a familiar NumPy- and Pandas-like
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: Dask arrays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main use-cases of Dask is the automatic generation of parallel array
    operations, which greatly simplifies the handling of arrays that don't fit into
    memory. The strategy employed by Dask is to split the array into a number of subunits
    that, in Dask array terminology, are called **chunks**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dask implement a NumPy-like interface for arrays in the `dask.array` module
    (which we will abbreviate as `da`). An array can be created from a NumPy-like
    array using the `da.from_array` function, which requires the specification of
    a chunk size. The `da.from_array` function will return a `da.array` object that
    will handle the splitting of the original array into subunits of the specified
    chunk size. In the following example, we create an array of `30` elements, and
    we split it into chunks with `10` elements each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `a_da` variable maintains a Dask graph that can be accessed using the `dask`
    attribute. To understand what Dask does under the hood, we can inspect its content.
    In the following example, we can see that the Dask graph contains four nodes.
    One of them is the source array, denoted by the `''array-original-4c76''` key,
    the other three keys in the `a_da.dask` dictionary are tasks that are used to
    access a chunk of the original array using the `dask.array.core.getarray` function
    and, as you can see, each task extracts a slice of 10 elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If we perform an operation on the `a_da` array, Dask will generate more subtasks
    that operate on the smaller chunks, opening the possibility of achieving parallelism.
    The interface exposed by `da.array` is compatible with common NumPy semantics
    and broadcasting rules. The complete code, shown as follows, demonstrates the
    good compatibility of Dask with NumPy broadcasting rules, element-wise operations,
    and other methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The value of pi can be calculated using the `compute` method, which can also
    be called with the `get` optional argument to specify a different scheduler (by
    default, `da.array` uses a multithreaded scheduler):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Even deceptively simple algorithms, such as the estimation of pi, may require
    a lot of tasks to be executed. Dask provides utilities to visualize the computational graph.
    The following figure shows part of the Dask graph for the estimation of pi, which
    can be obtained by executing the method `pi.visualize()`. In the graph, circles
    refer to transformations that get applied on the nodes, which are represented
    as rectangles. This example helps us to get a feel of the complexity of the Dask
    graph and to appreciate the scheduler''s job of creating an efficient execution
    plan that includes proper ordering of tasks and the selection of tasks that will
    be executed in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dask_graph-e1489790395473.png)'
  prefs: []
  type: TYPE_IMG
- en: Dask Bag and DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dask provides other data structures for automatic generation of computation
    graphs. In this subsection, we'll take a look at `dask.bag.Bag`, a generic collection
    of elements that can be used to code MapReduce-style algorithms, and `dask.dataframe.DataFrame`,
    a distributed version of `pandas.DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `Bag` can be easily created from a Python collection. For example, you can
    create a `Bag` from a list using the `from_sequence` factory function. The level
    of parallelism can be specified using the `npartitions` argument (this will distribute
    the `Bag` content into a number of partitions). In the following example, we create
    a `Bag` containing numbers from `0` to `99`, partitioned into four chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the next example, we will demonstrate how to perform a word count of a set
    of strings using an algorithm that's similar to MapReduce. Given our collection
    of sequences, we apply `str.split`, followed by `concat` to obtain a linear list
    of words in the documents. Then, for each word, we produce a dictionary that contains
    a word and the value `1` (refer to the *An introduction to MapReduce* section
    for an illustration). We then write a *Reduce* step using the `foldby` operator
    to calculate the word count.
  prefs: []
  type: TYPE_NORMAL
- en: The `foldby` transformation is useful to implement a Reduce step that combines
    the word counts without having to shuffle all the elements over the network. Imagine
    that our word dataset is divided into two partitions. A good strategy to calculate
    the total count is to first sum the word occurrences in each partition and then
    combine those partial sums to get the final result. The following figure illustrates
    the concept. On the left, we have our input partitions. The partial sum is calculated
    for each individual partition (this is done using a binary operation, **binop**),
    and then the final sums are computed by combining the partial sums using a **combine**
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_08CHPNO_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code illustrates how to use `Bag` and the `foldby` operator to
    compute the word count. For the `foldby` operator, we need to define two functions
    that take five arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`key`: This is a function that returns the key for the reduce operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binop`: This is a function that takes two arguments: `total` and `x`. Given
    a `total` value (the values accumulated so far), `binop` incorporates the next
    item into the total.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initial`: This is the initial value for the `binop` accumulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combine`: This is a function that combines the totals for each partition (in
    this case it is a simple sum).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initial_combine`: This is the initial value for the `combine` accumulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As we just saw, expressing complex operations in an efficient way using `Bag`
    can become cumbersome. For this reason, Dask provides another data structure designed
    for analytical workloads--`dask.dataframe.DataFrame`. A `DataFrame` can be initialized
    in Dask using a variety of methods, such as from `CSV` files on distributed filesystems,
    or directly from a `Bag`. Just like `da.array` provides an API that closely mirrors
    NumPy features, Dask `DataFrame` can be used as a distributed version of `pandas.DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a demonstration, we will re-implement the word count using a `DataFrame`.
    We first load the data to obtain a `Bag` of words, and then we convert the `Bag`
    to a `DataFrame` using the `to_dataframe` method. By passing a column name to
    the `to_dataframe` method, we can initialize a `DataFrame`, which contains a single
    column, named `words`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Dask `DataFrame` closely replicates the `pandas.DataFrame` API. To compute
    the word count, we only need to call the `value_counts` method on the words column,
    and Dask will automatically devise a parallel computation strategy. To trigger
    the calculation, it is sufficient to call the `compute` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'An interesting question one may ask is "*what kind of algorithm does DataFrame
    use under the hood?*". The answer can be found by looking at the upper part of
    the generated Dask graph, which is displayed in the following figure. The first
    two rectangles at the bottom represent two partitions of the dataset, which are
    stored as two `pd.Series` instances. To calculate the overall count, Dask will
    first execute `value_counts` on each of the `pd.Series` and then combine the counts along
    with the `value_counts_aggregate` step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_08CHPNO_05.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, both Dask `array` and `DataFrame` take advantage of the fast
    vectorized implementations of NumPy and Pandas to achieve excellent performance
    and stability.
  prefs: []
  type: TYPE_NORMAL
- en: Dask distributed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first iterations of the Dask project were designed to run on a single computer
    using a thread-based or a process-based scheduler. Recently, the implementation
    of a new distributed backend can be used to set up and run Dask graphs on a network
    of computers.
  prefs: []
  type: TYPE_NORMAL
- en: Dask distributed is not installed automatically with Dask. The library is available
    through the `conda` package manager (use the `$ conda install distributed` command )
    as well as `pip` (with the `$ pip install distributed` command).
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting started with Dask distributed is really easy. The most basic setup
    is obtained by instantiating a `Client` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: By default, Dask will start a few key processes (on the local machine) necessary
    for scheduling and executing distributed tasks through the `Client` instance.
    The main components of a Dask cluster are a single *scheduler* and a collection
    of *workers*.
  prefs: []
  type: TYPE_NORMAL
- en: The **scheduler** is the process responsible for distributing the work across
    the workers and to monitor and manage the results. Generally, when a task is submitted
    to the user, the scheduler will find a free worker and submit a task for execution.
    Once the worker is done, the scheduler is informed that the result is available.
  prefs: []
  type: TYPE_NORMAL
- en: A worker is a process that accepts incoming tasks and produces results. Workers
    can reside on different machines over the network. Workers execute tasks using
    `ThreadPoolExecutor`. This can be used to achieve parallelism when using functions
    that do not acquire the GIL (such as Numpy, Pandas, and Cython functions in `nogil`
    blocks). When executing pure Python code, it is advantageous to start many single-threaded
    worker processes as this will enable parallelism for code that acquires the GIL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Client` class can be used to submit tasks manually to the scheduler using
    familiar asynchronous methods. For example, to submit a function for execution
    on the cluster, one can use the `Client.map` and `Client.submit` methods. In the
    following code, we demonstrate the use of `Client.map` and `Client.submit` to
    calculate the square of a few numbers. The `Client` will submit a series of tasks
    to the scheduler and we will receive a `Future` instance for each task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: So far, this is quite similar to what we saw in the earlier chapters with `TheadPoolExecutor` and
    `ProcessPoolExecutor`. Note however, that Dask Distributed not only submits the
    tasks, but also caches the computation results on the worker memory. You can see
    caching in action by looking at the preceding code example. When we first invoke
    `client.submit`, the `square(2)` task is created and its status is set to *pending*.
    When we subsequently invoke `client.map`,  the `square(2)` task is resubmitted
    to the scheduler, but this time, rather than recalculating its value, the scheduler
    directly retrieves the result for the worker. As a result, the third `Future`
    returned by map already has a finished status.
  prefs: []
  type: TYPE_NORMAL
- en: 'Results from a collection of `Future` instances can be retrieved using the
    `Client.gather` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`Client` can also be used to run arbitrary Dask graphs. For example, we can
    trivially run our approximation of pi by passing the `client.get` function as
    an optional argument to `pi.compute`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This feature makes Dask extremely scalable as it is possible to develop and
    run algorithms on the local machine using one of the simpler schedulers and, in
    case the performance is not satisfactory, to reuse the same algorithms on a cluster
    of hundreds of machines.
  prefs: []
  type: TYPE_NORMAL
- en: Manual cluster setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To instantiate scheduler and workers manually, one can use the `dask-scheduler`
    and `dask-worker` command-line utilities. First, we can initialize a scheduler
    using the `dask-scheduler` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This will provide an address for the scheduler and a Web UI address that can
    be accessed to monitor the state of the cluster. Now, we can assign some workers
    to the scheduler; this can be accomplished using the `dask-worker` command and
    by passing the address of the scheduler to the worker. This will automatically
    start a worker with four threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The Dask scheduler is fairly resilient in the sense that if we add and remove
    a worker, it is able to track which results are unavailable and recompute them
    on-demand. Finally, in order to use the initialized scheduler from a Python session,
    it is sufficient to initialize a `Client` instance and provide the address for
    the scheduler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Dask also provides a convenient diagnostic Web UI that can be used to monitor
    the status and time spent for each of the tasks performed on the cluster. In the
    next figure, the **Task Stream** shows the time taken for executing the pi estimation.
    In the plot, each horizontal gray line corresponds to a thread used by the workers
    (in our case, we have one worker with four threads, also called **Worker Core**),
    and each rectangular box corresponds to a task, colored so that the same task
    types have the same color (for example, addition, power, or exponent). From this
    plot, you can observe how all the boxes are very small and far from each other.
    This means that the tasks are quite small compared to the overhead of communication.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, an increase in chunk size, which implies to an increase in the
    time required to run each task compared to the time of communication, will be
    beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_08CHPNO_06.png)'
  prefs: []
  type: TYPE_IMG
- en: Using PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, Apache Spark is one of the most popular projects for distributed computing.
    Developed in Scala, Spark was released in 2014, and integrates with HDFS and provides
    several advantages and improvements over the Hadoop MapReduce framework.
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to Hadoop MapReduce, Spark is designed to process data interactively
    and supports APIs for the Java, Scala, and Python programming languages. Given
    its different architecture, especially by the fact that Spark keep results in
    memory, Spark is generally much faster than Hadoop MapReduce.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Spark and PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setting up PySpark from scratch requires the installation of the Java and Scala
    runtimes, the compilation of the project from source, and the configuration of Python
    and Jupyter notebook so that they can be used alongside the Spark installation.
    An easier and less error-prone way to set up PySpark is to use an already configured
    Spark cluster made available through a **Docker** container.
  prefs: []
  type: TYPE_NORMAL
- en: Docker can be downloaded at [https://www.docker.com/](https://www.docker.com/) .
    If you're new to containers, you can read the next chapter for an introduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up a Spark cluster, it is sufficient to go in this chapter''s code files
    (where a file named `Dockerfile` is located) and issue the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will automatically download, install, and configure Spark, Python,
    and Jupyter notebook in an isolated environment. To start Spark and a Jupyter
    notebook session, you can execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The command will print a unique ID (called *container id*) that you can use
    to reference the application container and will start Spark and Jupyter notebook
    in the background. The `-p` option ensures that we can access the SparkUI and
    Jupyter network ports from the local machine. After issuing the command, you can
    open a browser to `http://127.0.0.1:8888` to access the Jupyter notebook session.
    You can test the correct initialization of Spark by creating a new notebook and
    executing the following content inside a cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This will initialize a `SparkContext` and take the first element in a collection
    (those new terms will be explained in detail later). Once the `SparkContext` is
    initialized, we can also head over to [http://127.0.0.1:4040](http://127.0.0.1:4040)
    to open the Spark Web UI.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the setup is complete, we will understand how Spark works and how to
    implement simple parallel algorithms using its powerful API.
  prefs: []
  type: TYPE_NORMAL
- en: Spark architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Spark cluster is a set of processes distributed over different machines. The
    **Driver Program** is a process, such as a Scala or Python interpreter, used by
    the user to submit the tasks to be executed.
  prefs: []
  type: TYPE_NORMAL
- en: The user can build task graphs, similar to Dask, using a special API and submit
    those tasks to the **Cluster Manager** that is responsible for assigning these
    tasks to **Executors**, processes responsible for executing the tasks. In a multi-user
    system, the Cluster Manager is also responsible for allocating resources on a
    per-user basis.
  prefs: []
  type: TYPE_NORMAL
- en: The user interacts with the Cluster Manager through the Driver Program. The
    class responsible for communication between the user and the Spark cluster is called
    `SparkContext`. This class is able to connect and configure the Executors on the
    cluster based on the resources available to the user.
  prefs: []
  type: TYPE_NORMAL
- en: For its most common use-cases, Spark manages its data through a data structure
    called **Resilient Distributed Datasets** (**RDD**), which represents a collection
    of items. RDDs are capable of handling massive datasets by separating their elements
    into partitions and operating on the partitions in parallel (note that this mechanism
    is mainly hidden from the user). RDDs can also be stored in memory (optionally,
    and when appropriate) for fast access and to cache expensive intermediate results.
  prefs: []
  type: TYPE_NORMAL
- en: Using RDDs, it is possible to define tasks and transformations (similarly to
    how we were automatically generating computation graphs in Dask) and, when requested,
    the Cluster Manager will automatically dispatch and execute tasks on the available
    Executors.
  prefs: []
  type: TYPE_NORMAL
- en: The Executors will receive the tasks from the Cluster Manager, execute them,
    and keep the results around if needed. Note that an Executor can have multiple
    cores and each node in the cluster may have multiple Executors. Generally speaking,
    Spark is fault tolerant on Executor's failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we show how the aforementioned components interact
    in a Spark cluster. The **Driver Program** interacts with the **Cluster Manager**
    that manages the **Executor** instances on different nodes (each Executor instance
    can also have multiple threads). Note that, even if the **Driver Program** doesn''t
    directly control the Executors, the results, which are stored in the **Executor**
    instances, are transferred directly between the Executors and the Driver Program. For
    this reason, it''s important that the **Driver Program** is network-reachable
    from the **Executor** processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_08CHPNO_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A natural question to ask is: How is Spark, a software written in Scala, able
    to execute Python code? The integration is done through the `Py4J` library, which
    maintains a Python process under-the-hood and communicates with it through sockets
    (a form of interprocess communication). In order to run the tasks, Executors maintain
    a series of Python processes so that they are able to process Python code in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: RDDs and variables defined in a Python process in the Driver Program are serialized,
    and the communication between the Cluster Manager and the Executors (including
    shuffling) is dealt with by Spark's Scala code. The extra serialization steps
    necessary for the Python and Scala interchange, all contribute to the overhead
    of communication; therefore, when using PySpark, extra care must be taken to ensure
    that the data structures used are serialized efficiently and that the data partitions
    are big enough so that the cost of communication is negligible compared to the
    cost of execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the additional Python processes needed for
    PySpark execution. These additional Python processes come with associated memory
    costs and an extra layer of indirection that complicate error reporting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_08CHPNO_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Despite these drawbacks, PySpark is still a widely used tool because it bridges
    the vivid Python ecosystem with the industrial strength of the Hadoop infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Resilient Distributed Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The easiest way to create an RDD in Python is with the `SparkContext.parallelize`
    method. This method was also used earlier where we parallelized a collection of
    integers between `0` and `1000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `rdd` collection will be divided into a number of partitions which, in
    this case, correspond to a default value of four (the default value can be changed
    using configuration options). To explicitly specify the number of partitions,
    one can pass an extra argument to `parallelize`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'RDDs support a lot of functional programming operators, similar to what we
    used back in [Chapter 6](2b46e5c0-5308-4073-b1c6-4232a881b39f.xhtml), *Implementing
    Concurrency*, with reactive programming and data streams (even though, in that
    case, the operators were designed to work on events over time rather than normal
    collections). For example, we may illustrate the basic `map` function which, by
    now, should be quite familiar. In the following code, we use `map` to calculate
    the square of a series of numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `map` function will return a new RDD but won''t compute anything just yet.
    In order to trigger the execution, you can use the `collect` method, which will
    retrieve all the elements in the collection, or `take`, which will return only
    the first ten elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'For a comparison between PySpark, Dask, and the other parallel programming
    libraries we explored in the earlier chapters, we will reimplement the approximation
    of pi. In the PySpark implementation, we will first create two RDDs of random
    numbers using `parallelize`, then we combine the datasets using the `zip` function
    (this is equivalent to Python''s `zip`), and we finally test whether the random
    points are inside the circle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: It's important to note that both the `zip` and `map` operations produce new
    RDDs and do not actually execute the instruction on the underlying data. In the preceding
    example, code execution is triggered when we call the `hit_test.sum` function,
    which returns an integer. This behavior is different from the Dask API where the
    whole computation (including the final result, `pi`) did not trigger the execution.
  prefs: []
  type: TYPE_NORMAL
- en: We can now move on to a more interesting application to demonstrate more RDD
    methods. We will learn how to count the number of visits each user of a website
    performs in a day. In a real-world scenario, the data would have been collected
    in a database and/or stored in a distributed filesystem, such as HDFS. However,
    in our example, we will generate some data that we will then analyze.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we generate a list of dictionaries, each containing
    a `user` (selected among twenty users) and a `timestamp`. The steps to produce
    the dataset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a pool of 20 users (the `users` variable).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a function that returns a random time between two dates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For 10,000 times, we choose a random user from our `users` pool and a random
    timestamp between the dates January 1, 2017 and January 7, 2017.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'With the dataset at hand, we can start asking questions and use PySpark to
    find the answers. One common question is "*How many times has a given user visited
    the website?.*" A naive way to compute this result can be achieved by grouping
    the entries RDD by user (using the `groupBy` operator) and counting how many items
    are present for each user. In PySpark, `groupBy` takes a function as argument
    to extract the grouping key for each element and returns a new RDD that contain
    tuples of the `(key, group)` form. In the following example, we use the user ID
    as the key for our `groupBy`, and we inspect the first element using `first`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The return value of `groupBy` contains a `ResultIterable` (which is basically
    a list) for each user ID. To count the number of visits per user, it''s sufficient
    to calculate the length of each `ResultIterable`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Even though this algorithm may work well in small datasets, `groupBy` requires
    us to collect and store the whole set of entries for each user in memory, and
    this can exceed the memory capacity of an individual node. Since we don't need
    the list but only the count, there's a better way to calculate this number without
    having to hold the list of visits for each user in memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'When dealing with an RDD of `(key, value)` pairs, it is possible to use `mapValues`
    to apply a function only to the values. In the preceding code, we can replace
    the `map(lambda kv: (kv[0], len(kv[1])))` call with `mapValues(len)` for better
    readability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more efficient calculation, we can leverage the `reduceByKey` function,
    which will perform a step similar to the Reduce step that we saw in the *An introduction
    to MapReduce* section. The `reduceByKey` function can be called from an RDD of
    tuples that contain a key as their first element and a value as their second element,
    and accepts a function as its first argument that will calculate the reduction.
     A simple example of the `reduceByKey` function is illustrated in the following
    snippet. We have a few string keys associated with integer numbers, and we want
    to get the sum of the values for each key; the reduction, expressed as a lambda,
    corresponds to the sum of the elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `reduceByKey` function is much more efficient than `groupBy` because the
    reduction is parallelizable and doesn''t require the in-memory storage of the
    groups; also, it limits the data shuffled between Executors (it performs similar
    operations to Dask''s `foldby`, which was explained earlier). At this point, we
    can rewrite our visit count calculation using `reduceByKey`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'With Spark''s RDD API, it is also easy to answer questions such as "*How many
    visits did the website receive each day?.*" This can be computed using `reduceByKey`
    with the appropriate key (which is the date extracted from the timestamp). In
    the following example, we demonstrate the calculation. Also, note the usage of
    the `sortByKey` operator to return the counts sorted by date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Spark DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For numerical and analytical tasks, Spark provides a convenient interface available
    through the `pyspark.sql` module (also called SparkSQL). The module includes a
    `spark.sql.DataFrame` class that can be used for efficient SQL-style queries similar
    to those of Pandas. Access to the SQL interface is provided through the `SparkSession`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`SparkSession` can then be used to create a `DataFrame` through the function
    `createDataFrame`. The function `createDataFrame` accepts either a RDD, a list,
    or a `pandas.DataFrame`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will create a `spark.sql.DataFrame` by converting
    an RDD, `rows`, which contains a collection of `Row` instances. The `Row` instances
    represent an association between a set of column names and a set of values, just
    like a row in a `pd.DataFrame`. In this example, we have two columns--`x` and `y`--to
    which we will associate random numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'After obtaining our collection of `Row` instances, we can combine them in a
    `DataFrame`, as follows. We can also inspect the `DataFrame` content using the `show`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`spark.sql.DataFrame` supports performing transformations on the distributed
    dataset using a convenient SQL syntax. For example, you can use the `selectExpr`
    method to calculate a value using a SQL expression. In the following code, we
    compute the hit test using the `x` and `y` columns and the `pow` SQL function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate the expressivity of SQL, we can also calculate the estimation
    of pi using a single expression. The expression involves using SQL functions such
    as `sum`, `pow`, `cast`, and `count`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Spark SQL follows the same syntax as Hive, a SQL engine for distributed datasets
    built on Hadoop. Refer to [https://cwiki.apache.org/confluence/display/Hive/LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)
    for a complete syntax reference.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames are a great way to leverage the power and optimization of Scala while
    using the Python interface. The main reason is that queries are interpreted symbolically
    by SparkSQL and the execution happens directly in Scala without having to pass
    intermediate results through Python. This greatly reduces the serialization overhead
    and takes advantage of the query optimizations performed by SparkSQL. Optimizations
    and query planning allows the use of SQL operators, such as `GROUP BY`, without
    incurring in performance penalties, such as the one we experienced using `groupBy`
    directly on an RDD.
  prefs: []
  type: TYPE_NORMAL
- en: Scientific computing with mpi4py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though Dask and Spark are great technologies widely used in the IT industry,
    they have not been widely adopted in academic research. High-performance supercomputers
    with thousands of processors have been used in academia for decades to run intense
    numerical applications. For this reason, supercomputers are generally configured
    using a very different software stack that focuses on a computationally-intensive
    algorithm implemented in a low-level language, such as C, Fortran, or even assembly.
  prefs: []
  type: TYPE_NORMAL
- en: The principal library used for parallel execution on these kinds of systems
    is **Message Passing Interface** (**MPI**), which, while less convenient or sophisticated
    than Dask or Spark, is perfectly capable of expressing parallel algorithms and
    achieving excellent performance. Note that, contrary to Dask and Spark, MPI does
    not follow the MapReduce model and is best used for running thousands of processes
    with very little data sent between them.
  prefs: []
  type: TYPE_NORMAL
- en: MPI works quite differently compared to what we've seen so far. Parallelism
    in MPI is achieved by running *the same script* in multiple processes (which possibly
    exist on different nodes); communication and synchronization between processes
    is handled by a designated process, which is commonly called **root** and is usually
    identified by a `0` ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will briefly demonstrate the main concepts of MPI using
    its `mpi4py` Python interface. In the following example, we demonstrate the simplest
    possible parallel code with MPI. The code imports the MPI module and retrieves
    `COMM_WORLD`, which is an interface that can be used to interact with other MPI
    processes. The `Get_rank` function will return an integer identifier for the current
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We can place the preceding code in a file, `mpi_example.py`, and execute it. Running
    this script normally won''t do anything special as it involves the execution of
    a single process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'MPI jobs are meant to be executed using the `mpiexec` command, which takes
    a `-n` option to indicate the number of parallel processes. Running the script
    using the following command will generate four independent executions of the same
    script, each with a different ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Distributing processes among the network is performed automatically through
    a resource manager (such as TORQUE). Generally, supercomputers are configured
    by the system administrator, which will also provide instructions on how to run
    MPI software.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a feel as to what an MPI program looks like, we will reimplement the
    approximation of *pi*. The complete code is shown here. The program will do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a random array of `N / n_procs` size for each process so that each process
    will test the same amount of samples (`n_procs` is obtained through the `Get_size`
    function)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each separate process, calculate the sum of the hit tests and store it in
    `hits_counts`, which will represent the partial counts for each process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `reduce` function to calculate the total sum of the partial counts.
    When using reduce, we need to specify the `root` argument to specify which process
    will receive the result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Print the final result only on the process corresponding to the root process:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now place the preceding code in a file named `mpi_pi.py` and execute
    it using `mpiexec`. The output shows how the four process executions are intertwined
    until we get to the `reduce` call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributed processing can be used to implement algorithms capable of handling
    massive datasets by distributing smaller tasks across a cluster of computers.
    Over the years, many software packages, such as Apache Hadoop, have been developed
    to implement performant and reliable execution of distributed software.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned about the architecture and usage of Python packages,
    such as Dask and PySpark, which provide powerful APIs to design and execute programs
    capable of scaling to hundreds of machines. We also briefly looked at MPI, a library
    that has been used for decades to distribute work on supercomputers designed for
    academic research.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we explored several techniques to improve the performance of
    our program, and to increase the speed of our programs and the size of the datasets
    we are able to process. In the next chapter, we will describe the strategies and
    best practices to write and maintain high-performance code.
  prefs: []
  type: TYPE_NORMAL
