- en: Distributed Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式处理
- en: In the last chapter, we introduced the concept of parallel processing and learned
    how to leverage multicore processors and GPUs. Now, we can step up our game a
    bit and turn our attention on distributed processing, which involves executing
    tasks across multiple machines to solve a certain problem.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了并行处理的概念，并学习了如何利用多核处理器和GPU。现在，我们可以将游戏提升到一个新的水平，并将注意力转向分布式处理，这涉及到在多台机器上执行任务以解决特定问题。
- en: In this chapter, we will illustrate the challenges, use cases, and examples
    of how to run code on a cluster of computers. Python offers easy-to-use and reliable
    packages for distribute processing, which will allow us to implement scalable
    and fault-tolerant code with relative ease.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将说明在计算机集群上运行代码的挑战、用例和示例。Python提供了易于使用且可靠的分布式处理包，这将使我们能够相对容易地实现可扩展和容错代码。
- en: 'The list of topics for this chapter is as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主题列表如下：
- en: Distributed computing and the MapReduce model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式计算和MapReduce模型
- en: Directed Acyclic Graphs with Dask
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Dask的定向无环图
- en: Writing parallel code with Dask's `array`, `Bag`, and `DataFrame` data structures
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Dask的`array`、`Bag`和`DataFrame`数据结构编写并行代码
- en: Distributing parallel algorithms with Dask Distributed
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Dask Distributed分发并行算法
- en: An introduction to PySpark
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark简介
- en: Spark's Resilient Distributed Datasets and DataFrame
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的弹性分布式数据集和DataFrame
- en: Scientific computing with `mpi4py`
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`mpi4py`进行科学计算
- en: Introduction to distributed computing
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式计算简介
- en: In today's world, computers, smartphones, and other devices have become an integral
    part of our lives. Every day, massive quantities of data is produced. Billions
    of people access services on the Internet, and companies are constantly collecting
    data to learn about their users to better target products and improve user experience.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今世界，计算机、智能手机和其他设备已成为我们生活的重要组成部分。每天，都会产生大量的数据。数十亿人通过互联网访问服务，公司不断收集数据以了解他们的用户，以便更好地定位产品和提升用户体验。
- en: Handling this ever increasing amount of data presents substantial challenges.
    Large companies and organizations often build clusters of machines designed to
    store, process, and analyze large and complex datasets. Similar datasets are also
    produced in data-intensive fields such as environmental sciences and health care.
    These large-scale datasets have been recently called **big data**. The analysis
    techniques applied to big data usually involve a combination of machine learning,
    information retrieval, and visualization.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这日益增长的大量数据带来了巨大的挑战。大型公司和组织通常构建机器集群，用于存储、处理和分析大型且复杂的数据集。在数据密集型领域，如环境科学和医疗保健，也产生了类似的数据库。这些大规模数据集最近被称为**大数据**。应用于大数据的分析技术通常涉及机器学习、信息检索和可视化的结合。
- en: Computing clusters have been used for decades in scientific computing, where
    the study of complex problems requires the use of parallel algorithms executed
    on high-performance distributed systems. For such applications, universities and
    other organizations provide and manage supercomputers for research and engineering
    purposes. Applications that run on supercomputers are generally focused on highly
    numerical workloads, such as protein and molecular simulations, quantum mechanical
    calculations, climate models, and much more.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 计算集群在科学计算中已经使用了数十年，在研究复杂问题时需要使用在高性能分布式系统上执行的并行算法。对于此类应用，大学和其他组织提供并管理超级计算机用于研究和工程目的。在超级计算机上运行的应用通常专注于高度数值化的工作负载，如蛋白质和分子模拟、量子力学计算、气候模型等。
- en: The challenges of programming for distributed systems are apparent if we think
    back on how the cost of communication increases as we distribute data and computational
    tasks across a local network. Network transfers are extremely slow compared to
    the processor speed, and when using distributed processing, it is even more important
    to keep network communications as limited as possible. This can be achieved using
    a few different strategies that favor local data processing and resort to data
    transfers only when strictly necessary.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为分布式系统编程的挑战显而易见，如果我们回顾一下，随着我们将数据和计算任务分散到本地网络中，通信成本是如何增加的。与处理器速度相比，网络传输极其缓慢，在使用分布式处理时，保持网络通信尽可能有限尤为重要。这可以通过使用一些不同的策略来实现，这些策略有利于本地数据处理，并且仅在绝对必要时才进行数据传输。
- en: Other challenges of distributed processing involve the general unreliability
    of computer networks. When you think that in a computing cluster there may be
    thousands of machines, it becomes clear that (probabilistically speaking) faulty
    nodes become very common. For this reason, distributed systems need to be able
    to handle node failures gracefully and without disrupting the ongoing work. Luckily,
    companies have invested a great deal of resources in developing fault-tolerant
    distributed engines that take care of these aspects automatically.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式处理的其他挑战包括计算机网络的一般不可靠性。当你想到在一个计算集群中可能有数千台机器时，很明显（从概率上讲），故障节点变得非常普遍。因此，分布式系统需要能够优雅地处理节点故障，而不会干扰正在进行的工作。幸运的是，公司已经投入了大量资源来开发容错分布式引擎，这些引擎可以自动处理这些方面。
- en: An introduction to MapReduce
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce简介
- en: '**MapReduce** is a programming model that allows you to express algorithms
    for efficient execution on a distributed system. The MapReduce model was first
    introduced by Google in 2004 ([https://research.google.com/archive/mapreduce.html](https://research.google.com/archive/mapreduce.html)),
    as a way to automatically partition datasets over different machines and for automatic
    local processing and the communication between *cluster nodes*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**MapReduce**是一种编程模型，允许你在分布式系统上高效地表达算法。MapReduce模型最早由Google在2004年提出（[https://research.google.com/archive/mapreduce.html](https://research.google.com/archive/mapreduce.html)），作为一种在多台机器上自动分区数据集、自动本地处理以及*集群节点*之间通信的方法。'
- en: The MapReduce framework was used in cooperation with a distributed filesystem,
    the **Google File System** (GFS or GoogleFS), which was designed to partition
    and replicate data across the computing cluster. Partitioning was useful for storing
    and processing datasets that wouldn't fit on a single node while replication ensured
    that the system was able to handle failures gracefully. MapReduce was used by
    Google, in conjunction with GFS, for indexing of their web pages. Later on, the
    MapReduce and GFS concepts were implemented by Doug Cutting (at the time, an employee
    at Yahoo!), resulting in the first versions of the **Hadoop Distributed File System**
    (**HDFS**) and Hadoop MapReduce.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce框架与分布式文件系统**Google文件系统**（GFS或GoogleFS）合作使用，该系统旨在将数据分区并复制到计算集群中。分区对于存储和处理无法适应单个节点的数据集很有用，而复制确保系统能够优雅地处理故障。Google使用MapReduce与GFS一起对他们的网页进行索引。后来，MapReduce和GFS概念由Doug
    Cutting（当时是雅虎的员工）实现，产生了**Hadoop分布式文件系统**（**HDFS**）和Hadoop MapReduce的第一个版本。
- en: 'The programming model exposed by MapReduce is actually quite simple. The idea
    is to express the computation as a combination of two, fairly generic, steps:
    *Map* and *Reduce*. Some readers will probably be familiar with Python''s `map`
    and `reduce` functions; however, in the context of MapReduce, the Map and Reduce
    steps are capable of representing a broader class of operations.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce暴露的编程模型实际上相当简单。其思想是将计算表达为两个相当通用的步骤的组合：*Map*和*Reduce*。一些读者可能熟悉Python的`map`和`reduce`函数；然而，在MapReduce的上下文中，Map和Reduce步骤能够表示更广泛的操作类别。
- en: Map takes a collection of data as input and produces a *transformation* on this
    data. What is generally emitted by Map is a series of key value pairs that can
    be passed to a Reduce step. The Reduce step will aggregate items with the same
    key and apply a function to the collection to form a usually smaller collection
    of values.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Map以一组数据作为输入，并对这些数据进行*转换*。Map通常输出一系列键值对，这些键值对可以被传递到Reduce步骤。Reduce步骤将具有相同键的项聚合起来，并对集合应用一个函数，形成一个通常更小的值集合。
- en: The estimation of *pi*, which was shown in the last chapter, can be trivially
    converted using a series of Map and Reduce steps. In that case, the input was
    a collection of pairs of random numbers. The transformation (Map step) was the
    hit test, and the Reduce step was counting the number of times the hit test was
    True.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中展示的*π*的估计可以通过一系列Map和Reduce步骤轻松转换。在这种情况下，输入是一系列随机数的对。转换（Map步骤）是击中测试，而Reduce步骤是计算击中测试为True的次数。
- en: The prototypical example of the MapReduce model is the implementation of a word
    count; the program takes a series of documents as input, and returns, for each
    word, the total number of occurrences in the document collection. The following
    figure illustrates the Map and Reduce steps of the word count program. On the
    left, we have the input documents. The Map operation will produce a (key, value)
    entry where the first element is the word and the second element is **1** (that's
    because every word contributes **1** to the final count).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 模型的典型示例是实现词频统计；程序接受一系列文档作为输入，并返回每个单词在文档集合中的总出现次数。以下图展示了词频统计程序的 Map
    和 Reduce 步骤。在左侧，我们有输入文档。Map 操作将生成一个（键，值）条目，其中第一个元素是单词，第二个元素是 **1**（这是因为每个单词都对最终计数贡献了
    **1**）。
- en: 'We then perform the reduce operation to aggregate all the elements of the same
    key and produce the global count for each of the words. In the figure, we can
    see how all values of the items with key **the** are summed to produce the final
    entry (**the, 4**):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们执行 reduce 操作，聚合相同键的所有元素，并为每个单词生成全局计数。在图中，我们可以看到所有键为 **the** 的项的值是如何相加以生成最终的条目（**the,
    4**）：
- en: '![](img/B06440_08CHPNO_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06440_08CHPNO_01.png)'
- en: If we implement our algorithm using the Map and Reduce operation, the framework
    implementation will ensure that data production and aggregation is done efficiently,
    by limiting the communication between nodes through clever algorithms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 Map 和 Reduce 操作实现我们的算法，框架实现将确保通过限制节点之间的通信通过巧妙的算法来高效地完成数据生产和聚合。
- en: However, how does MapReduce manage to keep communication to a minimum? Let's
    go through the journey of a MapReduce task. Imagine that you have a cluster with
    two nodes, and a partition of the data (this is usually found locally in each
    node) is loaded in each node from disk and is ready for processing. A mapper process
    is created in each node and processes the data to produce the intermediate results.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，MapReduce 是如何将通信保持在最低限度的？让我们回顾一下 MapReduce 任务的旅程。想象一下，你有一个包含两个节点的集群，数据分区（这通常在每个节点本地找到）从磁盘加载到每个节点，并准备好处理。在每个节点上创建了一个
    mapper 进程，并处理数据以生成中间结果。
- en: 'Next, it is necessary to send the data to the reducer for further processing.
    In order to do this, however, it is necessary that all the items that possess
    the same key are shipped to the same reducer. This operation is called **shuffling**
    and is the principal communication task in the MapReduce model:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，有必要将数据发送到 reducer 进行进一步处理。然而，为了做到这一点，所有具有相同键的项都必须发送到同一个 reducer。这个操作称为 **洗牌**，是
    MapReduce 模型中的主要通信任务：
- en: '![](img/B06440_08CHPNO_02.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06440_08CHPNO_02.png)'
- en: Note that, before the data exchange happens, it is necessary to assign a subset
    of keys to each reducer; this step is called **partitioning**.  Once a reducer
    receives its own partition of keys, it is free to process data and write the resulting
    output on disk.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在数据交换发生之前，有必要将键的子集分配给每个 reducer；这一步称为 **分区**。一旦 reducer 收到其自己的键分区，它就可以自由地处理数据并在磁盘上写入结果输出。
- en: The MapReduce framework (through the Apache Hadoop project) has been extensively
    used in its original form by many companies and organizations. More recently,
    new frameworks that extend the ideas introduced by MapReduce have been developed
    to create systems able to express more complex workflows, to use memory more efficiently
    and to support a lean and efficient execution of distributed tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 框架（通过 Apache Hadoop 项目）在其原始形式下已被许多公司和组织广泛使用。最近，一些新的框架被开发出来，以扩展 MapReduce
    引入的思想，以创建能够表达更复杂工作流程、更有效地使用内存并支持瘦型和高效分布式任务执行的系统。
- en: In the following sections, we will describe two of the most used libraries in
    the Python distributed landscape: Dask and PySpark.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将描述 Python 分布式领域中两个最常用的库：Dask 和 PySpark。
- en: Dask
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask
- en: '**Dask** is a project of Continuum Analytics (the same company that''s responsible
    for Numba and the `conda` package manager) and a pure Python library for parallel
    and distributed computation. It excels at performing data analysis tasks and is
    very well integrated in the Python ecosystem.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dask** 是 Continuum Analytics（负责 Numba 和 `conda` 软件包管理器的同一家公司）的一个项目，是一个用于并行和分布式计算的纯
    Python 库。它在执行数据分析任务方面表现出色，并且与 Python 生态系统紧密结合。'
- en: Dask was initially conceived as a package for bigger-than-memory calculations
    on a single machine. Recently, with the Dask Distributed project, its code has
    been adapted to execute tasks on a cluster with excellent performance and fault-tolerance
    capabilities. It supports MapReduce-style tasks as well as complex numerical algorithms.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Dask最初被构想为一个用于单机内存外计算的包。最近，随着Dask Distributed项目的推出，其代码已被调整以在具有出色性能和容错能力的集群上执行任务。它支持MapReduce风格的任务以及复杂的数值算法。
- en: Directed Acyclic Graphs
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有向无环图
- en: The idea behind Dask is quite similar to what we already saw in the last chapter
    with Theano and Tensorflow. We can use a familiar Pythonic API to build an execution
    plan, and the framework will automatically split the workflow into tasks that
    will be shipped and executed on multiple processes or computers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Dask背后的理念与我们已经在上一章中看到的Theano和Tensorflow非常相似。我们可以使用熟悉的Pythonic API来构建执行计划，而框架将自动将工作流程拆分成将在多个进程或计算机上传输和执行的任务。
- en: 'Dask expresses its variables and operations as a **Directed Acyclic Graph**
    (**DAG**) that can be represented through a simple Python dictionary. To briefly
    illustrate how this works, we will implement the sum of two numbers with Dask.
    We will define our computational graph by storing the values of our input variables
    in the dictionary. The `a` and `b` input variables will be given a value of `2`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Dask将它的变量和操作表示为一个**有向无环图**（**DAG**），可以通过一个简单的Python字典来表示。为了简要说明这是如何工作的，我们将使用Dask实现两个数的和。我们将通过在字典中存储输入变量的值来定义我们的计算图。输入变量`a`和`b`将被赋予值`2`：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Each variable represents a node in the DAG. The next step necessary to build
    our DAG is the execution of operations on the nodes we just defined. In Dask,
    a task can be defined by placing a tuple containing a Python function and its
    positional arguments in the `dsk` dictionary. To implement a `sum`, we can add
    a new node, named `result`, (the actual name is completely arbitrary) with a tuple
    containing the function we intend to execute, followed by its arguments. This
    is illustrated in the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个变量代表DAG中的一个节点。构建我们的DAG的下一步是执行我们刚刚定义的节点上的操作。在Dask中，一个任务可以通过在`dsk`字典中放置一个包含Python函数及其位置参数的元组来定义。为了实现求和，我们可以添加一个新的节点，命名为`result`（实际名称完全任意），包含我们打算执行的函数，后跟其参数。以下代码展示了这一点：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For better style and clarity, we can calculate the sum by replacing the `lambda`
    statement with the standard `operator.add` library function:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好的风格和清晰度，我们可以通过替换`lambda`语句为标准的`operator.add`库函数来计算和：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It's important to note that the arguments we intend to pass to the function
    are the `"a"` and `"b"` strings, which refer to the `a` and `b` nodes in the graph.
    Note that we didn't use any Dask-specific functions to define the DAG; this is
    the first indication of how the framework is flexible and lean since all manipulations
    are performed on simple and familiar Python dictionaries.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们打算传递给函数的参数是`"a"`和`"b"`字符串，它们指的是图中的`a`和`b`节点。请注意，我们没有使用任何Dask特定的函数来定义DAG；这是框架灵活和精简的第一个迹象，因为所有操作都是在简单且熟悉的Python字典上进行的。
- en: 'The execution of tasks is performed by a scheduler, which is a function that
    takes a DAG and the task or tasks we''d like to perform and returns the computed
    value. The default Dask scheduler is the `dask.get` function, which can be used
    as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 任务执行由调度器完成，调度器是一个函数，它接受一个DAG以及我们想要执行的任务或任务列表，并返回计算值。默认的Dask调度器是`dask.get`函数，可以使用以下方式使用：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: All the complexity is hidden behind the scheduler, which will take care of distributing
    the tasks across threads, processes, or even different machines. The `dask.get`
    scheduler is a synchronous and serial implementation that is useful for testing
    and debugging purposes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所有复杂性都隐藏在调度器后面，调度器将负责将任务分配到线程、进程甚至不同的机器上。`dask.get`调度器是一个同步和串行实现，适用于测试和调试目的。
- en: Defining graphs using a simple dictionary is useful to understand how Dask does
    its magic and for debugging purposes. Raw dictionaries can also be used to implement
    more complex algorithms not covered by the Dask API. Now, we will learn how Dask
    is capable of generating tasks automatically through a familiar NumPy- and Pandas-like
    interface.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单的字典定义图对于理解Dask如何施展魔法以及用于调试目的非常有用。原始字典也可以用来实现Dask API未涵盖的更复杂算法。现在，我们将学习Dask如何通过熟悉的NumPy和Pandas-like接口自动生成任务。
- en: Dask arrays
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask数组
- en: One of the main use-cases of Dask is the automatic generation of parallel array
    operations, which greatly simplifies the handling of arrays that don't fit into
    memory. The strategy employed by Dask is to split the array into a number of subunits
    that, in Dask array terminology, are called **chunks**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的主要用例之一是自动生成并行数组操作，这极大地简化了处理无法装入内存的数组。Dask 采取的策略是将数组分割成多个子单元，在 Dask 数组术语中，这些子单元被称为
    **chunks**。
- en: 'Dask implement a NumPy-like interface for arrays in the `dask.array` module
    (which we will abbreviate as `da`). An array can be created from a NumPy-like
    array using the `da.from_array` function, which requires the specification of
    a chunk size. The `da.from_array` function will return a `da.array` object that
    will handle the splitting of the original array into subunits of the specified
    chunk size. In the following example, we create an array of `30` elements, and
    we split it into chunks with `10` elements each:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 在 `dask.array` 模块（我们将简称为 `da`）中实现了一个类似于 NumPy 的数组接口。可以使用 `da.from_array`
    函数从一个类似于 NumPy 的数组创建数组，该函数需要指定块大小。`da.from_array` 函数将返回一个 `da.array` 对象，该对象将处理将原始数组分割成指定块大小的子单元。在以下示例中，我们创建了一个包含
    `30` 个元素的数组，并将其分割成每个块包含 `10` 个元素的块：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `a_da` variable maintains a Dask graph that can be accessed using the `dask`
    attribute. To understand what Dask does under the hood, we can inspect its content.
    In the following example, we can see that the Dask graph contains four nodes.
    One of them is the source array, denoted by the `''array-original-4c76''` key,
    the other three keys in the `a_da.dask` dictionary are tasks that are used to
    access a chunk of the original array using the `dask.array.core.getarray` function
    and, as you can see, each task extracts a slice of 10 elements:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`a_da` 变量维护一个 Dask 图，可以通过 `dask` 属性访问。为了了解 Dask 在底层做了什么，我们可以检查其内容。在以下示例中，我们可以看到
    Dask 图包含四个节点。其中一个是源数组，用 `''array-original-4c76''` 键表示，`a_da.dask` 字典中的其他三个键是用于使用
    `dask.array.core.getarray` 函数访问原始数组子块的任务，如您所见，每个任务提取了 10 个元素的一个切片：'
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If we perform an operation on the `a_da` array, Dask will generate more subtasks
    that operate on the smaller chunks, opening the possibility of achieving parallelism.
    The interface exposed by `da.array` is compatible with common NumPy semantics
    and broadcasting rules. The complete code, shown as follows, demonstrates the
    good compatibility of Dask with NumPy broadcasting rules, element-wise operations,
    and other methods:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在 `a_da` 数组上执行操作，Dask 将生成更多子任务来操作更小的子单元，从而打开实现并行化的可能性。`da.array` 暴露的接口与常见的
    NumPy 语义和广播规则兼容。以下代码展示了 Dask 与 NumPy 广播规则、逐元素操作和其他方法的良好兼容性：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The value of pi can be calculated using the `compute` method, which can also
    be called with the `get` optional argument to specify a different scheduler (by
    default, `da.array` uses a multithreaded scheduler):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `compute` 方法计算 π 的值，也可以通过 `get` 可选参数来指定不同的调度器（默认情况下，`da.array` 使用多线程调度器）：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Even deceptively simple algorithms, such as the estimation of pi, may require
    a lot of tasks to be executed. Dask provides utilities to visualize the computational graph.
    The following figure shows part of the Dask graph for the estimation of pi, which
    can be obtained by executing the method `pi.visualize()`. In the graph, circles
    refer to transformations that get applied on the nodes, which are represented
    as rectangles. This example helps us to get a feel of the complexity of the Dask
    graph and to appreciate the scheduler''s job of creating an efficient execution
    plan that includes proper ordering of tasks and the selection of tasks that will
    be executed in parallel:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是表面上简单的算法，如 π 的估计，也可能需要执行大量任务。Dask 提供了可视化计算图的工具。以下图显示了用于估计 π 的 Dask 图的一部分，可以通过执行
    `pi.visualize()` 方法获得。在图中，圆形代表应用于节点的转换，节点以矩形表示。这个例子帮助我们了解 Dask 图的复杂性，并欣赏调度器创建高效执行计划的工作，包括正确排序任务和选择并行执行的任务：
- en: '![](img/dask_graph-e1489790395473.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![Dask 图](img/dask_graph-e1489790395473.png)'
- en: Dask Bag and DataFrame
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask Bag 和 DataFrame
- en: Dask provides other data structures for automatic generation of computation
    graphs. In this subsection, we'll take a look at `dask.bag.Bag`, a generic collection
    of elements that can be used to code MapReduce-style algorithms, and `dask.dataframe.DataFrame`,
    a distributed version of `pandas.DataFrame`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 提供了其他数据结构用于自动生成计算图。在本小节中，我们将探讨 `dask.bag.Bag`，这是一个通用的元素集合，可用于编写 MapReduce
    风格的算法，以及 `dask.dataframe.DataFrame`，它是 `pandas.DataFrame` 的分布式版本。
- en: 'A `Bag` can be easily created from a Python collection. For example, you can
    create a `Bag` from a list using the `from_sequence` factory function. The level
    of parallelism can be specified using the `npartitions` argument (this will distribute
    the `Bag` content into a number of partitions). In the following example, we create
    a `Bag` containing numbers from `0` to `99`, partitioned into four chunks:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `Bag` 可以很容易地从 Python 集合中创建。例如，您可以使用 `from_sequence` 工厂函数从一个列表中创建一个 `Bag`。可以使用
    `npartitions` 参数指定并行级别（这将把 `Bag` 内容分布到多个分区中）。在以下示例中，我们创建了一个包含从 `0` 到 `99` 的数字的
    `Bag`，分为四个块：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the next example, we will demonstrate how to perform a word count of a set
    of strings using an algorithm that's similar to MapReduce. Given our collection
    of sequences, we apply `str.split`, followed by `concat` to obtain a linear list
    of words in the documents. Then, for each word, we produce a dictionary that contains
    a word and the value `1` (refer to the *An introduction to MapReduce* section
    for an illustration). We then write a *Reduce* step using the `foldby` operator
    to calculate the word count.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们将演示如何使用类似于 MapReduce 的算法对一组字符串进行词频统计。给定我们的序列集合，我们应用 `str.split`，然后使用
    `concat` 获取文档中的线性单词列表。然后，对于每个单词，我们生成一个包含单词和值 `1` 的字典（有关说明，请参阅 *MapReduce 简介* 部分）。然后，我们使用
    `foldby` 操作符编写一个 *Reduce* 步骤来计算词频。
- en: The `foldby` transformation is useful to implement a Reduce step that combines
    the word counts without having to shuffle all the elements over the network. Imagine
    that our word dataset is divided into two partitions. A good strategy to calculate
    the total count is to first sum the word occurrences in each partition and then
    combine those partial sums to get the final result. The following figure illustrates
    the concept. On the left, we have our input partitions. The partial sum is calculated
    for each individual partition (this is done using a binary operation, **binop**),
    and then the final sums are computed by combining the partial sums using a **combine**
    function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`foldby` 转换对于实现不需要在网络中重新排序所有元素即可合并词频的 Reduce 步骤非常有用。想象一下，我们的单词数据集被分为两个分区。计算总计数的一个好策略是首先计算每个分区的单词出现次数之和，然后将这些部分和组合起来得到最终结果。以下图示说明了这个概念。在左侧，我们有我们的输入分区。每个单独分区计算部分和（这是使用二进制操作
    **binop** 完成的），然后通过使用 **combine** 函数组合部分和来计算最终总和。'
- en: '![](img/B06440_08CHPNO_04.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06440_08CHPNO_04.png)'
- en: 'The following code illustrates how to use `Bag` and the `foldby` operator to
    compute the word count. For the `foldby` operator, we need to define two functions
    that take five arguments:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码说明了如何使用 `Bag` 和 `foldby` 操作符来计算词频。对于 `foldby` 操作符，我们需要定义两个函数，它们接受五个参数：
- en: '`key`: This is a function that returns the key for the reduce operation.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`key`: 这是一个返回 reduce 操作键的函数。'
- en: '`binop`: This is a function that takes two arguments: `total` and `x`. Given
    a `total` value (the values accumulated so far), `binop` incorporates the next
    item into the total.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binop`: 这是一个接受两个参数的函数：`total` 和 `x`。给定一个 `total` 值（到目前为止累积的值），`binop` 将下一个项目合并到总和中。'
- en: '`initial`: This is the initial value for the `binop` accumulation.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initial`: 这是 `binop` 累积的初始值。'
- en: '`combine`: This is a function that combines the totals for each partition (in
    this case it is a simple sum).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`combine`: 这是一个将每个分区的总和合并的函数（在这种情况下是一个简单的求和）。'
- en: '`initial_combine`: This is the initial value for the `combine` accumulation.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initial_combine`: 这是 `combine` 累积的初始值。'
- en: 'Now, let''s look at the code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看代码：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we just saw, expressing complex operations in an efficient way using `Bag`
    can become cumbersome. For this reason, Dask provides another data structure designed
    for analytical workloads--`dask.dataframe.DataFrame`. A `DataFrame` can be initialized
    in Dask using a variety of methods, such as from `CSV` files on distributed filesystems,
    or directly from a `Bag`. Just like `da.array` provides an API that closely mirrors
    NumPy features, Dask `DataFrame` can be used as a distributed version of `pandas.DataFrame`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚才看到的，使用`Bag`以有效的方式表达复杂操作可能会变得繁琐。因此，Dask提供另一种数据结构，专为分析工作负载设计--`dask.dataframe.DataFrame`。`DataFrame`可以在Dask中使用多种方法初始化，例如从分布式文件系统上的`CSV`文件，或直接从`Bag`。就像`da.array`提供了一个与NumPy功能紧密相似的API一样，Dask
    `DataFrame`可以用作`pandas.DataFrame`的分布式版本。
- en: 'As a demonstration, we will re-implement the word count using a `DataFrame`.
    We first load the data to obtain a `Bag` of words, and then we convert the `Bag`
    to a `DataFrame` using the `to_dataframe` method. By passing a column name to
    the `to_dataframe` method, we can initialize a `DataFrame`, which contains a single
    column, named `words`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 作为演示，我们将使用`DataFrame`重新实现词频。我们首先加载数据以获得一个单词的`Bag`，然后使用`to_dataframe`方法将`Bag`转换为`DataFrame`。通过将列名传递给`to_dataframe`方法，我们可以初始化一个`DataFrame`，它包含一个名为`words`的单列：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Dask `DataFrame` closely replicates the `pandas.DataFrame` API. To compute
    the word count, we only need to call the `value_counts` method on the words column,
    and Dask will automatically devise a parallel computation strategy. To trigger
    the calculation, it is sufficient to call the `compute` method:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Dask `DataFrame`紧密复制了`pandas.DataFrame` API。要计算词频，我们只需在单词列上调用`value_counts`方法，Dask将自动设计一个并行计算策略。要触发计算，只需调用`compute`方法：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'An interesting question one may ask is "*what kind of algorithm does DataFrame
    use under the hood?*". The answer can be found by looking at the upper part of
    the generated Dask graph, which is displayed in the following figure. The first
    two rectangles at the bottom represent two partitions of the dataset, which are
    stored as two `pd.Series` instances. To calculate the overall count, Dask will
    first execute `value_counts` on each of the `pd.Series` and then combine the counts along
    with the `value_counts_aggregate` step:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能的问题是一个人可能会问：“*DataFrame底层使用的是哪种算法？*”。答案可以通过查看生成的Dask图的顶部来找到，该图如下所示。底部的前两个矩形代表数据集的两个分区，它们存储为两个`pd.Series`实例。为了计算总数，Dask将首先在每个`pd.Series`上执行`value_counts`，然后结合`value_counts_aggregate`步骤：
- en: '![](img/B06440_08CHPNO_05.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06440_08CHPNO_05.png)'
- en: As you can see, both Dask `array` and `DataFrame` take advantage of the fast
    vectorized implementations of NumPy and Pandas to achieve excellent performance
    and stability.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Dask `array`和`DataFrame`都利用了NumPy和Pandas的快速向量化实现，以实现卓越的性能和稳定性。
- en: Dask distributed
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask分布式
- en: The first iterations of the Dask project were designed to run on a single computer
    using a thread-based or a process-based scheduler. Recently, the implementation
    of a new distributed backend can be used to set up and run Dask graphs on a network
    of computers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Dask项目的最初迭代是为了在单台计算机上运行而设计的，使用基于线程或进程的调度器。最近，新分布式后端的实现可以用来在计算机网络上设置和运行Dask图。
- en: Dask distributed is not installed automatically with Dask. The library is available
    through the `conda` package manager (use the `$ conda install distributed` command )
    as well as `pip` (with the `$ pip install distributed` command).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Dask分布式不是与Dask自动安装的。该库可以通过`conda`包管理器（使用`$ conda install distributed`命令）以及`pip`（使用`$
    pip install distributed`命令）获得。
- en: 'Getting started with Dask distributed is really easy. The most basic setup
    is obtained by instantiating a `Client` object:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 开始使用Dask分布式实际上非常简单。最基本的设置是通过实例化一个`Client`对象来获得的：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: By default, Dask will start a few key processes (on the local machine) necessary
    for scheduling and executing distributed tasks through the `Client` instance.
    The main components of a Dask cluster are a single *scheduler* and a collection
    of *workers*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Dask将通过`Client`实例启动一些关键进程（在本地机器上），这些进程对于调度和执行分布式任务是必要的。Dask集群的主要组件是一个单一的*scheduler*和一组*workers*。
- en: The **scheduler** is the process responsible for distributing the work across
    the workers and to monitor and manage the results. Generally, when a task is submitted
    to the user, the scheduler will find a free worker and submit a task for execution.
    Once the worker is done, the scheduler is informed that the result is available.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**调度器**是负责在工作器之间分配工作并监控和管理结果的进程。通常，当任务被提交给用户时，调度器会找到一个空闲的工作器并提交一个任务以供执行。一旦工作器完成，调度器就会被告知结果已可用。'
- en: A worker is a process that accepts incoming tasks and produces results. Workers
    can reside on different machines over the network. Workers execute tasks using
    `ThreadPoolExecutor`. This can be used to achieve parallelism when using functions
    that do not acquire the GIL (such as Numpy, Pandas, and Cython functions in `nogil`
    blocks). When executing pure Python code, it is advantageous to start many single-threaded
    worker processes as this will enable parallelism for code that acquires the GIL.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 工作器是一个接受传入任务并产生结果的进程。工作器可以驻留在网络上的不同机器上。工作器使用`ThreadPoolExecutor`执行任务。这可以用来在不需要获取GIL（例如，在`nogil`块中的Numpy、Pandas和Cython函数）的函数中使用并行性。当执行纯Python代码时，启动许多单线程工作器进程是有利的，因为这将为获取GIL的代码启用并行性。
- en: 'The `Client` class can be used to submit tasks manually to the scheduler using
    familiar asynchronous methods. For example, to submit a function for execution
    on the cluster, one can use the `Client.map` and `Client.submit` methods. In the
    following code, we demonstrate the use of `Client.map` and `Client.submit` to
    calculate the square of a few numbers. The `Client` will submit a series of tasks
    to the scheduler and we will receive a `Future` instance for each task:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`Client`类可以用来使用熟悉的异步方法手动将任务提交给调度器。例如，为了在集群上执行一个函数，可以使用`Client.map`和`Client.submit`方法。在下面的代码中，我们展示了如何使用`Client.map`和`Client.submit`来计算几个数字的平方。`Client`将向调度器提交一系列任务，我们将为每个任务接收一个`Future`实例：'
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: So far, this is quite similar to what we saw in the earlier chapters with `TheadPoolExecutor` and
    `ProcessPoolExecutor`. Note however, that Dask Distributed not only submits the
    tasks, but also caches the computation results on the worker memory. You can see
    caching in action by looking at the preceding code example. When we first invoke
    `client.submit`, the `square(2)` task is created and its status is set to *pending*.
    When we subsequently invoke `client.map`,  the `square(2)` task is resubmitted
    to the scheduler, but this time, rather than recalculating its value, the scheduler
    directly retrieves the result for the worker. As a result, the third `Future`
    returned by map already has a finished status.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这与我们在前几章中看到的`TheadPoolExecutor`和`ProcessPoolExecutor`非常相似。然而，请注意，Dask
    Distributed不仅提交任务，还将在工作器内存中缓存计算结果。您可以通过查看前面的代码示例来看到缓存的作用。当我们第一次调用`client.submit`时，`square(2)`任务被创建，其状态设置为*待处理*。当我们随后调用`client.map`时，`square(2)`任务被重新提交给调度器，但这次，调度器不是重新计算其值，而是直接从工作器检索结果。因此，map返回的第三个`Future`已经处于完成状态。
- en: 'Results from a collection of `Future` instances can be retrieved using the
    `Client.gather` method:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`Client.gather`方法检索来自`Future`实例集合的结果：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Client` can also be used to run arbitrary Dask graphs. For example, we can
    trivially run our approximation of pi by passing the `client.get` function as
    an optional argument to `pi.compute`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`Client`也可以用来运行任意的Dask图。例如，我们可以通过将`client.get`函数作为可选参数传递给`pi.compute`来简单地运行我们的π近似值：'
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This feature makes Dask extremely scalable as it is possible to develop and
    run algorithms on the local machine using one of the simpler schedulers and, in
    case the performance is not satisfactory, to reuse the same algorithms on a cluster
    of hundreds of machines.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特性使得Dask具有极高的可扩展性，因为它可以在本地机器上使用其中一个较简单的调度器开发和运行算法，如果性能不满意，还可以在由数百台机器组成的集群上重用相同的算法。
- en: Manual cluster setup
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动集群设置
- en: 'To instantiate scheduler and workers manually, one can use the `dask-scheduler`
    and `dask-worker` command-line utilities. First, we can initialize a scheduler
    using the `dask-scheduler` command:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要手动实例化调度器和工作者，可以使用`dask-scheduler`和`dask-worker`命令行工具。首先，我们可以使用`dask-scheduler`命令初始化调度器：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will provide an address for the scheduler and a Web UI address that can
    be accessed to monitor the state of the cluster. Now, we can assign some workers
    to the scheduler; this can be accomplished using the `dask-worker` command and
    by passing the address of the scheduler to the worker. This will automatically
    start a worker with four threads:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为调度器提供一个地址和一个可以用来监控集群状态的 Web UI 地址。现在，我们可以将一些工作线程分配给调度器；这可以通过使用 `dask-worker`
    命令并将调度器的地址传递给工作线程来实现。这将自动启动一个拥有四个线程的工作线程：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The Dask scheduler is fairly resilient in the sense that if we add and remove
    a worker, it is able to track which results are unavailable and recompute them
    on-demand. Finally, in order to use the initialized scheduler from a Python session,
    it is sufficient to initialize a `Client` instance and provide the address for
    the scheduler:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 调度器在容错方面相当强大，这意味着如果我们添加和删除一个工作线程，它能够追踪哪些结果不可用，并按需重新计算它们。最后，为了从 Python 会话中使用初始化的调度器，只需初始化一个
    `Client` 实例并提供调度器的地址即可：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Dask also provides a convenient diagnostic Web UI that can be used to monitor
    the status and time spent for each of the tasks performed on the cluster. In the
    next figure, the **Task Stream** shows the time taken for executing the pi estimation.
    In the plot, each horizontal gray line corresponds to a thread used by the workers
    (in our case, we have one worker with four threads, also called **Worker Core**),
    and each rectangular box corresponds to a task, colored so that the same task
    types have the same color (for example, addition, power, or exponent). From this
    plot, you can observe how all the boxes are very small and far from each other.
    This means that the tasks are quite small compared to the overhead of communication.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 还提供了一个方便的诊断 Web UI，可以用来监控集群上每个任务的状态和耗时。在下一张图中，**任务流**显示了执行 pi 估计所花费的时间。在图表中，每一条水平灰色线对应一个工作线程（在我们的例子中，我们有一个拥有四个线程的工作线程，也称为**工作核心**），每个矩形框对应一个任务，颜色相同以表示相同的任务类型（例如，加法、幂或指数）。从这个图表中，你可以观察到所有方块都非常小且彼此距离很远。这意味着与通信开销相比，任务相当小。
- en: In this case, an increase in chunk size, which implies to an increase in the
    time required to run each task compared to the time of communication, will be
    beneficial.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，块大小的增加，意味着与通信时间相比，每个任务运行所需的时间增加，这将是有益的。
- en: '![](img/B06440_08CHPNO_06.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06440_08CHPNO_06.png)'
- en: Using PySpark
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PySpark
- en: Nowadays, Apache Spark is one of the most popular projects for distributed computing.
    Developed in Scala, Spark was released in 2014, and integrates with HDFS and provides
    several advantages and improvements over the Hadoop MapReduce framework.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Apache Spark 是分布式计算中最受欢迎的项目之一。Spark 使用 Scala 开发，于 2014 年发布，与 HDFS 集成，并在 Hadoop
    MapReduce 框架之上提供了几个优势和改进。
- en: Contrary to Hadoop MapReduce, Spark is designed to process data interactively
    and supports APIs for the Java, Scala, and Python programming languages. Given
    its different architecture, especially by the fact that Spark keep results in
    memory, Spark is generally much faster than Hadoop MapReduce.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Hadoop MapReduce 不同，Spark 被设计为可以交互式处理数据，并支持 Java、Scala 和 Python 编程语言的 API。鉴于其不同的架构，特别是
    Spark 将结果保留在内存中的事实，Spark 通常比 Hadoop MapReduce 快得多。
- en: Setting up Spark and PySpark
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Spark 和 PySpark
- en: Setting up PySpark from scratch requires the installation of the Java and Scala
    runtimes, the compilation of the project from source, and the configuration of Python
    and Jupyter notebook so that they can be used alongside the Spark installation.
    An easier and less error-prone way to set up PySpark is to use an already configured
    Spark cluster made available through a **Docker** container.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始设置 PySpark 需要安装 Java 和 Scala 运行时，从源代码编译项目，并配置 Python 和 Jupyter notebook，以便它们可以与
    Spark 安装一起使用。设置 PySpark 的一个更简单且错误更少的方法是使用通过 **Docker** 容器提供的已配置 Spark 集群。
- en: Docker can be downloaded at [https://www.docker.com/](https://www.docker.com/) .
    If you're new to containers, you can read the next chapter for an introduction.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从 [https://www.docker.com/](https://www.docker.com/) 下载 Docker。如果您对容器还不太熟悉，可以阅读下一章以获取介绍。
- en: 'To set up a Spark cluster, it is sufficient to go in this chapter''s code files
    (where a file named `Dockerfile` is located) and issue the following command:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置 Spark 集群，只需进入本章的代码文件（其中有一个名为 `Dockerfile` 的文件）并执行以下命令：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This command will automatically download, install, and configure Spark, Python,
    and Jupyter notebook in an isolated environment. To start Spark and a Jupyter
    notebook session, you can execute the following command:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将自动在隔离环境中下载、安装和配置Spark、Python和Jupyter笔记本。要启动Spark和Jupyter笔记本会话，您可以执行以下命令：
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The command will print a unique ID (called *container id*) that you can use
    to reference the application container and will start Spark and Jupyter notebook
    in the background. The `-p` option ensures that we can access the SparkUI and
    Jupyter network ports from the local machine. After issuing the command, you can
    open a browser to `http://127.0.0.1:8888` to access the Jupyter notebook session.
    You can test the correct initialization of Spark by creating a new notebook and
    executing the following content inside a cell:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 命令将打印一个唯一的ID（称为*容器ID*），您可以使用它来引用应用程序容器，并将Spark和Jupyter笔记本在后台启动。`-p`选项确保我们可以从本地机器访问SparkUI和Jupyter网络端口。在发出命令后，您可以通过打开浏览器访问`http://127.0.0.1:8888`来访问Jupyter笔记本会话。您可以通过创建一个新的笔记本并在单元格中执行以下内容来测试Spark的正确初始化：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This will initialize a `SparkContext` and take the first element in a collection
    (those new terms will be explained in detail later). Once the `SparkContext` is
    initialized, we can also head over to [http://127.0.0.1:4040](http://127.0.0.1:4040)
    to open the Spark Web UI.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这将初始化一个`SparkContext`并获取集合中的第一个元素（这些新术语将在稍后详细解释）。一旦`SparkContext`初始化完成，我们还可以访问[http://127.0.0.1:4040](http://127.0.0.1:4040)来打开Spark
    Web UI。
- en: Now that the setup is complete, we will understand how Spark works and how to
    implement simple parallel algorithms using its powerful API.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在设置完成，我们将了解Spark的工作原理以及如何使用其强大的API实现简单的并行算法。
- en: Spark architecture
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark架构
- en: A Spark cluster is a set of processes distributed over different machines. The
    **Driver Program** is a process, such as a Scala or Python interpreter, used by
    the user to submit the tasks to be executed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Spark集群是在不同机器上分布的一组进程。**驱动程序**是一个进程，例如Scala或Python解释器，用户用它来提交要执行的任务。
- en: The user can build task graphs, similar to Dask, using a special API and submit
    those tasks to the **Cluster Manager** that is responsible for assigning these
    tasks to **Executors**, processes responsible for executing the tasks. In a multi-user
    system, the Cluster Manager is also responsible for allocating resources on a
    per-user basis.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以使用特殊的API构建类似于Dask的任务图，并将这些任务提交给**集群管理器**，该管理器负责将这些任务分配给**执行器**，即负责执行任务的进程。在多用户系统中，集群管理器还负责按用户分配资源。
- en: The user interacts with the Cluster Manager through the Driver Program. The
    class responsible for communication between the user and the Spark cluster is called
    `SparkContext`. This class is able to connect and configure the Executors on the
    cluster based on the resources available to the user.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 用户通过**驱动程序**与集群管理器交互。负责用户与Spark集群之间通信的类被称为`SparkContext`。这个类能够根据用户可用的资源连接和配置集群上的执行器。
- en: For its most common use-cases, Spark manages its data through a data structure
    called **Resilient Distributed Datasets** (**RDD**), which represents a collection
    of items. RDDs are capable of handling massive datasets by separating their elements
    into partitions and operating on the partitions in parallel (note that this mechanism
    is mainly hidden from the user). RDDs can also be stored in memory (optionally,
    and when appropriate) for fast access and to cache expensive intermediate results.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其最常见的使用场景，Spark通过一种称为**弹性分布式数据集**（**RDD**）的数据结构来管理其数据，它表示一组项目。RDDs通过将它们的元素分割成分区并在并行操作这些分区（注意，这种机制主要对用户隐藏）来处理大规模数据集。RDDs还可以存储在内存中（可选，且在适当的时候）以实现快速访问和缓存昂贵的中间结果。
- en: Using RDDs, it is possible to define tasks and transformations (similarly to
    how we were automatically generating computation graphs in Dask) and, when requested,
    the Cluster Manager will automatically dispatch and execute tasks on the available
    Executors.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RDDs，可以定义任务和转换（类似于我们在Dask中自动生成计算图的方式），当请求时，集群管理器将自动将任务调度和执行到可用的执行器上。
- en: The Executors will receive the tasks from the Cluster Manager, execute them,
    and keep the results around if needed. Note that an Executor can have multiple
    cores and each node in the cluster may have multiple Executors. Generally speaking,
    Spark is fault tolerant on Executor's failures.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器将从集群管理器接收任务，执行它们，并在需要时保留结果。请注意，一个执行器可以有多个核心，集群中的每个节点可能有多个执行器。一般来说，Spark对执行器的故障具有容错性。
- en: 'In the following diagram, we show how the aforementioned components interact
    in a Spark cluster. The **Driver Program** interacts with the **Cluster Manager**
    that manages the **Executor** instances on different nodes (each Executor instance
    can also have multiple threads). Note that, even if the **Driver Program** doesn''t
    directly control the Executors, the results, which are stored in the **Executor**
    instances, are transferred directly between the Executors and the Driver Program. For
    this reason, it''s important that the **Driver Program** is network-reachable
    from the **Executor** processes:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，我们展示了上述组件如何在Spark集群中交互。**驱动程序**与**集群管理器**交互，集群管理器负责管理不同节点上的**执行器**实例（每个执行器实例也可以有多个线程）。请注意，即使**驱动程序**不直接控制执行器，存储在**执行器**实例中的结果也会直接在执行器和驱动程序之间传输。因此，确保**驱动程序**可以从**执行器**进程网络访问是很重要的：
- en: '![](img/B06440_08CHPNO_07.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06440_08CHPNO_07.png)'
- en: 'A natural question to ask is: How is Spark, a software written in Scala, able
    to execute Python code? The integration is done through the `Py4J` library, which
    maintains a Python process under-the-hood and communicates with it through sockets
    (a form of interprocess communication). In order to run the tasks, Executors maintain
    a series of Python processes so that they are able to process Python code in parallel.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的问题就是：Spark，一个用Scala编写的软件，是如何执行Python代码的？集成是通过`Py4J`库完成的，该库在底层维护一个Python进程并通过套接字（一种进程间通信形式）与之通信。为了运行任务，执行器维护一系列Python进程，以便它们能够并行处理Python代码。
- en: RDDs and variables defined in a Python process in the Driver Program are serialized,
    and the communication between the Cluster Manager and the Executors (including
    shuffling) is dealt with by Spark's Scala code. The extra serialization steps
    necessary for the Python and Scala interchange, all contribute to the overhead
    of communication; therefore, when using PySpark, extra care must be taken to ensure
    that the data structures used are serialized efficiently and that the data partitions
    are big enough so that the cost of communication is negligible compared to the
    cost of execution.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序程序中的Python进程定义的RDD和变量会被序列化，集群管理器和执行器之间的通信（包括洗牌）由Spark的Scala代码处理。Python和Scala之间交换所需的额外序列化步骤，所有这些都增加了通信的开销；因此，当使用PySpark时，必须格外小心，确保使用的结构被有效地序列化，并且数据分区足够大，以便通信的成本与执行的成本相比可以忽略不计。
- en: 'The following diagram illustrates the additional Python processes needed for
    PySpark execution. These additional Python processes come with associated memory
    costs and an extra layer of indirection that complicate error reporting:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了PySpark执行所需的额外Python进程。这些额外的Python进程伴随着相关的内存成本和额外的间接层，这增加了错误报告的复杂性：
- en: '![](img/B06440_08CHPNO_08.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06440_08CHPNO_08.png)'
- en: Despite these drawbacks, PySpark is still a widely used tool because it bridges
    the vivid Python ecosystem with the industrial strength of the Hadoop infrastructure.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些缺点，PySpark仍然是一个广泛使用的工具，因为它将生动的Python生态系统与Hadoop基础设施的工业强度连接起来。
- en: Resilient Distributed Datasets
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性分布式数据集
- en: 'The easiest way to create an RDD in Python is with the `SparkContext.parallelize`
    method. This method was also used earlier where we parallelized a collection of
    integers between `0` and `1000`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中创建RDD的最简单方法是使用`SparkContext.parallelize`方法。这个方法之前也被用来并行化一个介于`0`和`1000`之间的整数集合：
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `rdd` collection will be divided into a number of partitions which, in
    this case, correspond to a default value of four (the default value can be changed
    using configuration options). To explicitly specify the number of partitions,
    one can pass an extra argument to `parallelize`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd`集合将被分成多个分区，在这种情况下，对应于默认值四（默认值可以通过配置选项更改）。要显式指定分区数，可以向`parallelize`传递额外的参数：'
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'RDDs support a lot of functional programming operators, similar to what we
    used back in [Chapter 6](2b46e5c0-5308-4073-b1c6-4232a881b39f.xhtml), *Implementing
    Concurrency*, with reactive programming and data streams (even though, in that
    case, the operators were designed to work on events over time rather than normal
    collections). For example, we may illustrate the basic `map` function which, by
    now, should be quite familiar. In the following code, we use `map` to calculate
    the square of a series of numbers:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: RDD支持许多函数式编程操作符，类似于我们在[第6章](2b46e5c0-5308-4073-b1c6-4232a881b39f.xhtml)“实现并发”中使用的，与反应式编程和数据流（尽管在这种情况下，操作符是为处理随时间推移的事件而不是普通集合而设计的）。例如，我们可以展示基本的`map`函数，到现在应该已经很熟悉了。在以下代码中，我们使用`map`来计算一系列数字的平方：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `map` function will return a new RDD but won''t compute anything just yet.
    In order to trigger the execution, you can use the `collect` method, which will
    retrieve all the elements in the collection, or `take`, which will return only
    the first ten elements:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`map`函数将返回一个新的RDD，但不会立即计算任何内容。为了触发执行，你可以使用`collect`方法，它将检索集合中的所有元素，或者使用`take`，它将只返回前十个元素：'
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'For a comparison between PySpark, Dask, and the other parallel programming
    libraries we explored in the earlier chapters, we will reimplement the approximation
    of pi. In the PySpark implementation, we will first create two RDDs of random
    numbers using `parallelize`, then we combine the datasets using the `zip` function
    (this is equivalent to Python''s `zip`), and we finally test whether the random
    points are inside the circle:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较PySpark、Dask以及我们在前几章中探索的其他并行编程库，我们将重新实现π的近似值。在PySpark实现中，我们首先使用`parallelize`创建两个包含随机数的RDD，然后使用`zip`函数（这相当于Python的`zip`）合并数据集，最后测试随机点是否在圆内：
- en: '[PRE26]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It's important to note that both the `zip` and `map` operations produce new
    RDDs and do not actually execute the instruction on the underlying data. In the preceding
    example, code execution is triggered when we call the `hit_test.sum` function,
    which returns an integer. This behavior is different from the Dask API where the
    whole computation (including the final result, `pi`) did not trigger the execution.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，`zip`和`map`操作都会生成新的RDD，实际上并不在底层数据上执行指令。在先前的例子中，代码执行是在我们调用`hit_test.sum`函数时触发的，该函数返回一个整数。这种行为与Dask
    API不同，在Dask API中，整个计算（包括最终结果`pi`）并没有触发执行。
- en: We can now move on to a more interesting application to demonstrate more RDD
    methods. We will learn how to count the number of visits each user of a website
    performs in a day. In a real-world scenario, the data would have been collected
    in a database and/or stored in a distributed filesystem, such as HDFS. However,
    in our example, we will generate some data that we will then analyze.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续到一个更有趣的应用，以展示更多的RDD方法。我们将学习如何计算网站每个用户每天访问的次数。在现实世界的场景中，数据已经被收集到数据库中，或者存储在分布式文件系统，如HDFS中。然而，在我们的例子中，我们将生成一些数据，然后进行分析。
- en: 'In the following code, we generate a list of dictionaries, each containing
    a `user` (selected among twenty users) and a `timestamp`. The steps to produce
    the dataset are as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们生成一个包含字典的列表，每个字典包含一个`user`（从二十个用户中选择）和一个`timestamp`。生成数据集的步骤如下：
- en: Create a pool of 20 users (the `users` variable).
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含20个用户的池（`users`变量）。
- en: Define a function that returns a random time between two dates.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数返回两个日期之间的随机时间。
- en: For 10,000 times, we choose a random user from our `users` pool and a random
    timestamp between the dates January 1, 2017 and January 7, 2017.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于10,000次，我们从`users`池中随机选择一个用户，并从2017年1月1日到2017年1月7日之间的随机时间戳。
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'With the dataset at hand, we can start asking questions and use PySpark to
    find the answers. One common question is "*How many times has a given user visited
    the website?.*" A naive way to compute this result can be achieved by grouping
    the entries RDD by user (using the `groupBy` operator) and counting how many items
    are present for each user. In PySpark, `groupBy` takes a function as argument
    to extract the grouping key for each element and returns a new RDD that contain
    tuples of the `(key, group)` form. In the following example, we use the user ID
    as the key for our `groupBy`, and we inspect the first element using `first`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用手头的数据集，我们可以开始提问并使用 PySpark 来找到答案。一个常见的问题是“*给定用户访问网站了多少次？*”。计算这个结果的一个简单方法是通过使用
    `groupBy` 操作符按用户分组（使用 `groupBy` 操作符）并计算每个用户有多少项。在 PySpark 中，`groupBy` 接收一个函数作为参数，用于提取每个元素的分组键，并返回一个新的
    RDD，其中包含 `(key, group)` 形式的元组。在下面的示例中，我们使用用户 ID 作为 `groupBy` 的键，并使用 `first` 检查第一个元素：
- en: '[PRE28]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The return value of `groupBy` contains a `ResultIterable` (which is basically
    a list) for each user ID. To count the number of visits per user, it''s sufficient
    to calculate the length of each `ResultIterable`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupBy` 的返回值包含一个 `ResultIterable`（基本上是一个列表），用于每个用户 ID。为了计算每个用户的访问次数，计算每个
    `ResultIterable` 的长度就足够了：'
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Even though this algorithm may work well in small datasets, `groupBy` requires
    us to collect and store the whole set of entries for each user in memory, and
    this can exceed the memory capacity of an individual node. Since we don't need
    the list but only the count, there's a better way to calculate this number without
    having to hold the list of visits for each user in memory.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个算法在小数据集中可能效果很好，但 `groupBy` 要求我们将每个用户的整个条目集收集并存储在内存中，这可能会超过单个节点的内存容量。由于我们不需要列表，只需要计数，因此有更好的方法来计算这个数字，而无需在内存中保留每个用户的访问列表。
- en: 'When dealing with an RDD of `(key, value)` pairs, it is possible to use `mapValues`
    to apply a function only to the values. In the preceding code, we can replace
    the `map(lambda kv: (kv[0], len(kv[1])))` call with `mapValues(len)` for better
    readability.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '当处理 `(key, value)` 对的 RDD 时，可以使用 `mapValues` 仅对值应用函数。在前面的代码中，我们可以将 `map(lambda
    kv: (kv[0], len(kv[1])))` 调用替换为 `mapValues(len)` 以提高可读性。'
- en: 'For a more efficient calculation, we can leverage the `reduceByKey` function,
    which will perform a step similar to the Reduce step that we saw in the *An introduction
    to MapReduce* section. The `reduceByKey` function can be called from an RDD of
    tuples that contain a key as their first element and a value as their second element,
    and accepts a function as its first argument that will calculate the reduction.
     A simple example of the `reduceByKey` function is illustrated in the following
    snippet. We have a few string keys associated with integer numbers, and we want
    to get the sum of the values for each key; the reduction, expressed as a lambda,
    corresponds to the sum of the elements:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更高效的计算，我们可以利用 `reduceByKey` 函数，它将执行类似于我们在 *MapReduce 简介* 部分中看到的 Reduce 步骤。`reduceByKey`
    函数可以从包含键作为第一个元素和值作为第二个元素的元组的 RDD 中调用，并接受一个作为其第一个参数的函数，该函数将计算减少。以下是对 `reduceByKey`
    函数的一个简单示例。我们有一些与整数数字关联的字符串键，我们想要获取每个键的值的总和；这个减少操作，用 lambda 表达式表示，对应于元素的总和：
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `reduceByKey` function is much more efficient than `groupBy` because the
    reduction is parallelizable and doesn''t require the in-memory storage of the
    groups; also, it limits the data shuffled between Executors (it performs similar
    operations to Dask''s `foldby`, which was explained earlier). At this point, we
    can rewrite our visit count calculation using `reduceByKey`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey` 函数比 `groupBy` 更高效，因为它的减少操作是可并行的，并且不需要在内存中存储组；此外，它还限制了在 Executors
    之间传输的数据量（它执行的操作类似于前面解释过的 Dask 的 `foldby`）。在这个阶段，我们可以使用 `reduceByKey` 重新编写我们的访问次数计算：'
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With Spark''s RDD API, it is also easy to answer questions such as "*How many
    visits did the website receive each day?.*" This can be computed using `reduceByKey`
    with the appropriate key (which is the date extracted from the timestamp). In
    the following example, we demonstrate the calculation. Also, note the usage of
    the `sortByKey` operator to return the counts sorted by date:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark 的 RDD API，回答诸如“*每天网站接收了多少次访问？*”等问题也很容易。这可以通过使用 `reduceByKey` 并提供适当的键（即从时间戳中提取的日期）来计算。在下面的示例中，我们展示了计算过程。同时，请注意
    `sortByKey` 操作符的使用，它按日期对计数进行排序：
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Spark DataFrame
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark DataFrame
- en: 'For numerical and analytical tasks, Spark provides a convenient interface available
    through the `pyspark.sql` module (also called SparkSQL). The module includes a
    `spark.sql.DataFrame` class that can be used for efficient SQL-style queries similar
    to those of Pandas. Access to the SQL interface is provided through the `SparkSession`
    class:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值和分析任务，Spark 通过 `pyspark.sql` 模块（也称为 SparkSQL）提供了一个方便的接口。该模块包括一个 `spark.sql.DataFrame`
    类，可以用于类似于 Pandas 的有效 SQL 样式查询。通过 `SparkSession` 类提供对 SQL 接口的访问：
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`SparkSession` can then be used to create a `DataFrame` through the function
    `createDataFrame`. The function `createDataFrame` accepts either a RDD, a list,
    or a `pandas.DataFrame`.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession` 可以通过 `createDataFrame` 函数创建一个 `DataFrame`。`createDataFrame`
    函数接受 RDD、列表或 `pandas.DataFrame`。'
- en: 'In the following example, we will create a `spark.sql.DataFrame` by converting
    an RDD, `rows`, which contains a collection of `Row` instances. The `Row` instances
    represent an association between a set of column names and a set of values, just
    like a row in a `pd.DataFrame`. In this example, we have two columns--`x` and `y`--to
    which we will associate random numbers:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将通过将包含 `Row` 实例集合的 RDD `rows` 转换为 `spark.sql.DataFrame` 来创建一个 `spark.sql.DataFrame`。`Row`
    实例代表一组列名和一组值之间的关联，就像 `pd.DataFrame` 中的行一样。在这个例子中，我们有两个列--`x` 和 `y`--我们将与随机数相关联：
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'After obtaining our collection of `Row` instances, we can combine them in a
    `DataFrame`, as follows. We can also inspect the `DataFrame` content using the `show`
    method:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得我们的 `Row` 实例集合后，我们可以将它们组合成一个 `DataFrame`，如下所示。我们还可以使用 `show` 方法检查 `DataFrame`
    的内容：
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`spark.sql.DataFrame` supports performing transformations on the distributed
    dataset using a convenient SQL syntax. For example, you can use the `selectExpr`
    method to calculate a value using a SQL expression. In the following code, we
    compute the hit test using the `x` and `y` columns and the `pow` SQL function:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.sql.DataFrame` 支持使用方便的 SQL 语法对分布式数据集进行转换。例如，您可以使用 `selectExpr` 方法使用
    SQL 表达式计算一个值。在以下代码中，我们使用 `x` 和 `y` 列以及 `pow` SQL 函数计算碰撞测试：'
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To demonstrate the expressivity of SQL, we can also calculate the estimation
    of pi using a single expression. The expression involves using SQL functions such
    as `sum`, `pow`, `cast`, and `count`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 SQL 的表达能力，我们还可以使用单个表达式计算 pi 的估计值。该表达式涉及使用 SQL 函数，如 `sum`、`pow`、`cast` 和
    `count`：
- en: '[PRE37]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Spark SQL follows the same syntax as Hive, a SQL engine for distributed datasets
    built on Hadoop. Refer to [https://cwiki.apache.org/confluence/display/Hive/LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)
    for a complete syntax reference.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 与基于 Hadoop 的分布式数据集的 SQL 引擎 Hive 使用相同的语法。有关完整的语法参考，请参阅 [https://cwiki.apache.org/confluence/display/Hive/LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)。
- en: DataFrames are a great way to leverage the power and optimization of Scala while
    using the Python interface. The main reason is that queries are interpreted symbolically
    by SparkSQL and the execution happens directly in Scala without having to pass
    intermediate results through Python. This greatly reduces the serialization overhead
    and takes advantage of the query optimizations performed by SparkSQL. Optimizations
    and query planning allows the use of SQL operators, such as `GROUP BY`, without
    incurring in performance penalties, such as the one we experienced using `groupBy`
    directly on an RDD.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 是一种利用 Scala 的强大功能和优化，同时使用 Python 接口的好方法。主要原因在于查询由 SparkSQL 符号解释，并且执行直接在
    Scala 中发生，无需通过 Python 传递中间结果。这大大减少了序列化开销，并利用了 SparkSQL 执行的查询优化。优化和查询规划允许使用 SQL
    操作符，如 `GROUP BY`，而不会产生性能惩罚，就像我们直接在 RDD 上使用 `groupBy` 时所经历的那样。
- en: Scientific computing with mpi4py
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 mpi4py 进行科学计算
- en: Even though Dask and Spark are great technologies widely used in the IT industry,
    they have not been widely adopted in academic research. High-performance supercomputers
    with thousands of processors have been used in academia for decades to run intense
    numerical applications. For this reason, supercomputers are generally configured
    using a very different software stack that focuses on a computationally-intensive
    algorithm implemented in a low-level language, such as C, Fortran, or even assembly.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Dask 和 Spark 是在 IT 行业广泛使用的优秀技术，但它们在学术研究中并没有得到广泛的应用。几十年来，学术界一直使用拥有数千个处理器的超级计算机来运行密集型数值应用。因此，超级计算机通常使用一个非常不同的软件栈进行配置，该软件栈专注于在低级语言（如
    C、Fortran 或甚至汇编）中实现的计算密集型算法。
- en: The principal library used for parallel execution on these kinds of systems
    is **Message Passing Interface** (**MPI**), which, while less convenient or sophisticated
    than Dask or Spark, is perfectly capable of expressing parallel algorithms and
    achieving excellent performance. Note that, contrary to Dask and Spark, MPI does
    not follow the MapReduce model and is best used for running thousands of processes
    with very little data sent between them.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些系统上用于并行执行的主要库是**消息传递接口**（**MPI**），虽然它不如Dask或Spark方便或复杂，但完全能够表达并行算法并实现出色的性能。请注意，与Dask和Spark不同，MPI不遵循MapReduce模型，并且最好用于运行成千上万的进程，它们之间发送的数据非常少。
- en: MPI works quite differently compared to what we've seen so far. Parallelism
    in MPI is achieved by running *the same script* in multiple processes (which possibly
    exist on different nodes); communication and synchronization between processes
    is handled by a designated process, which is commonly called **root** and is usually
    identified by a `0` ID.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们迄今为止所看到的不同，MPI的工作方式相当不同。MPI中的并行性是通过在多个进程中运行**相同的脚本**（这些进程可能存在于不同的节点上）来实现的；进程之间的通信和同步由一个指定的进程处理，通常称为**根**，通常由`0`
    ID标识。
- en: 'In this section, we will briefly demonstrate the main concepts of MPI using
    its `mpi4py` Python interface. In the following example, we demonstrate the simplest
    possible parallel code with MPI. The code imports the MPI module and retrieves
    `COMM_WORLD`, which is an interface that can be used to interact with other MPI
    processes. The `Get_rank` function will return an integer identifier for the current
    process:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要演示使用其`mpi4py` Python接口的MPI的主要概念。在以下示例中，我们展示了使用MPI的最简单的并行代码。代码导入MPI模块并检索`COMM_WORLD`，这是一个可以用来与其他MPI进程交互的接口。`Get_rank`函数将返回当前进程的整数标识符：
- en: '[PRE38]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can place the preceding code in a file, `mpi_example.py`, and execute it. Running
    this script normally won''t do anything special as it involves the execution of
    a single process:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面的代码放入一个名为`mpi_example.py`的文件中，并执行它。正常运行此脚本不会做任何特别的事情，因为它只涉及单个进程的执行：
- en: '[PRE39]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'MPI jobs are meant to be executed using the `mpiexec` command, which takes
    a `-n` option to indicate the number of parallel processes. Running the script
    using the following command will generate four independent executions of the same
    script, each with a different ID:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: MPI作业旨在使用`mpiexec`命令执行，该命令通过`-n`选项来指定并行进程的数量。使用以下命令运行脚本将生成四个独立的相同脚本执行，每个执行都有一个不同的ID：
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Distributing processes among the network is performed automatically through
    a resource manager (such as TORQUE). Generally, supercomputers are configured
    by the system administrator, which will also provide instructions on how to run
    MPI software.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过资源管理器（如TORQUE）自动在网络中分配进程。通常，超级计算机由系统管理员配置，系统管理员还将提供有关如何运行MPI软件的说明。
- en: 'To get a feel as to what an MPI program looks like, we will reimplement the
    approximation of *pi*. The complete code is shown here. The program will do the
    following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解MPI程序的样子，我们将重新实现π的近似。完整的代码如下。程序将执行以下操作：
- en: Create a random array of `N / n_procs` size for each process so that each process
    will test the same amount of samples (`n_procs` is obtained through the `Get_size`
    function)
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个进程创建一个大小为`N / n_procs`的随机数组，以便每个进程将测试相同数量的样本（`n_procs`通过`Get_size`函数获得）
- en: In each separate process, calculate the sum of the hit tests and store it in
    `hits_counts`, which will represent the partial counts for each process
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个单独的进程中，计算击中测试的总和并将其存储在`hits_counts`中，这将代表每个进程的部分计数
- en: Use the `reduce` function to calculate the total sum of the partial counts.
    When using reduce, we need to specify the `root` argument to specify which process
    will receive the result
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`reduce`函数计算部分计数的总和。在调用reduce时，我们需要指定`root`参数来指定哪个进程将接收结果
- en: 'Print the final result only on the process corresponding to the root process:'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只在根进程对应的进程中打印最终结果：
- en: '[PRE41]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can now place the preceding code in a file named `mpi_pi.py` and execute
    it using `mpiexec`. The output shows how the four process executions are intertwined
    until we get to the `reduce` call:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将前面的代码放入一个名为`mpi_pi.py`的文件中，并使用`mpiexec`执行它。输出显示了四个进程执行如何交织在一起，直到我们到达`reduce`调用：
- en: '[PRE42]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Distributed processing can be used to implement algorithms capable of handling
    massive datasets by distributing smaller tasks across a cluster of computers.
    Over the years, many software packages, such as Apache Hadoop, have been developed
    to implement performant and reliable execution of distributed software.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式处理可以用来实现能够处理大规模数据集的算法，通过在计算机集群中分配更小的任务来实现。多年来，许多软件包，如Apache Hadoop，已经被开发出来以实现分布式软件的高效和可靠执行。
- en: In this chapter, we learned about the architecture and usage of Python packages,
    such as Dask and PySpark, which provide powerful APIs to design and execute programs
    capable of scaling to hundreds of machines. We also briefly looked at MPI, a library
    that has been used for decades to distribute work on supercomputers designed for
    academic research.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了Python包的架构和使用方法，例如Dask和PySpark，它们提供了强大的API来设计和执行能够扩展到数百台机器的程序。我们还简要介绍了MPI，这是一个已经用于数十年的库，用于在为学术研究设计的超级计算机上分配工作。
- en: Throughout this book, we explored several techniques to improve the performance of
    our program, and to increase the speed of our programs and the size of the datasets
    we are able to process. In the next chapter, we will describe the strategies and
    best practices to write and maintain high-performance code.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们探讨了多种提高我们程序性能的技术，以及增加我们程序处理数据集速度和规模的方法。在下一章中，我们将描述编写和维护高性能代码的策略和最佳实践。
