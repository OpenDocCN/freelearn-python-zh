- en: Distributed Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式处理
- en: In the last chapter, we introduced the concept of parallel processing and learned
    how to leverage multicore processors and GPUs. Now, we can step up our game a
    bit and turn our attention on distributed processing, which involves executing
    tasks across multiple machines to solve a certain problem.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了并行处理的概念，并学习了如何利用多核处理器和GPU。现在，我们可以将游戏提升到一个新的水平，并将注意力转向分布式处理，这涉及到在多台机器上执行任务以解决特定问题。
- en: In this chapter, we will illustrate the challenges, use cases, and examples
    of how to run code on a cluster of computers. Python offers easy-to-use and reliable
    packages for distribute processing, which will allow us to implement scalable
    and fault-tolerant code with relative ease.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将说明在计算机集群上运行代码的挑战、用例和示例。Python提供了易于使用且可靠的分布式处理包，这将使我们能够相对容易地实现可扩展和容错代码。
- en: 'The list of topics for this chapter is as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主题列表如下：
- en: Distributed computing and the MapReduce model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式计算和MapReduce模型
- en: Directed Acyclic Graphs with Dask
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Dask的定向无环图
- en: Writing parallel code with Dask's `array`, `Bag`, and `DataFrame` data structures
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Dask的`array`、`Bag`和`DataFrame`数据结构编写并行代码
- en: Distributing parallel algorithms with Dask Distributed
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Dask Distributed分发并行算法
- en: An introduction to PySpark
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark简介
- en: Spark's Resilient Distributed Datasets and DataFrame
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的弹性分布式数据集和DataFrame
- en: Scientific computing with `mpi4py`
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`mpi4py`进行科学计算
- en: Introduction to distributed computing
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式计算简介
- en: In today's world, computers, smartphones, and other devices have become an integral
    part of our lives. Every day, massive quantities of data is produced. Billions
    of people access services on the Internet, and companies are constantly collecting
    data to learn about their users to better target products and improve user experience.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今世界，计算机、智能手机和其他设备已成为我们生活的重要组成部分。每天，都会产生大量的数据。数十亿人通过互联网访问服务，公司不断收集数据以了解他们的用户，以便更好地定位产品和提升用户体验。
- en: Handling this ever increasing amount of data presents substantial challenges.
    Large companies and organizations often build clusters of machines designed to
    store, process, and analyze large and complex datasets. Similar datasets are also
    produced in data-intensive fields such as environmental sciences and health care.
    These large-scale datasets have been recently called **big data**. The analysis
    techniques applied to big data usually involve a combination of machine learning,
    information retrieval, and visualization.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这日益增长的大量数据带来了巨大的挑战。大型公司和组织通常构建机器集群，用于存储、处理和分析大型且复杂的数据集。在数据密集型领域，如环境科学和医疗保健，也产生了类似的数据库。这些大规模数据集最近被称为**大数据**。应用于大数据的分析技术通常涉及机器学习、信息检索和可视化的结合。
- en: Computing clusters have been used for decades in scientific computing, where
    the study of complex problems requires the use of parallel algorithms executed
    on high-performance distributed systems. For such applications, universities and
    other organizations provide and manage supercomputers for research and engineering
    purposes. Applications that run on supercomputers are generally focused on highly
    numerical workloads, such as protein and molecular simulations, quantum mechanical
    calculations, climate models, and much more.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 计算集群在科学计算中已经使用了数十年，在研究复杂问题时需要使用在高性能分布式系统上执行的并行算法。对于此类应用，大学和其他组织提供并管理超级计算机用于研究和工程目的。在超级计算机上运行的应用通常专注于高度数值化的工作负载，如蛋白质和分子模拟、量子力学计算、气候模型等。
- en: The challenges of programming for distributed systems are apparent if we think
    back on how the cost of communication increases as we distribute data and computational
    tasks across a local network. Network transfers are extremely slow compared to
    the processor speed, and when using distributed processing, it is even more important
    to keep network communications as limited as possible. This can be achieved using
    a few different strategies that favor local data processing and resort to data
    transfers only when strictly necessary.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为分布式系统编程的挑战显而易见，如果我们回顾一下，随着我们将数据和计算任务分散到本地网络中，通信成本是如何增加的。与处理器速度相比，网络传输极其缓慢，在使用分布式处理时，保持网络通信尽可能有限尤为重要。这可以通过使用一些不同的策略来实现，这些策略有利于本地数据处理，并且仅在绝对必要时才进行数据传输。
- en: Other challenges of distributed processing involve the general unreliability
    of computer networks. When you think that in a computing cluster there may be
    thousands of machines, it becomes clear that (probabilistically speaking) faulty
    nodes become very common. For this reason, distributed systems need to be able
    to handle node failures gracefully and without disrupting the ongoing work. Luckily,
    companies have invested a great deal of resources in developing fault-tolerant
    distributed engines that take care of these aspects automatically.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式处理的其他挑战包括计算机网络的一般不可靠性。当你想到在一个计算集群中可能有数千台机器时，很明显（从概率上讲），故障节点变得非常普遍。因此，分布式系统需要能够优雅地处理节点故障，而不会干扰正在进行的工作。幸运的是，公司已经投入了大量资源来开发容错分布式引擎，这些引擎可以自动处理这些方面。
- en: An introduction to MapReduce
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce简介
- en: '**MapReduce** is a programming model that allows you to express algorithms
    for efficient execution on a distributed system. The MapReduce model was first
    introduced by Google in 2004 ([https://research.google.com/archive/mapreduce.html](https://research.google.com/archive/mapreduce.html)),
    as a way to automatically partition datasets over different machines and for automatic
    local processing and the communication between *cluster nodes*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**MapReduce**是一种编程模型，允许你在分布式系统上高效地表达算法。MapReduce模型最早由Google在2004年提出（[https://research.google.com/archive/mapreduce.html](https://research.google.com/archive/mapreduce.html)），作为一种在多台机器上自动分区数据集、自动本地处理以及*集群节点*之间通信的方法。'
- en: The MapReduce framework was used in cooperation with a distributed filesystem,
    the **Google File System** (GFS or GoogleFS), which was designed to partition
    and replicate data across the computing cluster. Partitioning was useful for storing
    and processing datasets that wouldn't fit on a single node while replication ensured
    that the system was able to handle failures gracefully. MapReduce was used by
    Google, in conjunction with GFS, for indexing of their web pages. Later on, the
    MapReduce and GFS concepts were implemented by Doug Cutting (at the time, an employee
    at Yahoo!), resulting in the first versions of the **Hadoop Distributed File System**
    (**HDFS**) and Hadoop MapReduce.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce框架与分布式文件系统**Google文件系统**（GFS或GoogleFS）合作使用，该系统旨在将数据分区并复制到计算集群中。分区对于存储和处理无法适应单个节点的数据集很有用，而复制确保系统能够优雅地处理故障。Google使用MapReduce与GFS一起对他们的网页进行索引。后来，MapReduce和GFS概念由Doug
    Cutting（当时是雅虎的员工）实现，产生了**Hadoop分布式文件系统**（**HDFS**）和Hadoop MapReduce的第一个版本。
- en: 'The programming model exposed by MapReduce is actually quite simple. The idea
    is to express the computation as a combination of two, fairly generic, steps:
    *Map* and *Reduce*. Some readers will probably be familiar with Python''s `map`
    and `reduce` functions; however, in the context of MapReduce, the Map and Reduce
    steps are capable of representing a broader class of operations.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce暴露的编程模型实际上相当简单。其思想是将计算表达为两个相当通用的步骤的组合：*Map*和*Reduce*。一些读者可能熟悉Python的`map`和`reduce`函数；然而，在MapReduce的上下文中，Map和Reduce步骤能够表示更广泛的操作类别。
- en: Map takes a collection of data as input and produces a *transformation* on this
    data. What is generally emitted by Map is a series of key value pairs that can
    be passed to a Reduce step. The Reduce step will aggregate items with the same
    key and apply a function to the collection to form a usually smaller collection
    of values.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Map以一组数据作为输入，并对这些数据进行*转换*。Map通常输出一系列键值对，这些键值对可以被传递到Reduce步骤。Reduce步骤将具有相同键的项聚合起来，并对集合应用一个函数，形成一个通常更小的值集合。
- en: The estimation of *pi*, which was shown in the last chapter, can be trivially
    converted using a series of Map and Reduce steps. In that case, the input was
    a collection of pairs of random numbers. The transformation (Map step) was the
    hit test, and the Reduce step was counting the number of times the hit test was
    True.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中展示的*π*的估计可以通过一系列Map和Reduce步骤轻松转换。在这种情况下，输入是一系列随机数的对。转换（Map步骤）是击中测试，而Reduce步骤是计算击中测试为True的次数。
- en: The prototypical example of the MapReduce model is the implementation of a word
    count; the program takes a series of documents as input, and returns, for each
    word, the total number of occurrences in the document collection. The following
    figure illustrates the Map and Reduce steps of the word count program. On the
    left, we have the input documents. The Map operation will produce a (key, value)
    entry where the first element is the word and the second element is **1** (that's
    because every word contributes **1** to the final count).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 模型的典型示例是实现词频统计；程序接受一系列文档作为输入，并返回每个单词在文档集合中的总出现次数。以下图展示了词频统计程序的 Map
    和 Reduce 步骤。在左侧，我们有输入文档。Map 操作将生成一个（键，值）条目，其中第一个元素是单词，第二个元素是 **1**（这是因为每个单词都对最终计数贡献了
    **1**）。
- en: 'We then perform the reduce operation to aggregate all the elements of the same
    key and produce the global count for each of the words. In the figure, we can
    see how all values of the items with key **the** are summed to produce the final
    entry (**the, 4**):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们执行 reduce 操作，聚合相同键的所有元素，并为每个单词生成全局计数。在图中，我们可以看到所有键为 **the** 的项的值是如何相加以生成最终的条目（**the,
    4**）：
- en: '![](img/B06440_08CHPNO_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06440_08CHPNO_01.png)'
- en: If we implement our algorithm using the Map and Reduce operation, the framework
    implementation will ensure that data production and aggregation is done efficiently,
    by limiting the communication between nodes through clever algorithms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 Map 和 Reduce 操作实现我们的算法，框架实现将确保通过限制节点之间的通信通过巧妙的算法来高效地完成数据生产和聚合。
- en: However, how does MapReduce manage to keep communication to a minimum? Let's
    go through the journey of a MapReduce task. Imagine that you have a cluster with
    two nodes, and a partition of the data (this is usually found locally in each
    node) is loaded in each node from disk and is ready for processing. A mapper process
    is created in each node and processes the data to produce the intermediate results.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，MapReduce 是如何将通信保持在最低限度的？让我们回顾一下 MapReduce 任务的旅程。想象一下，你有一个包含两个节点的集群，数据分区（这通常在每个节点本地找到）从磁盘加载到每个节点，并准备好处理。在每个节点上创建了一个
    mapper 进程，并处理数据以生成中间结果。
- en: 'Next, it is necessary to send the data to the reducer for further processing.
    In order to do this, however, it is necessary that all the items that possess
    the same key are shipped to the same reducer. This operation is called **shuffling**
    and is the principal communication task in the MapReduce model:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，有必要将数据发送到 reducer 进行进一步处理。然而，为了做到这一点，所有具有相同键的项都必须发送到同一个 reducer。这个操作称为 **洗牌**，是
    MapReduce 模型中的主要通信任务：
- en: '![](img/B06440_08CHPNO_02.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06440_08CHPNO_02.png)'
- en: Note that, before the data exchange happens, it is necessary to assign a subset
    of keys to each reducer; this step is called **partitioning**.  Once a reducer
    receives its own partition of keys, it is free to process data and write the resulting
    output on disk.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在数据交换发生之前，有必要将键的子集分配给每个 reducer；这一步称为 **分区**。一旦 reducer 收到其自己的键分区，它就可以自由地处理数据并在磁盘上写入结果输出。
- en: The MapReduce framework (through the Apache Hadoop project) has been extensively
    used in its original form by many companies and organizations. More recently,
    new frameworks that extend the ideas introduced by MapReduce have been developed
    to create systems able to express more complex workflows, to use memory more efficiently
    and to support a lean and efficient execution of distributed tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 框架（通过 Apache Hadoop 项目）在其原始形式下已被许多公司和组织广泛使用。最近，一些新的框架被开发出来，以扩展 MapReduce
    引入的思想，以创建能够表达更复杂工作流程、更有效地使用内存并支持瘦型和高效分布式任务执行的系统。
- en: In the following sections, we will describe two of the most used libraries in
    the Python distributed landscape: Dask and PySpark.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将描述 Python 分布式领域中两个最常用的库：Dask 和 PySpark。
- en: Dask
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask
- en: '**Dask** is a project of Continuum Analytics (the same company that''s responsible
    for Numba and the `conda` package manager) and a pure Python library for parallel
    and distributed computation. It excels at performing data analysis tasks and is
    very well integrated in the Python ecosystem.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dask** 是 Continuum Analytics（负责 Numba 和 `conda` 软件包管理器的同一家公司）的一个项目，是一个用于并行和分布式计算的纯
    Python 库。它在执行数据分析任务方面表现出色，并且与 Python 生态系统紧密结合。'
- en: Dask was initially conceived as a package for bigger-than-memory calculations
    on a single machine. Recently, with the Dask Distributed project, its code has
    been adapted to execute tasks on a cluster with excellent performance and fault-tolerance
    capabilities. It supports MapReduce-style tasks as well as complex numerical algorithms.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Dask最初被构想为一个用于单机内存外计算的包。最近，随着Dask Distributed项目的推出，其代码已被调整以在具有出色性能和容错能力的集群上执行任务。它支持MapReduce风格的任务以及复杂的数值算法。
- en: Directed Acyclic Graphs
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有向无环图
- en: The idea behind Dask is quite similar to what we already saw in the last chapter
    with Theano and Tensorflow. We can use a familiar Pythonic API to build an execution
    plan, and the framework will automatically split the workflow into tasks that
    will be shipped and executed on multiple processes or computers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Dask背后的理念与我们已经在上一章中看到的Theano和Tensorflow非常相似。我们可以使用熟悉的Pythonic API来构建执行计划，而框架将自动将工作流程拆分成将在多个进程或计算机上传输和执行的任务。
- en: 'Dask expresses its variables and operations as a **Directed Acyclic Graph**
    (**DAG**) that can be represented through a simple Python dictionary. To briefly
    illustrate how this works, we will implement the sum of two numbers with Dask.
    We will define our computational graph by storing the values of our input variables
    in the dictionary. The `a` and `b` input variables will be given a value of `2`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Dask将它的变量和操作表示为一个**有向无环图**（**DAG**），可以通过一个简单的Python字典来表示。为了简要说明这是如何工作的，我们将使用Dask实现两个数的和。我们将通过在字典中存储输入变量的值来定义我们的计算图。输入变量`a`和`b`将被赋予值`2`：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Each variable represents a node in the DAG. The next step necessary to build
    our DAG is the execution of operations on the nodes we just defined. In Dask,
    a task can be defined by placing a tuple containing a Python function and its
    positional arguments in the `dsk` dictionary. To implement a `sum`, we can add
    a new node, named `result`, (the actual name is completely arbitrary) with a tuple
    containing the function we intend to execute, followed by its arguments. This
    is illustrated in the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个变量代表DAG中的一个节点。构建我们的DAG的下一步是执行我们刚刚定义的节点上的操作。在Dask中，一个任务可以通过在`dsk`字典中放置一个包含Python函数及其位置参数的元组来定义。为了实现求和，我们可以添加一个新的节点，命名为`result`（实际名称完全任意），包含我们打算执行的函数，后跟其参数。以下代码展示了这一点：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For better style and clarity, we can calculate the sum by replacing the `lambda`
    statement with the standard `operator.add` library function:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好的风格和清晰度，我们可以通过替换`lambda`语句为标准的`operator.add`库函数来计算和：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It's important to note that the arguments we intend to pass to the function
    are the `"a"` and `"b"` strings, which refer to the `a` and `b` nodes in the graph.
    Note that we didn't use any Dask-specific functions to define the DAG; this is
    the first indication of how the framework is flexible and lean since all manipulations
    are performed on simple and familiar Python dictionaries.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们打算传递给函数的参数是`"a"`和`"b"`字符串，它们指的是图中的`a`和`b`节点。请注意，我们没有使用任何Dask特定的函数来定义DAG；这是框架灵活和精简的第一个迹象，因为所有操作都是在简单且熟悉的Python字典上进行的。
- en: 'The execution of tasks is performed by a scheduler, which is a function that
    takes a DAG and the task or tasks we''d like to perform and returns the computed
    value. The default Dask scheduler is the `dask.get` function, which can be used
    as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 任务执行由调度器完成，调度器是一个函数，它接受一个DAG以及我们想要执行的任务或任务列表，并返回计算值。默认的Dask调度器是`dask.get`函数，可以使用以下方式使用：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: All the complexity is hidden behind the scheduler, which will take care of distributing
    the tasks across threads, processes, or even different machines. The `dask.get`
    scheduler is a synchronous and serial implementation that is useful for testing
    and debugging purposes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所有复杂性都隐藏在调度器后面，调度器将负责将任务分配到线程、进程甚至不同的机器上。`dask.get`调度器是一个同步和串行实现，适用于测试和调试目的。
- en: Defining graphs using a simple dictionary is useful to understand how Dask does
    its magic and for debugging purposes. Raw dictionaries can also be used to implement
    more complex algorithms not covered by the Dask API. Now, we will learn how Dask
    is capable of generating tasks automatically through a familiar NumPy- and Pandas-like
    interface.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单的字典定义图对于理解Dask如何施展魔法以及用于调试目的非常有用。原始字典也可以用来实现Dask API未涵盖的更复杂算法。现在，我们将学习Dask如何通过熟悉的NumPy和Pandas-like接口自动生成任务。
- en: Dask arrays
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask数组
- en: One of the main use-cases of Dask is the automatic generation of parallel array
    operations, which greatly simplifies the handling of arrays that don't fit into
    memory. The strategy employed by Dask is to split the array into a number of subunits
    that, in Dask array terminology, are called **chunks**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的主要用例之一是自动生成并行数组操作，这极大地简化了处理无法装入内存的数组。Dask 采取的策略是将数组分割成多个子单元，在 Dask 数组术语中，这些子单元被称为
    **chunks**。
- en: 'Dask implement a NumPy-like interface for arrays in the `dask.array` module
    (which we will abbreviate as `da`). An array can be created from a NumPy-like
    array using the `da.from_array` function, which requires the specification of
    a chunk size. The `da.from_array` function will return a `da.array` object that
    will handle the splitting of the original array into subunits of the specified
    chunk size. In the following example, we create an array of `30` elements, and
    we split it into chunks with `10` elements each:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 在 `dask.array` 模块（我们将简称为 `da`）中实现了一个类似于 NumPy 的数组接口。可以使用 `da.from_array`
    函数从一个类似于 NumPy 的数组创建数组，该函数需要指定块大小。`da.from_array` 函数将返回一个 `da.array` 对象，该对象将处理将原始数组分割成指定块大小的子单元。在以下示例中，我们创建了一个包含
    `30` 个元素的数组，并将其分割成每个块包含 `10` 个元素的块：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `a_da` variable maintains a Dask graph that can be accessed using the `dask`
    attribute. To understand what Dask does under the hood, we can inspect its content.
    In the following example, we can see that the Dask graph contains four nodes.
    One of them is the source array, denoted by the `''array-original-4c76''` key,
    the other three keys in the `a_da.dask` dictionary are tasks that are used to
    access a chunk of the original array using the `dask.array.core.getarray` function
    and, as you can see, each task extracts a slice of 10 elements:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`a_da` 变量维护一个 Dask 图，可以通过 `dask` 属性访问。为了了解 Dask 在底层做了什么，我们可以检查其内容。在以下示例中，我们可以看到
    Dask 图包含四个节点。其中一个是源数组，用 `''array-original-4c76''` 键表示，`a_da.dask` 字典中的其他三个键是用于使用
    `dask.array.core.getarray` 函数访问原始数组子块的任务，如您所见，每个任务提取了 10 个元素的一个切片：'
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If we perform an operation on the `a_da` array, Dask will generate more subtasks
    that operate on the smaller chunks, opening the possibility of achieving parallelism.
    The interface exposed by `da.array` is compatible with common NumPy semantics
    and broadcasting rules. The complete code, shown as follows, demonstrates the
    good compatibility of Dask with NumPy broadcasting rules, element-wise operations,
    and other methods:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在 `a_da` 数组上执行操作，Dask 将生成更多子任务来操作更小的子单元，从而打开实现并行化的可能性。`da.array` 暴露的接口与常见的
    NumPy 语义和广播规则兼容。以下代码展示了 Dask 与 NumPy 广播规则、逐元素操作和其他方法的良好兼容性：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The value of pi can be calculated using the `compute` method, which can also
    be called with the `get` optional argument to specify a different scheduler (by
    default, `da.array` uses a multithreaded scheduler):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `compute` 方法计算 π 的值，也可以通过 `get` 可选参数来指定不同的调度器（默认情况下，`da.array` 使用多线程调度器）：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Even deceptively simple algorithms, such as the estimation of pi, may require
    a lot of tasks to be executed. Dask provides utilities to visualize the computational graph.
    The following figure shows part of the Dask graph for the estimation of pi, which
    can be obtained by executing the method `pi.visualize()`. In the graph, circles
    refer to transformations that get applied on the nodes, which are represented
    as rectangles. This example helps us to get a feel of the complexity of the Dask
    graph and to appreciate the scheduler''s job of creating an efficient execution
    plan that includes proper ordering of tasks and the selection of tasks that will
    be executed in parallel:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是表面上简单的算法，如 π 的估计，也可能需要执行大量任务。Dask 提供了可视化计算图的工具。以下图显示了用于估计 π 的 Dask 图的一部分，可以通过执行
    `pi.visualize()` 方法获得。在图中，圆形代表应用于节点的转换，节点以矩形表示。这个例子帮助我们了解 Dask 图的复杂性，并欣赏调度器创建高效执行计划的工作，包括正确排序任务和选择并行执行的任务：
- en: '![](img/dask_graph-e1489790395473.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![Dask 图](img/dask_graph-e1489790395473.png)'
- en: Dask Bag and DataFrame
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask Bag 和 DataFrame
- en: Dask provides other data structures for automatic generation of computation
    graphs. In this subsection, we'll take a look at `dask.bag.Bag`, a generic collection
    of elements that can be used to code MapReduce-style algorithms, and `dask.dataframe.DataFrame`,
    a distributed version of `pandas.DataFrame`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 提供了其他数据结构用于自动生成计算图。在本小节中，我们将探讨 `dask.bag.Bag`，这是一个通用的元素集合，可用于编写 MapReduce
    风格的算法，以及 `dask.dataframe.DataFrame`，它是 `pandas.DataFrame` 的分布式版本。
- en: 'A `Bag` can be easily created from a Python collection. For example, you can
    create a `Bag` from a list using the `from_sequence` factory function. The level
    of parallelism can be specified using the `npartitions` argument (this will distribute
    the `Bag` content into a number of partitions). In the following example, we create
    a `Bag` containing numbers from `0` to `99`, partitioned into four chunks:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `Bag` 可以很容易地从 Python 集合中创建。例如，您可以使用 `from_sequence` 工厂函数从一个列表中创建一个 `Bag`。可以使用
    `npartitions` 参数指定并行级别（这将把 `Bag` 内容分布到多个分区中）。在以下示例中，我们创建了一个包含从 `0` 到 `99` 的数字的
    `Bag`，分为四个块：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the next example, we will demonstrate how to perform a word count of a set
    of strings using an algorithm that's similar to MapReduce. Given our collection
    of sequences, we apply `str.split`, followed by `concat` to obtain a linear list
    of words in the documents. Then, for each word, we produce a dictionary that contains
    a word and the value `1` (refer to the *An introduction to MapReduce* section
    for an illustration). We then write a *Reduce* step using the `foldby` operator
    to calculate the word count.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们将演示如何使用类似于 MapReduce 的算法对一组字符串进行词频统计。给定我们的序列集合，我们应用 `str.split`，然后使用
    `concat` 获取文档中的线性单词列表。然后，对于每个单词，我们生成一个包含单词和值 `1` 的字典（有关说明，请参阅 *MapReduce 简介* 部分）。然后，我们使用
    `foldby` 操作符编写一个 *Reduce* 步骤来计算词频。
- en: The `foldby` transformation is useful to implement a Reduce step that combines
    the word counts without having to shuffle all the elements over the network. Imagine
    that our word dataset is divided into two partitions. A good strategy to calculate
    the total count is to first sum the word occurrences in each partition and then
    combine those partial sums to get the final result. The following figure illustrates
    the concept. On the left, we have our input partitions. The partial sum is calculated
    for each individual partition (this is done using a binary operation, **binop**),
    and then the final sums are computed by combining the partial sums using a **combine**
    function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`foldby` 转换对于实现不需要在网络中重新排序所有元素即可合并词频的 Reduce 步骤非常有用。想象一下，我们的单词数据集被分为两个分区。计算总计数的一个好策略是首先计算每个分区的单词出现次数之和，然后将这些部分和组合起来得到最终结果。以下图示说明了这个概念。在左侧，我们有我们的输入分区。每个单独分区计算部分和（这是使用二进制操作
    **binop** 完成的），然后通过使用 **combine** 函数组合部分和来计算最终总和。'
- en: '![](img/B06440_08CHPNO_04.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06440_08CHPNO_04.png)'
- en: 'The following code illustrates how to use `Bag` and the `foldby` operator to
    compute the word count. For the `foldby` operator, we need to define two functions
    that take five arguments:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码说明了如何使用 `Bag` 和 `foldby` 操作符来计算词频。对于 `foldby` 操作符，我们需要定义两个函数，它们接受五个参数：
- en: '`key`: This is a function that returns the key for the reduce operation.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`key`: 这是一个返回 reduce 操作键的函数。'
- en: '`binop`: This is a function that takes two arguments: `total` and `x`. Given
    a `total` value (the values accumulated so far), `binop` incorporates the next
    item into the total.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binop`: 这是一个接受两个参数的函数：`total` 和 `x`。给定一个 `total` 值（到目前为止累积的值），`binop` 将下一个项目合并到总和中。'
- en: '`initial`: This is the initial value for the `binop` accumulation.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initial`: 这是 `binop` 累积的初始值。'
- en: '`combine`: This is a function that combines the totals for each partition (in
    this case it is a simple sum).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`combine`: 这是一个将每个分区的总和合并的函数（在这种情况下是一个简单的求和）。'
- en: '`initial_combine`: This is the initial value for the `combine` accumulation.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initial_combine`: 这是 `combine` 累积的初始值。'
- en: 'Now, let''s look at the code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看代码：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we just saw, expressing complex operations in an efficient way using `Bag`
    can become cumbersome. For this reason, Dask provides another data structure designed
    for analytical workloads--`dask.dataframe.DataFrame`. A `DataFrame` can be initialized
    in Dask using a variety of methods, such as from `CSV` files on distributed filesystems,
    or directly from a `Bag`. Just like `da.array` provides an API that closely mirrors
    NumPy features, Dask `DataFrame` can be used as a distributed version of `pandas.DataFrame`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚才看到的，使用`Bag`以有效的方式表达复杂操作可能会变得繁琐。因此，Dask提供另一种数据结构，专为分析工作负载设计--`dask.dataframe.DataFrame`。`DataFrame`可以在Dask中使用多种方法初始化，例如从分布式文件系统上的`CSV`文件，或直接从`Bag`。就像`da.array`提供了一个与NumPy功能紧密相似的API一样，Dask
    `DataFrame`可以用作`pandas.DataFrame`的分布式版本。
- en: 'As a demonstration, we will re-implement the word count using a `DataFrame`.
    We first load the data to obtain a `Bag` of words, and then we convert the `Bag`
    to a `DataFrame` using the `to_dataframe` method. By passing a column name to
    the `to_dataframe` method, we can initialize a `DataFrame`, which contains a single
    column, named `words`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 作为演示，我们将使用`DataFrame`重新实现词频。我们首先加载数据以获得一个单词的`Bag`，然后使用`to_dataframe`方法将`Bag`转换为`DataFrame`。通过将列名传递给`to_dataframe`方法，我们可以初始化一个`DataFrame`，它包含一个名为`words`的单列：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Dask `DataFrame` closely replicates the `pandas.DataFrame` API. To compute
    the word count, we only need to call the `value_counts` method on the words column,
    and Dask will automatically devise a parallel computation strategy. To trigger
    the calculation, it is sufficient to call the `compute` method:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Dask `DataFrame`紧密复制了`pandas.DataFrame` API。要计算词频，我们只需在单词列上调用`value_counts`方法，Dask将自动设计一个并行计算策略。要触发计算，只需调用`compute`方法：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'An interesting question one may ask is "*what kind of algorithm does DataFrame
    use under the hood?*". The answer can be found by looking at the upper part of
    the generated Dask graph, which is displayed in the following figure. The first
    two rectangles at the bottom represent two partitions of the dataset, which are
    stored as two `pd.Series` instances. To calculate the overall count, Dask will
    first execute `value_counts` on each of the `pd.Series` and then combine the counts along
    with the `value_counts_aggregate` step:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能的问题是一个人可能会问：“*DataFrame底层使用的是哪种算法？*”。答案可以通过查看生成的Dask图的顶部来找到，该图如下所示。底部的前两个矩形代表数据集的两个分区，它们存储为两个`pd.Series`实例。为了计算总数，Dask将首先在每个`pd.Series`上执行`value_counts`，然后结合`value_counts_aggregate`步骤：
- en: '![](img/B06440_08CHPNO_05.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06440_08CHPNO_05.png)'
- en: As you can see, both Dask `array` and `DataFrame` take advantage of the fast
    vectorized implementations of NumPy and Pandas to achieve excellent performance
    and stability.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Dask `array`和`DataFrame`都利用了NumPy和Pandas的快速向量化实现，以实现卓越的性能和稳定性。
- en: Dask distributed
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask分布式
- en: The first iterations of the Dask project were designed to run on a single computer
    using a thread-based or a process-based scheduler. Recently, the implementation
    of a new distributed backend can be used to set up and run Dask graphs on a network
    of computers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Dask项目的最初迭代是为了在单台计算机上运行而设计的，使用基于线程或进程的调度器。最近，新分布式后端的实现可以用来在计算机网络上设置和运行Dask图。
- en: Dask distributed is not installed automatically with Dask. The library is available
    through the `conda` package manager (use the `$ conda install distributed` command )
    as well as `pip` (with the `$ pip install distributed` command).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Dask分布式不是与Dask自动安装的。该库可以通过`conda`包管理器（使用`$ conda install distributed`命令）以及`pip`（使用`$
    pip install distributed`命令）获得。
- en: 'Getting started with Dask distributed is really easy. The most basic setup
    is obtained by instantiating a `Client` object:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 开始使用Dask分布式实际上非常简单。最基本的设置是通过实例化一个`Client`对象来获得的：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: By default, Dask will start a few key processes (on the local machine) necessary
    for scheduling and executing distributed tasks through the `Client` instance.
    The main components of a Dask cluster are a single *scheduler* and a collection
    of *workers*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Dask将通过`Client`实例启动一些关键进程（在本地机器上），这些进程对于调度和执行分布式任务是必要的。Dask集群的主要组件是一个单一的*scheduler*和一组*workers*。
- en: The **scheduler** is the process responsible for distributing the work across
    the workers and to monitor and manage the results. Generally, when a task is submitted
    to the user, the scheduler will find a free worker and submit a task for execution.
    Once the worker is done, the scheduler is informed that the result is available.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**调度器**是负责在工作器之间分配工作并监控和管理结果的进程。通常，当任务被提交给用户时，调度器会找到一个空闲的工作器并提交一个任务以供执行。一旦工作器完成，调度器就会被告知结果已可用。'
- en: A worker is a process that accepts incoming tasks and produces results. Workers
    can reside on different machines over the network. Workers execute tasks using
    `ThreadPoolExecutor`. This can be used to achieve parallelism when using functions
    that do not acquire the GIL (such as Numpy, Pandas, and Cython functions in `nogil`
    blocks). When executing pure Python code, it is advantageous to start many single-threaded
    worker processes as this will enable parallelism for code that acquires the GIL.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 工作器是一个接受传入任务并产生结果的进程。工作器可以驻留在网络上的不同机器上。工作器使用`ThreadPoolExecutor`执行任务。这可以用来在不需要获取GIL（例如，在`nogil`块中的Numpy、Pandas和Cython函数）的函数中使用并行性。当执行纯Python代码时，启动许多单线程工作器进程是有利的，因为这将为获取GIL的代码启用并行性。
- en: 'The `Client` class can be used to submit tasks manually to the scheduler using
    familiar asynchronous methods. For example, to submit a function for execution
    on the cluster, one can use the `Client.map` and `Client.submit` methods. In the
    following code, we demonstrate the use of `Client.map` and `Client.submit` to
    calculate the square of a few numbers. The `Client` will submit a series of tasks
    to the scheduler and we will receive a `Future` instance for each task:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`Client`类可以用来使用熟悉的异步方法手动将任务提交给调度器。例如，为了在集群上执行一个函数，可以使用`Client.map`和`Client.submit`方法。在下面的代码中，我们展示了如何使用`Client.map`和`Client.submit`来计算几个数字的平方。`Client`将向调度器提交一系列任务，我们将为每个任务接收一个`Future`实例：'
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: So far, this is quite similar to what we saw in the earlier chapters with `TheadPoolExecutor` and
    `ProcessPoolExecutor`. Note however, that Dask Distributed not only submits the
    tasks, but also caches the computation results on the worker memory. You can see
    caching in action by looking at the preceding code example. When we first invoke
    `client.submit`, the `square(2)` task is created and its status is set to *pending*.
    When we subsequently invoke `client.map`,  the `square(2)` task is resubmitted
    to the scheduler, but this time, rather than recalculating its value, the scheduler
    directly retrieves the result for the worker. As a result, the third `Future`
    returned by map already has a finished status.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这与我们在前几章中看到的`TheadPoolExecutor`和`ProcessPoolExecutor`非常相似。然而，请注意，Dask
    Distributed不仅提交任务，还将在工作器内存中缓存计算结果。您可以通过查看前面的代码示例来看到缓存的作用。当我们第一次调用`client.submit`时，`square(2)`任务被创建，其状态设置为*待处理*。当我们随后调用`client.map`时，`square(2)`任务被重新提交给调度器，但这次，调度器不是重新计算其值，而是直接从工作器检索结果。因此，map返回的第三个`Future`已经处于完成状态。
- en: 'Results from a collection of `Future` instances can be retrieved using the
    `Client.gather` method:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`Client.gather`方法检索来自`Future`实例集合的结果：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Client` can also be used to run arbitrary Dask graphs. For example, we can
    trivially run our approximation of pi by passing the `client.get` function as
    an optional argument to `pi.compute`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`Client`也可以用来运行任意的Dask图。例如，我们可以通过将`client.get`函数作为可选参数传递给`pi.compute`来简单地运行我们的π近似值：'
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This feature makes Dask extremely scalable as it is possible to develop and
    run algorithms on the local machine using one of the simpler schedulers and, in
    case the performance is not satisfactory, to reuse the same algorithms on a cluster
    of hundreds of machines.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特性使得Dask具有极高的可扩展性，因为它可以在本地机器上使用其中一个较简单的调度器开发和运行算法，如果性能不满意，还可以在由数百台机器组成的集群上重用相同的算法。
- en: Manual cluster setup
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动集群设置
- en: 'To instantiate scheduler and workers manually, one can use the `dask-scheduler`
    and `dask-worker` command-line utilities. First, we can initialize a scheduler
    using the `dask-scheduler` command:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要手动实例化调度器和工作者，可以使用`dask-scheduler`和`dask-worker`命令行工具。首先，我们可以使用`dask-scheduler`命令初始化调度器：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will provide an address for the scheduler and a Web UI address that can
    be accessed to monitor the state of the cluster. Now, we can assign some workers
    to the scheduler; this can be accomplished using the `dask-worker` command and
    by passing the address of the scheduler to the worker. This will automatically
    start a worker with four threads:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The Dask scheduler is fairly resilient in the sense that if we add and remove
    a worker, it is able to track which results are unavailable and recompute them
    on-demand. Finally, in order to use the initialized scheduler from a Python session,
    it is sufficient to initialize a `Client` instance and provide the address for
    the scheduler:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Dask also provides a convenient diagnostic Web UI that can be used to monitor
    the status and time spent for each of the tasks performed on the cluster. In the
    next figure, the **Task Stream** shows the time taken for executing the pi estimation.
    In the plot, each horizontal gray line corresponds to a thread used by the workers
    (in our case, we have one worker with four threads, also called **Worker Core**),
    and each rectangular box corresponds to a task, colored so that the same task
    types have the same color (for example, addition, power, or exponent). From this
    plot, you can observe how all the boxes are very small and far from each other.
    This means that the tasks are quite small compared to the overhead of communication.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: In this case, an increase in chunk size, which implies to an increase in the
    time required to run each task compared to the time of communication, will be
    beneficial.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_08CHPNO_06.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Using PySpark
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, Apache Spark is one of the most popular projects for distributed computing.
    Developed in Scala, Spark was released in 2014, and integrates with HDFS and provides
    several advantages and improvements over the Hadoop MapReduce framework.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to Hadoop MapReduce, Spark is designed to process data interactively
    and supports APIs for the Java, Scala, and Python programming languages. Given
    its different architecture, especially by the fact that Spark keep results in
    memory, Spark is generally much faster than Hadoop MapReduce.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Spark and PySpark
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setting up PySpark from scratch requires the installation of the Java and Scala
    runtimes, the compilation of the project from source, and the configuration of Python
    and Jupyter notebook so that they can be used alongside the Spark installation.
    An easier and less error-prone way to set up PySpark is to use an already configured
    Spark cluster made available through a **Docker** container.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Docker can be downloaded at [https://www.docker.com/](https://www.docker.com/) .
    If you're new to containers, you can read the next chapter for an introduction.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up a Spark cluster, it is sufficient to go in this chapter''s code files
    (where a file named `Dockerfile` is located) and issue the following command:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This command will automatically download, install, and configure Spark, Python,
    and Jupyter notebook in an isolated environment. To start Spark and a Jupyter
    notebook session, you can execute the following command:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The command will print a unique ID (called *container id*) that you can use
    to reference the application container and will start Spark and Jupyter notebook
    in the background. The `-p` option ensures that we can access the SparkUI and
    Jupyter network ports from the local machine. After issuing the command, you can
    open a browser to `http://127.0.0.1:8888` to access the Jupyter notebook session.
    You can test the correct initialization of Spark by creating a new notebook and
    executing the following content inside a cell:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This will initialize a `SparkContext` and take the first element in a collection
    (those new terms will be explained in detail later). Once the `SparkContext` is
    initialized, we can also head over to [http://127.0.0.1:4040](http://127.0.0.1:4040)
    to open the Spark Web UI.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Now that the setup is complete, we will understand how Spark works and how to
    implement simple parallel algorithms using its powerful API.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Spark architecture
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Spark cluster is a set of processes distributed over different machines. The
    **Driver Program** is a process, such as a Scala or Python interpreter, used by
    the user to submit the tasks to be executed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The user can build task graphs, similar to Dask, using a special API and submit
    those tasks to the **Cluster Manager** that is responsible for assigning these
    tasks to **Executors**, processes responsible for executing the tasks. In a multi-user
    system, the Cluster Manager is also responsible for allocating resources on a
    per-user basis.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: The user interacts with the Cluster Manager through the Driver Program. The
    class responsible for communication between the user and the Spark cluster is called
    `SparkContext`. This class is able to connect and configure the Executors on the
    cluster based on the resources available to the user.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: For its most common use-cases, Spark manages its data through a data structure
    called **Resilient Distributed Datasets** (**RDD**), which represents a collection
    of items. RDDs are capable of handling massive datasets by separating their elements
    into partitions and operating on the partitions in parallel (note that this mechanism
    is mainly hidden from the user). RDDs can also be stored in memory (optionally,
    and when appropriate) for fast access and to cache expensive intermediate results.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Using RDDs, it is possible to define tasks and transformations (similarly to
    how we were automatically generating computation graphs in Dask) and, when requested,
    the Cluster Manager will automatically dispatch and execute tasks on the available
    Executors.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: The Executors will receive the tasks from the Cluster Manager, execute them,
    and keep the results around if needed. Note that an Executor can have multiple
    cores and each node in the cluster may have multiple Executors. Generally speaking,
    Spark is fault tolerant on Executor's failures.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we show how the aforementioned components interact
    in a Spark cluster. The **Driver Program** interacts with the **Cluster Manager**
    that manages the **Executor** instances on different nodes (each Executor instance
    can also have multiple threads). Note that, even if the **Driver Program** doesn''t
    directly control the Executors, the results, which are stored in the **Executor**
    instances, are transferred directly between the Executors and the Driver Program. For
    this reason, it''s important that the **Driver Program** is network-reachable
    from the **Executor** processes:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_08CHPNO_07.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'A natural question to ask is: How is Spark, a software written in Scala, able
    to execute Python code? The integration is done through the `Py4J` library, which
    maintains a Python process under-the-hood and communicates with it through sockets
    (a form of interprocess communication). In order to run the tasks, Executors maintain
    a series of Python processes so that they are able to process Python code in parallel.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: RDDs and variables defined in a Python process in the Driver Program are serialized,
    and the communication between the Cluster Manager and the Executors (including
    shuffling) is dealt with by Spark's Scala code. The extra serialization steps
    necessary for the Python and Scala interchange, all contribute to the overhead
    of communication; therefore, when using PySpark, extra care must be taken to ensure
    that the data structures used are serialized efficiently and that the data partitions
    are big enough so that the cost of communication is negligible compared to the
    cost of execution.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the additional Python processes needed for
    PySpark execution. These additional Python processes come with associated memory
    costs and an extra layer of indirection that complicate error reporting:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_08CHPNO_08.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: Despite these drawbacks, PySpark is still a widely used tool because it bridges
    the vivid Python ecosystem with the industrial strength of the Hadoop infrastructure.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Resilient Distributed Datasets
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The easiest way to create an RDD in Python is with the `SparkContext.parallelize`
    method. This method was also used earlier where we parallelized a collection of
    integers between `0` and `1000`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `rdd` collection will be divided into a number of partitions which, in
    this case, correspond to a default value of four (the default value can be changed
    using configuration options). To explicitly specify the number of partitions,
    one can pass an extra argument to `parallelize`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'RDDs support a lot of functional programming operators, similar to what we
    used back in [Chapter 6](2b46e5c0-5308-4073-b1c6-4232a881b39f.xhtml), *Implementing
    Concurrency*, with reactive programming and data streams (even though, in that
    case, the operators were designed to work on events over time rather than normal
    collections). For example, we may illustrate the basic `map` function which, by
    now, should be quite familiar. In the following code, we use `map` to calculate
    the square of a series of numbers:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `map` function will return a new RDD but won''t compute anything just yet.
    In order to trigger the execution, you can use the `collect` method, which will
    retrieve all the elements in the collection, or `take`, which will return only
    the first ten elements:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'For a comparison between PySpark, Dask, and the other parallel programming
    libraries we explored in the earlier chapters, we will reimplement the approximation
    of pi. In the PySpark implementation, we will first create two RDDs of random
    numbers using `parallelize`, then we combine the datasets using the `zip` function
    (this is equivalent to Python''s `zip`), and we finally test whether the random
    points are inside the circle:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It's important to note that both the `zip` and `map` operations produce new
    RDDs and do not actually execute the instruction on the underlying data. In the preceding
    example, code execution is triggered when we call the `hit_test.sum` function,
    which returns an integer. This behavior is different from the Dask API where the
    whole computation (including the final result, `pi`) did not trigger the execution.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: We can now move on to a more interesting application to demonstrate more RDD
    methods. We will learn how to count the number of visits each user of a website
    performs in a day. In a real-world scenario, the data would have been collected
    in a database and/or stored in a distributed filesystem, such as HDFS. However,
    in our example, we will generate some data that we will then analyze.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we generate a list of dictionaries, each containing
    a `user` (selected among twenty users) and a `timestamp`. The steps to produce
    the dataset are as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Create a pool of 20 users (the `users` variable).
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a function that returns a random time between two dates.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For 10,000 times, we choose a random user from our `users` pool and a random
    timestamp between the dates January 1, 2017 and January 7, 2017.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'With the dataset at hand, we can start asking questions and use PySpark to
    find the answers. One common question is "*How many times has a given user visited
    the website?.*" A naive way to compute this result can be achieved by grouping
    the entries RDD by user (using the `groupBy` operator) and counting how many items
    are present for each user. In PySpark, `groupBy` takes a function as argument
    to extract the grouping key for each element and returns a new RDD that contain
    tuples of the `(key, group)` form. In the following example, we use the user ID
    as the key for our `groupBy`, and we inspect the first element using `first`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The return value of `groupBy` contains a `ResultIterable` (which is basically
    a list) for each user ID. To count the number of visits per user, it''s sufficient
    to calculate the length of each `ResultIterable`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Even though this algorithm may work well in small datasets, `groupBy` requires
    us to collect and store the whole set of entries for each user in memory, and
    this can exceed the memory capacity of an individual node. Since we don't need
    the list but only the count, there's a better way to calculate this number without
    having to hold the list of visits for each user in memory.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'When dealing with an RDD of `(key, value)` pairs, it is possible to use `mapValues`
    to apply a function only to the values. In the preceding code, we can replace
    the `map(lambda kv: (kv[0], len(kv[1])))` call with `mapValues(len)` for better
    readability.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more efficient calculation, we can leverage the `reduceByKey` function,
    which will perform a step similar to the Reduce step that we saw in the *An introduction
    to MapReduce* section. The `reduceByKey` function can be called from an RDD of
    tuples that contain a key as their first element and a value as their second element,
    and accepts a function as its first argument that will calculate the reduction.
     A simple example of the `reduceByKey` function is illustrated in the following
    snippet. We have a few string keys associated with integer numbers, and we want
    to get the sum of the values for each key; the reduction, expressed as a lambda,
    corresponds to the sum of the elements:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `reduceByKey` function is much more efficient than `groupBy` because the
    reduction is parallelizable and doesn''t require the in-memory storage of the
    groups; also, it limits the data shuffled between Executors (it performs similar
    operations to Dask''s `foldby`, which was explained earlier). At this point, we
    can rewrite our visit count calculation using `reduceByKey`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With Spark''s RDD API, it is also easy to answer questions such as "*How many
    visits did the website receive each day?.*" This can be computed using `reduceByKey`
    with the appropriate key (which is the date extracted from the timestamp). In
    the following example, we demonstrate the calculation. Also, note the usage of
    the `sortByKey` operator to return the counts sorted by date:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Spark DataFrame
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For numerical and analytical tasks, Spark provides a convenient interface available
    through the `pyspark.sql` module (also called SparkSQL). The module includes a
    `spark.sql.DataFrame` class that can be used for efficient SQL-style queries similar
    to those of Pandas. Access to the SQL interface is provided through the `SparkSession`
    class:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`SparkSession` can then be used to create a `DataFrame` through the function
    `createDataFrame`. The function `createDataFrame` accepts either a RDD, a list,
    or a `pandas.DataFrame`.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will create a `spark.sql.DataFrame` by converting
    an RDD, `rows`, which contains a collection of `Row` instances. The `Row` instances
    represent an association between a set of column names and a set of values, just
    like a row in a `pd.DataFrame`. In this example, we have two columns--`x` and `y`--to
    which we will associate random numbers:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'After obtaining our collection of `Row` instances, we can combine them in a
    `DataFrame`, as follows. We can also inspect the `DataFrame` content using the `show`
    method:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`spark.sql.DataFrame` supports performing transformations on the distributed
    dataset using a convenient SQL syntax. For example, you can use the `selectExpr`
    method to calculate a value using a SQL expression. In the following code, we
    compute the hit test using the `x` and `y` columns and the `pow` SQL function:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To demonstrate the expressivity of SQL, we can also calculate the estimation
    of pi using a single expression. The expression involves using SQL functions such
    as `sum`, `pow`, `cast`, and `count`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Spark SQL follows the same syntax as Hive, a SQL engine for distributed datasets
    built on Hadoop. Refer to [https://cwiki.apache.org/confluence/display/Hive/LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)
    for a complete syntax reference.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames are a great way to leverage the power and optimization of Scala while
    using the Python interface. The main reason is that queries are interpreted symbolically
    by SparkSQL and the execution happens directly in Scala without having to pass
    intermediate results through Python. This greatly reduces the serialization overhead
    and takes advantage of the query optimizations performed by SparkSQL. Optimizations
    and query planning allows the use of SQL operators, such as `GROUP BY`, without
    incurring in performance penalties, such as the one we experienced using `groupBy`
    directly on an RDD.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Scientific computing with mpi4py
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though Dask and Spark are great technologies widely used in the IT industry,
    they have not been widely adopted in academic research. High-performance supercomputers
    with thousands of processors have been used in academia for decades to run intense
    numerical applications. For this reason, supercomputers are generally configured
    using a very different software stack that focuses on a computationally-intensive
    algorithm implemented in a low-level language, such as C, Fortran, or even assembly.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: The principal library used for parallel execution on these kinds of systems
    is **Message Passing Interface** (**MPI**), which, while less convenient or sophisticated
    than Dask or Spark, is perfectly capable of expressing parallel algorithms and
    achieving excellent performance. Note that, contrary to Dask and Spark, MPI does
    not follow the MapReduce model and is best used for running thousands of processes
    with very little data sent between them.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: MPI works quite differently compared to what we've seen so far. Parallelism
    in MPI is achieved by running *the same script* in multiple processes (which possibly
    exist on different nodes); communication and synchronization between processes
    is handled by a designated process, which is commonly called **root** and is usually
    identified by a `0` ID.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will briefly demonstrate the main concepts of MPI using
    its `mpi4py` Python interface. In the following example, we demonstrate the simplest
    possible parallel code with MPI. The code imports the MPI module and retrieves
    `COMM_WORLD`, which is an interface that can be used to interact with other MPI
    processes. The `Get_rank` function will return an integer identifier for the current
    process:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can place the preceding code in a file, `mpi_example.py`, and execute it. Running
    this script normally won''t do anything special as it involves the execution of
    a single process:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'MPI jobs are meant to be executed using the `mpiexec` command, which takes
    a `-n` option to indicate the number of parallel processes. Running the script
    using the following command will generate four independent executions of the same
    script, each with a different ID:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Distributing processes among the network is performed automatically through
    a resource manager (such as TORQUE). Generally, supercomputers are configured
    by the system administrator, which will also provide instructions on how to run
    MPI software.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a feel as to what an MPI program looks like, we will reimplement the
    approximation of *pi*. The complete code is shown here. The program will do the
    following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Create a random array of `N / n_procs` size for each process so that each process
    will test the same amount of samples (`n_procs` is obtained through the `Get_size`
    function)
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each separate process, calculate the sum of the hit tests and store it in
    `hits_counts`, which will represent the partial counts for each process
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `reduce` function to calculate the total sum of the partial counts.
    When using reduce, we need to specify the `root` argument to specify which process
    will receive the result
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Print the final result only on the process corresponding to the root process:'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can now place the preceding code in a file named `mpi_pi.py` and execute
    it using `mpiexec`. The output shows how the four process executions are intertwined
    until we get to the `reduce` call:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributed processing can be used to implement algorithms capable of handling
    massive datasets by distributing smaller tasks across a cluster of computers.
    Over the years, many software packages, such as Apache Hadoop, have been developed
    to implement performant and reliable execution of distributed software.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned about the architecture and usage of Python packages,
    such as Dask and PySpark, which provide powerful APIs to design and execute programs
    capable of scaling to hundreds of machines. We also briefly looked at MPI, a library
    that has been used for decades to distribute work on supercomputers designed for
    academic research.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we explored several techniques to improve the performance of
    our program, and to increase the speed of our programs and the size of the datasets
    we are able to process. In the next chapter, we will describe the strategies and
    best practices to write and maintain high-performance code.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
