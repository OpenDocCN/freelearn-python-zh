["```py\n<st c=\"4255\">@shared_task(ignore_result=False)</st> def add_login_task_wrapper(details): <st c=\"4327\">async def add_login_task(details):</st> try:\n            async with db_session() as sess:\n              async with sess.begin():\n                repo = LoginRepository(sess)\n                details_dict = loads(details)\n                print(details_dict)\n                login = Login(**details_dict)\n                result = await repo.insert_login(login)\n                if result:\n                    return str(True)\n                else:\n                    return str(False)\n        except Exception as e:\n            print(e)\n            return str(False)\n    return <st c=\"4774\">signature()</st> method includes having a tuple argument, as in the following snippet:\n\n```", "```py\n add_login_task_wrapper.s(<st c=\"5080\">delay()</st>, <st c=\"5089\">apply_async()</st>, or simply <st c=\"5114\">()</st> right after the <st c=\"5133\">signature()</st> call to run the task, if necessary. Now, let us explore Celery’s built-in signatures called *<st c=\"5237\">primitives</st>*, used in building simple and complex workflows.\n\t\t\t<st c=\"5295\">Utilizing Celery primitives</st>\n\t\t\t<st c=\"5323\">Now, Celery provides the following core</st> <st c=\"5364\">workflow operations called primitives, which are</st> <st c=\"5413\">also signature objects themselves that take a list of task signatures to build dynamic</st> <st c=\"5500\">workflow transactions:</st>\n\n\t\t\t\t*   `<st c=\"5522\">chain()</st>` <st c=\"5530\">– A Celery function that takes a series of signatures that are linked together to form a chain of callbacks executed from left</st> <st c=\"5658\">to right.</st>\n\t\t\t\t*   `<st c=\"5667\">group()</st>` <st c=\"5675\">– A Celery operator that takes a list of signatures that will execute</st> <st c=\"5746\">in parallel.</st>\n\t\t\t\t*   `<st c=\"5758\">chord()</st>` <st c=\"5766\">– A Celery operator that takes a list of signatures that will execute in parallel but with a callback that will consolidate</st> <st c=\"5891\">their results.</st>\n\n\t\t\t<st c=\"5905\">Let us first, in the next section, showcase Celery’s chained</st> <st c=\"5967\">workflow execution.</st>\n\t\t\t<st c=\"5986\">Implementing a sequential workflow</st>\n\t\t\t<st c=\"6021\">Celery primitives are the components of building dynamic Celery workflows.</st> <st c=\"6097\">The most commonly used primitive is the</st> *<st c=\"6137\">chain</st>* <st c=\"6142\">primitive, which can establish a pipeline of tasks with results passed from one task to another in a left-to-right manner.</st> <st c=\"6266\">Since it is dynamic, it can follow any specific</st> <st c=\"6314\">sequence based on the software specification, but it prefers smaller and straightforward tasks to avoid unwanted performance degradation.</st> *<st c=\"6452\">Figure 8</st>**<st c=\"6460\">.1</st>* <st c=\"6462\">shows a workflow diagram that the</st> `<st c=\"6497\">ch08-celery-redis</st>` <st c=\"6514\">project implemented for an efficient user</st> <st c=\"6557\">signup transaction:</st>\n\t\t\t![Figure 8.1 – Task signatures in a chain operation](img/B19383_08_001.jpg)\n\n\t\t\t<st c=\"6578\">Figure 8.1 – Task signatures in a chain operation</st>\n\t\t\t<st c=\"6627\">Similar to the</st> `<st c=\"6643\">add_user_login_task_wrapper()</st>` <st c=\"6672\">task,</st> `<st c=\"6679\">add_user_profile_task_wrapper()</st>` <st c=\"6710\">and</st> `<st c=\"6715\">show_complete_profile_task_wrapper()</st>` <st c=\"6751\">are asynchronous Celery tasks that can emit their respective signature to establish a dynamic workflow.</st> <st c=\"6856\">The following endpoint function calls the signatures of these tasks in sequence using the</st> `<st c=\"6946\">chain()</st>` <st c=\"6953\">primitive:</st>\n\n```", "```py\n\n\t\t\t<st c=\"7441\">The presence of</st> `<st c=\"7458\">()</st>` <st c=\"7460\">at the end of the</st> `<st c=\"7479\">chain()</st>` <st c=\"7486\">primitive means the execution of the chained sequence since</st> `<st c=\"7547\">chain()</st>` <st c=\"7554\">is also a signature but a predefined one.</st> <st c=\"7597\">Now, the purpose of the</st> `<st c=\"7621\">add_user_workflow()</st>` <st c=\"7640\">endpoint is to merge the</st> *<st c=\"7666\">INSERT</st>* <st c=\"7672\">transaction of the login credentials and the login profile details of the user instead of accessing two separate</st> <st c=\"7786\">endpoints for the whole process.</st> <st c=\"7819\">Also, it’s there to render the login credentials to the user after a successful workflow execution.</st> <st c=\"7919\">So, all three tasks are in one execution frame with one JSON input of combined user profile and login details to the initial task,</st> `<st c=\"8050\">add_user_login_task_wrapper()</st>`<st c=\"8079\">. But what if tasks need arguments?</st> <st c=\"8115\">Does the</st> `<st c=\"8124\">signature()</st>` <st c=\"8135\">method accept parameter(s) for its task?</st> <st c=\"8177\">Let’s take a look in the</st> <st c=\"8202\">next section.</st>\n\t\t\t<st c=\"8215\">Passing inputs to signatures</st>\n\t\t\t<st c=\"8244\">As mentioned earlier in this chapter, the</st> <st c=\"8286\">required arguments for the Celery tasks can be passed to the</st> `<st c=\"8348\">s()</st>` <st c=\"8351\">or</st> `<st c=\"8355\">signature()</st>` <st c=\"8366\">function.</st> <st c=\"8377\">In the given chained tasks, the</st> `<st c=\"8409\">add_user_login_task_wrapper()</st>` <st c=\"8438\">is the only task among the three that needs input from the API, as depicted in its</st> <st c=\"8522\">code here:</st>\n\n```", "```py\n @shared_task(ignore_result=False) <st c=\"9548\">def add_user_profile_task_wrapper(details):</st> async def add_user_profile_task(<st c=\"9624\">details</st>):\n        try:\n            async with db_session() as sess:\n              async with sess.begin():\n                … … … … … … <st c=\"9711\">role = profile_dict['role']</st> result = False <st c=\"9754\">if role == 0:</st><st c=\"9767\">repo = AdminRepository(sess)</st> admin = Administrator(**profile_dict)\n                    result = await repo.insert_admin(admin) <st c=\"9875\">elif role == 1:</st><st c=\"9890\">repo = DoctorRepository(sess)</st> doc = Doctor(**profile_dict)\n                    result = await repo.insert_doctor(doc) <st c=\"9989\">elif role == 2:</st><st c=\"10004\">repo = PatientRepository(sess)</st> patient = Patient(**profile_dict)\n                    result = await repo.insert_patient(patient)\n                … … … … … …\n                … … … … … …\n    return <st c=\"10335\">add_user_profile_task_wrapper()</st>, the <st c=\"10372\">details</st> parameter pertains to the returned value of <st c=\"10424\">add_user_login_task_wrapper()</st>. The first parameter will always receive the result of the preceding tasks. Now, the <st c=\"10539\">add_user_profile_task_wrapper()</st> task will check the role to determine what table to insert the profile information in. Then, it will return the *<st c=\"10683\">username</st>* as input to the final task, <st c=\"10720\">show_complete_login_task_wrapper()</st>, which will render the user credentials.\n\t\t\t<st c=\"10795\">The dynamic workflow must have strict exception handling from the inside of the tasks and from the outside of the Celery workflow execution to establish a continuous and blockage-free passing of results</st> <st c=\"10998\">or input from the initial task to</st> <st c=\"11033\">the end.</st>\n\t\t\t<st c=\"11041\">On the other hand, running independent Celery tasks requires a different Celery primitive operation called</st> `<st c=\"11149\">group()</st>`<st c=\"11156\">. Let us now scrutinize some parallel tasks from</st> <st c=\"11205\">our application.</st>\n\t\t\t<st c=\"11221\">Running independent and parallel tasks</st>\n\t\t\t<st c=\"11260\">The</st> `<st c=\"11265\">group()</st>` <st c=\"11272\">primitive can run tasks</st> <st c=\"11296\">concurrently and even return consolidated results from functional tasks.</st> <st c=\"11370\">Our sample grouped workflow, shown in</st> *<st c=\"11408\">Figure 8</st>**<st c=\"11416\">.2</st>*<st c=\"11418\">, focuses only on void tasks that serialize a list of records to CSV files, so no consolidation of results</st> <st c=\"11525\">is needed:</st>\n\t\t\t![Figure 8.2 – Task signatures in grouped workflow](img/B19383_08_002.jpg)\n\n\t\t\t<st c=\"11670\">Figure 8.2 – Task signatures in grouped workflow</st>\n\t\t\t<st c=\"11718\">The</st> `<st c=\"11723\">group()</st>` <st c=\"11730\">operation can accept varying Celery tasks with different arguments but prefers those that</st> *<st c=\"11821\">read from and write to files</st>*<st c=\"11849\">,</st> *<st c=\"11851\">perform database transactions</st>*<st c=\"11880\">,</st> *<st c=\"11882\">extract resources from API endpoints</st>*<st c=\"11918\">,</st> *<st c=\"11920\">download files from external storages</st>*<st c=\"11957\">, or</st> *<st c=\"11962\">perform any I/O operations</st>*<st c=\"11988\">. Our</st> `<st c=\"11994\">create_reports()</st>` <st c=\"12010\">endpoint function performs the grouped workflow presented in</st> *<st c=\"12072\">Figure 8</st>**<st c=\"12080\">.2</st>*<st c=\"12082\">, which aims to back up the list of user administrators, patients, and doctors to their respective CSV files.</st> <st c=\"12192\">The following is the code of the</st> <st c=\"12225\">endpoint</st> <st c=\"12234\">function:</st>\n\n```", "```py\n\n\t\t\t<st c=\"12891\">The</st> `<st c=\"12896\">create_reports()</st>` <st c=\"12912\">endpoint passes different filenames to the three tasks.</st> <st c=\"12969\">The</st> `<st c=\"12973\">generate_csv_admin_task_wrapper()</st>`<st c=\"13006\">method will back up all administrator records to</st> `<st c=\"13056\">dams_admin.csv</st>`<st c=\"13070\">,</st> `<st c=\"13072\">generate_csv_patient_task_wrapper()</st>` <st c=\"13107\">will dump all patient records to</st> `<st c=\"13141\">dams_patient.csv</st>`<st c=\"13157\">, and</st> `<st c=\"13163\">generate_csv_doctor_task_wrapper()</st>` <st c=\"13197\">will save all doctor profiles to</st> `<st c=\"13231\">dams_doctor.csv</st>`<st c=\"13246\">. All three will concurrently execute after running the</st> `<st c=\"13302\">group()</st>` <st c=\"13309\">operation.</st>\n\t\t\t<st c=\"13320\">But if the concern is to manage all the results of these concurrently running tasks, the</st> `<st c=\"13410\">chord()</st>` <st c=\"13417\">workflow operation, as shown in the next section, will be the best option for</st> <st c=\"13496\">this scenario.</st>\n\t\t\t<st c=\"13510\">Using callbacks to manage task results</st>\n\t\t\t<st c=\"13549\">The</st> `<st c=\"13554\">chord()</st>` <st c=\"13561\">primitive works like the</st> `<st c=\"13587\">group()</st>` <st c=\"13594\">operation except for its callback task requirement, which will handle and</st> <st c=\"13668\">manage all results of the independent tasks.</st> <st c=\"13714\">The following API endpoint generates a report on a doctor’s appointments and</st> <st c=\"13791\">laboratory requests:</st>\n\n```", "```py\n\n\t\t\t<st c=\"14283\">The</st> `<st c=\"14287\">derive_doctor_stats()</st>` <st c=\"14309\">method aims to execute the workflow shown in</st> *<st c=\"14355\">Figure 8</st>**<st c=\"14363\">.3</st>*<st c=\"14365\">, which uses the</st> `<st c=\"14382\">chord()</st>` <st c=\"14389\">operation to run</st> `<st c=\"14407\">count_patients_doctor_task_wrapper()</st>` <st c=\"14443\">to determine the number of patients of a particular doctor and</st> `<st c=\"14507\">count_request_doctor_task_wrapper()</st>` <st c=\"14542\">to extract the total number of laboratory requests of the same doctor.</st> <st c=\"14614\">The results of the tasks are stored in a list according to the order of their executions before passing it to the callback task,</st> `<st c=\"14743\">create_doctor_stats_task_wrapper()</st>`<st c=\"14777\">, for processing.</st> <st c=\"14795\">Unlike in the</st> `<st c=\"14809\">group()</st>` <st c=\"14816\">primitive, the results are managed by a callback task before returning the final result to the</st> <st c=\"14912\">API function:</st>\n\t\t\t![Figure 8.3 – Task signatures in chord() primitive](img/B19383_08_003.jpg)\n\n\t\t\t<st c=\"15032\">Figure 8.3 – Task signatures in chord() primitive</st>\n\t\t\t<st c=\"15081\">A sample output of the</st> `<st c=\"15105\">create_doctor_stats_task_wrapper()</st>` <st c=\"15139\">task will be like this: “</st>*<st c=\"15165\">Doctor HSP-200 has 2 patients and 0</st>* *<st c=\"15202\">lab requests.</st>*<st c=\"15215\">”</st>\n\t\t\t<st c=\"15217\">There are lots of ways to build complex dynamic workflows using combinations of</st> `<st c=\"15297\">chain()</st>`<st c=\"15304\">,</st> `<st c=\"15306\">group()</st>`<st c=\"15313\">, and</st> `<st c=\"15319\">chord()</st>`<st c=\"15326\">, which will implement the workflows that the Flask applications need to optimize some business processes.</st> <st c=\"15433\">It is possible for a chained task to call the</st> `<st c=\"15479\">group()</st>` <st c=\"15486\">primitive from the inside to spawn and run a group of independent tasks.</st> <st c=\"15560\">It is also feasible to use Celery’s</st> *<st c=\"15596\">subtasks</st>* <st c=\"15605\">to implement conditional task executions.</st> <st c=\"15647\">There are also miscellaneous primitives such as</st> `<st c=\"15695\">map()</st>`<st c=\"15700\">,</st> `<st c=\"15702\">starmap()</st>`<st c=\"15711\">, and</st> `<st c=\"15717\">chunks()</st>` <st c=\"15725\">that can manage</st> <st c=\"15741\">arguments of tasks in the workflow.</st> <st c=\"15778\">A Celery workflow is flexible and open to any implementation using its primitives and signatures since it targets dynamic workflows.</st> <st c=\"15911\">Celery workflows can read and execute workflows from XML files, such as BPMN workflows.</st> <st c=\"15999\">However, there is a workflow solution that can work on both dynamic and BPMN</st> <st c=\"16076\">workflows: SpiffWorkflow.</st>\n\t\t\t<st c=\"16101\">Creating BPMN and non-BPMN workflows with SpiffWorkflow</st>\n\t\t\t**<st c=\"16157\">SpiffWorkflow</st>** <st c=\"16171\">is a flexible Python execution engine for workflow activities.</st> <st c=\"16235\">Its latest installment focuses more on BPMN models, but it always</st> <st c=\"16300\">has strong support classes to build and run non-BPMN</st> <st c=\"16353\">workflows translated into Python and JSON.</st> <st c=\"16397\">The library has a</st> *<st c=\"16415\">BPMN interpreter</st>* <st c=\"16431\">that can execute tasks indicated in</st> <st c=\"16467\">BPMN diagrams created by BPMN modeling tools and</st> *<st c=\"16517\">serializers</st>* <st c=\"16528\">to run</st> <st c=\"16536\">JSON-based workflows.</st>\n\t\t\t<st c=\"16557\">To start SpiffWorkflow, we need to install some</st> <st c=\"16606\">required dependencies.</st>\n\t\t\t<st c=\"16628\">Setting up the development environment</st>\n\t\t\t<st c=\"16667\">No broker or server is needed to run</st> <st c=\"16705\">workflows with SpiffWorkflow.</st> <st c=\"16735\">However, installing the main plugin using the</st> `<st c=\"16781\">pip</st>` <st c=\"16784\">command is</st> <st c=\"16796\">a requirement:</st>\n\n```", "```py\n\n\t\t\t<st c=\"16836\">Then, for serialization and parsing purposes, install the</st> `<st c=\"16895\">lxml</st>` <st c=\"16899\">dependency:</st>\n\n```", "```py\n\n\t\t\t<st c=\"16928\">Since SpiffWorkflow uses the Celery client library for legacy support, install the</st> `<st c=\"17012\">celery</st>` <st c=\"17018\">module:</st>\n\n```", "```py\n\n\t\t\t<st c=\"17045\">Now, download and install a BPMN modeler tool that can provide BPMN diagrams supported by SpiffWorkflow.</st> <st c=\"17151\">This chapter uses the</st> *<st c=\"17173\">Camunda Modeler for Camunda 7 BPMN</st>* <st c=\"17207\">version to generate BPMN diagrams, which we can download from</st> [<st c=\"17270\">https://camunda.com/download/modeler/</st>](https://camunda.com/download/modeler/)<st c=\"17307\">.</st> *<st c=\"17309\">Figure 8</st>**<st c=\"17317\">.4</st>* <st c=\"17319\">provides a screenshot of the Camunda Modeler with a sample</st> <st c=\"17379\">BPMN diagram:</st>\n\t\t\t![Figure 8.4 – Camunda Modeler with BPMN model for Camunda 7](img/B19383_08_004.jpg)\n\n\t\t\t<st c=\"17627\">Figure 8.4 – Camunda Modeler with BPMN model for Camunda 7</st>\n\t\t\t<st c=\"17685\">The version of SpiffWorkflow used by this chapter can only parse and execute the BPMN model for the Camunda 7 platform.</st> <st c=\"17806\">Hopefully, its future releases can support Camunda 8 or higher versions of</st> <st c=\"17881\">BPMN diagrams.</st>\n\t\t\t<st c=\"17895\">Let us now create our workflow using the BPMN</st> <st c=\"17942\">modeler tool.</st>\n\t\t\t<st c=\"17955\">Creating a BPMN diagram</st>\n\t\t\t<st c=\"17979\">BPMN is an open standard for business</st> <st c=\"18017\">process diagrams.</st> <st c=\"18036\">It is a graphical mechanism to visualize and simulate a systematic set of activities in one process flow that goals a successful result.</st> <st c=\"18173\">A BPMN diagram has a set of graphical elements, called</st> *<st c=\"18228\">flow objects</st>*<st c=\"18240\">, composed of</st> *<st c=\"18254\">activities</st>*<st c=\"18264\">,</st> *<st c=\"18266\">events</st>*<st c=\"18272\">,</st> *<st c=\"18274\">sequence flows</st>*<st c=\"18288\">,</st> <st c=\"18290\">and</st> *<st c=\"18294\">gateways</st>*<st c=\"18302\">.</st>\n\t\t\t<st c=\"18303\">An activity represents work that needs execution inside a workflow process.</st> <st c=\"18380\">A work can be simple and atomic, such as a</st> *<st c=\"18423\">task</st>*<st c=\"18427\">, or complex, such as a</st> *<st c=\"18451\">sub-process</st>*<st c=\"18462\">. When an activity is atomic and cannot break down further, despite the complexity of the process, then that is considered a task.</st> <st c=\"18593\">A task in BPMN is denoted as a</st> *<st c=\"18624\">rounded-corner rectangle shape</st>* <st c=\"18654\">component.</st> <st c=\"18666\">There are several types of tasks, but SpiffWorkflow only supports</st> <st c=\"18732\">the following:</st>\n\n\t\t\t\t*   **<st c=\"18746\">Manual task</st>** <st c=\"18758\">– A non-automated task that a human can perform outside the context of</st> <st c=\"18830\">the workflow.</st>\n\t\t\t\t*   **<st c=\"18843\">Script task</st>** <st c=\"18855\">– A task that runs a</st> <st c=\"18877\">modeler-defined script.</st>\n\t\t\t\t*   **<st c=\"18900\">User task</st>** <st c=\"18910\">– A typical task that a human actor can carry out using some application-related operation, such as clicking</st> <st c=\"19020\">a button.</st>\n\n\t\t\t<st c=\"19029\">The tasks presented in the BPMN diagram of</st> *<st c=\"19073\">Figure 8</st>**<st c=\"19081\">.4</st>*<st c=\"19083\">, namely</st> **<st c=\"19092\">Doctor’s Specialization Form</st>**<st c=\"19120\">,</st> **<st c=\"19122\">List Specialized Doctors</st>**<st c=\"19146\">,</st> **<st c=\"19148\">Doctor’s Availability Form</st>**<st c=\"19174\">, and</st> **<st c=\"19180\">Patient Detail Form</st>**<st c=\"19199\">, are</st> *<st c=\"19205\">user tasks</st>*<st c=\"19215\">. Usually, user tasks can represent actions such as web form handling, console-based transactions with user inputs, or transactions in applications involving editing and submitting form data.</st> <st c=\"19407\">On the other hand, the</st> **<st c=\"19430\">Evaluate Form Data</st>** <st c=\"19448\">and</st> **<st c=\"19453\">Finalize Schedule</st>** <st c=\"19470\">tasks are considered</st> *<st c=\"19492\">script tasks</st>*<st c=\"19504\">.</st>\n\t\t\t<st c=\"19505\">A</st> *<st c=\"19508\">sequence flow</st>* <st c=\"19521\">is a one-directional line connector between activities or tasks.</st> <st c=\"19587\">The BPMN standard allows adding descriptions or labels to sequence flows to determine which paths to take from one activity</st> <st c=\"19711\">to another.</st>\n\t\t\t<st c=\"19722\">Now, the workflow will not work without</st> *<st c=\"19763\">start</st>* <st c=\"19768\">and</st> *<st c=\"19773\">stop events</st>*<st c=\"19784\">. An</st> *<st c=\"19789\">event</st>* <st c=\"19794\">is an occurrence along the workflow required to execute due to some triggers to produce some result.</st> <st c=\"19896\">The start event, represented by a</st> *<st c=\"19930\">small and open circle with a thin-lined boundary</st>*<st c=\"19978\">, triggers the start of the workflow.</st> <st c=\"20016\">The stop event, defined by a</st> *<st c=\"20045\">small, open circle with a single thick-lined boundary</st>*<st c=\"20098\">, ends the workflow activities.</st> <st c=\"20130\">Other than these two, there are</st> *<st c=\"20162\">cancel</st>*<st c=\"20168\">,</st> *<st c=\"20170\">signal</st>*<st c=\"20176\">,</st> *<st c=\"20178\">error</st>*<st c=\"20183\">,</st> *<st c=\"20185\">message</st>*<st c=\"20192\">,</st> *<st c=\"20194\">timer</st>*<st c=\"20199\">, and</st> *<st c=\"20205\">escalation</st>* <st c=\"20215\">events supported by SpiffWorkflow, and all these are represented</st> <st c=\"20281\">as circles.</st>\n\t\t\t<st c=\"20292\">The</st> *<st c=\"20297\">diamond-shaped component</st>* <st c=\"20321\">in</st> *<st c=\"20325\">Figure 8</st>**<st c=\"20333\">.4</st>* <st c=\"20335\">is a</st> *<st c=\"20341\">gateway</st>* <st c=\"20348\">component.</st> <st c=\"20360\">It diverges or converges its incoming or outgoing process flows.</st> <st c=\"20425\">It can control multiple incoming and multiple outgoing process flows.</st> <st c=\"20495\">SpiffWorkflow supports the following types</st> <st c=\"20538\">of gateways:</st>\n\n\t\t\t\t*   **<st c=\"20550\">Exclusive gateway</st>** <st c=\"20568\">– Caters to multiple incoming flows and will emit only one output flow based on</st> <st c=\"20649\">some evaluation.</st>\n\t\t\t\t*   **<st c=\"20665\">Parallel gateway</st>** <st c=\"20682\">– Emits an independent</st> <st c=\"20706\">process flow that will execute tasks without order but will wait for all the tasks</st> <st c=\"20789\">to finish.</st>\n\t\t\t\t*   **<st c=\"20799\">Event gateway</st>** <st c=\"20813\">– Emits an outgoing flow based on some events from an</st> <st c=\"20868\">outside source.</st>\n\t\t\t\t*   **<st c=\"20883\">Inclusive gateway</st>** <st c=\"20901\">– Caters to multiple incoming flows and can emit more than one output flow based on some</st> <st c=\"20991\">complex evaluation.</st>\n\n\t\t\t<st c=\"21010\">The gateway in</st> *<st c=\"21026\">Figure 8</st>**<st c=\"21034\">.4</st>* <st c=\"21036\">is an example of an exclusive gateway because it will allow</st> **<st c=\"21097\">the Finalize Schedule</st>** <st c=\"21118\">task execution to proceed if, and only if, the form data is complete.</st> <st c=\"21189\">Otherwise, it will redirect the sequence flow to the</st> **<st c=\"21242\">Doctor’s Specialization Form</st>** <st c=\"21270\">web form task again for</st> <st c=\"21295\">data re-entry.</st>\n\t\t\t<st c=\"21309\">Now, let us start the showcase on how SpiffWorkflow can interpret a BPMN diagram for</st> **<st c=\"21395\">business process</st>** **<st c=\"21412\">management</st>** <st c=\"21422\">(</st>**<st c=\"21424\">BPM</st>**<st c=\"21427\">).</st>\n\t\t\t<st c=\"21430\">Implementing the BPMN workflow</st>\n\t\t\t<st c=\"21461\">SpiffWorkflow can translate mainly the</st> *<st c=\"21501\">user</st>*<st c=\"21505\">,</st> *<st c=\"21507\">manual</st>*<st c=\"21513\">, and</st> *<st c=\"21519\">script</st>* <st c=\"21525\">tasks of a BPMN diagram.</st> <st c=\"21551\">So, it can best</st> <st c=\"21567\">handle business process optimization involving sophisticated web flows in a</st> <st c=\"21643\">web application.</st>\n\t\t\t<st c=\"21659\">Since there is nothing to configure in the</st> `<st c=\"21703\">create_app()</st>` <st c=\"21715\">factory or</st> `<st c=\"21727\">main.py</st>` <st c=\"21734\">module for SpiffWorkflow, the next step after dependency module installations and the BPMN diagram design is the view function implementation for the BPMN diagram simulation.</st> <st c=\"21910\">The view functions must initiate and execute SpiffWorkflow tasks to run the entire</st> <st c=\"21993\">BPMN workflow.</st>\n\t\t\t<st c=\"22007\">The first support class to call in the module script is</st> `<st c=\"22064\">CamundaParser</st>`<st c=\"22077\">, a support class found in the</st> `<st c=\"22108\">SpiffWorkflow.camunda.parser.CamundaParser</st>` <st c=\"22150\">module of SpiffWorkflow.</st> <st c=\"22176\">The</st> `<st c=\"22180\">CamundaParser</st>` <st c=\"22193\">class will parse the BPMN tags of the BPMN file based on the Camunda 7 standards.</st> <st c=\"22276\">The BPMN file is an XML document with tags corresponding to the</st> *<st c=\"22340\">flow objects</st>* <st c=\"22352\">of the workflow.</st> <st c=\"22370\">Now, the</st> `<st c=\"22379\">CamundaParser</st>` <st c=\"22392\">class will need the name or ID of the BPMN definition to load the document and verify if the XML schema of the BPMN document is</st> <st c=\"22521\">well formed and valid.</st> <st c=\"22544\">The following is the first portion of the</st> `<st c=\"22586\">/view/appointment.py</st>` <st c=\"22606\">module of the</st> `<st c=\"22621\">doctor</st>` <st c=\"22627\">Blueprint module that instantiates the</st> `<st c=\"22667\">CamundaParser</st>` <st c=\"22680\">class that will load our</st> `<st c=\"22706\">dams_appointment.bpmn</st>` <st c=\"22727\">file, the workflow design depicted in the BPMN workflow diagram of</st> *<st c=\"22795\">Figure 8</st>**<st c=\"22803\">.4</st>*<st c=\"22805\">:</st>\n\n```", "```py\n<st c=\"25054\">@doc_bp.route(\"/doctor/expertise\",</st><st c=\"25089\">methods = [\"GET\", \"POST\"])</st>\n<st c=\"25116\">async def choose_specialization():</st> if request.method == \"GET\":\n      return render_template(\"doc_specialization_form.html\")\n    session['specialization'] = request.form['specialization'] <st c=\"25379\">select_doctor()</st> to list all doctors with the specialization indicated by <st c=\"25452\">choose_specialization()</st>. The following snippet presents the code for the <st c=\"25525\">select_doctor()</st> view:\n\n```", "```py\n\n\t\t\t<st c=\"25802\">After the</st> `<st c=\"25813\">select_doctor()</st>` <st c=\"25828\">view, the user will choose a date and time for the appointment through the</st> `<st c=\"25904\">reserve_schedule()</st>` <st c=\"25922\">view.</st> <st c=\"25929\">The last view of the web flow is</st> `<st c=\"25962\">provide_patient_details()</st>`<st c=\"25987\">, which</st> <st c=\"25994\">will ask for the patient details needed for the diagnosis and payment.</st> <st c=\"26066\">The following code presents the implementation of the</st> `<st c=\"26120\">reserve_schedule()</st>` <st c=\"26138\">view:</st>\n\n```", "```py\n<st c=\"26792\">from SpiffWorkflow.bpmn.workflow import BpmnWorkflow</st>\n<st c=\"26845\">from SpiffWorkflow.camunda.parser.CamundaParser import CamundaParser</st>\n<st c=\"26914\">from SpiffWorkflow.bpmn.specs.bpmn_task_spec import TaskSpec</st>\n<st c=\"26975\">from SpiffWorkflow.camunda.specs.user_task import UserTask</st>\n<st c=\"27034\">from SpiffWorkflow.task import Task, TaskState</st>\n<st c=\"27081\">@doc_bp.route(\"/doctor/patient\", methods = [\"GET\", \"POST\"])</st>\n<st c=\"27141\">async def provide_patient_details():</st> if request.method == \"GET\": <st c=\"27207\">return render_template(\"doc_patient_form.html\"), 201</st><st c=\"27259\">form_data = dict()</st> form_data['specialization'] = <st c=\"27309\">session['specialization']</st> form_data['docid'] = <st c=\"27356\">session['docid']</st> form_data['appt_date'] = <st c=\"27398\">session['appt_date']</st> form_data['appt_time'] = <st c=\"27444\">session['appt_time']</st> form_data['ticketid'] = <st c=\"27489\">request.form['ticketid']</st> form_data['patientid'] = <st c=\"27539\">request.form['patientid']</st> form_data['priority_level'] = <st c=\"27595\">request.form['priority_level']</st><st c=\"27625\">workflow = BpmnWorkflow(spec)</st><st c=\"27655\">workflow.do_engine_steps()</st> ready_tasks: List[Task] = <st c=\"27709\">workflow.get_tasks(TaskState.READY)</st> while len(ready_tasks) > 0:\n        for task in ready_tasks:\n            if isinstance(<st c=\"27812\">task.task_spec</st>, UserTask):\n                upload_login_form_data(task, form_data)\n            workflow.run_task_from_id(task_id=<st c=\"27914\">task.id</st>)\n         else:\n            task_details:TaskSpec = <st c=\"27955\">task.task_spec</st> print(\"Complete Task \", <st c=\"27994\">task_details.name</st>) <st c=\"28014\">workflow.do_engine_steps()</st> ready_tasks = <st c=\"28055\">workflow.get_tasks(TaskState.READY)</st><st c=\"28090\">dashboard_page = workflow.data['finalize_sched']</st> if dashboard_page:\n      return render_template(\"doc_dashboard.html\"), 201\n    else:\n      return redirect(url_for(\"doc_bp.choose_specialization\"))\n```", "```py\n<st c=\"32287\">from SpiffWorkflow.util.deep_merge import DeepMerge</st>\n<st c=\"32339\">def upload_login_form_data(task: UserTask, form_data):</st> form = task.task_spec.form <st c=\"32422\">data = {}</st> if task.data is None:\n        task.data = {}\n    for field in form.fields:\n        if field.id == \"specialization\":\n            process_data = form_data[\"specialization\"]\n        elif field.id == \"docid\":\n            process_data = form_data[\"docid\"]\n        elif field.id == \"date_scheduled\":\n            process_data = form_data[\"appt_date\"]\n        … … … … … … <st c=\"32716\">update_data(data, field.id,  process_data)</st><st c=\"32757\">DeepMerge.merge(task.data, data)</st>\n<st c=\"32790\">@doc_bp.route(\"/doctor/patient\", methods = [\"GET\", \"POST\"])</st>\n<st c=\"32850\">async def provide_patient_details():</st> … … … … … …\n    while len(ready_tasks) > 0:\n        for task in ready_tasks:\n            if isinstance(task.task_spec, UserTask): <st c=\"32993\">upload_login_form_data(task, form_data)</st> else:\n                task_details:TaskSpec = task.task_spec\n                print(\"Complete Task \", task_details.name)\n            workflow.run_task_from_id(task_id=task.id)\n        … … … … … …\n        return redirect(url_for(\"doc_bp.choose_specialization\"))\n```", "```py\n def update_data(dct, name, value):\n    path = name.split('.')\n    current = dct\n    for component in path[:-1]:\n        if component not in current:\n            current[component] = {}\n        current = current[component]\n    current[path[-1]] = value\n```", "```py\n<st c=\"36314\">from SpiffWorkflow.specs.WorkflowSpec import WorkflowSpec</st>\n<st c=\"36372\">from SpiffWorkflow.specs.ExclusiveChoice import</st> <st c=\"36420\">ExclusiveChoice</st>\n<st c=\"36436\">from SpiffWorkflow.specs.Simple import Simple</st>\n<st c=\"36482\">from SpiffWorkflow.operators import Equal, Attrib</st>\n<st c=\"36532\">class PaymentWorkflowSpec(WorkflowSpec):</st> def __init__(self):\n        super().__init__() <st c=\"36613\">patient_pay = Simple(wf_spec=self, name='dams_patient_pay')</st> patient_pay.ready_event.connect(  callback=tx_patient_pay)\n        self.start.connect(taskspec=patient_pay) <st c=\"36772\">payment_verify = ExclusiveChoice(wf_spec=self, name='payment_check')</st> patient_pay.connect(taskspec=payment_verify)\n        patient_release = Simple(wf_spec=self, name='dams_patient_release')\n        cond = Equal(Attrib(name='amount'), Attrib(name='charge')) <st c=\"37013\">payment_verify.connect_if(condition=cond,</st> <st c=\"37054\">task_spec=patient_release)</st> patient_release.completed_event.connect( callback=tx_patient_release)\n        patient_hold = Simple(wf_spec=self, name='dams_patient_onhold')\n        payment_verify.connect(task_spec=patient_hold) <st c=\"37328\">WorkflowSpec</st> is responsible for the non-BPMN workflow implementation in Python format. The constructor of the <st c=\"37439\">WorkflowSpec</st> sub-class creates generic, simple, and atomic tasks using the <st c=\"37514\">Simple</st> API of the <st c=\"37532\">SpiffWorkflow.specs.Simple</st> module. The task can have more than one input and any number of output task variables. There is also an <st c=\"37663\">ExclusiveChoice</st> sub-class that works like a gateway for the workflow.\n\t\t\t<st c=\"37732\">Moreover, each task has a</st> `<st c=\"37759\">connect()</st>` <st c=\"37768\">method to</st> <st c=\"37778\">establish sequence flows.</st> <st c=\"37805\">It also has event variables, such as</st> `<st c=\"37842\">ready_event</st>`<st c=\"37853\">,</st> `<st c=\"37855\">cancelled_event</st>`<st c=\"37870\">,</st> `<st c=\"37872\">completed_event</st>`<st c=\"37887\">, and</st> `<st c=\"37893\">reached_event</st>`<st c=\"37906\">, that run their respective callback method, such as our</st> `<st c=\"37963\">tx_patient_pay()</st>`<st c=\"37979\">,</st> `<st c=\"37981\">tx_patient_release()</st>`<st c=\"38001\">, and</st> `<st c=\"38007\">tx_patient_onhold()</st>` <st c=\"38026\">methods.</st> <st c=\"38036\">Calling these event objects marks a transition from one task’s current state</st> <st c=\"38113\">to another.</st>\n\t\t\t<st c=\"38124\">The</st> `<st c=\"38129\">Attrib</st>` <st c=\"38135\">helper class recognizes a task variable and retrieves its data for comparison performed by internal API classes, such as</st> `<st c=\"38257\">Equal</st>`<st c=\"38262\">,</st> `<st c=\"38264\">NotEqual</st>`<st c=\"38272\">, and</st> `<st c=\"38278\">LessThan</st>`<st c=\"38286\">, of the</st> `<st c=\"38295\">SpiffWorkflow.operators</st>` <st c=\"38318\">module.</st>\n\t\t\t<st c=\"38326\">Let us now run our</st> `<st c=\"38346\">PaymentWorkflowSpec</st>` <st c=\"38365\">workflow using a</st> <st c=\"38383\">view function.</st>\n\t\t\t<st c=\"38397\">Running a non-BPMN workflow</st>\n\t\t\t<st c=\"38425\">Since this is not a Camunda-based workflow, running</st> <st c=\"38477\">the workflow does not need a parser.</st> <st c=\"38515\">Immediately wrap and instantiate the custom</st> `<st c=\"38559\">WorkflowSpec</st>` <st c=\"38571\">sub-class inside the</st> `<st c=\"38593\">Workflow</st>` <st c=\"38601\">class and call</st> `<st c=\"38617\">get_tasks()</st>` <st c=\"38628\">inside the view function to prepare the non-BPMN workflow for the task traversal and executions.</st> <st c=\"38726\">But the following</st> `<st c=\"38744\">start_payment_form()</st>` <st c=\"38764\">function opts for individual access of tasks using the workflow instance’s</st> `<st c=\"38840\">get_tasks_from_spec_name()</st>` <st c=\"38866\">function instead of using a</st> `<st c=\"38895\">while</st>` <st c=\"38900\">loop for</st> <st c=\"38910\">task traversal:</st>\n\n```", "```py\n start_tasks: list[Task] = workflow_instance.get_tasks_from_spec_name( name='Start')\n    for task in start_tasks:\n        if task.state == TaskState.READY:\n            workflow_instance.run_task_from_id( task_id=task.id)\n```", "```py\n patient_pay_task: list[Task] = workflow_instance.get_tasks_from_spec_name( name='dams_patient_pay')\n    for task in patient_pay_task:\n        if task.state == TaskState.READY:\n            task.set_data(ticketid=ticketid, patientid=patientid, charge=charge, amount=amount, discount=discount, status=status, date_released=date_released)\n            workflow_instance.run_task_from_id(  task_id=task.id)\n```", "```py\n payment_check_task: list[Task] = workflow_instance.get_tasks_from_spec_name( name='payment_check')\n    for task in payment_check_task:\n        if task.state == TaskState.READY:\n            workflow_instance.run_task_from_id( task_id=task.id)\n```", "```py\n for_releasing = False\n    patient_release_task: list[Task] = workflow_instance.get_tasks_from_spec_name( name='dams_patient_release')\n    for task in patient_release_task:\n        if task.state == TaskState.READY:\n            for_releasing = True\n            workflow_instance.run_task_from_id( task_id=task.id)\n```", "```py\n patient_onhold_task: list[Task] = workflow_instance.get_tasks_from_spec_name( name='dams_patient_onhold')\n    for task in patient_onhold_task:\n        if task.state == TaskState.READY:\n            workflow_instance.run_task_from_id( task_id=task.id)\n    if for_releasing == True:\n        return redirect(url_for('payment_bp.release_patient'), code=307)\n    else:\n       return redirect(url_for('payment_bp.hold_patient'), code=307)\n```", "```py\n docker run --name zeebe --rm -p 26500-26502:26500-26502 -d --network=ch08-network camunda/zeebe:latest\n```", "```py\n pip install pyzeebe\n```", "```py\n<st c=\"46011\">from pyzeebe import ZeebeWorker, create_insecure_channel</st> import asyncio\nfrom modules.models.config import db_session, init_db\nfrom modules.doctors.repository.diagnosis import DiagnosisRepository\nprint('starting the Zeebe worker...')\nprint('initialize database connectivity...')\ninit_db()\nchannel = create_insecure_channel() <st c=\"46479\">ZeebeWorker</st> worker with its constructor parameters. The <st c=\"46535\">initdb()</st> call is included in the module because our tasks will need CRUD transactions:\n\n```", "```py\n\n\t\t\t<st c=\"47117\">The</st> `<st c=\"47122\">select_diagnosis()</st>` <st c=\"47140\">method is a</st> `<st c=\"47153\">pyzeebe</st>` <st c=\"47160\">worker decorated with the</st> `<st c=\"47187\">@worker.task()</st>` <st c=\"47201\">annotation.</st> <st c=\"47214\">The</st> `<st c=\"47218\">task_type</st>` <st c=\"47227\">attribute of the</st> `<st c=\"47245\">@worker.task()</st>` <st c=\"47259\">annotation indicates its</st> `<st c=\"47285\">ServiceTask</st>` <st c=\"47296\">name in the</st> <st c=\"47308\">BPMN model.</st> <st c=\"47321\">The decorator can also include other attributes, such as</st> `<st c=\"47378\">exception_handler</st>` <st c=\"47395\">and</st> `<st c=\"47400\">timeout_ms</st>`<st c=\"47410\">. Now,</st> `<st c=\"47417\">select_diagnosis()</st>` <st c=\"47435\">looks for all patients’ diagnoses from the database with</st> `<st c=\"47493\">docid</st>` <st c=\"47499\">and</st> `<st c=\"47503\">patientid</st>` <st c=\"47512\">parameters as filters to the search.</st> <st c=\"47550\">It returns a dictionary with a key named</st> `<st c=\"47591\">data</st>` <st c=\"47595\">handling</st> <st c=\"47605\">the result:</st>\n\n```", "```py\n\n\t\t\t<st c=\"47924\">On the other hand, this</st> `<st c=\"47949\">retrieve_analysis()</st>` <st c=\"47968\">task takes</st> `<st c=\"47980\">records</st>` <st c=\"47987\">from</st> `<st c=\"47993\">select_diagnosis()</st>` <st c=\"48011\">in string form but is serialized back to the list form with</st> `<st c=\"48072\">json.loads()</st>`<st c=\"48084\">. This task will extract</st> <st c=\"48108\">only all resolutions from the patients’ records</st> <st c=\"48156\">and return them to the caller.</st> <st c=\"48188\">The task returns a</st> <st c=\"48207\">dictionary also.</st>\n\t\t\t<st c=\"48223\">The</st> *<st c=\"48228\">local parameter names</st>* <st c=\"48249\">and the</st> *<st c=\"48258\">dictionary keys</st>* <st c=\"48273\">returned by the worker’s tasks must be</st> *<st c=\"48313\">BPMN variable names</st>* <st c=\"48332\">because the client will also fetch these local parameters to assign values and dictionary keys for the output extraction for the preceding</st> `<st c=\"48472\">ServiceTask</st>` <st c=\"48483\">task.</st>\n\t\t\t<st c=\"48489\">Since our Flask client application uses its event loop, our worker must run on a separate event loop using</st> `<st c=\"48597\">asyncio</st>` <st c=\"48604\">to avoid exceptions.</st> <st c=\"48626\">The following</st> `<st c=\"48640\">worker_tasks.py</st>` <st c=\"48655\">snippet shows how to run the worker on an</st> `<st c=\"48698\">asyncio</st>` <st c=\"48705\">environment:</st>\n\n```", "```py\n from celery import shared_task\nimport asyncio <st c=\"49771\">from pyzeebe import ZeebeClient, create_insecure_channel</st> channel = create_insecure_channel(hostname=\"localhost\", port=26500) <st c=\"49959\">ZeebeClient</st> instance. The port to connect the Zeebe client is <st c=\"50021\">26500</st>:\n\n```", "```py\n<st c=\"50666\">@shared_task(ignore_result=False)</st>\n<st c=\"50700\">def run_zeebe_task(docid, patientid):</st> async def zeebe_task(docid, patientid):\n        try:\n            process_instance_key, result = await <st c=\"50821\">client.run_process_with_result(</st><st c=\"50852\">bpmn_process_id</st>= \"Process_Diagnostics\", <st c=\"50894\">variables</st>={\"<st c=\"50907\">docid</st>\": docid, \"<st c=\"50925\">patientid</st>\":patientid}, variables_to_fetch =[\"<st c=\"50972\">result</st>\"], timeout=10000)\n            return result\n        except Exception as e:\n            print(e)\n            return {} <st c=\"51147\">ZeebeClient</st> has two asynchronous methods that can execute process definitions in the BPMN file, and these are <st c=\"51258\">run_process()</st> and <st c=\"51276\">run_process_with_result()</st>. Both methods pass values to the first task of the workflow, but only <st c=\"51372\">run_process_with_result()</st> returns an output value. The given <st c=\"51433\">run_zeebe_task()</st> method will execute the first <st c=\"51480\">ServiceTask</st> task, the worker’s <st c=\"51511\">select_diagnosis()</st> task, pass values to its <st c=\"51555\">docid</st> and <st c=\"51565\">patientid</st> parameters, and retrieve the dictionary output of the last <st c=\"51634\">ServiceTask</st> task, <st c=\"51652\">retrieve_analysis()</st>, indicated by the <st c=\"51690\">result</st> key. A <st c=\"51704\">ServiceTask</st> task’s parameters are considered BPMN variables that the BPMN file or the <st c=\"51790\">ZeebeClient</st> operations can fetch at any time. Likewise, the key of the dictionary returned by <st c=\"51884\">ServiceTask</st> becomes a BPMN variable, too. So, the <st c=\"51934\">variables</st> parameter of the <st c=\"51961\">run_process_with_result()</st> method fetches the local parameters of the first worker’s task, and its <st c=\"52059\">variables_to_fetch</st> property retrieves the returned dictionary of any <st c=\"52128\">ServiceTask</st> task indicated by the key name.\n\t\t\t<st c=\"52171\">To enable the</st> `<st c=\"52186\">ZeebeClient</st>` <st c=\"52197\">operations, run Celery and the Redis broker.</st> <st c=\"52243\">Let us now implement API endpoints that will simulate the</st> <st c=\"52301\">diagnosis workflow.</st>\n\t\t\t<st c=\"52320\">Building API endpoints</st>\n\t\t\t<st c=\"52343\">The following API endpoint passes the</st> <st c=\"52381\">filename of the BPMN file to the</st> `<st c=\"52415\">pyzeebe</st>` <st c=\"52422\">client by calling the</st> `<st c=\"52445\">deploy_zeebe_wf()</st>` <st c=\"52462\">Celery task:</st>\n\n```", "```py\n\n\t\t\t<st c=\"52804\">Afterward, the following</st> `<st c=\"52830\">extract_analysis_text()</st>` <st c=\"52853\">endpoint can run the workflow by calling the</st> `<st c=\"52899\">run_zeebe_task()</st>` <st c=\"52915\">Celery task:</st>\n\n```", "```py\n\n\t\t\t<st c=\"53265\">The given endpoint will also pass the</st> `<st c=\"53304\">docid</st>` <st c=\"53309\">and</st> `<st c=\"53314\">patientid</st>` <st c=\"53323\">values to the</st> <st c=\"53338\">client task.</st>\n\t\t\t<st c=\"53350\">The</st> `<st c=\"53355\">pyzeebe</st>` <st c=\"53362\">library has many limitations, such as supporting</st> `<st c=\"53412\">UserTask</st>` <st c=\"53420\">and web flows and implementing workflows that</st> <st c=\"53466\">call API endpoints for results.</st> <st c=\"53499\">Although connecting our Flask application to the enterprise Camunda platform can address these problems with</st> `<st c=\"53608\">pyzeebe</st>`<st c=\"53615\">, it is a practical and clever approach to use the Airflow 2.x</st> <st c=\"53678\">platform instead.</st>\n\t\t\t<st c=\"53695\">Using Airflow 2.x in orchestrating API endpoints</st>\n\t\t\t**<st c=\"53744\">Airflow 2.x</st>** <st c=\"53756\">is an open source platform that provides workflow authorization, monitoring, scheduling, and maintenance with its easy-to-use UI dashboard.</st> <st c=\"53897\">It can manage</st> **<st c=\"53911\">extract, transform, load</st>** <st c=\"53935\">(</st>**<st c=\"53937\">ETL</st>**<st c=\"53940\">) workflows</st> <st c=\"53953\">and</st> <st c=\"53957\">data analytics.</st>\n\t\t\t<st c=\"53972\">Airflow uses Flask Blueprints internally and allows</st> <st c=\"54024\">customization just by adding custom Blueprints in its Airflow directory.</st> <st c=\"54098\">However, the main goal of this</st> <st c=\"54129\">chapter is to use Airflow as an API orchestration tool to run sets of workflow activities that consume API services</st> <st c=\"54245\">for resources.</st>\n\t\t\t<st c=\"54259\">Let us begin with the installation of the Airflow</st> <st c=\"54310\">2.x platform.</st>\n\t\t\t<st c=\"54323\">Installing and configuring Airflow 2.x</st>\n\t\t\t<st c=\"54362\">There is no direct Airflow 2.x installation for</st> <st c=\"54410\">the Windows platform yet.</st> <st c=\"54437\">But there is a Docker image that can run Airflow on Windows and operating systems with low</st> <st c=\"54528\">memory resources.</st> <st c=\"54546\">Our approach was to install Airflow directly on WSL2 (Ubuntu) through Windows PowerShell and also use Ubuntu to implement our Flask application for</st> <st c=\"54694\">this topic.</st>\n\t\t\t<st c=\"54705\">Now, follow the</st> <st c=\"54722\">next procedures:</st>\n\n\t\t\t\t1.  <st c=\"54738\">For Windows users, run the</st> `<st c=\"54766\">wsl</st>` <st c=\"54769\">command on PowerShell and log in to its home account using the</st> *<st c=\"54833\">WSL credentials</st>*<st c=\"54848\">.</st>\n\t\t\t\t2.  <st c=\"54849\">Then, run the</st> `<st c=\"54864\">cd ~</st>` <st c=\"54868\">Linux command to ensure all installations happen in the</st> <st c=\"54925\">home directory.</st>\n\t\t\t\t3.  <st c=\"54940\">After installing Python 11.x and all its required Ubuntu libraries, create a virtual environment (for example,</st> `<st c=\"55052\">ch08-airflow-env</st>`<st c=\"55068\">) using the</st> `<st c=\"55081\">python3 -m venv</st>` <st c=\"55096\">command for the</st> `<st c=\"55113\">airflow</st>` <st c=\"55120\">module installation.</st>\n\t\t\t\t4.  <st c=\"55141\">Activate the virtual environment by running the</st> `<st c=\"55190\">source <</st>``<st c=\"55198\">venv_folder>/bin/activate</st>` <st c=\"55224\">command.</st>\n\t\t\t\t5.  <st c=\"55233\">Next, find a directory in the system that can be the Airflow core directory where all Airflow configurations and customizations happen.</st> <st c=\"55370\">In our case, it is the</st> `<st c=\"55393\">/</st>``<st c=\"55394\">mnt/c/Alibata/Development/Server/Airflow</st>` <st c=\"55434\">folder.</st>\n\t\t\t\t6.  <st c=\"55442\">Open the</st> `<st c=\"55452\">bashrc</st>` <st c=\"55458\">configuration file and add the</st> `<st c=\"55490\">AIRFLOW_HOME</st>` <st c=\"55502\">variable with the Airflow core directory path.</st> <st c=\"55550\">The following is a sample of registering</st> <st c=\"55591\">the variable:</st>\n\n    ```", "```py\n     pip install apache-airflow\n    ```", "```py\n\n    \t\t\t\t7.  <st c=\"55748\">Initialize its metadata database and generate configuration files in the</st> `<st c=\"55822\">AIRFLOW_HOME</st>` <st c=\"55834\">directory using the</st> `<st c=\"55855\">airflow db</st>` `<st c=\"55866\">migrate</st>` <st c=\"55873\">command.</st>\n\t\t\t\t8.  <st c=\"55882\">Create an administrator</st> <st c=\"55907\">account for its UI dashboard using the</st> <st c=\"55946\">following command:</st> `<st c=\"55965\">airflow users create --username <user> --password <pass> --firstname <fname> --lastname <lname> --role Admin --email <xxxx@yyyy.com></st>`<st c=\"56097\">. The role value should</st> <st c=\"56121\">be</st> `<st c=\"56124\">Admin</st>`<st c=\"56129\">.</st>\n\t\t\t\t9.  <st c=\"56130\">Verify if the user account is added to its database using the</st> `<st c=\"56193\">airflow users</st>` `<st c=\"56207\">list</st>` <st c=\"56211\">command.</st>\n\t\t\t\t10.  <st c=\"56220\">At this point, log in to the</st> *<st c=\"56250\">root account</st>* <st c=\"56262\">and activate the virtual environment using</st> `<st c=\"56306\">root</st>`<st c=\"56310\">. Run the scheduler using the</st> `<st c=\"56340\">airflow</st>` `<st c=\"56348\">scheduler</st>` <st c=\"56357\">command.</st>\n\t\t\t\t11.  <st c=\"56366\">With the root account, run the server using the</st> `<st c=\"56415\">airflow webserver --port 8080</st>` <st c=\"56444\">command.</st> <st c=\"56454\">Port</st> `<st c=\"56459\">8080</st>` <st c=\"56463\">is its</st> <st c=\"56471\">default port.</st>\n\t\t\t\t12.  <st c=\"56484\">Lastly, access the Airflow portal at</st> `<st c=\"56522\">http://localhost:8080</st>` <st c=\"56543\">and use your</st> `<st c=\"56557\">Admin</st>` <st c=\"56562\">account to log in to</st> <st c=\"56584\">the dashboard.</st>\n\n\t\t\t*<st c=\"56598\">Figure 8</st>**<st c=\"56607\">.9</st>* <st c=\"56609\">shows the home dashboard of</st> <st c=\"56638\">Airflow 2.x:</st>\n\t\t\t![Figure 8.9 – The home page of the Airflow 2.x UI](img/B19383_08_009.jpg)\n\n\t\t\t<st c=\"57451\">Figure 8.9 – The home page of the Airflow 2.x UI</st>\n\t\t\t<st c=\"57499\">An Airflow architecture is composed of the</st> <st c=\"57543\">following components:</st>\n\n\t\t\t\t*   **<st c=\"57564\">Web server</st>** <st c=\"57575\">– Runs the UI management dashboard and executes and</st> <st c=\"57628\">monitors tasks.</st>\n\t\t\t\t*   **<st c=\"57643\">Scheduler</st>** <st c=\"57653\">– Checks the status of tasks, updates tasks’ state details in the metadata database, and queues the next</st> <st c=\"57759\">task</st> <st c=\"57764\">for executions.</st>\n\t\t\t\t*   **<st c=\"57779\">Metadata database</st>** <st c=\"57797\">– Stores the states of a task,</st> **<st c=\"57829\">cross-communications</st>** <st c=\"57849\">(</st>**<st c=\"57851\">XComs</st>**<st c=\"57856\">) data, and</st> **<st c=\"57869\">directed acyclic graph</st>** <st c=\"57891\">(</st>**<st c=\"57893\">DAG</st>**<st c=\"57896\">) variables; processes perform read and write</st> <st c=\"57942\">operations in</st> <st c=\"57957\">this database.</st>\n\t\t\t\t*   **<st c=\"57971\">Executor</st>** <st c=\"57980\">– Executes tasks and updates the</st> <st c=\"58014\">metadata database.</st>\n\n\t\t\t<st c=\"58032\">Next, let us create</st> <st c=\"58053\">workflow tasks.</st>\n\t\t\t<st c=\"58068\">Creating tasks</st>\n\t\t\t<st c=\"58083\">Airflow uses DAG files to implement tasks and their sequence flows.</st> <st c=\"58152\">A DAG is a high-level design of the workflow and exclusive</st> <st c=\"58210\">tasks based on their task definitions, schedules, relationships, and dependencies.</st> <st c=\"58294\">Airflow provides the API classes that implement a DAG in Python code.</st> <st c=\"58364\">But, before creating DAG files, open the</st> `<st c=\"58405\">AIRFLOW_HOME</st>` <st c=\"58417\">directory and create a</st> `<st c=\"58441\">dags</st>` <st c=\"58445\">sub-folder inside it.</st> *<st c=\"58468\">Figure 8</st>**<st c=\"58476\">.10</st>* <st c=\"58479\">shows our Airflow core directory with the created</st> `<st c=\"58530\">dags</st>` <st c=\"58534\">folder:</st>\n\t\t\t![Figure 8.10 – Custom dags folder in AIRFLOW_HOME](img/B19383_08_010.jpg)\n\n\t\t\t<st c=\"58764\">Figure 8.10 – Custom dags folder in AIRFLOW_HOME</st>\n\t\t\t<st c=\"58812\">One of the files in our</st> `<st c=\"58837\">$AIRFLOW_HOME/dag</st>` <st c=\"58854\">directory is</st> `<st c=\"58868\">report_login_count_dag.py</st>`<st c=\"58893\">, which builds a sequence flow composed of two orchestrated API executions, each with</st> <st c=\"58979\">service tasks.</st> *<st c=\"58994\">Figure 8</st>**<st c=\"59002\">.11</st>* <st c=\"59005\">provides an overview of the</st> <st c=\"59034\">workflow design:</st>\n\t\t\t![Figure 8.11 – An overview of an Airflow DAG](img/B19383_08_011.jpg)\n\n\t\t\t<st c=\"59172\">Figure 8.11 – An overview of an Airflow DAG</st>\n\t\t\t`<st c=\"59215\">DAG</st>` <st c=\"59219\">is an API class from the</st> `<st c=\"59245\">airflow</st>` <st c=\"59252\">module that implements an entire workflow activity.</st> <st c=\"59305\">It is composed of different</st> *<st c=\"59333\">operators</st>* <st c=\"59342\">that represent tasks.</st> <st c=\"59365\">A DAG file can implement more than one DAG if needed.</st> <st c=\"59419\">The following code is the</st> `<st c=\"59445\">DAG</st>` <st c=\"59448\">script in the</st> `<st c=\"59463\">report_login_count_dag.py</st>` <st c=\"59488\">file that implements the workflow depicted in</st> *<st c=\"59535\">Figure 8</st>**<st c=\"59543\">.11</st>*<st c=\"59546\">:</st>\n\n```", "```py\n task1 = <st c=\"60420\">SimpleHttpOperator</st>( <st c=\"60441\">task_id=\"list_all_login\",</st><st c=\"60466\">method=\"GET\",</st><st c=\"60480\">http_conn_id=\"packt_dag\",</st><st c=\"60506\">endpoint=\"/ch08/login/list/all\",</st> headers={\"Content-Type\": \"application/json\"}, <st c=\"60586\">response_check=lambda response:</st> <st c=\"60617\">handle_response(response),</st><st c=\"60644\">dag=dag</st> )\n    task2 = <st c=\"60663\">PythonOperator</st>( <st c=\"60680\">task_id='count_login',</st><st c=\"60702\">python_callable=count_login,</st><st c=\"60731\">provide_context=True,</st><st c=\"60753\">do_xcom_push=True,</st><st c=\"60772\">dag=dag</st> )\n```", "```py\n<st c=\"62852\">def handle_response(response, **context):</st> if response.status_code == 201:\n        print(\"executed API successfully...\")\n        return True\n    else:\n        print(\"executed with errors...\")\n        return False\n```", "```py\n def count_login(<st c=\"63623\">ti, **context</st>): <st c=\"63641\">data = ti.xcom_pull(task_ids=['list_all_login'])</st> if not len(data):\n        raise ValueError('Data is empty') <st c=\"63742\">records_dict = json.loads(data[0])</st> count = len(records_dict[\"records\"]) <st c=\"63814\">ti.xcom_push(key=\"records\", value=count)</st> return count\n    task3 = <st c=\"63876\">SimpleHttpOperator</st>( <st c=\"63897\">task_id='report_count',</st><st c=\"63920\">method=\"GET\",</st><st c=\"63934\">http_conn_id=\"packt_dag\",</st><st c=\"63960\">endpoint=\"/ch08/login/report/count\",</st> data={\"login_count\": \"{{ <st c=\"64023\">task_instance.xcom_pull( task_ids=['list_all_login','count_login'], key='records')[0]</st> }}\"},\n        headers={\"Content-Type\": \"application/json\"},\n        dag=dag\n    )\n    … … … … … … <st c=\"64215\">Task3</st> is also a <st c=\"64232\">SimpleHTTPOperator</st> operator, but its goal is to call an HTTP <st c=\"64293\">GET</st> API and pass a request parameter, <st c=\"64331\">login_count</st>, with a value derived from XCom data. Operators can access Airflow built-in objects, such as <st c=\"64436\">dag_run</st> and <st c=\"64448\">task_instance</st>, using the <st c=\"64473\">{{ }}</st> Jinja2 delimiter. In <st c=\"64500\">Task3</st>, <st c=\"64507\">task_instance</st>, using its <st c=\"64532\">xcom_pull()</st> function, retrieves from the list of tasks the XCom variable records. The result of <st c=\"64628\">xcom_pull()</st> is always a list with the value of the XCom variable at its *<st c=\"64700\">0 index</st>*.\n\t\t\t<st c=\"64708\">The last portion of the DAG file is where to place the sequence flow of the DAG’s task.</st> <st c=\"64797\">There are two ways to establish dependency from one task to another.</st> `<st c=\"64866\">>></st>`<st c=\"64868\">, or the</st> *<st c=\"64877\">upstream dependency</st>*<st c=\"64896\">, connects a flow from left to right, which means the execution of the task from the right depends on the success of the left task.</st> <st c=\"65028\">The other one,</st> `<st c=\"65043\"><<</st>` <st c=\"65045\">or the</st> *<st c=\"65053\">downstream dependency</st>*<st c=\"65074\">, follows the</st> <st c=\"65087\">reverse flow.</st> <st c=\"65102\">If two or more tasks depend on the same task, brackets enclose those dependent tasks, such as the</st> `<st c=\"65200\">task1 >> [task2, task3]</st>` <st c=\"65223\">flow, where</st> `<st c=\"65236\">task2</st>` <st c=\"65241\">and</st> `<st c=\"65246\">task3</st>` <st c=\"65251\">are dependent tasks of</st> `<st c=\"65275\">task1</st>`<st c=\"65280\">. In the given DAG file, it is just a sequential flow from</st> `<st c=\"65339\">task1</st>` <st c=\"65344\">to</st> `<st c=\"65348\">task4</st>`<st c=\"65353\">.</st>\n\t\t\t<st c=\"65354\">What executes our tasks are called</st> *<st c=\"65390\">executors</st>*<st c=\"65399\">. The</st> <st c=\"65405\">default executor is</st> `<st c=\"65425\">SequentialExecutor</st>`<st c=\"65443\">, which runs the task flows one task at a time.</st> `<st c=\"65491\">LocalExecutor</st>` <st c=\"65504\">runs the workflow sequentially, but the tasks may run in parallel mode.</st> <st c=\"65577\">There is</st> `<st c=\"65586\">CeleryExecutor</st>`<st c=\"65600\">, which runs workflows composed of Celery tasks, and</st> `<st c=\"65653\">KubernetesExecutor</st>`<st c=\"65671\">, which runs tasks on</st> <st c=\"65693\">a cluster.</st>\n\t\t\t<st c=\"65703\">To deploy and re-deploy the DAG files,</st> *<st c=\"65743\">restart</st>* <st c=\"65750\">the scheduler and the web server.</st> <st c=\"65785\">Let us now implement an API endpoint function that will run the DAG deployed in the</st> <st c=\"65869\">Airflow server.</st>\n\t\t\t<st c=\"65884\">Utilizing Airflow built-in REST endpoints</st>\n\t\t\t<st c=\"65926\">To trigger the DAG is to run the workflow.</st> <st c=\"65970\">Running</st> <st c=\"65978\">a DAG requires using the Airflow UI’s DAG page, applying Airflow APIs for console-based triggers, or consuming Airflow’s built-in REST API with the Flask application or Postman.</st> <st c=\"66156\">This chapter implemented the</st> `<st c=\"66185\">ch08-airflow</st>` <st c=\"66197\">project to provide the</st> `<st c=\"66221\">report_login_count</st>` <st c=\"66239\">DAG with API endpoints for</st> `<st c=\"66267\">Task1</st>` <st c=\"66272\">and</st> `<st c=\"66277\">Task3</st>` <st c=\"66282\">executions and also to trigger the workflow using some Airflow REST endpoints.</st> <st c=\"66362\">The following is a custom endpoint function that triggers the</st> `<st c=\"66424\">report_login_count</st>` <st c=\"66442\">DAG with a</st> `<st c=\"66454\">dag_run_id</st>` <st c=\"66464\">value of a</st> <st c=\"66476\">UUID type:</st>\n\n```", "```py\n\n\t\t\t<st c=\"67085\">Airflow requires</st> *<st c=\"67103\">Basic authentication</st>* <st c=\"67123\">before consuming its REST endpoints.</st> <st c=\"67161\">Any REST access must include an</st> `<st c=\"67193\">Authorization</st>` <st c=\"67206\">header with</st> <st c=\"67219\">the generated token of a valid username and password.</st> <st c=\"67273\">Also, install a REST client module, such as</st> `<st c=\"67317\">requests</st>`<st c=\"67325\">, to consume the API libraries.</st> <st c=\"67357\">Running</st> `<st c=\"67365\">/api/v1/dags/report_login_count/dagRuns</st>` <st c=\"67404\">with an HTTP</st> `<st c=\"67418\">POST</st>` <st c=\"67422\">request will give us a JSON response</st> <st c=\"67460\">like this:</st>\n\n```", "```py\n\n\t\t\t<st c=\"67932\">Then, running the following</st> <st c=\"67961\">Airflow REST endpoint using the same</st> `<st c=\"67998\">dag_run_id</st>` <st c=\"68008\">value will provide us with the result of</st> <st c=\"68050\">the workflow:</st>\n\n```", "```py\n\n\t\t\t<st c=\"68676\">The given HTTP</st> `<st c=\"68692\">GET</st>` <st c=\"68695\">request API</st> <st c=\"68707\">will provide us a JSON result</st> <st c=\"68738\">like so:</st>\n\n```", "```py\n\n\t\t\t<st c=\"68816\">Airflow is a big platform that can offer us many solutions, especially in building pipelines of tasks for data transformation, batch processing, and data analytics.</st> <st c=\"68981\">Its strength is also in implementing API orchestration for microservices.</st> <st c=\"69055\">But for complex, long-running, and distributed workflow transactions, it is Temporal.io that can provide durable, reliable, and</st> <st c=\"69183\">scalable solutions.</st>\n\t\t\t<st c=\"69202\">Implementing workflows using Temporal.io</st>\n\t\t\t<st c=\"69243\">The</st> **<st c=\"69248\">Temporal.io</st>** <st c=\"69259\">server manages loosely coupled workflows and activities, those not limited by the architecture of the Temporal.io platform.</st> <st c=\"69384\">Thus, all workflow components are coded from the ground up</st> <st c=\"69443\">without hooks and callable methods appearing in the implementation.</st> <st c=\"69511\">The server expects the execution of activities rather than tasks.</st> <st c=\"69577\">In BPMN, an activity is more complex than a task.</st> <st c=\"69627\">The server is responsible for building a fault-tolerant workflow because it can recover failed activity execution by restarting its execution from</st> <st c=\"69774\">the start.</st>\n\t\t\t<st c=\"69784\">So, let us begin this topic with the Temporal.io</st> <st c=\"69834\">server setup.</st>\n\t\t\t<st c=\"69847\">Setting up the environment</st>\n\t\t\t<st c=\"69874\">The Temporal.io server has an installer for</st> *<st c=\"69919\">macOS</st>*<st c=\"69924\">,</st> *<st c=\"69926\">Windows</st>*<st c=\"69933\">, and</st> *<st c=\"69939\">Linux</st>* <st c=\"69944\">platforms.</st> <st c=\"69956\">For Windows users, download the ZIP file from the</st> [<st c=\"70006\">https://temporal.download/cli/archive/latest?platform=windows&arch=amd64</st>](https://temporal.download/cli/archive/latest?platform=windows&arch=amd64) <st c=\"70078\">link.</st> <st c=\"70085\">Then, unzip the file to the local</st> <st c=\"70119\">machine.</st> <st c=\"70128\">Start the server using the</st> `<st c=\"70155\">temporal server</st>` `<st c=\"70171\">start-dev</st>` <st c=\"70180\">command.</st>\n\t\t\t<st c=\"70189\">Now, to integrate our Flask application with the server, install the</st> `<st c=\"70259\">temporalio</st>` <st c=\"70269\">module to the virtual environment using the</st> `<st c=\"70314\">pip</st>` <st c=\"70317\">command.</st> <st c=\"70327\">Establish a server connection in the</st> `<st c=\"70364\">main.py</st>` <st c=\"70371\">module of the application using the</st> `<st c=\"70408\">Client</st>` <st c=\"70414\">class of the</st> `<st c=\"70428\">temporalio</st>` <st c=\"70438\">module.</st> <st c=\"70447\">The following</st> `<st c=\"70461\">main.py</st>` <st c=\"70468\">script shows how to instantiate a</st> `<st c=\"70503\">Client</st>` <st c=\"70509\">instance:</st>\n\n```", "```py\n\n\t\t\t<st c=\"70844\">The</st> `<st c=\"70849\">connect_temporal()</st>` <st c=\"70867\">method instantiates the Client API class and creates a</st> `<st c=\"70923\">temporal_client</st>` <st c=\"70938\">environment variable in the Flask platform for the API endpoints to run the workflow.</st> <st c=\"71025\">Since</st> `<st c=\"71031\">main.py</st>` <st c=\"71038\">is the entry point module, an event loop will execute the method during the Flask</st> <st c=\"71121\">server startup.</st>\n\t\t\t<st c=\"71136\">After setting up the Temporal.io server and its connection to the Flask application, let us discuss the distinct approach to</st> <st c=\"71261\">workflow implementation.</st>\n\t\t\t<st c=\"71286\">Implementing activities and a workflow</st>\n\t\t\t<st c=\"71325\">Temporal uses the code-first approach of implementing a workflow and its activities.</st> <st c=\"71411\">Activities in a Temporal platform must be</st> *<st c=\"71453\">idempotent</st>*<st c=\"71463\">, meaning its parameters and results are non-changing through the</st> <st c=\"71529\">course or history of its executions.</st> <st c=\"71566\">The following is an example of a complex but</st> <st c=\"71611\">idempotent activity:</st>\n\n```", "```py\n<st c=\"72810\">@workflow.defn(sandboxed=False)</st> class ReserveAppointmentWorkflow():\n    def __init__(self) -> None: <st c=\"73046\">@workflow.defn</st> decorator implements a workflow. An example is our <st c=\"73112\">ReserveAppointmentWorkflow</st> class.\n\t\t\t<st c=\"73145\">It maintains the same execution state starting from the beginning, making it a deterministic workflow.</st> <st c=\"73249\">It also manages all its states through replays to determine some exceptions and provide recovery after a</st> <st c=\"73354\">non-deterministic state.</st>\n\t\t\t<st c=\"73378\">Moreover, Temporal workflows are designed to run continuously without time limits but with proper scheduling to handle long-running and complex activities.</st> <st c=\"73535\">However, using threads for concurrency is not allowed in Temporal workflows.</st> <st c=\"73612\">It must have an instance method decorated by</st> `<st c=\"73657\">@workflow.run</st>` <st c=\"73670\">to create a continuous loop for its activities.</st> <st c=\"73719\">The following</st> `<st c=\"73733\">run()</st>` <st c=\"73738\">method accepts a request model with appointment details from the user and loops until the cancellation of the appointment, where</st> `<st c=\"73868\">appointmentwf.status</st>` <st c=\"73888\">becomes</st> `<st c=\"73897\">False</st>`<st c=\"73902\">:</st>\n\n```", "```py\n <st c=\"75220\">@workflow.query</st> def details(self) -> AppointmentWf:\n        return self.appointmentwf\n```", "```py\n<st c=\"75924\">from temporalio.client import Client</st>\n<st c=\"75961\">from temporalio.worker import Worker,</st> <st c=\"75999\">UnsandboxedWorkflowRunner</st> import asyncio\nfrom modules.admin.activities.workflow import reserve_schedule, close_schedule\nfrom modules.models.workflow import appt_queue_id\nfrom modules.workflows.transactions import ReserveAppointmentWorkflow\nasync def main(): <st c=\"76258\">client = await Client.connect(\"localhost:7233\")</st> worker = <st c=\"76315\">Worker(</st><st c=\"76322\">client,</st><st c=\"76330\">task_queue=appt_queue_id,</st><st c=\"76356\">workflows=[ReserveAppointmentWorkflow],</st><st c=\"76396\">activities=[reserve_schedule, close_schedule],</st><st c=\"76443\">workflow_runner=UnsandboxedWorkflowRunner,</st> ) <st c=\"76489\">await worker.run()</st> if __name__ == \"__main__\":\n    print(\"Temporal worker started…\") <st c=\"76679\">Worker</st> instance needs to know what workflows to queue and activities to run before the client application triggers their executions. Now, passing the <st c=\"76829\">UnsandboxedWorkflowRunner</st> object to the <st c=\"76869\">workflow_runner</st> parameter indicates that our worker will be running as an independent Python application outside the context of our Flask platform or any sandbox environment, thus the setting of the <st c=\"77068\">sandboxed</st> parameter in the <st c=\"77095\">@workflow.defn</st> decorator of every workflow class to <st c=\"77147\">False</st>. To run the worker, call and await the <st c=\"77192\">run()</st> method of the <st c=\"77212\">Worker</st> instance.\n\t\t\t<st c=\"77228\">Lastly, after implementing the</st> <st c=\"77259\">workflows, activities, and the worker, it is time to trigger the workflow</st> <st c=\"77334\">for execution.</st>\n\t\t\t<st c=\"77348\">Running activities</st>\n\t\t\t<st c=\"77367\">The</st> `<st c=\"77372\">ch08-temporal</st>` <st c=\"77385\">project is a</st> <st c=\"77398\">RESTful application in Flask, so to run a workflow, an API endpoint must import and use</st> `<st c=\"77487\">app.temporal_client</st>` <st c=\"77506\">to connect to the server and to invoke the</st> `<st c=\"77550\">start_workflow()</st>` <st c=\"77566\">method that will trigger the</st> <st c=\"77596\">workflow execution.</st>\n\t\t\t<st c=\"77615\">The</st> `<st c=\"77620\">start_workflow()</st>` <st c=\"77636\">method requires the workflow’s “</st>*<st c=\"77669\">run</st>*<st c=\"77673\">” method, the single model object parameter, the unique workflow ID, and</st> `<st c=\"77747\">task_queue</st>`<st c=\"77757\">. The following API endpoint triggers the execution of our</st> `<st c=\"77816\">ReserveAppointmentWorkflow</st>` <st c=\"77842\">class:</st>\n\n```", "```py\n\n\t\t\t<st c=\"78287\">After a successful workflow trigger, another API can query the details or results of the workflow by extracting the workflow’s</st> `<st c=\"78415\">WorkflowHandler</st>` <st c=\"78430\">class from the client using its</st> *<st c=\"78463\">workflow ID</st>*<st c=\"78474\">. The following endpoint function shows how to retrieve the result of the</st> <st c=\"78548\">completed workflow:</st>\n\n```", "```py\n\n\t\t\t<st c=\"79137\">To prove that Temporal workflows</st> <st c=\"79170\">can respond to cancellation events, the following API invokes the</st> `<st c=\"79237\">cancel()</st>` <st c=\"79245\">method from the</st> `<st c=\"79262\">WorkflowHandler</st>` <st c=\"79277\">class for its workflow to throw a</st> `<st c=\"79312\">CancelledError</st>` <st c=\"79326\">exception, leading to the execution of the</st> `<st c=\"79370\">close_schedule()</st>` <st c=\"79386\">activity:</st>\n\n```", "```py\n\n\t\t\t<st c=\"79728\">Although there is still a lot to discuss about the architecture and the behavior of these big-time workflow</st> <st c=\"79836\">solutions, the main goal is to highlight the feasibility of integrating different workflow engines into the asynchronous Flask platform and take into consideration workarounds for integrations to work with</st> <st c=\"80043\">Flask applications.</st>\n\t\t\t<st c=\"80062\">Summary</st>\n\t\t\t<st c=\"80070\">This chapter proved that</st> `<st c=\"80096\">Flask[async]</st>` <st c=\"80108\">can work with different workflow engines, starting with Celery tasks.</st> `<st c=\"80179\">Flask[async]</st>`<st c=\"80191\">, combined with the workflows created by Celery’s signatures and primitives, works well in building chained, grouped, and</st> <st c=\"80313\">chorded processes.</st>\n\t\t\t<st c=\"80331\">Then,</st> `<st c=\"80338\">Flask[async]</st>` <st c=\"80350\">was proven to work with SpiffWorkflow for some BPMN serialization that focuses on</st> `<st c=\"80433\">UserTask</st>` <st c=\"80441\">and</st> `<st c=\"80446\">ScriptTask</st>` <st c=\"80456\">tasks.</st> <st c=\"80464\">Also, this chapter even considered solving BPMN enterprise problems using the Zeebe/Camunda platform that showcases</st> `<st c=\"80580\">ServiceTask</st>` <st c=\"80591\">tasks.</st>\n\t\t\t<st c=\"80598\">Moreover,</st> `<st c=\"80609\">Flask[async]</st>` <st c=\"80621\">created an environment with Airflow 2.x to implement pipelines of tasks building an API orchestration.</st> <st c=\"80725\">In the last part, the chapter established the integration between</st> `<st c=\"80791\">Flask[async]</st>` <st c=\"80803\">and Temporal.io and demonstrated the implementation of deterministic and</st> <st c=\"80877\">distributed workflows.</st>\n\t\t\t<st c=\"80899\">This chapter provided a clear picture of the extensibility, usability, and scalability of the Flask framework in building scientific and big data applications and even BPMN-related and ETL-involved</st> <st c=\"81098\">business processes.</st>\n\t\t\t<st c=\"81117\">The next chapter will discuss the different authentication and authorization mechanisms to secure</st> <st c=\"81216\">Flask applications.</st>\n\n```", "```py\n\n```", "```py\n\n```", "```py\n\n```", "```py\n\n```", "```py\n\n```", "```py\n\n```", "```py\n\n```", "```py\n\n```", "```py\n\n```", "```py\n\n```"]