<html><head></head><body>
  <div id="_idContainer077">
    <h1 class="chapterNumber">10</h1>
    <h1 id="_idParaDest-165" class="chapterTitle">Deploying on AWS</h1>
    <p class="normal">In the previous chapters, we ran our different microservices directly in the host operating system, as it is sometimes the quickest way to get started, while also being a useful approach in general—especially for smaller installations or development where everything can be contained in a virtual environment. However, if the application requires a database or a compiled extension, then things start to be tightly coupled to the operating system and version. Other developers with slightly different systems will start to run into problems, and the more differences between a development environment and a production one, the more trouble you will have when releasing your software.</p>
    <p class="normal"><strong class="keyword">Virtual Machines</strong> (<strong class="keyword">VMs</strong>) can be a good<a id="_idIndexMarker599"/> solution, as they provide an isolated environment in which to run your code. A VM is essentially a piece of software pretending to be a real computer, in which there is a real operating system running in the pretend computer. If you've ever used an Amazon EC2 instance or a Google Compute Engine instance, then you have used a virtual machine. It's possible to run them locally using tools such as VMware or VirtualBox.</p>
    <p class="normal">However, VMs are heavyweights, precisely because they emulate a full computer. Using one from scratch involves installing an operating<a id="_idIndexMarker600"/> system or using a tool such as HashiCorp's Packer (<a href="https://www.packer.io/"><span class="url">https://www.packer.io/</span></a>) to build a disk image—the sort of thing that comes prebuilt for you when selecting an AWS or GCP instance.</p>
    <p class="normal">The big revolution came with <strong class="keyword">Docker</strong>, an open-source virtualization tool first released in 2013. Docker allows<a id="_idIndexMarker601"/> the use of isolated environments called <em class="italic">containers</em> to run applications in a very portable way.</p>
    <p class="normal">Cloud computing providers, such as Amazon Web Services (AWS), Google Cloud, and Microsoft Azure, allow people to rent space on their computers and make creating virtual machines and containers much easier. Provisioning these cloud resources, along with attached storage and databases, can be done with a few mouse clicks, or a few commands typed in a terminal. They can also be configured using configuration files to describe resources, using<a id="_idIndexMarker602"/> Infrastructure-as-Code tools, such as HashiCorp's <strong class="keyword">Terraform</strong>.</p>
    <p class="normal">In this chapter, we present Docker and explain how to run Quart-based microservices with it. We then cover deploying a container-based application using some common orchestration tools, such as Docker Compose, Docker Swarm, and, briefly, Kubernetes. Many of these topics could fill entire books by themselves, so this chapter will be an overview that relies on the installation instructions provided by the tools themselves to get started.</p>
    <p class="normal">Most of the cloud computing providers will also have their own versions of these tools, modified to better integrate with the other services that they offer. If you are already using a particular company's services, it is worth investigating their tools. At the same time, it is also worth knowing the cloud-agnostic versions, as taking a more independent approach can make migrating from one provider to another much easier.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Note that some of the instructions in this chapter may result in incurring a charge from AWS. While we will keep those costs to a minimum, it is important to understand what costs may be incurred by checking with AWS and also to unsubscribe from any unused resources after trying things out.</p>
    </div>
    <h1 id="_idParaDest-166" class="title">What is Docker?</h1>
    <p class="normal">The <strong class="keyword">Docker</strong> (<a href="https://www.docker.com/"><span class="url">https://www.docker.com/</span></a>) project is a <em class="italic">container</em> platform, which<a id="_idIndexMarker603"/> lets you run your applications<a id="_idIndexMarker604"/> in isolated environments. Using the Linux feature called <code class="Code-In-Text--PACKT-">cgroups</code> (<a href="https://en.wikipedia.org/wiki/Cgroups"><span class="url">https://en.wikipedia.org/wiki/Cgroups</span></a>), Docker creates<a id="_idIndexMarker605"/> isolated environments called containers that run on Linux without a VM. On macOS and Windows, installing Docker will create a lightweight VM for you to run containers in, although this is a seamless process. This means that macOS, Windows, and Linux users can all develop container-based applications without worrying about any interoperability trouble and deploy them to a Linux server where they will run natively.</p>
    <p class="normal">Today, Docker is almost synonymous with containers, but there<a id="_idIndexMarker606"/> are other container runtimes, such as <strong class="keyword">CRI-O</strong> (<a href="https://cri-o.io/"><span class="url">https://cri-o.io/</span></a>), and historical projects<a id="_idIndexMarker607"/> such as <strong class="keyword">rkt</strong> and <strong class="keyword">CoreOS</strong> that, together with<a id="_idIndexMarker608"/> Docker, helped shape the standardized ecosystem that we have today.</p>
    <p class="normal">Because containers do not rely on emulation when running on Linux, there is little performance difference between running code inside a container and outside. As there is an emulation layer on macOS and Windows, while it is possible to run containers in production on these platforms, there is little benefit to doing so. It is possible to package up everything needed to run an application inside a container image and distribute it for use anywhere that can run a container.</p>
    <p class="normal">As a Docker<a id="_idIndexMarker609"/> user, you just need to choose which image you want to run, and Docker does all the heavy lifting by interacting with the Linux kernel. An image in this context is the sum of all the instructions required to create a set of running processes on top of a Linux kernel, to run one container. An image includes all the resources necessary to run a Linux distribution. For instance, you can run whatever version of Ubuntu you want in a Docker container even if the host OS is of a different distribution.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">As containers operate best on a Linux-based system, the rest of this chapter assumes that everything is installed under a Linux distribution, such as Ubuntu.</p>
    </div>
    <p class="normal">We used Docker in <em class="chapterRef">Chapter 5</em>, <em class="italic">Splitting the Monolith</em>, when discussing metrics and monitoring, so you may already have Docker installed. With some older Linux distributions, you may have a very old version of Docker available. Installing a newer one directly from Docker itself is a good idea, to get the latest features and security patches. If you have a Docker installation, feel free to jump directly to the next section of this chapter, <em class="italic">Introduction to Docker</em>. If not, you can visit <a href="https://www.docker.com/get-docker"><span class="url">https://www.docker.com/get-docker</span></a> to download it and find the installation instructions. The community<a id="_idIndexMarker610"/> edition is good enough for building, running, and installing containers. Installing Docker on Linux is straightforward—you can probably find a package for your Linux distribution.</p>
    <p class="normal">For macOS, if you have Homebrew (<a href="https://brew.sh"><span class="url">https://brew.sh</span></a>) installed, then you can<a id="_idIndexMarker611"/> simply use <code class="Code-In-Text--PACKT-">brew install docker</code>. Otherwise, follow the instructions on Docker's website. Under Windows, Docker can either use the <strong class="keyword">Windows Subsystem for Linux</strong> (<strong class="keyword">WSL2</strong>), or the built-in Hyper-V to<a id="_idIndexMarker612"/> run a virtual machine. We recommend WSL, as it is the most straightforward to get working.</p>
    <p class="normal">If the installation was successful, you should be able to run the <code class="Code-In-Text--PACKT-">docker</code> command in your shell. Try the <code class="Code-In-Text--PACKT-">version</code> command to verify your installation, like this:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> docker version
Client:
 Cloud integration: 1.0.14
 Version:       	20.10.6
 API version:   	1.41
 Go version:    	go1.16.3
 Git commit:    	370c289
 Built:         	Fri Apr  9 22:46:57 2021
 OS/Arch:       	darwin/amd64
 Context:       	default
 Experimental:  	true
Server: Docker Engine - Community
 Engine:
  Version:      	20.10.6
  API version:  	1.41 (minimum version 1.12)
  Go version:   	go1.13.15
  Git commit:   	8728dd2
  Built:        	Fri Apr  9 22:44:56 2021
  OS/Arch:      	linux/amd64
  Experimental: 	false
 containerd:
  Version:      	1.4.4
  GitCommit:    	05f951a3781f4f2c1911b05e61c160e9c30eaa8e
 runc:
  Version:      	1.0.0-rc93
  GitCommit:    	12644e614e25b05da6fd08a38ffa0cfe1903fdec
 docker-init:
  Version:      	0.19.0
  GitCommit:    	de40ad0
</code></pre>
    <p class="normal">A Docker installation is composed of a Docker Engine, which controls the running containers<a id="_idIndexMarker613"/> and a command-line interface. It also includes Docker Compose, which is a way of arranging multiple containers that will work together, as well as Kubernetes, an orchestration tool for deploying and managing container-based applications.</p>
    <p class="normal">The engine provides an HTTP API, which can be reached locally through a UNIX socket (usually, <code class="Code-In-Text--PACKT-">/var/run/docker.sock</code>) or through the network. This means it is possible to control a Docker Engine that is running on a different computer to the Docker client, or orchestration tooling.</p>
    <p class="normal">Now that Docker is installed on your system, let's discover how it works.</p>
    <h1 id="_idParaDest-167" class="title">Introduction to Docker</h1>
    <p class="normal">Let's experiment<a id="_idIndexMarker614"/> with Docker containers. Running a container that you can enter commands in is as simple as the following:</p>
    <pre class="programlisting con"><code class="hljs-con">docker run --interactive --tty ubuntu:20.04 bash
</code></pre>
    <p class="normal">With this command, we are telling Docker to run the Ubuntu image, which will be fetched from Docker Hub, a central registry of public images. We are providing a tag of <code class="Code-In-Text--PACKT-">20.04</code> after the image name so that we download the container image that represents the Ubuntu 20.04 operating system. This won't contain everything that a regular Ubuntu installation has, but anything that's missing is installable.</p>
    <p class="normal">We also tell Docker to run interactively—the <code class="Code-In-Text--PACKT-">-i</code> argument—and to assign a <code class="Code-In-Text--PACKT-">tty</code> with the <code class="Code-In-Text--PACKT-">-t</code> argument, so that we can type commands inside the container. By default, Docker assumes that you want to start a container that runs in the background, serving requests. By using these two options and asking that the command <code class="Code-In-Text--PACKT-">bash</code> is run inside the container, we can get a shell that we can use just like a Linux shell, outside the container.</p>
    <p class="normal">Every existing Linux distribution out there provides a base image, not just Ubuntu. There are also pared-down base images for running Python, Ruby, or other environments, and base Linux images, such as Alpine, which aim to be even smaller. The size of the image is important because every time you want to update it or run it in a new place, it must be downloaded. Alpine is a little over 5MB in size, whereas the <code class="Code-In-Text--PACKT-">ubuntu:20.04</code> image is nearly 73MB. You can compare sizes and manage the images your Docker Engine knows about with the following commands – the second command will remove any local copy of the <code class="Code-In-Text--PACKT-">ubuntu:20.04</code> image, so if you run that, you will need to download that image again to use it:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> docker images
REPOSITORY        	TAG      	IMAGE ID   	CREATED    	SIZE
python            	3.9      	a6a0779c5fb2   2 days ago 	886MB
ubuntu            	20.04    	7e0aa2d69a15   3 weeks ago	72.7MB
alpine            	latest   	6dbb9cc54074   4 weeks ago	5.61MB
<span class="hljs-con-meta">$</span> docker rmi ubuntu:20.04
Untagged: ubuntu:20.04
Untagged: ubuntu@sha256:cf31af331f38d1d7158470e095b132acd126a7180a54f263d386da88eb681d93
Deleted: sha256:7e0aa2d69a153215c790488ed1fcec162015e973e49962d438e18249d16fa9bd
Deleted: sha256:3dd8c8d4fd5b59d543c8f75a67cdfaab30aef5a6d99aea3fe74d8cc69d4e7bf2
Deleted: sha256:8d8dceacec7085abcab1f93ac1128765bc6cf0caac334c821e01546bd96eb741
Deleted: sha256:ccdbb80308cc5ef43b605ac28fac29c6a597f89f5a169bbedbb8dec29c987439
</code></pre>
    <p class="normal">You might think that the size means the Ubuntu image is always a better choice than the Python base image, but the Ubuntu image doesn't contain Python, and so to use it we must build an image that contains everything we need and install our own software on top of that. Rather<a id="_idIndexMarker615"/> than do all of this set up by hand, we can use a <strong class="keyword">Dockerfile</strong> (<a href="https://docs.docker.com/engine/reference/builder/"><span class="url">https://docs.docker.com/engine/reference/builder/</span></a>).</p>
    <p class="normal">The standard name for these Docker configuration files is a Dockerfile, and the following is a basic example of one:</p>
    <pre class="programlisting code"><code class="hljs-code">FROM ubuntu:20.04
RUN apt-get update &amp;&amp; apt-get install -y python3
CMD ["bash"]
</code></pre>
    <p class="normal">A Dockerfile is a text<a id="_idIndexMarker616"/> file with a set of instructions. Each line starts with the instruction in uppercase, followed by its arguments. In our example, there are these three instructions:</p>
    <ul>
      <li class="bullet"><code class="Code-In-Text--PACKT-">FROM</code>: Points to the base image to use</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">RUN</code>: Runs the commands in the container once the base image is installed</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">CMD</code>: The command to run when the container is executed by Docker</li>
    </ul>
    <p class="normal">Now we should build our image and give it a useful name so that we can refer to it later on. Here we will run docker build and tag the new image with the name <code class="Code-In-Text--PACKT-">ubuntu-with-python</code>, while using the current directory for a build environment – by default, this is also where <code class="Code-In-Text--PACKT-">docker</code> <code class="Code-In-Text--PACKT-">build</code> looks for a Dockerfile:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> docker build -t ubuntu-with-python .
[+] Building 7.9s (6/6) FINISHED
 =&gt; [internal] load build definition from Dockerfile              	0.0s
 =&gt; =&gt; transferring dockerfile: 125B                              	0.0s
 =&gt; [internal] load .dockerignore                                 	0.0s
 =&gt; =&gt; transferring context: 2B                                   	0.0s
 =&gt; [internal] load metadata for docker.io/library/ubuntu:20.04   	0.0s
 =&gt; [1/2] FROM docker.io/library/ubuntu:20.04                     	0.0s
 =&gt; [2/2] RUN apt-get update &amp;&amp; apt-get install -y python3        	7.3s
 =&gt; exporting to image                                            	0.4s
 =&gt; =&gt; exporting layers                                           	0.4s
 =&gt; =&gt; writing image sha256:02602f606721f36e95fbda83af09baaa9f8256e83030197e5df69fd444e5c604                                             	0.0s
 =&gt; =&gt; naming to docker.io/library/ubuntu-with-python             	0.0s
Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them
</code></pre>
    <p class="normal">Now we can run our new image in the same way we ran the Ubuntu image earlier:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> docker run -it ubuntu-with-python bash
root@42b83b0933f4:/# python3
Python 3.8.5 (default, Jan 27 2021, 15:41:15)
[GCC 9.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
<span class="hljs-con-meta">&gt;</span>&gt;&gt;
</code></pre>
    <p class="normal">When Docker creates images, it creates a cache that has every instruction from<a id="_idIndexMarker617"/> the Dockerfile. If you run the <code class="Code-In-Text--PACKT-">build</code> command a second time, without changing the file, it should be done within seconds. Permuting or changing instructions rebuilds the image, starting at the first change. For this reason, a good strategy when writing these files is to sort instructions so that the most stable ones (the ones you rarely change) are at the top.</p>
    <p class="normal">Another good piece of advice is to clean up each instruction. For example, when we run <code class="Code-In-Text--PACKT-">apt-get update</code> and <code class="Code-In-Text--PACKT-">apt-get install</code> above, this downloads a lot of package index files, and the <code class="Code-In-Text--PACKT-">.deb</code> packages that, once installed, we no longer need. </p>
    <p class="normal">We can make our resulting image smaller by cleaning up after ourselves, which must be done in the same <code class="Code-In-Text--PACKT-">RUN</code> command so that the data we are removing is not written out as part of the container's image:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> cat Dockerfile
FROM ubuntu:20.04
RUN apt-get update &amp;&amp; \
	apt-get install -y python3 &amp;&amp; \
	apt-get clean &amp;&amp; \
	rm -fr /var/lib/apt/lists
CMD ["bash"]
<span class="hljs-con-meta">$</span> docker build -t cleaned-ubuntu-python .
<span class="hljs-con-meta">$</span> docker images
REPOSITORY          	TAG     IMAGE ID   	  CREATED          SIZE
cleaned-ubuntu-python 	latest  6bbca8ae76fe   3 seconds ago    112MB
ubuntu-with-python  	latest  dd51cfc39b5a   34 minutes ago   140MB
</code></pre>
    <p class="normal">One great feature that Docker offers is the ability to share, publish, and reuse images with other<a id="_idIndexMarker618"/> developers. Docker Hub (<a href="https://hub.docker.com"><span class="url">https://hub.docker.com</span></a>) is to Docker containers what PyPI is to Python packages.</p>
    <p class="normal">In the previous example, the Ubuntu<a id="_idIndexMarker619"/> base image was pulled from the Hub by Docker, and there are numerous pre-existing images you can use. For instance, if you want to launch a Linux distribution that is tweaked for Python, you can look at the Python page on the official Docker Hub website and pick one (<a href="https://hub.docker.com/_/python/"><span class="url">https://hub.docker.com/_/python/</span></a>).</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">python:version</code> images are Debian-based, and are an excellent starting point for any Python project.</p>
    <p class="normal">The Python<a id="_idIndexMarker620"/> images based on <strong class="keyword">Alpine Linux</strong> are also quite popular, because they produce the smallest images to run Python. They are more than ten times smaller than other images, which means they are much faster<a id="_idIndexMarker621"/> to download and set up for people wanting to run your project in Docker (refer to <a href="http://gliderlabs.viewdocs.io/docker-alpine/"><span class="url">http://gliderlabs.viewdocs.io/docker-alpine/</span></a>).</p>
    <p class="normal">To use Python 3.9 from the Alpine base image, you can create a Dockerfile like this:</p>
    <pre class="programlisting code"><code class="hljs-code">FROM python:3.9-alpine
CMD ["python3.9"]
</code></pre>
    <p class="normal">Building and running this Dockerfile places you in a Python 3.9 shell. The<a id="_idIndexMarker622"/> Alpine set is great if you run a Python application that does not require a lot of system-level dependencies nor any compilation. It is important to note, however, that Alpine has a specific set of compilation tools that are sometimes incompatible with some projects.</p>
    <p class="normal">For a Quart-based microservice project, the slightly larger Debian-based Python images are probably a simpler choice, because of its standard compilation environment and stability. Moreover, once the base image is downloaded, it is cached and reused, so you do not need to download everything again.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Note that it is important to use images from trusted people and organizations on Docker Hub, since anyone can upload an image. Beyond the risk of running malicious code, there's also the problem of using a Linux image that is not up to date with the latest security patches. Docker also supports digitally signing images to help verify that an image is the one you expect, with no modifications.</p>
    </div>
    <h1 id="_idParaDest-168" class="title">Running Quart in Docker</h1>
    <p class="normal">To run a Quart application<a id="_idIndexMarker623"/> in Docker, we can<a id="_idIndexMarker624"/> use the base Python image. From there, installing the app and its dependencies can be done via pip, which is already installed in the Python image.</p>
    <p class="normal">Assuming your project has a <code class="Code-In-Text--PACKT-">requirements.txt</code> file for its pinned dependencies, and a <code class="Code-In-Text--PACKT-">setup.py</code> file that installs the project, creating an image for your project can be done by instructing Docker on how to use the <code class="Code-In-Text--PACKT-">pip</code> command.</p>
    <p class="normal">In the following example, we introduce the <code class="Code-In-Text--PACKT-">COPY</code> command, which will recursively copy files and directories from outside the container into the image. We also add the <code class="Code-In-Text--PACKT-">EXPOSE</code> directive to indicate to anyone running the container that this port should be exposed to the outside world. We still need to connect that exposed port when we run the container with the <code class="Code-In-Text--PACKT-">-p</code> option. Any process inside the container can listen to any ports that it wants to, and communicate with itself using localhost, but anything outside the container won't be able to reach the inside unless that port has been exposed. It's also worth noting that localhost inside the container only refers to the container, not the computer that's hosting the running containers; so, if you need to communicate with other services, you will need to use its real IP address:</p>
    <pre class="programlisting code"><code class="hljs-code">FROM python:3.9
COPY . /app/
RUN pip install -r /app/requirements.txt
RUN pip install /app/
CMD ["hypercorn", "—bind", "0.0.0.0:5000", "myservice:app"]
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">3.9</code> tag here will get the latest Python 3.9 image that was uploaded to Docker Hub. Now we can run our new container, exposing the port it needs:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> docker run -p 5000:5000 quart_basic
[2021-05-15 15:34:56 +0000] [1] [INFO] Running on http://0.0.0.0:5000 (CTRL + C to quit)
<span class="hljs-con-meta">#</span> In another terminal:
<span class="hljs-con-meta">$</span> curl localhost:5000
{}
</code></pre>
    <p class="normal">Press <span class="keyStroke">Ctrl</span> + <span class="keyStroke">C</span> to stop<a id="_idIndexMarker625"/> the container, or from another terminal<a id="_idIndexMarker626"/> window, find the container and tell it to stop:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> docker ps
CONTAINER ID   IMAGE      	COMMAND             	CREATED     	STATUS                    	PORTS                                   	NAMES
040f7f01d90b   quart_basic   "hypercorn —bind 0.…"   2 seconds ago   Up Less than a second   0.0.0.0:5000-&gt;5000/tcp, :::5000-&gt;5000/tcp    stoic_bhabha
<span class="hljs-con-meta">$</span> docker stop 040f7f01d90b
040f7f01d90b
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">COPY</code> command automatically creates the top-level <code class="Code-In-Text--PACKT-">app</code> directory in the container and copies everything from "<code class="Code-In-Text--PACKT-">.</code>" in it. One important detail to remember with the <code class="Code-In-Text--PACKT-">COPY</code> command is that any change to the local directory ("<code class="Code-In-Text--PACKT-">.</code>") invalidates the Docker cache, and builds from that step. To tweak this mechanism, you can create a <code class="Code-In-Text--PACKT-">.dockerignore</code> file where you can list files and directories that should be ignored by Docker, such as the <code class="Code-In-Text--PACKT-">.git</code> directory that stores all the history and metadata about your version control.</p>
    <p class="normal">We are not using a virtual environment inside the container, as we are already in an isolated environment. We also run our Quart application using Hypercorn, a good practice for production use as we discussed in <em class="chapterRef">Chapter 9</em>, <em class="italic">Packaging and Running Python</em>.</p>
    <p class="normal">That is why the <code class="Code-In-Text--PACKT-">CMD</code> instruction, which<a id="_idIndexMarker627"/> tells the container what command to run when it starts, uses <strong class="keyword">Hypercorn</strong>. <code class="Code-In-Text--PACKT-">CMD</code> can take a normal shell command as an argument, but this does get interpreted by the shell inside the container, meaning that it could go wrong if there are<a id="_idIndexMarker628"/> symbols the shell interprets differently, such as <code class="Code-In-Text--PACKT-">*</code> and <code class="Code-In-Text--PACKT-">?</code>. It's much safer to provide a list, in a format<a id="_idIndexMarker629"/> you may be familiar with, if you have ever used the Python subprocess module (<a href="https://docs.python.org/3/library/subprocess.html"><span class="url">https://docs.python.org/3/library/subprocess.html</span></a>) or used exec<a id="_idIndexMarker630"/> system calls.</p>
    <p class="normal">The next thing we need to do is orchestrate different containers so that they can work together. Let's see in the next section how we can do that.</p>
    <h1 id="_idParaDest-169" class="title">Docker-based deployments</h1>
    <p class="normal">Deploying a microservice at scale can be done by running several containers spread across either one<a id="_idIndexMarker631"/> or several instances. When we are developing our application locally, we are limited to what our one desktop or laptop computer can provide; but for a production service, it may run on dozens or hundreds of servers, with each one running a container that is providing different parts of the application. Each of the options for deploying your application in the cloud, or scaling it up to meet your needs, will involve running more instances, to run more containers.</p>
    <p class="normal">The first to examine is Docker Compose, which is aimed at smaller-scale deployments, mostly contained in a single instance, but running multiple containers. This is ideal for a development environment, a staging environment, or a prototype. Other options we will look at are Docker Swarm and Kubernetes, which provide different levels of complexity for someone deploying an application, but also increasing levels of flexibility and power. Both options will also need someone to run cloud instances or bare-metal servers on which to run the containers.</p>
    <p class="normal">Once your Docker image is created, every host that runs a Docker daemon can be used to run as many containers as you want within the limits of the physical resources. We will examine several different options, to gain a broad overview of the features and complexity involved.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">There is no need to over-complicate your initial application. It might be tempting to go with a large Kubernetes cluster, but if your application does not need to scale that way, it's a wasted effort. Use the metrics collected about your application and the knowledge of upcoming business changes to adjust to plan for what you need, not what you might want.</p>
    </div>
    <p class="normal">To experiment with the <strong class="keyword">Terraform</strong>, <strong class="keyword">Docker Swarm</strong>, and <strong class="keyword">Kubernetes</strong> examples in this book and on <a href="https://github.com/PacktPublishing/Python-Microservices-Development-2nd-Edition/tree/main/CodeSample"><span class="url">https://github.com/PacktPublishing/Python-Microservices-Development-2nd-Edition/tree/main/CodeSamples</span></a>, you will need to create<a id="_idIndexMarker632"/> an account on AWS by visiting <a href="https://aws.amazon.com/"><span class="url">https://aws.amazon.com/</span></a>.</p>
    <p class="normal">Once you<a id="_idIndexMarker633"/> have set up the account, visit the <strong class="keyword">Identity and Access Management</strong> (<strong class="keyword">IAM</strong>) page to create a service<a id="_idIndexMarker634"/> user that can create and change resources. You could use your root—or main—account to do all of the work, but it is better to create service accounts for this purpose, as it means that any leaked access keys or secrets can be easily revoked—and new ones created—without causing major trouble for accessing the account in general. We should follow the principle of least privilege, as we discussed in <em class="chapterRef">Chapter 7</em>, <em class="italic">Securing Your Services</em>.</p>
    <p class="normal">Once on the IAM page, click <strong class="screenText">Add User</strong> and request <strong class="screenText">Programmatic Access</strong> so that you can obtain API keys to use this account in a program.</p>
    <figure class="mediaobject"><img src="../Images/B17108_10_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.1: IAM Add user page in AWS</p>
    <p class="normal">Create a group to control the user's permissions more easily. Grant this new group the permissions to modify EC2 instances, since it covers most of what we will be changing.</p>
    <figure class="mediaobject"><img src="../Images/B17108_10_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.2: Naming the group and setting permissions</p>
    <p class="normal">Once the group is created, you<a id="_idIndexMarker635"/> will get a chance to download the new <strong class="keyword">Access Key ID</strong> and <strong class="keyword">Secret Access Key</strong>. These will be used<a id="_idIndexMarker636"/> to grant access to any programs that we use to create instances and other cloud resources.</p>
    <p class="normal">Most of these tools<a id="_idIndexMarker637"/> are Infrastructure-as-Code. That is to say, you will have a configuration file, or set of files, that describe what your running services will look like, and what resources they require. This configuration should also be kept in version control so that any changes can be managed. Whether it is in the same source control repository as your code will depend on how you need to deploy the software: If you are continuously deploying new versions, then it can be helpful to keep the configuration alongside the application, but in many cases, it is much clearer to keep it separate, especially if the CI pipelines will be difficult to coordinate between the deployments and the code's own test and packaging tasks.</p>
    <h2 id="_idParaDest-170" class="title">Terraform</h2>
    <p class="normal">Before we head further<a id="_idIndexMarker638"/> into the different container<a id="_idIndexMarker639"/> orchestration tools, it's worth mentioning a different sort of tool that can organize the underlying cloud instances. <strong class="keyword">Terraform</strong>, made by the company HashiCorp, is a widely adopted tool for defining resources as code. Using Terraform modules, you can define how you want your cloud instances to be set up, what the security groups will be, what storage is present, along with a whole suite of other variables. Describing an Amazon EC2 instance in Terraform will look a little like the following configuration snippet, with a full example at <a href="https://github.com/PacktPublishing/Python-Microservices-Development-2nd-Edition"><span class="url">https://github.com/PacktPublishing/Python-Microservices-Development-2nd-Edition</span></a>:</p>
    <pre class="programlisting code"><code class="hljs-code">resource "aws_instance" "swarm_cluster" {
  count     	= 3
  ami       	= data.aws_ami.ubuntu_focal.id
  instance_type = "t3.micro" # Small for this demo.
  vpc_security_group_ids = [aws_security_group.swarm_security_group.id]
  key_name           	= "your ssh key name here"
  root_block_device {
	volume_type = "gp2"
	volume_size = "40" # GiB
	encrypted   = true
  }
}
</code></pre>
    <p class="normal">Here we are defining<a id="_idIndexMarker640"/> a resource called <code class="Code-In-Text--PACKT-">swarm_cluster</code>, in which will create<a id="_idIndexMarker641"/> three new instances, using an <code class="Code-In-Text--PACKT-">Ubuntu Focal</code> base image. We set the instance size to <code class="Code-In-Text--PACKT-">t3.micro</code> because we are trying things out and want to minimize the cost.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">This snippet of Terraform code does depend on other parts not quoted here, such as the security group resource, and discovering the latest identifier for an Ubuntu Focal image, but the full example is available at <a href="https://github.com/PacktPublishing/Python-Microservices-Development-2nd-Edition/tree/main/CodeSample"><span class="url">https://github.com/PacktPublishing/Python-Microservices-Development-2nd-Edition/tree/main/CodeSamples</span></a>.</p>
    </div>
    <p class="normal">Using Terraform, we can create and destroy our cloud resources in a CI/CD pipeline in a similar way that we test and deploy our application code. The following has in-depth tutorials and<a id="_idIndexMarker642"/> worked examples, and there are many community-provided modules to perform common tasks: <a href="https://learn.hashicorp.com/terraform"><span class="url">https://learn.hashicorp.com/terraform</span></a>.</p>
    <p class="normal">Terraform's <code class="Code-In-Text--PACKT-">plan</code> command will show you what changes will be made to your cloud infrastructure when you run <code class="Code-In-Text--PACKT-">terraform apply</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> terraform plan
...
<span class="hljs-con-meta">  #</span> module.docker-swarm.aws_instance.managers[2] will be created
  + resource "aws_instance" "managers" {
  	+ ami                              	= "ami-0440ba4c79a163c0e"
  	+ arn                              	= (known after apply)
  	+ associate_public_ip_address      	= (known after apply)
  	+ availability_zone                	= (known after apply)
…
<span class="hljs-con-meta">$</span> terraform apply
[terraform plan output]
Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.
  Enter a value: yes
aws_vpc.main: Creating...
module.docker-swarm.aws_iam_policy.swarm-access-role-policy: Creating...
module.docker-swarm.aws_sns_topic.alarms: Creating...
module.docker-swarm.aws_iam_role.ec2: Creating...
module.docker-swarm.aws_s3_bucket.terraform[0]: Creating...
module.docker-swarm.aws_sns_topic.alarms: Creation complete after 0s [id=arn:aws:sns:eu-central-1:944409847308:swarm-vpc-alarms]
module.docker-swarm.aws_iam_policy.swarm-access-role-policy: Creation complete after 1s [id=arn:aws:iam::944409847308:policy/swarm-vpc-swarm-ec2-policy]
...
</code></pre>
    <p class="normal">Once you are done with any<a id="_idIndexMarker643"/> experiments, you can run <code class="Code-In-Text--PACKT-">terraform destroy</code> to clear up any resources managed by Terraform—although this is a dangerous command for a production service!</p>
    <h2 id="_idParaDest-171" class="title">Service discovery</h2>
    <p class="normal">While Docker tries to provide<a id="_idIndexMarker644"/> all the tools<a id="_idIndexMarker645"/> to deal with clusters of containers, managing them can become quite complex. When done properly, it requires sharing some configuration across hosts, and to make sure that bringing containers up and down is partially automated.</p>
    <p class="normal">We very quickly come across scenarios that complicate a static configuration. If we need to move a microservice to a new AWS region, or a different cloud provider entirely, then how do we tell all the other microservices that use it? If we add a new feature that's controlled by a feature flag, how do we quickly turn it on and off? On a smaller scale, how does a load balancer know about all the containers that should receive traffic?</p>
    <p class="normal">Service discovery<a id="_idIndexMarker646"/> is an orchestration method that aims to solve these problems. Tools such as <strong class="keyword">Consul</strong> (<a href="https://www.consul.io/"><span class="url">https://www.consul.io/</span></a>) and <strong class="keyword">etcd</strong> (<a href="https://etcd.io/"><span class="url">https://etcd.io/</span></a>) allow values to be stored behind<a id="_idIndexMarker647"/> well-known keys and updated dynamically.</p>
    <p class="normal">Instead of deploying your service with full knowledge of all the URLs it might connect to, you provide it with the address of a service discovery tool, and the list of keys it should look up for each element. When a microservice starts up, and at regular intervals, it can check where it should be sending traffic, or whether a feature should be turned on.</p>
    <p class="normal">We will use <code class="Code-In-Text--PACKT-">etcd</code> as an example, with a basic Quart service, while also utilizing the <code class="Code-In-Text--PACKT-">etcd3</code> Python library. Assuming you have <code class="Code-In-Text--PACKT-">etcd</code> running with the default options after following the instructions on their website, we can<a id="_idIndexMarker648"/> add some configuration-updating code<a id="_idIndexMarker649"/> to our service, and have an endpoint that returns the URL we would contact, if the application was more complete:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># etcd_basic.py</span>
<span class="hljs-keyword">from</span> quart <span class="hljs-keyword">import</span> Quart, current_app
<span class="hljs-keyword">import</span> etcd3
<span class="hljs-comment"># Can read this map from a traditional config file</span>
settings_map = {
    <span class="hljs-string">"dataservice_url"</span>: <span class="hljs-string">"/services/dataservice/url"</span>,
}
settings_reverse_map = {v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> settings_map.items()}
etcd_client = etcd3.client()
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">load_settings</span><span class="hljs-function">():</span>
    config = <span class="hljs-built_in">dict</span>()
    <span class="hljs-keyword">for</span> setting, etcd_key <span class="hljs-keyword">in</span> settings_map.items():
        config[setting] = etcd_client.get(etcd_key)[<span class="hljs-number">0</span>].decode(<span class="hljs-string">"utf-8"</span>)
    <span class="hljs-keyword">return</span> config
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">create_app</span><span class="hljs-function">(</span><span class="hljs-params">name=__name__</span><span class="hljs-function">):</span>
    app = Quart(name)
    app.config.update(load_settings())
    <span class="hljs-keyword">return</span> app
app = create_app()
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">watch_callback</span><span class="hljs-function">(</span><span class="hljs-params">event</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">global</span> app
    <span class="hljs-keyword">for</span> update <span class="hljs-keyword">in</span> event.events:
    <span class="hljs-comment"># Determine which setting to update, and convert from bytes to str</span>
        config_option = settings_reverse_map[update.key.decode(<span class="hljs-string">"utf-8"</span>)]
        app.config[config_option] = update.value.decode(<span class="hljs-string">"utf-8"</span>)
<span class="hljs-comment"># Start to watch for dataservice url changes</span>
<span class="hljs-comment"># You can also watch entire areas with add_watch_prefix_callback</span>
watch_id = etcd_client.add_watch_callback(<span class="hljs-string">"/services/dataservice/url"</span>, watch_callback)
<span class="hljs-meta">@app.route(</span><span class="hljs-string">"/api"</span><span class="hljs-meta">)</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">what_is_url</span><span class="hljs-function">():</span>
    <span class="hljs-keyword">return</span> {<span class="hljs-string">"url"</span>: app.config[<span class="hljs-string">"dataservice_url"</span>]}
app.run()
</code></pre>
    <p class="normal">In this example, we load the keys in the <code class="Code-In-Text--PACKT-">settings_map</code> when the application starts, including <code class="Code-In-Text--PACKT-">/services/dataservice/url</code>, which we can<a id="_idIndexMarker650"/> then validate and use. Any time<a id="_idIndexMarker651"/> that value changes in <code class="Code-In-Text--PACKT-">etcd</code>, the <code class="Code-In-Text--PACKT-">watch_callback</code> function will be run in its own thread, and the app's configuration updated:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> curl http://127.0.0.1:5000/api
{"url":"https://firsturl.example.com/api"}
<span class="hljs-con-meta">$</span> etcdctl put "/services/dataservice/url" "https://secondurl.example.com/api"
OK
<span class="hljs-con-meta">$</span> curl http://127.0.0.1:5000/api
{"url":"https://secondurl.example.com/api"}
</code></pre>
    <p class="normal">Updating the live configuration is a simple command!</p>
    <p class="normal">If your application has configuration options that depend on each other, such as pairs of access tokens, it is best to encode them in a single option so that they are updated in a single operation. If something fails and only one of a co-dependent set of configuration settings is updated, your application will behave in unwanted and unexpected ways.</p>
    <h2 id="_idParaDest-172" class="title">Docker Compose</h2>
    <p class="normal">The commands<a id="_idIndexMarker652"/> required to run several containers on the same host can be quite long once you need to add names and networks<a id="_idIndexMarker653"/> and bind several sockets. Docker Compose (<a href="https://docs.docker.com/compose/"><span class="url">https://docs.docker.com/compose/</span></a>) simplifies the task<a id="_idIndexMarker654"/> by letting you define multiple containers' configuration in a single configuration file, as well as how those containers depend on each other. This utility is installed on macOS and Windows alongside Docker. For Linux distributions, there should be a system<a id="_idIndexMarker655"/> package available to install it, or you can obtain an installation script by following the instructions at <a href="https://docs.docker.com/compose/install/"><span class="url">https://docs.docker.com/compose/install/</span></a>.</p>
    <p class="normal">Once the script is installed on your system, create a <code class="Code-In-Text--PACKT-">yaml</code> file containing the information about services and networks that you want to run. The default filename is <code class="Code-In-Text--PACKT-">docker-compose.yml</code>, and so we will use that name for our examples to make the commands simpler.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">The compose<a id="_idIndexMarker656"/> configuration file has many options that let you define every aspect of the deployment of several containers. It's like a Makefile for a group of containers. This URL lists all options: <a href="https://docs.docker.com/compose/compose-file/"><span class="url">https://docs.docker.com/compose/compose-file/</span></a>.</p>
    </div>
    <p class="normal">In the following example, the <code class="Code-In-Text--PACKT-">.yaml</code> file is placed one directory above two of our Jeeves microservices and defines three services: the <code class="Code-In-Text--PACKT-">dataservice</code> and the <code class="Code-In-Text--PACKT-">tokendealer</code>, which are built locally from their Dockerfile; the third is RabbitMQ, and we use an image published on Docker Hub to run that:</p>
    <pre class="programlisting gen"><code class="hljs">version: '3'
networks:
  jeeves:
services:
  dataservice:
    networks:
     - jeeves
    build:
        context: dataservice/
    ports:
     - "8080:8080"
  tokendealer:
    networks:
     - jeeves
    build:
        context: tokendealer/
    ports:
     - "8090:8090"
  rabbitmq:
    image: "rabbitmq:latest"
    networks:
     - jeeves
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">Compose</code> file also creates networks with its <code class="Code-In-Text--PACKT-">networks</code> sections, allowing the containers to communicate<a id="_idIndexMarker657"/> with each other. They will get private DNS entries so they can be referred<a id="_idIndexMarker658"/> to with the image names, such as <code class="Code-In-Text--PACKT-">dataservice</code>, <code class="Code-In-Text--PACKT-">tokendealer</code>, and <code class="Code-In-Text--PACKT-">rabbitmq</code> in the example above. To build and run those three containers, you can use the <code class="Code-In-Text--PACKT-">up</code> command as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> docker compose up
[+] Building 9.2s (9/9) FINISHED
 ...
[+] Running 3/0
 <img src="../Images/Icon_3.png" alt=""/> Container pythonmicroservices_tokendealer_1  Created           	0.1s
 <img src="../Images/Icon_3.png" alt=""/> Container pythonmicroservices_dataservice_1  Created           	0.1s
 <img src="../Images/Icon_3.png" alt=""/> Container pythonmicroservices_rabbitmq_1  Created         	0.1s
Attaching to dataservice_1, rabbitmq_1, tokendealer_1
</code></pre>
    <p class="normal">The first time that command is executed, the two local container images will be built. These will either be static, or you can assign volumes to them to mount in the source code and continue developing on them.</p>
    <p class="normal">Using Docker Compose is great when you want to provide a full working stack for your microservices, which includes every piece of software needed to run it. For instance, if you are using<a id="_idIndexMarker659"/> a Postgres database, you can use the Postgres image (<a href="https://hub.docker.com/_/postgres/"><span class="url">https://hub.docker.com/_/postgres/</span></a>) and link it to your service in a Docker Compose file.</p>
    <p class="normal">Containerizing everything, even the databases, is a great way to showcase your software, or simply a good option for development purposes. However, as we stated earlier, a Docker container should be seen as an ephemeral filesystem. So if you use a container for your database, make sure that the directory where the data is written is mounted<a id="_idIndexMarker660"/> on the host filesystem. In most cases, however, the database service is usually its dedicated server on a production<a id="_idIndexMarker661"/> deployment. Using a container does not make much sense and adds only a little bit of overhead.</p>
    <h2 id="_idParaDest-173" class="title">Docker Swarm</h2>
    <p class="normal">Docker has a built-in cluster<a id="_idIndexMarker662"/> functionality called <strong class="keyword">swarm</strong> mode (<a href="https://docs.docker.com/engine/swarm/"><span class="url">https://docs.docker.com/engine/swarm/</span></a>). This mode has an impressive list of features, which lets you manage all your container clusters from a single utility. This makes it ideal<a id="_idIndexMarker663"/> for smaller deployments or ones that do not need to scale up and down as flexibly to meet changing demands.</p>
    <p class="normal">Once you have deployed a cluster, you need to set up a load balancer so that all the instances of your cluster are sharing the workload. The load balancer is commonly software such as nginx, OpenResty, or HAProxy, and is the entry point to distribute the incoming requests on clusters.</p>
    <p class="normal">To set up<a id="_idIndexMarker664"/> a swarm, all we really need are three EC2 instances, provided we can connect to them using port <code class="Code-In-Text--PACKT-">22</code> for SSH access to configure them, and port <code class="Code-In-Text--PACKT-">2377</code> for Docker's own communication. We should also allow any ports that our application needs, such as port <code class="Code-In-Text--PACKT-">443</code> for HTTPS connections.</p>
    <p class="normal">To create a swarm, we must create a manager node that will organize the rest. Using one of the nodes you have just created, connect to it using SSH, and convert it to a Docker Swarm<a id="_idIndexMarker665"/> manager:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> sudo docker swarm init —advertise-addr &lt;Public IP Address&gt;
Swarm initialized: current node (l6u7ljqhiaosbeecn4jjlm6vt) is now a manager.
</code></pre>
    <p class="normal">To add a worker to this swarm, run the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">docker swarm join —token &lt;some long token&gt; 52.212.189.167:2377
</code></pre>
    <p class="normal">To add a manager to this swarm, run <code class="Code-In-Text--PACKT-">docker swarm join-token manager</code> and follow the instructions.</p>
    <p class="normal">Copy the <code class="Code-In-Text--PACKT-">docker</code> <code class="Code-In-Text--PACKT-">swarm</code> command provided, and paste it into an SSH session on the other instances you have created. You may need to run <code class="Code-In-Text--PACKT-">sudo</code> to gain root access before the commands will work. On the manager node, we can now see all our workers:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> sudo docker node ls
ID                        	HOSTNAME       	STATUS	AVAILABILITY   MANAGER STATUS 	ENGINE VERSION
6u81yvbwbvb0fspe06yzlsi13 	ip-172-31-17-183 	Ready 	Active   	20.10.6
l6u7ljqhiaosbeecn4jjlm6vt *  	ip-172-31-26-31	Ready 	Active     	Leader       	20.10.6
873cp1742grhkzoo5xd2aiqls 	ip-172-31-28-17	Ready 	Active     	20.10.6
</code></pre>
    <p class="normal">Now all we need to do is create our services:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> sudo docker service create —replicas 1 —name dataservice jeeves/dataservice
sikcno6s3582tdr91dj1fvsse
overall progress: 1 out of 1 tasks
1/1: running   [==================================================&gt;]
verify: Service converged
<span class="hljs-con-meta">$</span> sudo docker service ls
ID         	    NAME          MODE         REPLICAS   IMAGE       	PORTS
sikcno6s3582   dataservice   replicated   1/1    	jeeves/dataservice:latest
</code></pre>
    <p class="normal">From here, we can<a id="_idIndexMarker666"/> scale our service up and down as we need to. To create five copies of our dataservice, we would issue a scale command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> sudo docker service scale dataservice=5
</code></pre>
    <p class="normal">As long as our manager<a id="_idIndexMarker667"/> node remains available, and some<a id="_idIndexMarker668"/> of the worker nodes are up, then our container service will remain active. We can terminate one of the cloud instances and watch things rebalance to the remaining instances with <code class="Code-In-Text--PACKT-">docker service ps</code>. Adding more nodes is as easy as adjusting a variable in the Terraform configuration and re-running <code class="Code-In-Text--PACKT-">terraform</code> <code class="Code-In-Text--PACKT-">apply</code>, before then joining them to the swarm.</p>
    <p class="normal">Looking after the suite of cloud instances is still work, but this environment provides a neat way of providing a resilient container deployment, especially early on in an application's life.</p>
    <h2 id="_idParaDest-174" class="title">Kubernetes</h2>
    <p class="normal">Originally designed by Google, but now<a id="_idIndexMarker669"/> maintained by an<a id="_idIndexMarker670"/> independent foundation, <strong class="keyword">Kubernetes</strong> (<a href="https://kubernetes.io/"><span class="url">https://kubernetes.io/</span></a>, also known as k8s) provides<a id="_idIndexMarker671"/> a platform-independent way of automating work with containerized systems, allowing you to describe the system in terms of different components, and issuing commands to a controller to adjust settings.</p>
    <p class="normal">Like Docker Swarm, Kubernetes also runs on a cluster of servers. It's possible to run this cluster yourself, although some cloud providers do have a service that makes managing the fleet of instances much easier. A good example<a id="_idIndexMarker672"/> of this is the <strong class="keyword">eksctl</strong> utility for AWS (<a href="https://eksctl.io/"><span class="url">https://eksctl.io/</span></a>). While not created by Amazon, it is an officially supported client for creating clusters in Amazon's Elastic Kubernetes Service.</p>
    <p class="normal">Rather than create<a id="_idIndexMarker673"/> all the AWS resources yourself, or create Terraform configuration to do so, <code class="Code-In-Text--PACKT-">eksctl</code> performs all the work for you, with sensible defaults for experimenting with Kubernetes. To get started, it is best to use the AWS credentials we created for earlier examples and to install both <code class="Code-In-Text--PACKT-">eksctl</code> and <code class="Code-In-Text--PACKT-">kubectl</code>—the Kubernetes command line. The AWS credentials will be used by <code class="Code-In-Text--PACKT-">eksctl</code> to create the cluster and other necessary resources, and once done, <code class="Code-In-Text--PACKT-">kubectl</code> can be used to deploy services and software. Unlike Docker Swarm, kubectl's administrative commands are designed to be run from your own computer:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> eksctl create cluster —name=jeeves-cluster-1 —nodes=4 —region=eu-west-1
2021-05-27 20:13:44 [<img src="../Images/Icon_1.png" alt=""/>]  eksctl version 0.51.0
2021-05-27 20:13:44 [<img src="../Images/Icon_1.png" alt=""/>]  using region eu-west-1
2021-05-27 20:13:44 [<img src="../Images/Icon_1.png" alt=""/>]  setting availability zones to [eu-west-1a eu-west-1c eu-west-1b]
2021-05-27 20:13:44 [<img src="../Images/Icon_1.png" alt=""/>]  subnets for eu-west-1a - public:192.168.0.0/19 private:192.168.96.0/19
2021-05-27 20:13:44 [<img src="../Images/Icon_1.png" alt=""/>]  subnets for eu-west-1c - public:192.168.32.0/19 private:192.168.128.0/19
2021-05-27 20:13:44 [<img src="../Images/Icon_1.png" alt=""/>]  subnets for eu-west-1b - public:192.168.64.0/19 private:192.168.160.0/19
2021-05-27 20:13:44 [<img src="../Images/Icon_1.png" alt=""/>]  nodegroup "ng-4e138761" will use "ami-0736921a175c8cebf" [AmazonLinux2/1.19]
2021-05-27 20:13:45 [<img src="../Images/Icon_1.png" alt=""/>]  using Kubernetes version 1.19
...
</code></pre>
    <p class="normal">It will take a few minutes to create the cluster, but once done, it will write the credentials <code class="Code-In-Text--PACKT-">kubectl</code> needs to the correct file, so no further setup should be needed. We told <code class="Code-In-Text--PACKT-">eksctl</code> to create four nodes, and that's exactly what it has done:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> kubectl get nodes
NAME                                       	STATUS   ROLES	AGE 	VERSION
ip-192-168-2-113.eu-west-1.compute.internal	Ready	&lt;none&gt;   8m56s   v1.19.6-eks-49a6c0
ip-192-168-37-156.eu-west-1.compute.internal   Ready	&lt;none&gt;   9m1s	v1.19.6-eks-49a6c0
ip-192-168-89-123.eu-west-1.compute.internal   Ready	&lt;none&gt;   9m1s	v1.19.6-eks-49a6c0
ip-192-168-90-188.eu-west-1.compute.internal   Ready	&lt;none&gt;   8m59s   v1.19.6-eks-49a6c0
</code></pre>
    <p class="normal">For the moment, we have nothing<a id="_idIndexMarker674"/> running on our <code class="Code-In-Text--PACKT-">k8s</code> cluster, so we shall create some work for it to do. The fundamental unit of work for <code class="Code-In-Text--PACKT-">k8s</code> is a <code class="Code-In-Text--PACKT-">pod</code>, which describes a set of running containers on the cluster. We have not created any of our own yet, but there are some running in a different namespace to help k8s do its own work of managing the rest of the tasks we set it. Namespaces like this can be useful for grouping sets of tasks together, making it easier to understand what is important when looking at the cluster:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> kubectl get pods
No resources found in default namespace…
<span class="hljs-con-meta">$</span> kubectl get pods —namespace kube-system
NAME                   	READY   STATUS	RESTARTS      AGE
aws-node-6xnrt         	1/1     Running      0      	29m
aws-node-rhgmd         	1/1     Running      0      	28m
aws-node-v497d         	1/1     Running      0      	29m
aws-node-wcbh7         	1/1 	Running   0      	29m
coredns-7f85bf9964-n8jmj	1/1 	Running   0      	36m
coredns-7f85bf9964-pk7sq	1/1 	Running   0      	36m
kube-proxy-4r7fw       	1/1 	Running   0      	29m
kube-proxy-dw9sv       	1/1 	Running   0      	29m
kube-proxy-p7qqv       	1/1 	Running   0      	28m
kube-proxy-t7spn       	1/1 	Running   0      	29m
</code></pre>
    <p class="normal">A Pod is a low-level description of some work for the cluster, and to help make life easier, there are higher-level abstractions for different types of work, such as a <code class="Code-In-Text--PACKT-">Deployment</code> for a stateless application, such as a web interface or proxy, a <code class="Code-In-Text--PACKT-">StatefulSet</code> for when your workload needs storage attached rather than keeping its data in a different service, as well as Jobs and CronJobs for one-off tasks and scheduled repeating tasks, respectively.</p>
    <p class="normal">Kubernetes accepts manifests<a id="_idIndexMarker675"/> of instructions that it should apply. A good starting point is to set up nginx, with a manifest such as this one:</p>
    <pre class="programlisting code"><code class="hljs-code"># nginx.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.21.0
          ports:
          - containerPort: 80
</code></pre>
    <p class="normal">We include some metadata about the type of resource we are requesting—a <code class="Code-In-Text--PACKT-">Deployment</code>—and its name, and then dive into the specification for the service. Down at the bottom of the file, we can see that we've asked for a container based on the <code class="Code-In-Text--PACKT-">nginx:1.21.0 image</code>, and that it should have port <code class="Code-In-Text--PACKT-">80</code> open. One layer up, we describe this container specification as a template that we use to create three different copies and run them on our cluster.</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> kubectl apply -f nginx.yml
deployment.apps/nginx-deployment created
<span class="hljs-con-meta">$</span> kubectl get pods
NAME                                READY     STATUS	   RESTARTS   AGE
nginx-deployment-6c4ccd94bc-8qftq   1/1 	Running   0      	21s
nginx-deployment-6c4ccd94bc-hqt8c   1/1 	Running   0      	21s
nginx-deployment-6c4ccd94bc-v7zpl   1/1 	Running   0      	21s
</code></pre>
    <p class="normal">Using kubectl's <code class="Code-In-Text--PACKT-">describe</code> subcommand, we get even more<a id="_idIndexMarker676"/> information about what was created for us:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> kubectl describe deployment nginx-deployment
Name:               	nginx-deployment
Namespace:          	default
CreationTimestamp:  	Thu, 27 May 2021 21:06:47 +0100
Labels:             	app=nginx
Annotations:        	deployment.kubernetes.io/revision: 1
Selector:           	app=nginx
Replicas:           	3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:       	RollingUpdate
MinReadySeconds:    	0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
	Image:    	nginx:1.21.0
	Port:     	80/TCP
	Host Port:	0/TCP
	Environment:  &lt;none&gt;
	Mounts:   	&lt;none&gt;
  Volumes:    	&lt;none&gt;
Conditions:
  Type       	Status  Reason
  ——       	———  ———
  Available  	True	MinimumReplicasAvailable
  Progressing	True	NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-6c4ccd94bc (3/3 replicas created)
Events:
  Type	Reason         	Age   From               	Message
  ——	———         	——  ——               	———-
  Normal  ScalingReplicaSet  14m   deployment-controller  Scaled up replica set nginx-deployment-6c4ccd94bc to 3
</code></pre>
    <p class="normal">If we decide we need more nginx containers, we can update the manifest. Change the number of replicas in our<a id="_idIndexMarker677"/> <code class="Code-In-Text--PACKT-">yaml</code> file from three to eight, and re-apply the manifest:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span>  kubectl get pods -l app=nginx
NAME                                READY   STATUS             RESTARTS   AGE
nginx-deployment-6c4ccd94bc-7g74n   0/1     ContainerCreating   0      	2s
nginx-deployment-6c4ccd94bc-8qftq   1/1     Running             0      	17m
nginx-deployment-6c4ccd94bc-crw2t   1/1     Running             0      	2s
nginx-deployment-6c4ccd94bc-fb7cf   0/1     ContainerCreating   0      	2s
nginx-deployment-6c4ccd94bc-hqt8c   1/1     Running             0      	17m
nginx-deployment-6c4ccd94bc-v7zpl   1/1     Running             0      	17m
nginx-deployment-6c4ccd94bc-zpd4v   1/1     Running             0      	2s
nginx-deployment-6c4ccd94bc-zwtcv   1/1     Running             0      	2s
</code></pre>
    <p class="normal">A similar change could be performed to upgrade the version of nginx, and Kubernetes has several strategies to perform updates of a service so that end users are unlikely to notice it happening. For example, it is possible to create an entirely new Pod of containers and redirect traffic to it, but it's also possible to do rolling updates inside a Pod, where a container is only destroyed when its replacement has successfully started. How can you tell the container was successfully started? Kubernetes allows you to describe what it should look for to check whether a container can do its work, and how long it should wait for a container to start, with its liveness and readiness checks.</p>
    <p class="normal">If you have been following along with the examples, remember to delete the cloud resources when you are done, as they cost. To remove just the nginx-deeployment we created, use <code class="Code-In-Text--PACKT-">kubectl</code>.</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> kubectl delete -f nginx.yml
deployment.apps "nginx-deployment" deleted
</code></pre>
    <p class="normal">But to destroy the entire cluster, return to using <code class="Code-In-Text--PACKT-">eksctl</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> eksctl delete cluster —name=jeeves-cluster-1 —region=eu-west-1
2021-05-27 21:33:22 [<img src="../Images/Icon_1.png" alt=""/>]  eksctl version 0.51.0
2021-05-27 21:33:22 [<img src="../Images/Icon_1.png" alt=""/>]  using region eu-west-1
2021-05-27 21:33:22 [<img src="../Images/Icon_1.png" alt=""/>]  deleting EKS cluster "jeeves-cluster-1"
2021-05-27 21:33:23 [<img src="../Images/Icon_1.png" alt=""/>]  deleted 0 Fargate profile(s)
2021-05-27 21:33:23 [<img src="../Images/Icon_2.png" alt=""/>]  kubeconfig has been updated
2021-05-27 21:33:23 [<img src="../Images/Icon_1.png" alt=""/>]  cleaning up AWS load balancers created by Kubernetes objects of Kind Service or Ingress
2021-05-27 21:33:25 [<img src="../Images/Icon_1.png" alt=""/>]  2 sequential tasks: { delete nodegroup "ng-4e138761", delete cluster control plane "jeeves-cluster-1" [async] }
2021-05-27 21:33:26 [<img src="../Images/Icon_1.png" alt=""/>]  will delete stack "eksctl-jeeves-cluster-1-nodegroup-ng-4e138761"
...
</code></pre>
    <p class="normal">This is a very brief overview<a id="_idIndexMarker678"/> of an enormously powerful tool, as the topic could cover an entire book by itself. For those who need it, the time spent learning Kubernetes is well spent, but as ever, you must assess the needs of your own application, and whether something simpler will get the job done.</p>
    <h1 id="_idParaDest-175" class="title">Summary</h1>
    <p class="normal">In this chapter, we looked at how microservices can be containerized with containers, and how you can create a deployment entirely based on Docker images. Containers are a well-established technology that is widely used to run internet services. The most important thing to keep in mind is that a containerized application is ephemeral: it is designed to be destroyed and recreated on demand, and any data that is not externalized using a mount point is lost.</p>
    <p class="normal">For provisioning and clustering your services, there is no generic solution, as the tools you use will depend on your needs. From a simple Docker Compose setup to a full Kubernetes cluster, each option provides different complexity and benefits. The best choice often depends on where you are to deploy your services, how your teams work, and how large your application needs to be in the present—there is no sense in planning for an unknowable future.</p>
    <p class="normal">The best way to tackle this problem is to take baby steps by first deploying everything manually, then automating where it makes sense. Automation is great, but can rapidly become difficult if you use a toolset you do not fully understand, or that is too complex for your needs.</p>
    <p class="normal">As a guide, consider:</p>
    <ul>
      <li class="bullet">Docker Compose when you need to deploy multiple containers in a small environment, and do not need to manage a large infrastructure.</li>
      <li class="bullet">Docker Swarm when you need flexibility in how many containers are deployed, to respond to a changing situation, and are happy to manage a larger cloud infrastructure.</li>
      <li class="bullet">Kubernetes when automation and flexibility are paramount, and you have people and time available to manage the infrastructure and handle the complexity.</li>
    </ul>
    <p class="normal">You will not be locked into one orchestration tool once you choose it, as the containers you build can be used in any of them, but moving to a different orchestration tool can be hard work, depending on how complex your configuration is.</p>
    <p class="normal">In that vein, to make their services easier to use and more appealing, cloud providers have built-in features to handle deployments. The three largest cloud providers are currently AWS, Google Cloud, and Microsoft Azure, although many other good options exist.</p>
  </div>
</body></html>