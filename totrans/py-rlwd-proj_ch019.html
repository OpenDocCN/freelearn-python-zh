<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="pandoc" name="generator"/>
<title>ch019.xhtml</title>

<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body epub:type="bodymatter">

<h1 data-number="19">Chapter 15<br/>
Project 5.1: Modeling Base Application</h1>
<p>The next step in the pipeline from acquisition to clean-and-convert is the analysis and some preliminary modeling of the data. This may lead us to use the data for a more complex model or perhaps machine learning. This chapter will guide you through creating another application in the three-stage pipeline to acquire, clean, and model a collection of data. This first project will create the application with placeholders for more detailed and application-specific modeling components. This makes it easier to insert small statistical models that can be replaced with more elaborate processing if needed.</p>
<p>In this chapter, we’ll look at two parts of data analysis:</p>
<ul>
<li><p>CLI architecture and how to design a more complex pipeline of processes for gathering and analyzing data</p></li>
<li><p>The core concepts of creating a statistical model of the data</p></li>
</ul>
<p>Viewed from a distance, all analytical work can be considered to be creating a simplified model of important features of some complicated processes. Even something as simple-sounding as computing an average suggests a simple model of the central tendency of a variable. Adding a standard deviation suggests an expected range for the variable’s values and – further – assigns a probability to values outside the range.</p>
<p>Models, can, of course, be considerably more detailed. Our purpose is to start down the path of modeling in a way that builds flexible, extensible application software. Each application will have unique modeling requirements, depending on the nature of the data, and the nature of the questions being asked about the data. For some processes, means and standard deviations are adequate for spotting outliers. For other processes, a richer and more detailed simulation may be required to estimate the expected distribution of data.</p>
<p>We’ll start the modeling by looking at variables in isolation, sometimes called <em>univariate </em>statistics. This will examine a variety of commonly recognized distributions of data. These distributions generally have a few parameters that can be discovered from the given data. In this chapter, we’ll also look at measures like mean, median, standard deviation, variance, and standard deviation. These can be used to describe data that has a normal or Gaussian distribution. The objective is to create create a CLI application separate from an analytic notebook used to present results. This creates a higher degree of automation for modeling. The results may then be presented in an analytical notebook.</p>
<p>There is a longer-term aspect to having automated model creation. Once a data model has been created, an analyst can also look at changes to a model and what implications the changes should have on the way an enterprise operates. For example, an application may perform a monthly test to be sure new data matches the established mean, median, and standard deviation reflecting the expected normal distribution of data. In the event that a batch of data doesn’t fit the established model, further investigation is required to uncover the root cause of this change. </p>

<h2 data-number="19.1">15.1  Description</h2>
<p>This application will create a report on a dataset presenting a number of statistics. This automates the ongoing monitoring aspect of an Analysis Notebook, reducing the manual steps and creating reproducible results. The automated computations stem from having a statistical model for the data, often created in an analysis notebook, where alternative models are explored. This reflects variables with values in an expected range.</p>
<p>For industrial monitoring, this is part of an activity called <strong>Gage repeatability</strong> <strong>and</strong> <strong>r</strong><strong>eproducibility</strong>. The activity seeks to confirm that measurements are repeatable and reproducible. This is described as looking at a “measurement instrument.” While we often think of an instrument as being a machine or a device, the definition is actually very broad. A survey or questionnaire is a measurement instrument focused on people’s responses to questions.</p>
<p>When these computed statistics deviate from expectations, it suggests something has changed, and the analyst can use these unexpected values to investigate the root cause of the deviation. Perhaps some enterprise process has changed, leading to shifts in some metrics. Or, perhaps some enterprise software has been upgraded, leading to changes to the source data or encodings used to create the clean data. More complex still, it may be that the instrument doesn’t actually measure what we thought it measured; this new discrepancy may expose a gap in our understanding.</p>
<p>The repeatability of the model’s measurements is central to the usability of the measurements. Consider a ruler that’s so worn down over years of use that it is no longer square or accurate. This single instrument will produce different results depending on what part of the worn end is used to make the measurement. This kind of measurement variability may obscure the variability in manufacturing a part. Understanding the causes of changes is challenging and can require thinking “outside the box” — challenging assumptions about the real-world process, the measurements of the process, and the model of those measurements.</p>
<p>Exploratory data analysis can be challenging and exhilarating precisely because there aren’t obvious, simple answers to explain why a measurement has changed.</p>
<p>The implementation of this preliminary model is through an application, separate from the previous stages in the pipeline to acquire and clean the data. With some careful design, this stage can be combined with those previous stages, creating a combined sequence of operations to acquire, clean, and create the summary statistics.</p>
<p>This application will overlap with the analysis notebook and the initial inspection notebook. Some of the observations made during those earlier ad-hoc analysis stages will be turned into fixed, automated processing.</p>
<p>This is the beginning of creating a more complicated machine-learning model of the data. In some cases, a statistical model using linear or logistic regression is adequate, and a more complex artificial intelligence model isn’t needed. In other cases, the inability to create a simple statistical model can point toward a need to create and tune the hyperparameters of a more complicated model.</p>
<p>The objective of this application is to save a statistical summary report that can be aggregated with and compared to other summary reports. The ideal structure will be a document in an easy-to-parse notation. JSON is suggested, but other easier-to-read formats like TOML are also sensible.</p>
<p>There are three key questions about data distribution:</p>
<ol>
<li><div><p>What is the <strong>location </strong>or expected value for the output being measured?</p>
</div></li>
<li><div><p>What is the <strong>spread </strong>or expected variation for this variable?</p>
</div></li>
<li><div><p>What is the general <strong>shape</strong>, e.g., is it symmetric or skewed in some way?</p>
</div></li>
</ol>
<p>For more background on these questions, see <a class="url" href="https://www.itl.nist.gov/div898/handbook/ppc/section1/ppc131.htm">https://www.itl.nist.gov/div898/handbook/ppc/section1/ppc131.htm</a></p>
<p>This summary processing will become part of an automated acquire, clean, and summarize operation. The <strong>User Experience </strong>(<strong>UX</strong>) will be a command-line application. Our expected command line should look something like the following:</p>
<div><div><pre class="console">% python src/summarize.py -o summary/series_1/2023/03 data/clean/Series_1.ndj</pre>
</div>
</div>
<p>The <code>-o</code> option specifies the path to an output sub-directory. The output filename added to this path will be derived from the source file name. The source file name often encodes information on the applicable date range for the extracted data.</p>
<div><div><p>The Anscombe’s Quartet data doesn’t change and wouldn’t really have an “applicable date” value.</p>
<p>We’ve introduced the <strong>idea </strong>of periodic enterprise extractions. None of the projects actually specify a data source subject to periodic change.</p>
<p>Some web services like <a class="url" href="http://www.yelp.com">http://www.yelp.com</a> have health-code data for food-service businesses; this is subject to periodic change and serves as a good source of analytic data.</p>
</div>
</div>
<p>Now that we’ve seen the expectations, we can turn to an approach to the implementation. </p>


<h2 data-number="19.2">15.2  Approach</h2>
<p>We’ll take some guidance from the C4 model ( <a class="url" href="https://c4model.com">https://c4model.com</a>) when looking at our approach:</p>
<ul>
<li><p><strong>Context</strong>: For this project, a context diagram would show a user creating analytical reports. You may find it helpful to draw this diagram.</p></li>
<li><p><strong>Containers</strong>: There only seems to be one container: the user’s personal computer.</p></li>
<li><p><strong>Components</strong>: We’ll address the components below.</p></li>
<li><p><strong>Code</strong>: We’ll touch on this to provide some suggested directions.</p></li>
</ul>
<p>The heart of this application is a module to summarize data in a way that lets us test whether it fits the expectations of a model. The statistical model is a simplified reflection of the underlying real-world processes that created the source data. The model’s simplifications include assumptions about events, measurements, internal state changes, and other details of the processing being observed.</p>
<p>For very simple cases — like Anscombe’s Quartet data — there are only two variables, which leaves a single relationship in the model. Each of the four sample collections in the quartet has a distinct relationship. Many of the summary statistics, however, are the same, making the relationship often surprising.</p>
<p>For other datasets, with more variables and more relationships, there are numerous choices available to the analyst. The <em>NIST</em> <em>Engineering Statistics Handbook </em>has an approach to modeling. See <a class="url" href="https://www.itl.nist.gov/div898/handbook/index.htm">https://www.itl.nist.gov/div898/handbook/index.htm</a> for the design of a model and analysis of the results of the model.</p>
<p>As part of the preliminary work, we will distinguish between two very broad categories of statistical summaries:</p>
<ul>
<li><p><strong>Univariate statistics</strong>: These are variables viewed in isolation.</p></li>
<li><p><strong>Multivariate statistics</strong>: These are variables in pairs (or higher-order groupings) with an emphasis on the relationship between the variable’s values.</p></li>
</ul>
<p>For univariate statistics, we need to understand the distribution of the data. This means measuring the location (the center or expected values), the spread (or scale), and the shape of the distribution. Each of these measurement areas has several well-known statistical functions that can be part of the summary application.</p>
<p>We’ll look at the multivariate statistics in the next chapter. We’ll start the univariate processing by looking at the application in a general way, and then focus on the statistical measures, the inputs, and finally, the outputs. </p>

<h3 data-number="19.2.1">15.2.1  Designing a summary app</h3>
<p>This application has a command-line interface to create a summary from the cleaned data. The input file(s) are the samples to be summarized. The summary must be in a form that’s easy to process by subsequent software. This can be a JSON- or a TOML-formatted file with the summary data.</p>
<p>The summaries will be ”measures of location,” sometimes called a ”central tendency.” See <a class="url" href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda351.htm">https://www.itl.nist.gov/div898/handbook/eda/section3/eda351.htm</a>.</p>
<p>The output must include enough context to understand the data source, and the variable being measured. The output also includes the measured values to a sensible number of decimal places. It’s important to avoid introducing additional digits into floating-point values when those digits are little more than noise.</p>
<p>A secondary feature of this application is to create an easy-to-read presentation of the summary. This can be done by using tools like <strong>Docutils </strong>to transform a reStructuredText report into HTML or a PDF. A tool like <strong>Pandoc </strong>could also be used to convert a source report into something that isn’t simply text. The technique explored in <a href="ch018.xhtml#x1-31300014"><em>Chapter</em><em> 14</em></a>, <a href="ch018.xhtml#x1-31300014"><em>Project 4.2: Creating Reports</em></a> is to use Jupyter{Book} to create a document suitable for publication.</p>
<p>We’ll start by looking at some of the measures of location that need to be computed. </p>


<h3 data-number="19.2.2">15.2.2  Describing the distribution</h3>
<p>As noted above, there are three aspects of the distribution of a variable. The data tends to scatter around a central tendency value; we’ll call this the location. There will be an expected limit on the scattering; we’ll call this the spread. There may be a shape that’s symmetric or skewed in some way. The reasons for scattering may include measurement variability, as well as variability in the process being measured.</p>
<p>The NIST Handbook defines three commonly-used measures of location:</p>
<ul>
<li><p><strong>mean</strong>: The sum of the variable’s values divided by the count of values: <em>X</em> = <img alt="∑ --XNi" src="img/file60.jpg"/>.</p></li>
<li><p><strong>median</strong>: The value of a value that is in the center of the distribution. Half the values are less than or equal to this value, and half the values are greater than or equal to this value. First, sort the values into ascending order. If there’s an odd number, it’s the value in the center. For an even number of values, split the difference between the two center-most values.</p></li>
<li><p><strong>mode</strong>: The most common value. For some of the Anscombe Quartet data series, this isn’t informative because all of the values are unique.</p></li>
</ul>
<p>These functions are first-class parts of the built-in <code>statistics</code> module, making them relatively easy to compute.</p>
<p>There are some alternatives that may be needed when the data is polluted by outliers. There are techniques like <em>Mid-Mean </em>and <em>Trimmed Mean </em>to discard data outside some range of percentiles.</p>
<p>The question of an ”outlier” is a sensitive topic. An outlier may reflect a measurement problem. An outlier may also hint that the processing being measured is quite a bit more complicated than is revealed in a set of samples. Another, separate set of samples may reveal a different mean or a larger standard deviation. The presence of outliers may suggest more study is needed to understand the nature of these values.</p>
<p>There are three commonly-used measures for the scale or spread of the data:</p>
<ul>
<li><p><strong>Variance </strong>and standard deviation. The variance is — essentially — the average of the squared distance of each sample from the mean: <em>s</em><sup>2</sup> = <img alt="∑ Xi−X¯ -(N−1)-" src="img/file61.jpg"/>. The standard deviation is the square root of the variance.</p></li>
<li><p><strong>Range </strong>is the difference between the largest and smallest values.</p></li>
<li><p><strong>Median absolute deviation </strong>is the median of the distance of each sample from the mean: MAD<sub>Y</sub> = median(|<em>Y</em> <sub>i</sub> −<em>Ỹ</em>|). See <a href="ch011.xhtml#x1-1610007"><em>Chapter</em><em> 7</em></a>, <a href="ch011.xhtml#x1-1610007"><em>Data Inspection Features</em></a>.</p></li>
</ul>
<p>The variance and standard deviation functions are first-class parts of the built-in <code>statistics</code> module. The range can be computed using the built-in <code>min()</code> and <code>max()</code> functions. A median absolute deviation function can be built using functions in the <code>statistics</code> module.</p>
<p>There are also measures for skewness and kurtosis of a distribution. We’ll leave these as extras to add to the application once the base statistical measures are in place. </p>


<h3 data-number="19.2.3">15.2.3  Use cleaned data model</h3>
<p>It’s essential to use the cleaned, normalized data for this summary processing. There is some overlap between an inspection notebook and this more detailed analysis. An initial inspection may also look at some measures of location and range to determine if the data can be used or contains errors or problems. During the inspection activities, it’s common to start creating an intuitive model of the data. This leads to formulating hypotheses about the data and considering experiments to confirm or reject those hypotheses.</p>
<p>This application formalizes hypothesis testing. Some functions from an initial data inspection notebook may be refactored into a form where those functions can be used on the cleaned data. The essential algorithm may be similar to the raw data version of the function. The data being used, however, will be the cleaned data.</p>
<p>This leads to a sidebar design decision. When we look back at the data inspection notebook, we’ll see some overlaps. </p>


<h3 data-number="19.2.4">15.2.4  Rethink the data inspection functions</h3>
<p>Because Python programming can be generic — independent of any specific data type — it’s tempting to try to unify the raw data processing and the cleaned data processing. The desire manifests as an attempt to write exactly one version of some algorithm, like the Median Absolute Deviation function that’s usable for <em>both </em>raw and cleaned data.</p>
<p>This is not always an achievable goal. In some situations, it may not even be desirable.</p>
<p>A function to process raw data must often do some needed cleaning and filtering. These overheads are later refactored and implemented in the pipeline to create cleaned data. To be very specific, the <code>if</code> conditions used to exclude bad data can be helpful during the inspection. These conditions will become part of the clean-and-convert applications. Once this is done, they are no longer relevant for working with the cleaned data.</p>
<p>Because the extra data cleanups are required for inspecting raw data, but not required for analyzing cleaned data, it can be difficult to create a single process that covers both cases. The complications required to implement this don’t seem to be worth the effort.</p>
<p>There are some additional considerations. One of these is the general design pattern followed by Python’s <code>statistics</code> module. This module works with sequences of atomic values. Our applications will read (and write) complicated <code>Sample</code> objects that are not atomic Python integer or float values. This means our applications will extract sequences of atomic values from sequences of complicated <code>Sample</code> objects.</p>
<p>The raw data, on the other hand, may not have a very sophisticated class definition. This means the decomposition of complicated objects isn’t part of the raw data processing.</p>
<p>For some very, very large datasets the decomposition of complicated multivariate objects to individual values may happen as the data is being read. Rather than ingest millions of objects, the application may extract a single attribute for processing.</p>
<p>This might lead to input processing that has the following pattern:</p>
<div><div><pre class="source-code">from collections.abc import Iterator, Callable
import json
from pathlib import Path
from typing import TypeAlias

from analysis_model import Sample

Extractor: TypeAlias = Callable[[Sample], float]

def attr_iter(some_path: Path, extractor: Extractor) -&gt; Iterator[float]:
    with some_path.open() as source:
        for line in source:
            document = Sample(**json.loads(line))
            yield extractor(document)

def x_values(some_path: Path) -&gt; list[float]:
    return list(attr_iter(some_path, lambda sample: sample.x))</pre>
</div>
</div>
<p>This example defines a generic function, <code>attr_iter()</code>, to read an ND JSON file to build instances of some class, <code>Sample</code>. (The details of the <code>Sample</code> class are omitted.)</p>
<p>The <code>x_values()</code> function uses the generic <code>attr_iter()</code> function with a concrete lambda object to extract a specific variable’s value, and create a list object. This list object can then be used with various statistical functions.</p>
<p>While a number of individual <code>Sample</code> objects are created, they aren’t retained. Only the values of the <code>x</code> attribute are saved, reducing the amount of memory used to create summary statistics from a large collection of complicated values. </p>


<h3 data-number="19.2.5">15.2.5  Create new results model</h3>
<p>The statistical summary contains three broad kinds of data:</p>
<ul>
<li><p>Metadata to specify what source data is used to create the summary.</p></li>
<li><p>Metadata to specify what measures are being used.</p></li>
<li><p>The computed values for location, shape, and spread.</p></li>
</ul>
<p>In some enterprise applications, source data is described by a range of dates defining the earliest and latest samples. In some cases, more details are required to describe the complete context. For example, the software to acquire raw data may have been upgraded in the past. This means older data may be incomplete. This means the context for processing data may require some additional details on software versions or releases in addition to the range of dates and data sources.</p>
<p>Similarly, the measures being used may shift over time. The computation of skewness, for example, may switch from the <strong>Fisher-Pearson </strong>formula to the <strong>adjusted Fisher-Pearson </strong>formula. This suggests the version information for the summary program should also be recorded along with the results computed.</p>
<p>Each of these metadata values provides necessary context and background information on the data source, the method of collection, and any computations of derived data. This context may be helpful in uncovering the root cause of changes. In some cases, the context is a way to catalog underlying assumptions about a process or a measurement instrument; seeing this context may allow an analyst to challenge assumptions and locate the root cause of a problem.</p>
<p>The application must create a result document that looks something like the following example:</p>
<div><div><pre class="source-code">[identification]
    date = "2023-03-27T10:04:00"
[creator]
    title = "Some Summary App"
    version = 4.2
[source]
    title = "Anscombe’s Quartet"
    path = "data/clean/Series_1.ndj"
[x.location]
    mean = 9.0
[x.spread]
    variance = 11.0
[y.location]
    mean = 7.5
[y.spread]
    variance = 4.125</pre>
</div>
</div>
<p>This file can be parsed by the <code>toml</code> or <code>tomllib</code> module to create a nested collection of dictionaries. The secondary feature of the summary application is to read this file and write a report, perhaps using Markdown or ReStructuredText that provides the data in a readable format suitable for publication.</p>
<p>For Python 3.11 or newer, the <code>tomllib</code> module is built in. For older Python installations, the <code>toml</code> module needs to be installed.</p>
<p>Now that we’ve seen the overall approach, we can look at the specific deliverable files. </p>



<h2 data-number="19.3">15.3  Deliverables</h2>
<p>This project has the following deliverables:</p>
<ul>
<li><p>Documentation in the <code>docs</code> folder.</p></li>
<li><p>Acceptance tests in the <code>tests/features</code> and <code>tests/steps</code> folders.</p></li>
<li><p>Unit tests for model module classes in the <code>tests</code> folder.</p></li>
<li><p>Mock objects for the <code>csv_extract</code> module tests will be part of the unit tests.</p></li>
<li><p>Unit tests for the <code>csv_extract</code> module components that are in the <code>tests</code> folder.</p></li>
<li><p>An application to summarize the cleaned data in a TOML file.</p></li>
<li><p>An application secondary feature to transform the TOML file to an HTML page or PDF file with the summary.</p></li>
</ul>
<p>In some cases, especially for particularly complicated applications, the summary statistics may be best implemented as a separate module. This module can then be expanded and modified without making significant changes to the overall application.</p>
<p>The idea is to distinguish between these aspects of this application:</p>
<ul>
<li><p>The CLI, which includes argument parsing and sensible handling of input and output paths.</p></li>
<li><p>The statistical model, which evolves as our understanding of the problem domain and the data evolve.</p></li>
<li><p>The data classes, which describe the structure of the samples, independent of any specific purpose.</p></li>
</ul>
<p>For some applications, these aspects do not involve a large number of classes or functions. In a case where the definitions are small, a single Python module will do nicely. For other applications, particularly those where initial assumptions turned out to be invalid and significant changes were made, having separate modules can permit more flexibility, and more agility with respect to future changes.</p>
<p>We’ll look at a few of these deliverables in a little more detail. We’ll start with some suggestions for creating the acceptance tests. </p>

<h3 data-number="19.3.1">15.3.1  Acceptance testing</h3>
<p>The acceptance tests need to describe the overall application’s behavior from the user’s point of view. The scenarios will follow the UX concept of a command-line application to acquire data and write output files. Because the input data has been cleaned and converted, there are few failure modes for this application; extensive testing of potential problems isn’t as important as it was in earlier data-cleaning projects.</p>
<p>For relatively simple datasets, the results of the statistical summaries are known in advance. This leads to features that might look like the following example:</p>
<div><div><pre class="source-code">Feature: Summarize an Anscombe Quartet Series.

Scenario: When requested, the application creates a TOML summary of a series.
  Given the "clean/series_1.ndj" file exists
  When we run command "python src/summarize.py \
        -o summary/series_1/2023/03 data/clean/Series_\1.ndj"
  Then the "summary/series_1/2023/03/summary.toml" file exists
  And the value of "summary[’creator’][’title’]" is "Anscombe Summary App"
  And the value of "summary[’source’][’path’]" is "data/clean/Series_1.ndj"
  And the value of "summary[’x’][’location’][’mean’]" is "9.0"</pre>
</div>
</div>
<p>We could continue the scenario with a number of additional <code>Then</code> steps to validate each of the locations and the spread and shape the statistical summaries.</p>
<p>The step definitions will be similar to step definitions for a number of previous projects. Specifically, the <code>When</code> step will use the <code>subprocess.run()</code> function to execute the given application with the required command-line arguments.</p>
<p>The first of the <code>Then</code> steps will need to read — and parse — the TOML file. The resulting summary object can be placed in the <code>context</code> object. Subsequent <code>Then</code> steps can examine the structure to locate the individual values, and confirm the values match the acceptance test expectations.</p>
<div><div><p>It is often helpful to extract a small subset of data to use for acceptance testing. Instead of processing millions of rows, a few dozen rows are adequate to confirm the application has read and summarized data. The data only needs to be representative of the larger set of samples under consideration.</p>
<p>Because the chosen subset is part of the testing suite; it rarely changes. This makes the results predictable.</p>
</div>
</div>
<p>As the data collection process evolves, it’s common to have changes to the data sources. This will lead to changes in the data cleaning. This may, in turn, lead to changes in the summary application as new codes or new outliers must be handled properly. The evolution of the data sources implies that the test data suite will also need to evolve to expose any of the special, edge, or corner cases.</p>
<p>Ideally, the test data suite is a mixture of ordinary — no surprises — data, mixed with representative examples of each of the special, atypical cases. As this test data suite evolves, the acceptance test scenario will also evolve.</p>
<p>The TOML file is relatively easy to parse and verify. The secondary feature of this application — expanding on the TOML output to add extensive Markdown — also works with text files. This makes it relatively easy to confirm with test scenarios that read and write text files.</p>
<p>The final publication, whether done by Pandoc or a combination of Pandoc and a LaTeX toolchain, isn’t the best subject for automated testing. A good copy editor or trusted associate needs to make sure the final document meets the stakeholder’s expectations. </p>


<h3 data-number="19.3.2">15.3.2  Unit testing</h3>
<p>It’s important to have unit testing for the various components that are unique to this application. The clean data class definition, for example, is created by another application, with its own test suite. The unit tests for this application don’t need to repeat those tests. Similarly, the <code>statistics</code> module has extensive unit tests; this application’s unit tests do not need to replicate any of that testing.</p>
<p>This further suggests that the <code>statistics</code> module should be replaced with <code>Mock</code> objects. Those mock objects can — generally — return <code>sentinel</code> objects that will appear in the resulting TOML-format summary document.</p>
<p>This suggests test cases structured like the following example:</p>
<div><div><pre class="source-code">from pytest import fixture
from unittest.mock import Mock, call, sentinel
import summary_app

@fixture
def mocked_mean(monkeypatch):
    mean = Mock(
        return_value=sentinel.MEAN
    )
    monkeypatch.setattr(summary_app, ’mean’, mean)
    return mean

@fixture
def mocked_variance(monkeypatch):
    variance = Mock(
        return_value=sentinel.VARIANCE
    )
    monkeypatch.setattr(summary_app, ’variance’, variance)
    return variance

def test_var_summary(mocked_mean, mocked_variance):
    sample_data = sentinel.SAMPLE
    result = summary_app.variable_summary(sample_data)
    assert result == {
        "location": {"mean": sentinel.MEAN},
        "spread": {"variance": sentinel.VARIANCE},
    }
    assert mocked_mean.mock_calls == [call(sample_data)]
    assert mocked_variance.mock_calls == [call(sample_data)]</pre>
</div>
</div>
<p>The two test fixtures provide mock results, using <code>sentinel</code> objects. Using <code>sentinel</code> objects allows easy comparison to be sure the results of the mocked functions were not manipulated unexpectedly by the application.</p>
<p>The test case, <code>test_var_summary()</code>, provides a mocked source of data in the form of another <code>sentinel</code> object. The results have the expected structure and the expected <code>sentinel</code> objects.</p>
<p>The final part of the test confirms the sample data — untouched — was provided to the mocked statistical functions. This confirms the application doesn’t filter or transform the data in any way. The results are the expected <code>sentinel</code> objects; this confirms the module didn’t adulterate the results of the <code>statistics</code> module. And the final check confirms that the mocked functions were called exactly once with the expected parameters.</p>
<p>This kind of unit test, with numerous mocks, is essential for focusing the testing on the new application code, and avoiding tests of other modules or packages. </p>


<h3 data-number="19.3.3">15.3.3  Application secondary feature</h3>
<p>A secondary feature of this application transforms the TOML summary into a more readable HTML or PDF file. This feature is a variation of the kinds of reporting done with Jupyter Lab (and associated tools like <strong>Jupyter</strong> {<strong>Book</strong>}).</p>
<p>There’s an important distinction between these two classes of reports:</p>
<ul>
<li><p>The Jupyter Lab reports involve discovery. The report content is always new.</p></li>
<li><p>The summary application’s reports involve confirmation of expectations. The report content should not be new or surprising.</p></li>
</ul>
<p>In some cases, the report will be used to confirm (or deny) an expected trend is continuing. The application applies the trend model to the data. If the results don’t match expectations, this suggests follow-up action is required. Ideally, it means the model is incorrect, and the trend is changing. The less-than-ideal case is the observation of an unexpected change in the applications providing the source data.</p>
<p>This application decomposes report writing into three distinct steps:</p>
<ol>
<li><div><p><strong>Content</strong>: This is the TOML file with the essential statistical measures.</p>
</div></li>
<li><div><p><strong>Structure</strong>: The secondary feature creates an intermediate markup file in Markdown or the RST format. This has an informative structure around the essential content.</p>
</div></li>
<li><div><p><strong>Presentation</strong>: The final publication document is created from the structured markup plus any templates or style sheets that are required.</p>
</div></li>
</ol>
<p>The final presentation is kept separate from the document’s content and structure.</p>
<div><div><p>An HTML document’s final presentation is created by a browser. Using a tool like <strong>Pandoc </strong>to create HTML from Markdown is — properly — replacing one markup language with another markup language.</p>
</div>
</div>
<p>Creating a PDF file is a bit more complicated. We’ll leave this in the extras section at the end of this chapter.</p>
<p>The first step toward creating a nicely formatted document is to create the initial Markdown or ReStructuredText document from the summary. In many cases, this is easiest done with the <strong>Jinja </strong>package. See <a class="url" href="https://jinja.palletsprojects.com/en/3.1.x/">https://jinja.palletsprojects.com/en/3.1.x/</a></p>
<p>One common approach is the following sequence of steps:</p>
<ol>
<li><div><p>Write a version of the report using Markdown (or RST).</p>
</div></li>
<li><div><p>Locate a template and style sheets that produce the desired HTML page when converted by the <strong>Pandoc </strong>or <strong>Docutils </strong>applications.</p>
</div></li>
<li><div><p>Refactor the source file to replace the content with <strong>Jinja </strong>placeholders. This becomes the template report.</p>
</div></li>
<li><div><p>Write an application to parse the TOML, then apply the TOML details to the template file.</p>
</div></li>
</ol>
<p>When using <strong>Jinja </strong>to enable filling in the template, it must be added to the <code>requirements.txt</code> file. If <strong>ReStructuredText </strong>(<strong>RST</strong>) is used, then the <strong>docutils </strong>project is also useful and should be added to the <code>requirements.txt</code> file.</p>
<p>If Markdown is used to create the report, then <strong>Pandoc </strong>is one way to handle the conversion from Markdown to HTML. Because <strong>Pandoc </strong>also converts RST to HTML, the <strong>docutils </strong>project is not required.</p>
<p>Because the parsed TOML is a dictionary, fields can be extracted by the Jinja template. We might have a Markdown template file with a structure like the following:</p>
<div><div><pre class="source-code"># Summary of {{ summary[’source’][’name’] }}

Created {{ summary[’identification’][’date’] }}

Some interesting notes about the project...

## X-Variable

Some interesting notes about this variable...

Mean = {{ summary[’x’][’location’][’mean’] }}

etc.</pre>
</div>
</div>
<p>The <code>{{</code><code> some-expression</code><code> }}</code> constructs are placeholders. This is where Jinja will evaluate the Python expression and replace the placeholders with the resulting value. Because of Jinja’s clever implementation, a name like <code>summary[’x’][’location’][’mean’]</code> can be written as <code>summary.x.location.mean</code>, also.</p>
<p>The lines with <code>#</code> and <code>##</code> are the way Markdown specifies the section headings. For more information on Markdown, see <a class="url" href="https://daringfireball.net/projects/markdown/">https://daringfireball.net/projects/markdown/</a>. Note that there are a large number of Markdown extensions, and it’s important to be sure the rendering engine (like Pandoc) supports the extensions you’d like to use.</p>
<p>The Jinja template language has numerous options for conditional and repeating document sections. This includes <code>{%</code><code> for</code><code> name</code><code> in</code><code> sequence</code><code> %}</code> and <code>{%</code><code> if</code><code> condition</code><code> %}</code> constructs to create extremely sophisticated templates. With these constructs, a single template can be used for a number of closely related situations with optional sections to cover special situations.</p>
<p>The application program to inject values from the <code>summary</code> object into the template shouldn’t be much more complicated than the examples shown on the Jinja basics page. See <a class="url" href="https://jinja.palletsprojects.com/en/3.1.x/api/#basics">https://jinja.palletsprojects.com/en/3.1.x/api/#basics</a> for some applications that load a template and inject values.</p>
<p>This program’s output is a file with a name like <code>summary_report.md</code>. This file would be ready for conversion to any of a large number of other formats.</p>
<p>The process of converting a Markdown file to HTML is handled by the <strong>Pandoc</strong> application. See <a class="url" href="https://pandoc.org/demos.html">https://pandoc.org/demos.html</a>. The command might be as complicated as the following:</p>
<div><div><pre class="source-code">pandoc -s --toc -c pandoc.css summary_report.md -o summary_report.html</pre>
</div>
</div>
<p>The <code>pandoc.css</code> file can provide the CSS styles to create a body that’s narrow enough to be printed on an ordinary US letter or A4 paper.</p>
<p>The application that creates the <code>summary_report.md</code> file can use <code>subprocess.run()</code> to execute the <strong>Pandoc </strong>application and create the desired HTML file. This provides a command-line UX that results in a readable document, ready to be distributed. </p>



<h2 data-number="19.4">15.4  Summary</h2>
<p>In this chapter we have created a foundation for building and using a statistical model of source data. We’ve looked at the following topics:</p>
<ul>
<li><p>Designing and building a more complex pipeline of processes for gathering and analyzing data.</p></li>
<li><p>Some of the core concepts behind creating a statistical model of some data.</p></li>
<li><p>Use of the built-in <code>statistics</code> library.</p></li>
<li><p>Publishing the results of the statistical measures.</p></li>
</ul>
<p>This application tends to be relatively small. The actual computations of the various statistical values leverage the built-in <code>statistics</code> library and tend to be very small. It often seems like there’s far more programming involved in parsing the CLI argument values, and creating the required output file, than doing the “real work” of this application.</p>
<p>This is a consequence of the way we’ve been separating the various concerns in data acquisition, cleaning, and analysis. We’ve partitioned the work into several, isolated stages along a pipeline:</p>
<ol>
<li><div><p>Acquiring raw data, generally in text form. This can involve database access or RESTful API access, or complicated file parsing problems.</p>
</div></li>
<li><div><p>Cleaning and converting the raw data to a more useful, native Python form. This can involve complications of parsing text and rejecting outlier values.</p>
</div></li>
<li><div><p>Summarizing and analyzing the cleaned data. This can focus on the data model and reporting conclusions about the data.</p>
</div></li>
</ol>
<p>The idea here is the final application can grow and adapt as our understanding of the data matures. In the next chapter, we’ll add features to the summary program to create deeper insights into the available data. </p>


<h2 data-number="19.5">15.5  Extras</h2>
<p>Here are some ideas for you to add to this project. </p>

<h3 data-number="19.5.1">15.5.1  Measures of shape</h3>
<p>The measurements of shape often involve two computations for skewness and kurtosis. These functions are not part of Python’s built-in <code>statistics</code> library.</p>
<p>It’s important to note that there are a very large number of distinct, well-understood distributions of data. The normal distribution is one of many different ways data can be distributed.</p>
<p>See <a class="url" href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda366.htm">https://www.itl.nist.gov/div898/handbook/eda/section3/eda366.htm</a>.</p>
<p>One measure of skewness is the following:</p>
<div><img alt=" ∑(Y− ¯Y)3 ----iN---- g1 = s3 " class="math-display" src="img/file62.jpg"/>
</div>
<p>Where <em>Ȳ</em> is the mean, and <em>s </em>is the standard deviation.</p>
<p>A symmetric distribution will have a skewness, <em>g</em><sub>1</sub>, near zero. Larger numbers indicate a ”long tail” opposite a large concentration of data around the mean.</p>
<p>One measure of kurtosis is the following:</p>
<div><img alt=" ∑ (Y −Y¯)4 ---iN----- kurtosis = s4 " class="math-display" src="img/file63.jpg"/>
</div>
<p>The kurtosis for the standard normal distribution is 3. A value larger than 3 suggests more data is in the tails; it’s ”flatter” or ”wider” than the standard normal distribution. A value less than three is ”taller” or ”narrower” than the standard.</p>
<p>These metrics can be added to the application to compute some additional univariate descriptive statistics. </p>


<h3 data-number="19.5.2">15.5.2  Creating PDF reports</h3>
<p>In the <a href="#x1-3360003"><em>Application secondary feature</em></a> section we looked at creating a Markdown or RST document with the essential content, some additional information, and an organizational structure. The intent was to use a tool like <strong>Pandoc </strong>to convert the Markdown to HTML. The HTML can be rendered by a browser to present an easy-to-read summary report.</p>
<p>Publishing this document as a PDF requires a tool that can create the necessary output file. There are two common choices:</p>
<ul>
<li><p>Use the <strong>ReportLab </strong>tool: <a class="url" href="https://www.reportlab.com/dev/docs/">https://www.reportlab.com/dev/docs/</a>. This is a commercial product with some open-source components.</p></li>
<li><p>Use the <strong>Pandoc </strong>tool coupled with a LaTeX processing tool.</p></li>
</ul>
<p>See <a href="ch018.xhtml#x1-3190002"><em>Preparing a report</em></a> of <a href="ch018.xhtml#x1-31300014"><em>Chapter</em><em> 14</em></a>, <a href="ch018.xhtml#x1-31300014"><em>Project 4.2: Creating Reports</em></a> for some additional thoughts on using LaTeX to create PDF files. While this involves a large number of separate components, it has the advantage of having the most capabilities.</p>
<p>It’s often best to learn the LaTeX tools separately. The TeXLive project maintains a number of tools useful for rendering LaTeX. For macOS users, the MacTex project offers the required binaries. An online tool like Overleaf is also useful for handling LaTeX. Sort out any problems by creating small <code>hello_world.tex</code> example documents to see how the LaTeX tools work.</p>
<p>Once the basics of the LaTeX tools are working, it makes sense to add the <strong>Pandoc </strong>tool to the environment.</p>
<p>Neither of these tools are Python-based and don’t use <strong>conda </strong>or <strong>pip</strong> installers.</p>
<p>As noted in <a href="ch018.xhtml#x1-31300014"><em>Chapter</em><em> 14</em></a>, <a href="ch018.xhtml#x1-31300014"><em>Project 4.2: Creating Reports</em></a>, there are a lot of components to this tool chain. This is a large number of separate installs that need to be managed. The results, however, can be very nice when a final PDF is created from a few CLI interactions. </p>


<h3 data-number="19.5.3">15.5.3  Serving the HTML report from the data API</h3>
<p>In <a href="ch016.xhtml#x1-27600012"><em>Chapter</em><em> 12</em></a>, <a href="ch016.xhtml#x1-27600012"><em>Project 3.8: Integrated Data Acquisition Web Service</em></a> we created a RESTful API service to provide cleaned data.</p>
<p>This service can be expanded to provide several other things. The most notable addition is the HTML summary report.</p>
<p>The process of creating a summary report will look like this:</p>
<ol>
<li><div><p>A user makes a request for a summary report for a given time period.</p>
</div></li>
<li><div><p>The RESTful API creates a “task” to be performed in the background and responds with the status showing the task has been created.</p>
</div></li>
<li><div><p>The user checks back periodically to see if the processing has finished. Some clever JavaScript programming can display an animation while an application program checks to see if the work is completed.</p>
</div></li>
<li><div><p>Once the processing is complete, the user can download the final report.</p>
</div></li>
</ol>
<p>This means two new resources paths will need to be added to the OpenAPI specification. These two new resources are:</p>
<ul>
<li><p>Requests to create a new summary. A POST request creates the task to build a summary and a GET request shows the status. A <code>2023.03/summarize</code> path will parallel the <code>2023.02/creation</code> path used to create the series.</p></li>
<li><p>Requests for a summary report. A GET request will download a given statistical summary report. Perhaps a <code>2023.03/report</code> path would be appropriate.</p></li>
</ul>
<p>As we add features to the RESTful API, we need to consider the resource names more and more carefully. The first wave of ideas sometimes fails to reflect the growing understanding of the user’s needs.</p>
<div><div><p>In retrospect, the <code>2023.02/create</code> path, defined in <a href="ch016.xhtml#x1-27600012"><em>Chapter</em><em> 12</em></a>, <a href="ch016.xhtml#x1-27600012"><em>Project 3.8: Integrated Data Acquisition Web Service</em></a>, may not have been the best name.</p>
<p>There’s an interesting tension between requests to create a resource and the resulting resource. The request to create a series is clearly distinct from the resulting series. Yet, they can both be meaningfully thought of as instances of “series.” The creation request is a kind of <em>future</em>: an expectation that will be fulfilled later.</p>
<p>An alternative naming scheme is to use <code>2023.02/creation</code> for series, and use <code>2023.03/create/series</code> and <code>2023.03/create/summary</code> as distinct paths to manage the long-running background that does the work.</p>
</div>
</div>
<p>The task being performed in the background will execute a number of steps:</p>
<ol>
<li><div><p>Determine if the request requires new data or existing data. If new data is needed, it is acquired, and cleaned. This is the existing process to acquire the series of data points.</p>
</div></li>
<li><div><p>Determine if the requested summary does not already exist. (For new data, of course, it will not exist.) If a summary is needed, it is created.</p>
</div></li>
</ol>
<p>Once the processing is complete, the raw data, cleaned data, and summary can all be available as resources on the API server. The user can request to download any of these resources.</p>
<p>It’s essential to be sure each of the components for the task work in isolation before attempting to integrate them as part of a web service. It’s far easier to diagnose and debug problems with summary reporting outside the complicated world of web services. </p>



</body>
</html>
